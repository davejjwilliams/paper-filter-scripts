{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4934af8f",
   "metadata": {},
   "source": [
    "# Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fadbe",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab58f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Shape: (177, 28)\n",
      "All Papers Shape: (692, 11)\n"
     ]
    }
   ],
   "source": [
    "df_relevant = pd.read_excel('results/final/final_results.xlsx', sheet_name='relevant_papers')\n",
    "df_all_papers = pd.read_csv('results/ICSE_all_papers.csv')\n",
    "\n",
    "print(\"Relevant Shape:\", df_relevant.shape)\n",
    "print(\"All Papers Shape:\", df_all_papers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676387c9",
   "metadata": {},
   "source": [
    "### Creating Combined and Non-Relevant Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a29c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Paper List\n",
    "relevant_papers = df_relevant['title'].tolist()\n",
    "\n",
    "# Remove relevant papers from all papers to create final non-relevant set\n",
    "df_non_relevant = df_all_papers[~df_all_papers['title'].isin(relevant_papers)].copy()\n",
    "\n",
    "# Make sure all non-relevant papers are marked as such\n",
    "df_non_relevant['relevant'] = False\n",
    "\n",
    "# Re-order columns to match relevant dataframe\n",
    "common_columns = ['reviewer', 'relevant', 'year', 'title', 'authors', 'url', 'abstract', 'artifact_available', 'artifact_reusable', 'artifact_functional', 'ai']\n",
    "df_non_relevant = df_non_relevant[common_columns]\n",
    "\n",
    "# Add extra columns: 'task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text'\n",
    "extra_columns = ['task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text']\n",
    "for col in extra_columns:\n",
    "    df_non_relevant[col] = None\n",
    "\n",
    "# Combine relevant and non-relevant dataframes\n",
    "df_combined = pd.concat([df_relevant, df_non_relevant], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4613e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape: (692, 28)\n",
      "Num Unique Papers: 692\n",
      "Num Relevant Papers in Combined DF: 177\n",
      "Num Unique Papers in Relevant DF: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Shape:\", df_combined.shape)\n",
    "print(\"Num Unique Papers:\", df_combined['title'].nunique())\n",
    "print(\"Num Relevant Papers in Combined DF:\", df_combined[df_combined['relevant'] == True].shape[0])\n",
    "print(\"Num Unique Papers in Relevant DF:\", df_relevant['title'].nunique()) # Should match number above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100580d7",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We now have access to four dataframes for analysis:\n",
    "\n",
    "- `df_combined`: contains all papers and final columns from our spreadsheet (non-relevant papers just have None values in the fields we completed for the relevant papers)\n",
    "- `df_relevant`: contains all relevant papers as rows and the final columns we intend to use for analysis\n",
    "- `df_non_relevant`: contains all non-relevant papers. Our finals columns are present but all filled with None values as we didn't perform data extraction for these papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64a7b",
   "metadata": {},
   "source": [
    "## Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719ef68",
   "metadata": {},
   "source": [
    "### Number of Papers at Each Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Papers in Combined DF: 692\n",
      "\n",
      "Total Papers from AI Keywords:\n",
      "ai\n",
      "False    388\n",
      "True     304\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Relevant Papers:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total numbers\n",
    "print(\"Total Papers in Combined DF:\", df_combined.shape[0])\n",
    "\n",
    "print(\"\\nTotal Papers from AI Keywords:\")\n",
    "print(df_combined['ai'].value_counts())\n",
    "\n",
    "print(\"\\nTotal Relevant Papers:\")\n",
    "print(df_combined['relevant'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers Per Year:\n",
      "year\n",
      "2023    210\n",
      "2024    236\n",
      "2025    246\n",
      "Name: title, dtype: int64\n",
      "\n",
      "AI Papers Per Year:\n",
      "year\n",
      "2023     59\n",
      "2024     99\n",
      "2025    146\n",
      "Name: title, dtype: int64\n",
      "\n",
      "Relevant Papers Per Year:\n",
      "year\n",
      "2023    32\n",
      "2024    55\n",
      "2025    90\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "print(\"Papers Per Year:\")\n",
    "print(df_combined.groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nAI Papers Per Year:\")\n",
    "print(df_combined[df_combined['ai'] == True].groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nRelevant Papers Per Year:\")\n",
    "print(df_combined[df_combined['relevant'] == True].groupby('year')['title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029cd15",
   "metadata": {},
   "source": [
    "### Numbers of Relevant Papers (Papers with LLM-based Empirical Studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfef7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevant vs Non-Relevant Counts:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Relevant Papers by Year:\n",
      "year\n",
      "2025    90\n",
      "2024    55\n",
      "2023    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Relevant vs Non-Relevant Counts:\")\n",
    "print(df_combined[\"relevant\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nRelevant Papers by Year:\")\n",
    "print(df_relevant[\"year\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe95f7",
   "metadata": {},
   "source": [
    "### Geo-location of SE Research Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e76ac",
   "metadata": {},
   "source": [
    "# RQ1 - What Tasks are being tackled in LLM SE studies, and are they fairly evaluated against existing non-LLM techniques?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'task' (short-text)\n",
    "- 'non_llm_approaches' (bool)\n",
    "- 'dataset_type' (short-text)\n",
    "- programming_language (short-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a225d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list\n",
      "code generation                                   26\n",
      "program repair                                    24\n",
      "test generation                                   23\n",
      "vulnerability detection                           16\n",
      "bug detection                                     12\n",
      "code translation                                   8\n",
      "clone detection                                    6\n",
      "code completion                                    6\n",
      "code summarisation                                 6\n",
      "code comprehension                                 5\n",
      "type detection                                     5\n",
      "log parsing                                        5\n",
      "code search                                        5\n",
      "fuzzing                                            4\n",
      "bug reproduction                                   3\n",
      "commit message generation                          3\n",
      "fault localisation                                 3\n",
      "code memorisation detection                        3\n",
      "test repair                                        2\n",
      "formal verification                                2\n",
      "program analysis                                   2\n",
      "traceability link recovery                         2\n",
      "code retrieval                                     2\n",
      "code refinement                                    1\n",
      "regression testing                                 1\n",
      "SO post editing                                    1\n",
      "AI generated code detection                        1\n",
      "security patch detection                           1\n",
      "model completion                                   1\n",
      "configuration validation                           1\n",
      "smart contract auditing                            1\n",
      "UI design repair                                   1\n",
      "detection of code design issues                    1\n",
      "code adaptation                                    1\n",
      "comment repair                                     1\n",
      "bug report comprehension                           1\n",
      "inconsistency prediction in decentralised apps     1\n",
      "machine-generated code detection                   1\n",
      "security injection                                 1\n",
      "code optimisation                                  1\n",
      "root cause analysis                                1\n",
      "privacy inconsistencies detection                  1\n",
      "figurative language detection                      1\n",
      "binary software composition analysis               1\n",
      "API misuse detection                               1\n",
      "program comprehension                              1\n",
      "exception handling recommender                     1\n",
      "static analysis                                    1\n",
      "comment generation                                 1\n",
      "code review                                        1\n",
      "log understanding                                  1\n",
      "emotion-cause extraction                           1\n",
      "code idioms detection                              1\n",
      "log generation                                     1\n",
      "CI/CD workflow generation                          1\n",
      "requirements analysis                              1\n",
      "model extraction                                   1\n",
      "mutant generation                                  1\n",
      "assert generation                                  1\n",
      "SO posts summarisation                             1\n",
      "question answering                                 1\n",
      "patch correctness assessment                       1\n",
      "algorithm classification                           1\n",
      "optimal thread coarsening factor                   1\n",
      "heterogeneous device mapping                       1\n",
      "vulnerability alert prediction                     1\n",
      "vulnerability repair                               1\n",
      "GUI test case migration                            1\n",
      "software effort estimation                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['task_list'] = df_relevant['task'].apply(\n",
    "    lambda x: [task.strip() for task in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(df_relevant['task_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce60bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of tasks per paper:\n",
      "num_tasks\n",
      "1     157\n",
      "2      11\n",
      "3       5\n",
      "4       3\n",
      "13      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple tasks: 20\n",
      "Percentage covering multiple tasks: 11.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at how many papers cover multiple tasks\n",
    "df_relevant['num_tasks'] = df_relevant['task_list'].apply(len)\n",
    "\n",
    "print(\"Distribution of number of tasks per paper:\")\n",
    "print(df_relevant['num_tasks'].value_counts().sort_index())\n",
    "print(f\"\\nPapers covering multiple tasks: {(df_relevant['num_tasks'] > 1).sum()}\")\n",
    "print(f\"Percentage covering multiple tasks: {(df_relevant['num_tasks'] > 1).mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9992a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_llm_approaches_bool\n",
      "True     114\n",
      "False     63\n",
      "Name: count, dtype: int64\n",
      "Overall: Non-LLM techniques in 114 out of 177 papers (64.4%)\n"
     ]
    }
   ],
   "source": [
    "# Consolidating to just Boolean values\n",
    "df_relevant['non_llm_approaches_bool'] = df_relevant['non_llm_approaches'].apply(lambda x: 'true' in str(x).strip().lower())\n",
    "print(df_relevant['non_llm_approaches_bool'].value_counts())\n",
    "number_non_llm_present = df_relevant['non_llm_approaches_bool'].sum()\n",
    "total = df_relevant.shape[0]\n",
    "\n",
    "print(f\"Overall: Non-LLM techniques in {number_non_llm_present} out of {total} papers ({(number_non_llm_present/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17017377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Non-LLM techniques in 27 out of 32 papers (84.4%)\n",
      "2024: Non-LLM techniques in 36 out of 55 papers (65.5%)\n",
      "2025: Non-LLM techniques in 51 out of 90 papers (56.7%)\n"
     ]
    }
   ],
   "source": [
    "number_non_llm_present_2023 = df_relevant[df_relevant['year'] == 2023]['non_llm_approaches_bool'].sum()\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "print(f\"2023: Non-LLM techniques in {number_non_llm_present_2023} out of {total_2023} papers ({(number_non_llm_present_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "number_non_llm_present_2024 = df_relevant[df_relevant['year'] == 2024]['non_llm_approaches_bool'].sum()\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "print(f\"2024: Non-LLM techniques in {number_non_llm_present_2024} out of {total_2024} papers ({(number_non_llm_present_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "number_non_llm_present_2025 = df_relevant[df_relevant['year'] == 2025]['non_llm_approaches_bool'].sum()\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "print(f\"2025: Non-LLM techniques in {number_non_llm_present_2025} out of {total_2025} papers ({(number_non_llm_present_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3154521b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer</th>\n",
       "      <th>relevant</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>artifact_available</th>\n",
       "      <th>artifact_reusable</th>\n",
       "      <th>artifact_functional</th>\n",
       "      <th>ai</th>\n",
       "      <th>task</th>\n",
       "      <th>non_llm_approaches</th>\n",
       "      <th>models_open_closed</th>\n",
       "      <th>num_models</th>\n",
       "      <th>model_families</th>\n",
       "      <th>model_scale</th>\n",
       "      <th>model_size_free_text</th>\n",
       "      <th>model_sizes_reported</th>\n",
       "      <th>model_config</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>programming_language</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost_free_text</th>\n",
       "      <th>artefact_manual</th>\n",
       "      <th>contamination</th>\n",
       "      <th>contamination_free_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>task_list</th>\n",
       "      <th>num_tasks</th>\n",
       "      <th>non_llm_approaches_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>DW</td>\n",
       "      <td>True</td>\n",
       "      <td>2025</td>\n",
       "      <td>Decoding Secret Memorization in Code LLMs Thro...</td>\n",
       "      <td>Nie, Yuqing, Wang, Chong, Wang, Kailong, Xu, G...</td>\n",
       "      <td>https://doi.org/10.1109/ICSE55347.2025.00229</td>\n",
       "      <td>Code Large Language Models (LLMs) have demonst...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>5</td>\n",
       "      <td>StableCode; CodeGen; DeepSeekCoder; CodeLlama;...</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>StableCode-3B,3B;CodeGen2.5-7B-multi,7B;DeepSe...</td>\n",
       "      <td>full</td>\n",
       "      <td>inference</td>\n",
       "      <td>code</td>\n",
       "      <td>HTML; Java; JavaScript; Python</td>\n",
       "      <td>gpu</td>\n",
       "      <td>All the experiments are conducted on a server ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Training Data Decontamination. Data cleaning s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>DW</td>\n",
       "      <td>True</td>\n",
       "      <td>2024</td>\n",
       "      <td>Traces of Memorisation in Large Language Model...</td>\n",
       "      <td>Al-Kaswan, Ali, Izadi, Maliheh, van Deursen, Arie</td>\n",
       "      <td>https://doi.org/10.1145/3597503.3639133</td>\n",
       "      <td>Large language models have gained significant ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>37</td>\n",
       "      <td>GPT-NEO;GPT-2;Pythia;CodeGen-NL;CodeGen-Mono;C...</td>\n",
       "      <td>SMALL/MEDIUM</td>\n",
       "      <td>GPT-NEO-125M,GPT-NEO-1.3B,GPT-NEO-2.7B,GPT-2-1...</td>\n",
       "      <td>full</td>\n",
       "      <td>inference</td>\n",
       "      <td>code, text</td>\n",
       "      <td>Python</td>\n",
       "      <td>gpu</td>\n",
       "      <td>we allocated 8 CPU cores with 32GB of RAM and ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper is about contamination/memorisation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>MK</td>\n",
       "      <td>True</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unveiling Memorization in Code Models</td>\n",
       "      <td>Yang, Zhou, Zhao, Zhipeng, Wang, Chenyu, Shi, ...</td>\n",
       "      <td>https://doi.org/10.1145/3597503.3639074</td>\n",
       "      <td>The availability of large-scale datasets, adva...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>6</td>\n",
       "      <td>CodeParrot; PolyCoder; GPT-NEO; InCoder; StarC...</td>\n",
       "      <td>SMALL/MEDIUM</td>\n",
       "      <td>CodeParrot is a GPT-2 model with 1.5 billion p...</td>\n",
       "      <td>some</td>\n",
       "      <td>inference</td>\n",
       "      <td>code, documentation (text), configuration (tex...</td>\n",
       "      <td>Python</td>\n",
       "      <td>time;gpu</td>\n",
       "      <td>... we run them an NVIDIA GeForce\\nA5000 GPU w...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>To mitigate this threat, we choose a state-of-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reviewer  relevant  year  \\\n",
       "171       DW      True  2025   \n",
       "172       DW      True  2024   \n",
       "173       MK      True  2024   \n",
       "\n",
       "                                                 title  \\\n",
       "171  Decoding Secret Memorization in Code LLMs Thro...   \n",
       "172  Traces of Memorisation in Large Language Model...   \n",
       "173              Unveiling Memorization in Code Models   \n",
       "\n",
       "                                               authors  \\\n",
       "171  Nie, Yuqing, Wang, Chong, Wang, Kailong, Xu, G...   \n",
       "172  Al-Kaswan, Ali, Izadi, Maliheh, van Deursen, Arie   \n",
       "173  Yang, Zhou, Zhao, Zhipeng, Wang, Chenyu, Shi, ...   \n",
       "\n",
       "                                              url  \\\n",
       "171  https://doi.org/10.1109/ICSE55347.2025.00229   \n",
       "172       https://doi.org/10.1145/3597503.3639133   \n",
       "173       https://doi.org/10.1145/3597503.3639074   \n",
       "\n",
       "                                              abstract  artifact_available  \\\n",
       "171  Code Large Language Models (LLMs) have demonst...               False   \n",
       "172  Large language models have gained significant ...               False   \n",
       "173  The availability of large-scale datasets, adva...               False   \n",
       "\n",
       "     artifact_reusable  artifact_functional    ai  \\\n",
       "171              False                False  True   \n",
       "172              False                False  True   \n",
       "173              False                False  True   \n",
       "\n",
       "                            task non_llm_approaches models_open_closed  \\\n",
       "171  code memorisation detection              False               open   \n",
       "172  code memorisation detection              False               open   \n",
       "173  code memorisation detection              False               open   \n",
       "\n",
       "     num_models                                     model_families  \\\n",
       "171           5  StableCode; CodeGen; DeepSeekCoder; CodeLlama;...   \n",
       "172          37  GPT-NEO;GPT-2;Pythia;CodeGen-NL;CodeGen-Mono;C...   \n",
       "173           6  CodeParrot; PolyCoder; GPT-NEO; InCoder; StarC...   \n",
       "\n",
       "      model_scale                               model_size_free_text  \\\n",
       "171        MEDIUM  StableCode-3B,3B;CodeGen2.5-7B-multi,7B;DeepSe...   \n",
       "172  SMALL/MEDIUM  GPT-NEO-125M,GPT-NEO-1.3B,GPT-NEO-2.7B,GPT-2-1...   \n",
       "173  SMALL/MEDIUM  CodeParrot is a GPT-2 model with 1.5 billion p...   \n",
       "\n",
       "    model_sizes_reported model_config  \\\n",
       "171                 full    inference   \n",
       "172                 full    inference   \n",
       "173                 some    inference   \n",
       "\n",
       "                                          dataset_type  \\\n",
       "171                                               code   \n",
       "172                                         code, text   \n",
       "173  code, documentation (text), configuration (tex...   \n",
       "\n",
       "               programming_language      cost  \\\n",
       "171  HTML; Java; JavaScript; Python       gpu   \n",
       "172                          Python       gpu   \n",
       "173                          Python  time;gpu   \n",
       "\n",
       "                                        cost_free_text artefact_manual  \\\n",
       "171  All the experiments are conducted on a server ...            True   \n",
       "172  we allocated 8 CPU cores with 32GB of RAM and ...            True   \n",
       "173  ... we run them an NVIDIA GeForce\\nA5000 GPU w...            True   \n",
       "\n",
       "     contamination                            contamination_free_text  topic  \\\n",
       "171           True  Training Data Decontamination. Data cleaning s...    NaN   \n",
       "172           True      The paper is about contamination/memorisation    NaN   \n",
       "173           True  To mitigate this threat, we choose a state-of-...    NaN   \n",
       "\n",
       "                         task_list  num_tasks  non_llm_approaches_bool  \n",
       "171  [code memorisation detection]          1                    False  \n",
       "172  [code memorisation detection]          1                    False  \n",
       "173  [code memorisation detection]          1                    False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['task_list'].apply(lambda x: 'code memorisation detection' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b40c9",
   "metadata": {},
   "source": [
    "# RQ2 - What models are being used?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'models_open_closed' (open/closed/both)\n",
    "- 'num_models' (int)\n",
    "- 'model_families' (list of short text)\n",
    "- 'model_sizes_reported' (NA/none/some/full - currently unfinished)\n",
    "- 'model_scale' (currently unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07775a87",
   "metadata": {},
   "source": [
    "### Open vs. Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b5c9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models_open_closed\n",
       "open      71\n",
       "both      65\n",
       "closed    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant['models_open_closed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80376d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open models in 136 out of 177 papers (76.8%)\n",
      "Closed models in 106 out of 177 papers (59.9%)\n"
     ]
    }
   ],
   "source": [
    "total = df_relevant.shape[0]\n",
    "number_open = df_relevant[~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_closed = df_relevant[~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "\n",
    "print(f\"Open models in {number_open} out of {total} papers ({(number_open/total)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed} out of {total} papers ({(number_closed/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eda04f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Papers:\n",
      "Only open models in 22 out of 32 papers (68.8%)\n",
      "Only closed models in 4 out of 32 papers (12.5%)\n",
      "Open models in 28 out of 32 papers (87.5%)\n",
      "Closed models in 10 out of 32 papers (31.2%)\n",
      "Both model types in 6 out of 32 papers (18.8%)\n",
      "\n",
      "2024 Papers:\n",
      "Only open models in 25 out of 55 papers (45.5%)\n",
      "Only closed models in 13 out of 55 papers (23.6%)\n",
      "Open models in 42 out of 55 papers (76.4%)\n",
      "Closed models in 30 out of 55 papers (54.5%)\n",
      "Both model types in 17 out of 55 papers (30.9%)\n",
      "\n",
      "2025 Papers:\n",
      "Only open models in 24 out of 90 papers (26.7%)\n",
      "Only closed models in 24 out of 90 papers (26.7%)\n",
      "Open models in 66 out of 90 papers (73.3%)\n",
      "Closed models in 66 out of 90 papers (73.3%)\n",
      "Both model types in 42 out of 90 papers (46.7%)\n",
      "Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\n"
     ]
    }
   ],
   "source": [
    "total_2023 = df_relevant[df_relevant[\"year\"] == 2023].shape[0]\n",
    "number_open_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"2023 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2023} out of {total_2023} papers ({(number_open_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2023} out of {total_2023} papers ({(number_closed_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2023} out of {total_2023} papers ({(number_open_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2023} out of {total_2023} papers ({(number_closed_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2023} out of {total_2023} papers ({(number_both_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant[\"year\"] == 2024].shape[0]\n",
    "number_open_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2024 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2024} out of {total_2024} papers ({(number_open_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2024} out of {total_2024} papers ({(number_closed_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2024} out of {total_2024} papers ({(number_open_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2024} out of {total_2024} papers ({(number_closed_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2024} out of {total_2024} papers ({(number_both_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant[\"year\"] == 2025].shape[0]\n",
    "number_open_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2025 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2025} out of {total_2025} papers ({(number_open_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2025} out of {total_2025} papers ({(number_closed_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2025} out of {total_2025} papers ({(number_open_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2025} out of {total_2025} papers ({(number_closed_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2025} out of {total_2025} papers ({(number_both_2025/total_2025)*100:.1f}%)\")\n",
    "\n",
    "print(\"Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ba1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with X models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_models\n",
       "1     48\n",
       "3     24\n",
       "2     23\n",
       "5     17\n",
       "4     17\n",
       "6     13\n",
       "7      9\n",
       "8      7\n",
       "9      5\n",
       "10     4\n",
       "11     3\n",
       "16     2\n",
       "19     2\n",
       "15     1\n",
       "12     1\n",
       "37     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of papers with X models:\")\n",
    "df_relevant['num_models'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5e9b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Paper counts per model family:\n",
      "model_families_list\n",
      "GPT-4                           47\n",
      "GPT-3.5                         44\n",
      "CodeBERT                        34\n",
      "CodeLlama                       26\n",
      "CodeT5                          22\n",
      "CodeGen                         19\n",
      "StarCoder                       18\n",
      "GraphCodeBERT                   18\n",
      "Llama                           17\n",
      "RoBERTa                         15\n",
      "ChatGPT                         14\n",
      "BERT                            13\n",
      "DeepSeekCoder                   12\n",
      "UniXcoder                       11\n",
      "Codex                           10\n",
      "InCoder                          9\n",
      "T5                               8\n",
      "Claude                           7\n",
      "Gemini                           7\n",
      "ChatGLM                          6\n",
      "DeepSeek                         6\n",
      "CodeQwen                         6\n",
      "GPT-3                            6\n",
      "UnixCoder                        6\n",
      "PLBART                           6\n",
      "CodeGPT                          5\n",
      "CodeParrot                       4\n",
      "Copilot                          4\n",
      "WizardCoder                      4\n",
      "GPT-2                            4\n",
      "PolyCoder                        4\n",
      "Mistral                          3\n",
      "Gemma                            3\n",
      "text-davinci                     3\n",
      "Pythia                           3\n",
      "Vicuna                           3\n",
      "DistilBERT                       3\n",
      "LineVul                          3\n",
      "CodeT5+                          3\n",
      "text-embedding                   3\n",
      "TransCoder                       2\n",
      "Codestral                        2\n",
      "Qwen                             2\n",
      "GPT3.5                           2\n",
      "CodeGemma                        2\n",
      "Incoder                          2\n",
      "CodeGeeX                         2\n",
      "VulBERTa                         2\n",
      "CuBERT                           2\n",
      "UniLog                           2\n",
      "BART                             2\n",
      "GPT-J                            2\n",
      "GPT-Neo                          2\n",
      "GPT-NEO                          2\n",
      "GPT-C                            2\n",
      "SynCoBERT                        2\n",
      "CoTexT                           2\n",
      "Phi                              2\n",
      "TFix                             2\n",
      "ALBERT                           2\n",
      "seBERT                           2\n",
      "Code-davinci                     2\n",
      "OpenDevin                        2\n",
      "SantaCoder                       2\n",
      "Longformer                       1\n",
      "CoditT5                          1\n",
      "Unixcoder                        1\n",
      "CodeBert                         1\n",
      "GPT4                             1\n",
      "SVulD                            1\n",
      "Poro                             1\n",
      "ChatDev                          1\n",
      "Self-collaboration               1\n",
      "MetaGPT                          1\n",
      "AutoGPT                          1\n",
      "Multi-Turn Program Synthesis     1\n",
      "AgentCoder                       1\n",
      "DetectGPT                        1\n",
      "GPT-2 Output Detector            1\n",
      "GPTZero                          1\n",
      "GPTSniffer                       1\n",
      "Starcoder                        1\n",
      "LLM-Parser                       1\n",
      "LILAC                            1\n",
      "Repilot                          1\n",
      "RAP-Gen                          1\n",
      "ChatRepair                       1\n",
      "FitRepair                        1\n",
      "AlphaRe-pair                     1\n",
      "Mixtral                          1\n",
      "VGX                              1\n",
      "ReVeal                           1\n",
      "Devign                           1\n",
      "VULGEN                           1\n",
      "COME                             1\n",
      "CCT5                             1\n",
      "NNGen                            1\n",
      "ALL-MINILM-L6-V210               1\n",
      "UniTrans                         1\n",
      "Shipwright                       1\n",
      "MagiCoder                        1\n",
      "Magicoder                        1\n",
      "Parfum                           1\n",
      "TOGA                             1\n",
      "AthenTest                        1\n",
      "SEQ Graph& HYBRID                1\n",
      "AppMap Naive                     1\n",
      "OpenCodeInterpreter              1\n",
      "AutoCodeRover                    1\n",
      "CodeShell                        1\n",
      "LLama                            1\n",
      "CoCoSoDa                         1\n",
      "CodeRetriever                    1\n",
      "HedgeCode                        1\n",
      "SYNCOBERT                        1\n",
      "Vercel                           1\n",
      "GPT-4, CodeBERT                  1\n",
      "Moatless Tools                   1\n",
      "Agentless                        1\n",
      "FuzzGPT                          1\n",
      "TitanFuzz                        1\n",
      "Aider                            1\n",
      "SWE-Agent                        1\n",
      "DeBERTa                          1\n",
      "OPT                              1\n",
      "GPT3-5                           1\n",
      "Tulu                             1\n",
      "Guanaco                          1\n",
      "PaLM                             1\n",
      "StarChat                         1\n",
      "CAT-LM                           1\n",
      "Exlong                           1\n",
      "GLTR                             1\n",
      "ContraBERT                       1\n",
      "ChatUniTest                      1\n",
      "TestGen-LLM                      1\n",
      "RustAssistant                    1\n",
      "Sonnet                           1\n",
      "Stable-Code                      1\n",
      "BigBird                          1\n",
      "Sapling                          1\n",
      "KeyBERT                          1\n",
      "DISCO                            1\n",
      "PDBERT                           1\n",
      "GPTBigCode                       1\n",
      "Sentence-BERT                    1\n",
      "Airboros                         1\n",
      "CodeBERTa                        1\n",
      "Flan                             1\n",
      "code-davinci                     1\n",
      "UnifiedQA                        1\n",
      "VRepair                          1\n",
      "CodeReviewer                     1\n",
      "GrammarT5                        1\n",
      "Transformer                      1\n",
      "LSTM                             1\n",
      "GIN                              1\n",
      "TypeFix                          1\n",
      "PyTER                            1\n",
      "CoCoNuT                          1\n",
      "AlphaRepair                      1\n",
      "LANCE                            1\n",
      "XLNet                            1\n",
      "RepresentThemAll                 1\n",
      "Curie                            1\n",
      "Davinci                          1\n",
      "ELECTRA                          1\n",
      "MiniLM                           1\n",
      "DOBF                             1\n",
      "VulRepair                        1\n",
      "VulMaster                        1\n",
      "PanguCoder                       1\n",
      "flan-alpaca                      1\n",
      "SPT-Code                         1\n",
      "ProphetNet-Code                  1\n",
      "T5-learning                      1\n",
      "JavaBERT                         1\n",
      "DeepDebug                        1\n",
      "C-BERT                           1\n",
      "CugLM                            1\n",
      "TreeBERT                         1\n",
      "PLBart                           1\n",
      "ATLAS                            1\n",
      "CoCoNut                          1\n",
      "Hoppity                          1\n",
      "Sequencer                        1\n",
      "CEDAR                            1\n",
      "Transformers                     1\n",
      "GPT-NeoX                         1\n",
      "FAIR                             1\n",
      "OSCAR                            1\n",
      "Transcoder*                      1\n",
      "IRGen                            1\n",
      "Deep-SE                          1\n",
      "GPT2SP                           1\n",
      "sentenceBERT                     1\n",
      "SDA-Trans                        1\n",
      "StableCode                       1\n",
      "CodeGen-NL                       1\n",
      "CodeGen-Mono                     1\n",
      "CodeGen-Multi                    1\n",
      "CodeGen2                         1\n",
      "PyCodeGPT                        1\n",
      "GPT-Code-Clippy                  1\n",
      "BERTOverflow                     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['model_families_list'] = df_relevant['model_families'].apply(\n",
    "    lambda x: [model_family.strip() for model_family in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(\"Overall Paper counts per model family:\")\n",
    "print(df_relevant['model_families_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7389b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "CodeBERT            11\n",
       "RoBERTa              8\n",
       "BERT                 7\n",
       "CodeT5               7\n",
       "Codex                5\n",
       "T5                   5\n",
       "GraphCodeBERT        4\n",
       "DistilBERT           3\n",
       "PLBART               2\n",
       "GPT-J                2\n",
       "CodeGen              2\n",
       "BART                 2\n",
       "InCoder              2\n",
       "GPT-Neo              2\n",
       "XLNet                1\n",
       "MiniLM               1\n",
       "ELECTRA              1\n",
       "ALBERT               1\n",
       "RepresentThemAll     1\n",
       "seBERT               1\n",
       "Code-davinci         1\n",
       "Curie                1\n",
       "Davinci              1\n",
       "T5-learning          1\n",
       "CodeGPT              1\n",
       "JavaBERT             1\n",
       "DOBF                 1\n",
       "CuBERT               1\n",
       "ProphetNet-Code      1\n",
       "SPT-Code             1\n",
       "CoTexT               1\n",
       "C-BERT               1\n",
       "GPT-C                1\n",
       "CugLM                1\n",
       "TreeBERT             1\n",
       "GPT-2                1\n",
       "SynCoBERT            1\n",
       "DeepDebug            1\n",
       "UniXcoder            1\n",
       "GPT-NeoX             1\n",
       "PLBart               1\n",
       "GPT-3                1\n",
       "CodeParrot           1\n",
       "Copilot              1\n",
       "ATLAS                1\n",
       "CoCoNut              1\n",
       "Hoppity              1\n",
       "Sequencer            1\n",
       "TFix                 1\n",
       "CEDAR                1\n",
       "Transformers         1\n",
       "TransCoder           1\n",
       "Transcoder*          1\n",
       "SDA-Trans            1\n",
       "BERTOverflow         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2023]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24cb0987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "CodeBERT           11\n",
       "GPT-3.5             9\n",
       "GPT-4               8\n",
       "CodeT5              8\n",
       "ChatGPT             8\n",
       "CodeGen             8\n",
       "GraphCodeBERT       7\n",
       "UniXcoder           5\n",
       "BERT                5\n",
       "Codex               4\n",
       "RoBERTa             4\n",
       "InCoder             4\n",
       "StarCoder           4\n",
       "Llama               3\n",
       "ChatGLM             3\n",
       "UnixCoder           3\n",
       "PLBART              3\n",
       "T5                  3\n",
       "GPT-3               3\n",
       "GPT-2               3\n",
       "text-davinci        3\n",
       "PolyCoder           3\n",
       "CodeGeeX            2\n",
       "CodeParrot          2\n",
       "UniLog              2\n",
       "GPT-NEO             2\n",
       "Copilot             2\n",
       "Vicuna              2\n",
       "Pythia              2\n",
       "CodeGPT             2\n",
       "Airboros            1\n",
       "code-davinci        1\n",
       "SantaCoder          1\n",
       "WizardCoder         1\n",
       "Sentence-BERT       1\n",
       "GPTBigCode          1\n",
       "seBERT              1\n",
       "VulBERTa            1\n",
       "PDBERT              1\n",
       "ALBERT              1\n",
       "KeyBERT             1\n",
       "LSTM                1\n",
       "TFix                1\n",
       "DISCO               1\n",
       "text-embedding      1\n",
       "Flan                1\n",
       "CodeBERTa           1\n",
       "SynCoBERT           1\n",
       "CodeLlama           1\n",
       "Transformer         1\n",
       "GIN                 1\n",
       "flan-alpaca         1\n",
       "UnifiedQA           1\n",
       "GrammarT5           1\n",
       "GPT-C               1\n",
       "VRepair             1\n",
       "CodeReviewer        1\n",
       "VulMaster           1\n",
       "VulRepair           1\n",
       "PanguCoder          1\n",
       "TypeFix             1\n",
       "CoTexT              1\n",
       "CodeT5+             1\n",
       "AlphaRepair         1\n",
       "PyTER               1\n",
       "CoCoNuT             1\n",
       "LANCE               1\n",
       "IRGen               1\n",
       "Deep-SE             1\n",
       "FAIR                1\n",
       "OSCAR               1\n",
       "sentenceBERT        1\n",
       "GPT2SP              1\n",
       "CodeGen-Mono        1\n",
       "CodeGen-NL          1\n",
       "CodeGen-Multi       1\n",
       "CodeGen2            1\n",
       "PyCodeGPT           1\n",
       "GPT-Code-Clippy     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2024]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a8ec5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "GPT-4                           39\n",
       "GPT-3.5                         35\n",
       "CodeLlama                       25\n",
       "Llama                           14\n",
       "StarCoder                       14\n",
       "CodeBERT                        12\n",
       "DeepSeekCoder                   12\n",
       "CodeGen                          9\n",
       "Claude                           7\n",
       "Gemini                           7\n",
       "CodeT5                           7\n",
       "GraphCodeBERT                    7\n",
       "ChatGPT                          6\n",
       "CodeQwen                         6\n",
       "DeepSeek                         6\n",
       "UniXcoder                        5\n",
       "UnixCoder                        3\n",
       "LineVul                          3\n",
       "WizardCoder                      3\n",
       "ChatGLM                          3\n",
       "Mistral                          3\n",
       "Gemma                            3\n",
       "RoBERTa                          3\n",
       "InCoder                          3\n",
       "Codestral                        2\n",
       "Qwen                             2\n",
       "CodeT5+                          2\n",
       "CodeGemma                        2\n",
       "GPT3.5                           2\n",
       "OpenDevin                        2\n",
       "CodeGPT                          2\n",
       "Incoder                          2\n",
       "text-embedding                   2\n",
       "Phi                              2\n",
       "GPT-3                            2\n",
       "AthenTest                        1\n",
       "ChatDev                          1\n",
       "SVulD                            1\n",
       "Code-davinci                     1\n",
       "Multi-Turn Program Synthesis     1\n",
       "GPT4                             1\n",
       "AgentCoder                       1\n",
       "Self-collaboration               1\n",
       "Copilot                          1\n",
       "AutoGPT                          1\n",
       "MetaGPT                          1\n",
       "Magicoder                        1\n",
       "CoditT5                          1\n",
       "Unixcoder                        1\n",
       "Longformer                       1\n",
       "Shipwright                       1\n",
       "MagiCoder                        1\n",
       "TransCoder                       1\n",
       "Parfum                           1\n",
       "UniTrans                         1\n",
       "SantaCoder                       1\n",
       "ALL-MINILM-L6-V210               1\n",
       "CodeBert                         1\n",
       "TOGA                             1\n",
       "Poro                             1\n",
       "BERT                             1\n",
       "SEQ Graph& HYBRID                1\n",
       "Devign                           1\n",
       "VULGEN                           1\n",
       "COME                             1\n",
       "CCT5                             1\n",
       "NNGen                            1\n",
       "ReVeal                           1\n",
       "Mixtral                          1\n",
       "VGX                              1\n",
       "LILAC                            1\n",
       "LLM-Parser                       1\n",
       "Starcoder                        1\n",
       "GPTSniffer                       1\n",
       "FitRepair                        1\n",
       "ChatRepair                       1\n",
       "RAP-Gen                          1\n",
       "AlphaRe-pair                     1\n",
       "GLTR                             1\n",
       "Sapling                          1\n",
       "OpenCodeInterpreter              1\n",
       "Stable-Code                      1\n",
       "BigBird                          1\n",
       "ContraBERT                       1\n",
       "CuBERT                           1\n",
       "VulBERTa                         1\n",
       "ChatUniTest                      1\n",
       "TestGen-LLM                      1\n",
       "RustAssistant                    1\n",
       "Sonnet                           1\n",
       "GPTZero                          1\n",
       "GPT-2 Output Detector            1\n",
       "DetectGPT                        1\n",
       "Repilot                          1\n",
       "Aider                            1\n",
       "AppMap Naive                     1\n",
       "AutoCodeRover                    1\n",
       "SWE-Agent                        1\n",
       "Vercel                           1\n",
       "Agentless                        1\n",
       "TitanFuzz                        1\n",
       "Moatless Tools                   1\n",
       "HedgeCode                        1\n",
       "GPT-4, CodeBERT                  1\n",
       "Codex                            1\n",
       "SYNCOBERT                        1\n",
       "CoCoSoDa                         1\n",
       "CodeRetriever                    1\n",
       "LLama                            1\n",
       "FuzzGPT                          1\n",
       "CodeShell                        1\n",
       "PLBART                           1\n",
       "Pythia                           1\n",
       "OPT                              1\n",
       "Exlong                           1\n",
       "DeBERTa                          1\n",
       "CodeParrot                       1\n",
       "CAT-LM                           1\n",
       "PolyCoder                        1\n",
       "StarChat                         1\n",
       "PaLM                             1\n",
       "Vicuna                           1\n",
       "Guanaco                          1\n",
       "Tulu                             1\n",
       "GPT3-5                           1\n",
       "StableCode                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2025]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94f1a0",
   "metadata": {},
   "source": [
    "# RQ3 - How well do authors tackle the problem of data leakage/contamination?\n",
    "\n",
    "DF Columns to use:\n",
    "- contamination (bool)\n",
    "- contamination_free_text (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "547d1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All years: Contamination reported in 58 out of 177 papers (32.8%)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "total = df_relevant.shape[0]\n",
    "contamination_reported = df_relevant['contamination'].sum()\n",
    "\n",
    "print(f\"All years: Contamination reported in {contamination_reported} out of {total} papers ({(contamination_reported/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb11e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Contamination reported in 6 out of 32 papers (18.8%)\n",
      "2024: Contamination reported in 14 out of 55 papers (25.5%)\n",
      "2025: Contamination reported in 38 out of 90 papers (42.2%)\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "contamination_2023 = df_relevant[df_relevant['year'] == 2023]['contamination'].sum()\n",
    "print(f\"2023: Contamination reported in {contamination_2023} out of {total_2023} papers ({(contamination_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "contamination_2024 = df_relevant[df_relevant['year'] == 2024]['contamination'].sum()\n",
    "print(f\"2024: Contamination reported in {contamination_2024} out of {total_2024} papers ({(contamination_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "contamination_2025 = df_relevant[df_relevant['year'] == 2025]['contamination'].sum()\n",
    "print(f\"2025: Contamination reported in {contamination_2025} out of {total_2025} papers ({(contamination_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfc14a",
   "metadata": {},
   "source": [
    "# RQ4 - How replicable are LLM-based studies?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'model_config' (short-text list)\n",
    "- 'artifact_available' (bool)\n",
    "- 'artifact_reusable' (bool)\n",
    "- 'artifact_functional' (bool)\n",
    "- 'artefact_manual' (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d775ea",
   "metadata": {},
   "source": [
    "## ACM Badge Artifact Availability - Relevant vs. Non-Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96259443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of relevant papers with artifact available: 18.64% (33/177)\n",
      "Proportion of other papers with artifact available: 41.36% (213/515)\n"
     ]
    }
   ],
   "source": [
    "# Calculate proportions in the combined dataset\n",
    "total_relevant = df_relevant.shape[0]\n",
    "total_non_relevant = df_non_relevant.shape[0]\n",
    "\n",
    "relevant_with_artifact = df_relevant['artifact_available'].sum()\n",
    "non_relevant_with_artifact = df_non_relevant['artifact_available'].sum()\n",
    "\n",
    "prop_relevant = relevant_with_artifact / total_relevant\n",
    "prop_non_relevant = non_relevant_with_artifact / total_non_relevant\n",
    "\n",
    "print(f\"Proportion of relevant papers with artifact available: {prop_relevant:.2%} ({relevant_with_artifact}/{total_relevant})\")\n",
    "print(f\"Proportion of other papers with artifact available: {prop_non_relevant:.2%} ({non_relevant_with_artifact}/{total_non_relevant})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637d335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 - Proportion of relevant papers with artifact available: 18.75% (6/32)\n",
      "2024 - Proportion of relevant papers with artifact available: 16.36% (9/55)\n",
      "2025 - Proportion of relevant papers with artifact available: 20.00% (18/90)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of relevant papers with artifacts available per year\n",
    "total_relevant_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "artifact_avail_2023 = df_relevant[(df_relevant['year'] == 2023) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2023 = artifact_avail_2023 / total_relevant_2023\n",
    "print(f\"2023 - Proportion of relevant papers with artifact available: {prop_relevant_2023:.2%} ({artifact_avail_2023}/{total_relevant_2023})\")\n",
    "\n",
    "total_relevant_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "artifact_avail_2024 = df_relevant[(df_relevant['year'] == 2024) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2024 = artifact_avail_2024 / total_relevant_2024\n",
    "print(f\"2024 - Proportion of relevant papers with artifact available: {prop_relevant_2024:.2%} ({artifact_avail_2024}/{total_relevant_2024})\")\n",
    "\n",
    "total_relevant_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "artifact_avail_2025 = df_relevant[(df_relevant['year'] == 2025) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2025 = artifact_avail_2025 / total_relevant_2025\n",
    "print(f\"2025 - Proportion of relevant papers with artifact available: {prop_relevant_2025:.2%} ({artifact_avail_2025}/{total_relevant_2025})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e39d00",
   "metadata": {},
   "source": [
    "## Manual Artefact Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62cde1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact_manual\n",
      "True     144\n",
      "False     24\n",
      "DEAD       9\n",
      "Name: count, dtype: int64\n",
      "9 dead links in the 177 relevant papers\n",
      "2023 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     24\n",
      "False     5\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2024 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     44\n",
      "False     8\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2025 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     76\n",
      "False    11\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_relevant[\"artefact_manual\"].value_counts())\n",
    "print(f\"{df_relevant[df_relevant['artefact_manual'] == 'DEAD'].shape[0]} dead links in the {df_relevant.shape[0]} relevant papers\")\n",
    "\n",
    "# Per year\n",
    "print(\"2023 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artefact_manual'].value_counts())\n",
    "print(\"\\n2024 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artefact_manual'].value_counts())\n",
    "print(\"\\n2025 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artefact_manual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa8bd7",
   "metadata": {},
   "source": [
    "## How many papers have manually identified artefacts but no ACM badge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb9938c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with manually identified artefacts (including DEAD links): 153 out of 177 relevant papers (86.4%)\n",
      "Number of papers with ACM artifact available badge: 33 out of 177 relevant papers (18.6%)\n",
      "Number of papers with manually identified artefacts but no ACM artifact available badge: 124 out of 177 relevant papers (70.1%)\n",
      "Number of papers with no manually identified artefacts and no ACM artifact available badge: 20 out of 177 relevant papers (11.3%)\n"
     ]
    }
   ],
   "source": [
    "num_manually_identified = df_relevant[df_relevant['artefact_manual'].isin([True, 'DEAD'])].shape[0]\n",
    "print(f\"Number of papers with manually identified artefacts (including DEAD links): {num_manually_identified} out of {df_relevant.shape[0]} relevant papers ({(num_manually_identified/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_artifact_available = df_relevant[df_relevant['artifact_available']].shape[0]\n",
    "print(f\"Number of papers with ACM artifact available badge: {num_artifact_available} out of {df_relevant.shape[0]} relevant papers ({(num_artifact_available/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_manually_identified_no_badge = df_relevant[(df_relevant['artefact_manual'].isin([True, 'DEAD'])) & (~df_relevant['artifact_available'])].shape[0]\n",
    "print(f\"Number of papers with manually identified artefacts but no ACM artifact available badge: {num_manually_identified_no_badge} out of {df_relevant.shape[0]} relevant papers ({(num_manually_identified_no_badge/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_no_manual_no_badge = df_relevant[(~df_relevant['artefact_manual'].isin([True, 'DEAD'])) & (~df_relevant['artifact_available'])].shape[0]\n",
    "print(f\"Number of papers with no manually identified artefacts and no ACM artifact available badge: {num_no_manual_no_badge} out of {df_relevant.shape[0]} relevant papers ({(num_no_manual_no_badge/df_relevant.shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336064ed",
   "metadata": {},
   "source": [
    "## How many papers with ACM badges have dead links?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8523134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with artifacts available but DEAD links: 2\n",
      "Number of papers with artifacts reusable but DEAD links: 1\n",
      "Number of papers with artifacts functional but DEAD links: 0\n",
      "Two total papers have ACM badges with dead links. These papers both have the artifact available badge, and one of them even has the artifact reusable badge.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artefact_manual</th>\n",
       "      <th>artifact_available</th>\n",
       "      <th>artifact_reusable</th>\n",
       "      <th>artifact_functional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLM Assistance for Memory Safety</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ChatGPT-Based Test Generation for Refactoring ...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Automated Generation of Accessibility Test Rep...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Make LLM a Testing Expert: Bringing Human-like...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Demystifying and Detecting Misuses of Deep Lea...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Large Language Models are Edge-Case Generators...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Context-Aware Bug Reproduction for Mobile Apps</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>AI-Based Question Answering Assistance for Ana...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Large Language Models are Few-Shot Testers: Ex...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title artefact_manual  \\\n",
       "13                    LLM Assistance for Memory Safety            DEAD   \n",
       "57   ChatGPT-Based Test Generation for Refactoring ...            DEAD   \n",
       "75   Automated Generation of Accessibility Test Rep...            DEAD   \n",
       "89   Make LLM a Testing Expert: Bringing Human-like...            DEAD   \n",
       "96   Demystifying and Detecting Misuses of Deep Lea...            DEAD   \n",
       "131  Large Language Models are Edge-Case Generators...            DEAD   \n",
       "134     Context-Aware Bug Reproduction for Mobile Apps            DEAD   \n",
       "140  AI-Based Question Answering Assistance for Ana...            DEAD   \n",
       "149  Large Language Models are Few-Shot Testers: Ex...            DEAD   \n",
       "\n",
       "     artifact_available  artifact_reusable  artifact_functional  \n",
       "13                False              False                False  \n",
       "57                False              False                False  \n",
       "75                False              False                False  \n",
       "89                False              False                False  \n",
       "96                False              False                False  \n",
       "131               False              False                False  \n",
       "134               False              False                False  \n",
       "140                True               True                False  \n",
       "149                True              False                False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of papers with artifacts available but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_available'])].shape[0])\n",
    "print(\"Number of papers with artifacts reusable but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_reusable'])].shape[0])\n",
    "print(\"Number of papers with artifacts functional but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_functional'])].shape[0])\n",
    "\n",
    "print(\"Two total papers have ACM badges with dead links. These papers both have the artifact available badge, and one of them even has the artifact reusable badge.\")\n",
    "df_relevant[df_relevant['artefact_manual'] == 'DEAD'][['title', 'artefact_manual', 'artifact_available', 'artifact_reusable', 'artifact_functional']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518a064",
   "metadata": {},
   "source": [
    "# RQ5 - How sustainable is LLM-based SE research?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'cost' (short-text list)\n",
    "- 'cost_free_text' (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb184c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
