{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4934af8f",
   "metadata": {},
   "source": [
    "# Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fadbe",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab58f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Shape: (177, 28)\n",
      "All Papers Shape: (692, 11)\n"
     ]
    }
   ],
   "source": [
    "df_relevant = pd.read_excel('results/final/final_results.xlsx', sheet_name='relevant_papers')\n",
    "df_all_papers = pd.read_csv('results/ICSE_all_papers.csv')\n",
    "\n",
    "print(\"Relevant Shape:\", df_relevant.shape)\n",
    "print(\"All Papers Shape:\", df_all_papers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676387c9",
   "metadata": {},
   "source": [
    "### Creating Combined and Non-Relevant Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a29c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Paper List\n",
    "relevant_papers = df_relevant['title'].tolist()\n",
    "\n",
    "# Remove relevant papers from all papers to create final non-relevant set\n",
    "df_non_relevant = df_all_papers[~df_all_papers['title'].isin(relevant_papers)].copy()\n",
    "\n",
    "# Make sure all non-relevant papers are marked as such\n",
    "df_non_relevant['relevant'] = False\n",
    "\n",
    "# Re-order columns to match relevant dataframe\n",
    "common_columns = ['reviewer', 'relevant', 'year', 'title', 'authors', 'url', 'abstract', 'artifact_available', 'artifact_reusable', 'artifact_functional', 'ai']\n",
    "df_non_relevant = df_non_relevant[common_columns]\n",
    "\n",
    "# Add extra columns: 'task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text'\n",
    "extra_columns = ['task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text']\n",
    "for col in extra_columns:\n",
    "    df_non_relevant[col] = None\n",
    "\n",
    "# Combine relevant and non-relevant dataframes\n",
    "df_combined = pd.concat([df_relevant, df_non_relevant], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4613e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape: (692, 28)\n",
      "Num Unique Papers: 692\n",
      "Num Relevant Papers in Combined DF: 177\n",
      "Num Unique Papers in Relevant DF: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Shape:\", df_combined.shape)\n",
    "print(\"Num Unique Papers:\", df_combined['title'].nunique())\n",
    "print(\"Num Relevant Papers in Combined DF:\", df_combined[df_combined['relevant'] == True].shape[0])\n",
    "print(\"Num Unique Papers in Relevant DF:\", df_relevant['title'].nunique()) # Should match number above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100580d7",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We now have access to four dataframes for analysis:\n",
    "\n",
    "- `df_combined`: contains all papers and final columns from our spreadsheet (non-relevant papers just have None values in the fields we completed for the relevant papers)\n",
    "- `df_relevant`: contains all relevant papers as rows and the final columns we intend to use for analysis\n",
    "- `df_non_relevant`: contains all non-relevant papers. Our finals columns are present but all filled with None values as we didn't perform data extraction for these papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64a7b",
   "metadata": {},
   "source": [
    "## Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719ef68",
   "metadata": {},
   "source": [
    "### Number of Papers at Each Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Papers in Combined DF: 692\n",
      "\n",
      "Total Papers from AI Keywords:\n",
      "ai\n",
      "False    388\n",
      "True     304\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Relevant Papers:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total numbers\n",
    "print(\"Total Papers in Combined DF:\", df_combined.shape[0])\n",
    "\n",
    "print(\"\\nTotal Papers from AI Keywords:\")\n",
    "print(df_combined['ai'].value_counts())\n",
    "\n",
    "print(\"\\nTotal Relevant Papers:\")\n",
    "print(df_combined['relevant'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers Per Year:\n",
      "year\n",
      "2023    210\n",
      "2024    236\n",
      "2025    246\n",
      "Name: title, dtype: int64\n",
      "\n",
      "AI Papers Per Year:\n",
      "year\n",
      "2023     59\n",
      "2024     99\n",
      "2025    146\n",
      "Name: title, dtype: int64\n",
      "\n",
      "Relevant Papers Per Year:\n",
      "year\n",
      "2023    32\n",
      "2024    55\n",
      "2025    90\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "print(\"Papers Per Year:\")\n",
    "print(df_combined.groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nAI Papers Per Year:\")\n",
    "print(df_combined[df_combined['ai'] == True].groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nRelevant Papers Per Year:\")\n",
    "print(df_combined[df_combined['relevant'] == True].groupby('year')['title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029cd15",
   "metadata": {},
   "source": [
    "### Numbers of Relevant Papers (Papers with LLM-based Empirical Studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfef7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevant vs Non-Relevant Counts:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Relevant Papers by Year:\n",
      "year\n",
      "2025    90\n",
      "2024    55\n",
      "2023    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Relevant vs Non-Relevant Counts:\")\n",
    "print(df_combined[\"relevant\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nRelevant Papers by Year:\")\n",
    "print(df_relevant[\"year\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe95f7",
   "metadata": {},
   "source": [
    "### Geo-location of SE Research Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e76ac",
   "metadata": {},
   "source": [
    "# RQ1 - What Tasks are being tackled in LLM SE studies, and are they fairly evaluated against existing non-LLM techniques?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'task' (short-text)\n",
    "- 'non_llm_approaches' (bool)\n",
    "- 'dataset_type' (short-text)\n",
    "- programming_language (short-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e4a75",
   "metadata": {},
   "source": [
    "## Tasks Tackled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a225d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list\n",
      "code generation                                   26\n",
      "program repair                                    24\n",
      "test generation                                   23\n",
      "vulnerability detection                           16\n",
      "bug detection                                     12\n",
      "code translation                                   8\n",
      "clone detection                                    6\n",
      "code completion                                    6\n",
      "code summarisation                                 6\n",
      "code comprehension                                 5\n",
      "type detection                                     5\n",
      "log parsing                                        5\n",
      "code search                                        5\n",
      "fuzzing                                            4\n",
      "bug reproduction                                   3\n",
      "commit message generation                          3\n",
      "fault localisation                                 3\n",
      "code memorisation detection                        3\n",
      "test repair                                        2\n",
      "formal verification                                2\n",
      "program analysis                                   2\n",
      "traceability link recovery                         2\n",
      "code retrieval                                     2\n",
      "code refinement                                    1\n",
      "regression testing                                 1\n",
      "SO post editing                                    1\n",
      "AI generated code detection                        1\n",
      "security patch detection                           1\n",
      "software model evolution                           1\n",
      "configuration validation                           1\n",
      "smart contract auditing                            1\n",
      "UI design repair                                   1\n",
      "detection of code design issues                    1\n",
      "code adaptation                                    1\n",
      "comment repair                                     1\n",
      "bug report comprehension                           1\n",
      "inconsistency prediction in decentralised apps     1\n",
      "machine-generated code detection                   1\n",
      "security injection                                 1\n",
      "code optimisation                                  1\n",
      "root cause analysis                                1\n",
      "privacy inconsistencies detection                  1\n",
      "figurative language detection                      1\n",
      "binary software composition analysis               1\n",
      "API misuse detection                               1\n",
      "program comprehension                              1\n",
      "exception handling recommender                     1\n",
      "static analysis                                    1\n",
      "comment generation                                 1\n",
      "code review                                        1\n",
      "log understanding                                  1\n",
      "emotion-cause extraction                           1\n",
      "code idioms detection                              1\n",
      "log generation                                     1\n",
      "CI/CD workflow generation                          1\n",
      "requirements analysis                              1\n",
      "model extraction                                   1\n",
      "mutant generation                                  1\n",
      "assert generation                                  1\n",
      "SO posts summarisation                             1\n",
      "question answering                                 1\n",
      "patch correctness assessment                       1\n",
      "algorithm classification                           1\n",
      "optimal thread coarsening factor                   1\n",
      "heterogeneous device mapping                       1\n",
      "vulnerability alert prediction                     1\n",
      "vulnerability repair                               1\n",
      "GUI test case migration                            1\n",
      "software effort estimation                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['task_list'] = df_relevant['task'].apply(\n",
    "    lambda x: [task.strip() for task in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(df_relevant['task_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273d4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tasks:\n",
      "    \"AI generated code detection\":\"\",\n",
      "    \"API misuse detection\":\"\",\n",
      "    \"CI/CD workflow generation\":\"\",\n",
      "    \"GUI test case migration\":\"\",\n",
      "    \"SO post editing\":\"\",\n",
      "    \"SO posts summarisation\":\"\",\n",
      "    \"UI design repair\":\"\",\n",
      "    \"algorithm classification\":\"\",\n",
      "    \"assert generation\":\"\",\n",
      "    \"binary software composition analysis\":\"\",\n",
      "    \"bug detection\":\"\",\n",
      "    \"bug report comprehension\":\"\",\n",
      "    \"bug reproduction\":\"\",\n",
      "    \"clone detection\":\"\",\n",
      "    \"code adaptation\":\"\",\n",
      "    \"code completion\":\"\",\n",
      "    \"code comprehension\":\"\",\n",
      "    \"code generation\":\"\",\n",
      "    \"code idioms detection\":\"\",\n",
      "    \"code memorisation detection\":\"\",\n",
      "    \"code optimisation\":\"\",\n",
      "    \"code refinement\":\"\",\n",
      "    \"code retrieval\":\"\",\n",
      "    \"code review\":\"\",\n",
      "    \"code search\":\"\",\n",
      "    \"code summarisation\":\"\",\n",
      "    \"code translation\":\"\",\n",
      "    \"comment generation\":\"\",\n",
      "    \"comment repair\":\"\",\n",
      "    \"commit message generation\":\"\",\n",
      "    \"configuration validation\":\"\",\n",
      "    \"detection of code design issues\":\"\",\n",
      "    \"emotion-cause extraction\":\"\",\n",
      "    \"exception handling recommender\":\"\",\n",
      "    \"fault localisation\":\"\",\n",
      "    \"figurative language detection\":\"\",\n",
      "    \"formal verification\":\"\",\n",
      "    \"fuzzing\":\"\",\n",
      "    \"heterogeneous device mapping\":\"\",\n",
      "    \"inconsistency prediction in decentralised apps\":\"\",\n",
      "    \"log generation\":\"\",\n",
      "    \"log parsing\":\"\",\n",
      "    \"log understanding\":\"\",\n",
      "    \"machine-generated code detection\":\"\",\n",
      "    \"model extraction\":\"\",\n",
      "    \"mutant generation\":\"\",\n",
      "    \"optimal thread coarsening factor\":\"\",\n",
      "    \"patch correctness assessment\":\"\",\n",
      "    \"privacy inconsistencies detection\":\"\",\n",
      "    \"program analysis\":\"\",\n",
      "    \"program comprehension\":\"\",\n",
      "    \"program repair\":\"\",\n",
      "    \"question answering\":\"\",\n",
      "    \"regression testing\":\"\",\n",
      "    \"requirements analysis\":\"\",\n",
      "    \"root cause analysis\":\"\",\n",
      "    \"security injection\":\"\",\n",
      "    \"security patch detection\":\"\",\n",
      "    \"smart contract auditing\":\"\",\n",
      "    \"software effort estimation\":\"\",\n",
      "    \"software model evolution\":\"\",\n",
      "    \"static analysis\":\"\",\n",
      "    \"test generation\":\"\",\n",
      "    \"test repair\":\"\",\n",
      "    \"traceability link recovery\":\"\",\n",
      "    \"type detection\":\"\",\n",
      "    \"vulnerability alert prediction\":\"\",\n",
      "    \"vulnerability detection\":\"\",\n",
      "    \"vulnerability repair\":\"\",\n",
      "Num Unique Tasks: 69\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Tasks:\")\n",
    "\n",
    "for task in sorted(df_relevant['task_list'].explode().unique()):\n",
    "    print(f\"    \\\"{task}\\\":\\\"\\\",\")\n",
    "\n",
    "print(\"Num Unique Tasks:\", df_relevant['task_list'].explode().nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e857069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relevant Papers by Topic:\n",
      "topic_list\n",
      "code_recommendation             43\n",
      "verification_validation         39\n",
      "program_comprehension           31\n",
      "automated_repair                30\n",
      "security_privacy                20\n",
      "issue_detection_localisation    16\n",
      "miscellaneous                    9\n",
      "log_analysis                     7\n",
      "emerging                         6\n",
      "smart_contracts                  2\n",
      "requirements_engineering         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "task_to_topic = {\n",
    "    \"AI generated code detection\":\"emerging\",\n",
    "    \"API misuse detection\":\"issue_detection_localisation\",\n",
    "    \"CI/CD workflow generation\":\"miscellaneous\",\n",
    "    \"GUI test case migration\":\"verification_validation\",\n",
    "    \"SO post editing\":\"miscellaneous\",\n",
    "    \"SO posts summarisation\":\"miscellaneous\",\n",
    "    \"UI design repair\":\"automated_repair\",\n",
    "    \"algorithm classification\":\"miscellaneous\",\n",
    "    \"assert generation\":\"verification_validation\",\n",
    "    \"binary software composition analysis\":\"program_comprehension\",\n",
    "    \"bug detection\":\"issue_detection_localisation\",\n",
    "    \"bug report comprehension\":\"program_comprehension\",\n",
    "    \"bug reproduction\":\"verification_validation\",\n",
    "    \"clone detection\":\"program_comprehension\",\n",
    "    \"code adaptation\":\"code_recommendation\",\n",
    "    \"code completion\":\"code_recommendation\",\n",
    "    \"code comprehension\":\"program_comprehension\",\n",
    "    \"code generation\":\"code_recommendation\",\n",
    "    \"code idioms detection\":\"program_comprehension\",\n",
    "    \"code memorisation detection\":\"emerging\",\n",
    "    \"code optimisation\":\"code_recommendation\",\n",
    "    \"code refinement\":\"code_recommendation\",\n",
    "    \"code retrieval\":\"program_comprehension\",\n",
    "    \"code review\":\"verification_validation\",\n",
    "    \"code search\":\"code_recommendation\",\n",
    "    \"code summarisation\":\"program_comprehension\",\n",
    "    \"code translation\":\"code_recommendation\",\n",
    "    \"comment generation\":\"program_comprehension\",\n",
    "    \"comment repair\":\"automated_repair\",\n",
    "    \"commit message generation\":\"program_comprehension\",\n",
    "    \"configuration validation\":\"verification_validation\",\n",
    "    \"detection of code design issues\":\"issue_detection_localisation\",\n",
    "    \"emotion-cause extraction\":\"miscellaneous\",\n",
    "    \"exception handling recommender\":\"code_recommendation\",\n",
    "    \"fault localisation\":\"issue_detection_localisation\",\n",
    "    \"figurative language detection\":\"miscellaneous\",\n",
    "    \"formal verification\":\"verification_validation\",\n",
    "    \"fuzzing\":\"verification_validation\",\n",
    "    \"heterogeneous device mapping\":\"miscellaneous\",\n",
    "    \"inconsistency prediction in decentralised apps\":\"smart_contracts\",\n",
    "    \"log generation\":\"log_analysis\",\n",
    "    \"log parsing\":\"log_analysis\",\n",
    "    \"log understanding\":\"log_analysis\",\n",
    "    \"machine-generated code detection\":\"emerging\",\n",
    "    \"software model evolution\":\"miscellaneous\",\n",
    "    \"model extraction\":\"emerging\",\n",
    "    \"mutant generation\":\"verification_validation\",\n",
    "    \"optimal thread coarsening factor\":\"miscellaneous\",\n",
    "    \"patch correctness assessment\":\"automated_repair\",\n",
    "    \"privacy inconsistencies detection\":\"security_privacy\",\n",
    "    \"program analysis\":\"program_comprehension\",\n",
    "    \"program comprehension\":\"program_comprehension\",\n",
    "    \"program repair\":\"automated_repair\",\n",
    "    \"question answering\":\"miscellaneous\",\n",
    "    \"regression testing\":\"verification_validation\",\n",
    "    \"requirements analysis\":\"requirements_engineering\",\n",
    "    \"root cause analysis\":\"verification_validation\",\n",
    "    \"security injection\":\"security_privacy\",\n",
    "    \"security patch detection\":\"security_privacy\",\n",
    "    \"smart contract auditing\":\"smart_contracts\",\n",
    "    \"software effort estimation\":\"miscellaneous\",\n",
    "    \"static analysis\":\"verification_validation\",\n",
    "    \"test generation\":\"verification_validation\",\n",
    "    \"test repair\":\"automated_repair\",\n",
    "    \"traceability link recovery\":\"program_comprehension\",\n",
    "    \"type detection\":\"program_comprehension\",\n",
    "    \"vulnerability alert prediction\":\"security_privacy\",\n",
    "    \"vulnerability detection\":\"security_privacy\",\n",
    "    \"vulnerability repair\":\"automated_repair\",\n",
    "}\n",
    "\n",
    "# Map tasks to new topic column\n",
    "df_relevant['topic_list'] = df_relevant['task_list'].apply(\n",
    "    lambda tasks: list({task_to_topic.get(task, \"other\") for task in tasks})\n",
    ")\n",
    "\n",
    "print(\"\\nRelevant Papers by Topic:\")\n",
    "print(df_relevant['topic_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f5c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vulnerability detection\n",
      "- Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on Code Naturalness (2025)\n",
      "- GVI: Guided Vulnerability Imagination for Boosting Deep Vulnerability Detectors (2025)\n",
      "- Leveraging Large Language Models to Detect npm Malicious Packages (2025)\n",
      "- Similar but Patched Code Considered Harmful: The Impact of Similar but Patched Code on Recurring Vulnerability Detection and How to Remove Them (2025)\n",
      "- Vulnerability Detection with Code Language Models: How Far Are We? (2025)\n",
      "- Metamorphic-Based Many-Objective Distillation of LLMs for Code-Related Tasks (2025)\n",
      "- VGX: Large-Scale Sample Generation for Boosting Learning-Based Software Vulnerability Analyses (2024)\n",
      "- Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports (2024)\n",
      "- Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks (2024)\n",
      "- GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis (2024)\n",
      "- TRACED: Execution-aware Pre-training for Source Code (2024)\n",
      "- Fine-Grained Commit-Level Vulnerability Type Prediction by CWE Tree Structure (2023)\n",
      "- VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning (2023)\n",
      "- Enhancing Deep Learning-Based Vulnerability Detection by Building Behavior Graph Model (2023)\n",
      "- CoLeFunDa: Explainable Silent Vulnerability Fix Identification (2023)\n",
      "- CodeImprove: Program Adaptation for Deep Code Models (2025)\n",
      "\n",
      "test generation\n",
      "- Test Intention Guided LLM-Based Unit Test Generation (2025)\n",
      "- A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs (2025)\n",
      "- Feature-Driven End-to-End Test Generation (2025)\n",
      "- What You See Is What You Get: Attention-Based Self-Guided Automatic Unit Test Generation (2025)\n",
      "- Rug: Turbo LLM for Rust Unit Test Generation (2025)\n",
      "- LWDIFF: An LLM-Assisted Differential Testing Framework for WebAssembly Runtimes (2025)\n",
      "- Automating a Complete Software Test Process Using LLMs: An Automotive Case Study (2025)\n",
      "- ChatGPT-Based Test Generation for Refactoring Engines Enhanced by Feature Analysis on Examples (2025)\n",
      "- Understanding the Effectiveness of Coverage Criteria for Large Language Models: A Special Angle from Jailbreak Attacks (2025)\n",
      "- exLong: Generating Exceptional Behavior Tests with Large Language Models (2025)\n",
      "- LLM Based Input Space Partitioning Testing for Library APIs (2025)\n",
      "- TOGLL: Correct and Strong Test Oracle Generation with LLMs (2025)\n",
      "- Automated Generation of Accessibility Test Reports from Recorded User Transcripts (2025)\n",
      "- Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions (2024)\n",
      "- ChatGPT Incorrectness Detection in Software Reviews (2024)\n",
      "- Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model (2024)\n",
      "- Knowledge Graph Driven Inference Testing for Question Answering Software (2024)\n",
      "- Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing (2023)\n",
      "- CCTest: Testing and Repairing Code Completion Systems (2023)\n",
      "- Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction (2023)\n",
      "- CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models (2023)\n",
      "- Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning (2023)\n",
      "- MTTM: Metamorphic Testing for Textual Content Moderation Software (2023)\n",
      "\n",
      "test repair\n",
      "- NIODebugger: A Novel Approach to Repair Non-Idempotent-Outcome Tests with LLM-Based Agent (2025)\n",
      "- Comprehensive Semantic Repair of Obsolete GUI Test Scripts for Mobile Applications (2024)\n",
      "\n",
      "code refinement\n",
      "- Intention is All You Need: Refining Your Code from Your Intention (2025)\n",
      "\n",
      "code generation\n",
      "- Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding (2025)\n",
      "- Fixing Large Language Models' Specification Misunderstanding for Better Code Generation (2025)\n",
      "- LiCoEval: Evaluating LLMs on License Compliance in Code Generation (2025)\n",
      "- Towards Understanding the Characteristics of Code Generation Errors Made by Large Language Models (2025)\n",
      "- Unseen Horizons: Unveiling the Real Capability of LLM Code Generation Beyond the Familiar (2025)\n",
      "- Model Editing for LLMs4Code: How Far are We? (2025)\n",
      "- LLMs Meet Library Evolution: Evaluating Deprecated API Usage in LLM-Based Code Completion (2025)\n",
      "- HumanEvo: An Evolution-Aware Benchmark for More Realistic Evaluation of Repository-Level Code Generation (2025)\n",
      "- Calibration and Correctness of Language Models for Code (2025)\n",
      "- SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents (2025)\n",
      "- ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation (2025)\n",
      "- The Seeds of the FUTURE Sprout from History: Fuzzing for Unveiling Vulnerabilities in Prospective Deep-Learning Libraries (2025)\n",
      "- Iterative Generation of Adversarial Example for Deep Code Models (2025)\n",
      "- LLM-Aided Automatic Modeling for Security Protocol Verification (2025)\n",
      "- 3DGen: AI-Assisted Generation of Provably Correct Binary Format Parsers (2025)\n",
      "- Can an LLM Find Its Way around a Spreadsheet? (2025)\n",
      "- Evaluating Large Language Models in Class-Level Code Generation (2024)\n",
      "- Demystifying and Detecting Misuses of Deep Learning APIs (2024)\n",
      "- Code Search is All You Need? Improving Code Suggestions with Code Search (2024)\n",
      "- Are Prompt Engineering and TODO Comments Friends or Foes? An Evaluation on GitHub Copilot (2024)\n",
      "- GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code (2024)\n",
      "- On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study (2024)\n",
      "- CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models (2024)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models (2023)\n",
      "- EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning (2024)\n",
      "\n",
      "comment repair\n",
      "- Code Comment Inconsistency Detection and Rectification Using a Large Language Model (2025)\n",
      "\n",
      "program repair\n",
      "- Template-Guided Program Repair in the Era of Large Language Models (2025)\n",
      "- Dockerfile Flakiness: Characterization and Repair (2025)\n",
      "- RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code (2025)\n",
      "- Aligning the Objective of LLM-Based Program Repair (2025)\n",
      "- SpecRover: Code Intent Extraction via LLMs (2025)\n",
      "- The Fact Selection Problem in LLM-Based Program Repair (2025)\n",
      "- RepairAgent: An Autonomous, LLM-Based Agent for Program Repair (2025)\n",
      "- Knowledge-Enhanced Program Repair for Data Science Code (2025)\n",
      "- FixDrive: Automatically Repairing Autonomous Vehicle Driving Behaviour for $0.08 per Violation (2025)\n",
      "- Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models (2025)\n",
      "- PyTy: Repairing Static Type Errors in Python (2024)\n",
      "- Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources (2024)\n",
      "- Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation (2024)\n",
      "- Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors (2024)\n",
      "- Recommending Root-Cause and Mitigation Steps for Cloud Incidents Using Large Language Models (2023)\n",
      "- Automated Repair of Programs from Large Language Models (2023)\n",
      "- Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training (2023)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- Automated Program Repair in the Era of Large Pre-Trained Language Models (2023)\n",
      "- Impact of Code Language Models on Automated Program Repair (2023)\n",
      "- CCTest: Testing and Repairing Code Completion Systems (2023)\n",
      "- Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning (2023)\n",
      "- Out of Context: How important is Local Context in Neural Program Repair? (2024)\n",
      "- Rete: Learning Namespace Representation for Program Repair (2023)\n",
      "\n",
      "SO post editing\n",
      "- Towards Better Answers: Automated Stack Overflow Post Updating (2025)\n",
      "\n",
      "code comprehension\n",
      "- LLM Assistance for Memory Safety (2025)\n",
      "- Reasoning Runtime Behavior of a Program with LLM: How Far Are We? (2025)\n",
      "- Iterative Generation of Adversarial Example for Deep Code Models (2025)\n",
      "- On Calibration of Pre-trained Code Models (2024)\n",
      "- GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code (2024)\n",
      "\n",
      "code translation\n",
      "- LLM Assistance for Memory Safety (2025)\n",
      "- InterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-Based Code Translation (2025)\n",
      "- Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code (2024)\n",
      "- On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study (2024)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models (2023)\n",
      "- ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning (2023)\n",
      "- Syntax and Domain Aware Model for Unsupervised Program Translation (2023)\n",
      "\n",
      "bug detection\n",
      "- Boosting Static Resource Leak Detection via LLM-Based Resource-Oriented Intention Inference (2025)\n",
      "- ClozeMaster: Fuzzing Rust Compiler by Harnessing LLMs for Infilling Masked Real Programs (2025)\n",
      "- Planning a Large Language Model for Static Detection of Runtime Errors in Code Snippets (2025)\n",
      "- BDefects4NN: A Backdoor Defect Database for Controlled Localization Studies in Neural Networks (2025)\n",
      "- On Calibration of Pre-trained Code Models (2024)\n",
      "- RepresentThemAll: A Universal Learning Representation of Bug Reports (2023)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation (2023)\n",
      "- On the Applicability of Language Models to Block-Based Programs (2023)\n",
      "- CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back (2023)\n",
      "- ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning (2023)\n",
      "- On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization (2024)\n",
      "\n",
      "security patch detection\n",
      "- Repository-Level Graph Representation Learning for Enhanced Security Patch Detection (2025)\n",
      "\n",
      "software model evolution\n",
      "- Software Model Evolution with Large Language Models: Experiments on Simulated, Public, and Industrial Datasets (2025)\n",
      "\n",
      "commit message generation\n",
      "- An Empirical Study on Commit Message Generation Using LLMs via In-Context Learning (2025)\n",
      "- Context Conquers Parameters: Outperforming Proprietary LLM in Commit Message Generation (2025)\n",
      "- CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back (2023)\n",
      "\n",
      "traceability link recovery\n",
      "- LiSSA: Toward Generic Traceability Link Recovery through Retrieval-Augmented Generation (2025)\n",
      "- Recovering Trace Links Between Software Documentation And Code (2024)\n",
      "\n",
      "log parsing\n",
      "- LibreLog: Accurate and Efficient Unsupervised Log Parsing Using Open-Source Large Language Models (2025)\n",
      "- Unleashing the True Potential of Semantic-Based Log Parsing with Pre-Trained Language Models (2025)\n",
      "- LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing (2024)\n",
      "- DivLog: Log Parsing with Prompt Enhanced In-Context Learning (2024)\n",
      "- Log Parsing with Prompt-Based Few-Shot Learning (2023)\n",
      "\n",
      "AI generated code detection\n",
      "- An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We? (2025)\n",
      "\n",
      "code summarisation\n",
      "- Model Editing for LLMs4Code: How Far are We? (2025)\n",
      "- Source Code Summarization in the Era of Large Language Models (2025)\n",
      "- Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization) (2024)\n",
      "- On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study (2024)\n",
      "- Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training (2023)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "\n",
      "program analysis\n",
      "- Large Language Models for Safe Minimization (2025)\n",
      "- Treefix: Enabling Execution with a Tree of Prefixes (2025)\n",
      "\n",
      "regression testing\n",
      "- Ranking Relevant Tests for Order-Dependent Flaky Tests (2025)\n",
      "\n",
      "code search\n",
      "- Towards More Trustworthy Deep Code Models by Enabling Out-of-Distribution Detection (2025)\n",
      "- Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming (2025)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- CoCoSoDa: Effective Contrastive Learning for Code Search (2023)\n",
      "- ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning (2023)\n",
      "\n",
      "fuzzing\n",
      "- Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models (2025)\n",
      "- Fuzz4All: Universal Fuzzing with Large Language Models (2024)\n",
      "- Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer (2024)\n",
      "- Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries (2024)\n",
      "\n",
      "inconsistency prediction in decentralised apps\n",
      "- Hyperion: Unveiling DApp Inconsistencies Using LLM and Dataflow-Guided Symbolic Execution (2025)\n",
      "\n",
      "formal verification\n",
      "- Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification (2025)\n",
      "- SpecGen: Automated Generation of Formal Program Specifications via Large Language Models (2025)\n",
      "\n",
      "bug report comprehension\n",
      "- ChatGPT Inaccuracy Mitigation during Technical Report Understanding: Are We There Yet? (2025)\n",
      "\n",
      "UI design repair\n",
      "- DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models (2025)\n",
      "\n",
      "type detection\n",
      "- Neurosymbolic Modular Refinement Type Inference (2025)\n",
      "- TIGER: A Generating-Then-Ranking Framework for Practical Python Type Inference (2025)\n",
      "- On Calibration of Pre-trained Code Models (2024)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models (2023)\n",
      "\n",
      "smart contract auditing\n",
      "- Combining Fine-Tuning and LLM-Based Agents for Intuitive Smart Contract Auditing with Justifications (2025)\n",
      "\n",
      "configuration validation\n",
      "- Large Language Models as Configuration Validators (2025)\n",
      "\n",
      "detection of code design issues\n",
      "- An LLM-Based Agent-Oriented Approach for Automated Code Design Issue Localization (2025)\n",
      "\n",
      "code completion\n",
      "- RLCoder: Reinforcement Learning for Repository-Level Code Completion (2025)\n",
      "- When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference (2024)\n",
      "- Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training (2023)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- On the Applicability of Language Models to Block-Based Programs (2023)\n",
      "- Language Models for Code Completion: A Practical Evaluation (2024)\n",
      "\n",
      "clone detection\n",
      "- Metamorphic-Based Many-Objective Distillation of LLMs for Code-Related Tasks (2025)\n",
      "- On Calibration of Pre-trained Code Models (2024)\n",
      "- TRACED: Execution-aware Pre-training for Source Code (2024)\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning (2023)\n",
      "- A Multiple Representation Transformer with Optimized Abstract Syntax Tree for Efficient Code Clone Detection (2025)\n",
      "\n",
      "code adaptation\n",
      "- Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation through Prompt Engineering (2025)\n",
      "\n",
      "fault localisation\n",
      "- BDefects4NN: A Backdoor Defect Database for Controlled Localization Studies in Neural Networks (2025)\n",
      "- Large Language Models for Test-Free Fault Localization (2024)\n",
      "- Fonte: Finding Bug Inducing Commits from Failures (2023)\n",
      "\n",
      "code optimisation\n",
      "- Search-Based LLMs for Code Optimization (2025)\n",
      "\n",
      "security injection\n",
      "- Prompt-to-SQL Injections in LLM-Integrated Web Applications: Risks and Defenses (2025)\n",
      "\n",
      "machine-generated code detection\n",
      "- Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers (2025)\n",
      "\n",
      "root cause analysis\n",
      "- COCA: Generative Root Cause Analysis for Distributed Systems with Code Knowledge (2025)\n",
      "\n",
      "API misuse detection\n",
      "- Demystifying and Detecting Misuses of Deep Learning APIs (2024)\n",
      "\n",
      "binary software composition analysis\n",
      "- BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching (2024)\n",
      "\n",
      "code idioms detection\n",
      "- Streamlining Java Programming: Uncovering Well-Formed Idioms with IdioMine (2024)\n",
      "\n",
      "privacy inconsistencies detection\n",
      "- Are Your Requests Your True Needs? Checking Excessive Data Collection in VPA App (2024)\n",
      "\n",
      "figurative language detection\n",
      "- Shedding Light on Software Engineering-specific Metaphors and Idioms (2024)\n",
      "\n",
      "emotion-cause extraction\n",
      "- Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs (2024)\n",
      "\n",
      "program comprehension\n",
      "- An Empirical Study on Noisy Label Learning for Program Understanding (2024)\n",
      "\n",
      "exception handling recommender\n",
      "- Programming Assistant for Exception Handling with CodeBERT (2024)\n",
      "\n",
      "static analysis\n",
      "- TRACED: Execution-aware Pre-training for Source Code (2024)\n",
      "\n",
      "comment generation\n",
      "- Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning (2024)\n",
      "\n",
      "bug reproduction\n",
      "- Prompting Is All You Need: Automated Android Bug Replay with Large Language Models (2024)\n",
      "- CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace (2024)\n",
      "- Context-Aware Bug Reproduction for Mobile Apps (2023)\n",
      "\n",
      "code review\n",
      "- Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study (2024)\n",
      "\n",
      "log understanding\n",
      "- KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding (2024)\n",
      "\n",
      "CI/CD workflow generation\n",
      "- Toward Automatically Completing GitHub Workflows (2024)\n",
      "\n",
      "log generation\n",
      "- UniLog: Automatic Logging via LLM and In-Context Learning (2024)\n",
      "\n",
      "model extraction\n",
      "- Practical and Efficient Model Extraction of Sentiment Analysis APIs (2023)\n",
      "\n",
      "requirements analysis\n",
      "- AI-Based Question Answering Assistance for Analyzing Natural-Language Requirements (2023)\n",
      "\n",
      "code retrieval\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "- FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations (2024)\n",
      "\n",
      "question answering\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "\n",
      "mutant generation\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "\n",
      "assert generation\n",
      "- An Empirical Comparison of Pre-Trained Models of Source Code (2023)\n",
      "\n",
      "SO posts summarisation\n",
      "- Automated Summarization of Stack Overflow Posts (2023)\n",
      "\n",
      "patch correctness assessment\n",
      "- CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back (2023)\n",
      "\n",
      "algorithm classification\n",
      "- FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations (2024)\n",
      "\n",
      "heterogeneous device mapping\n",
      "- FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations (2024)\n",
      "\n",
      "optimal thread coarsening factor\n",
      "- FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations (2024)\n",
      "\n",
      "vulnerability alert prediction\n",
      "- Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation (2023)\n",
      "\n",
      "vulnerability repair\n",
      "- CoLeFunDa: Explainable Silent Vulnerability Fix Identification (2023)\n",
      "\n",
      "GUI test case migration\n",
      "- Learning-based Widget Matching for Migrating GUI Test Cases (2024)\n",
      "\n",
      "software effort estimation\n",
      "- Fine-SE: Integrating Semantic Features and Expert Features for Software Effort Estimation (2024)\n",
      "\n",
      "code memorisation detection\n",
      "- Decoding Secret Memorization in Code LLMs Through Token-Level Characterization (2025)\n",
      "- Traces of Memorisation in Large Language Models for Code (2024)\n",
      "- Unveiling Memorization in Code Models (2024)\n"
     ]
    }
   ],
   "source": [
    "# print each paper per task\n",
    "for task in df_relevant['task_list'].explode().unique():\n",
    "    if pd.isna(task):\n",
    "        continue\n",
    "    print(f\"\\n{task}\")\n",
    "    papers_for_task = df_relevant[df_relevant['task_list'].apply(lambda tasks: task in tasks)]\n",
    "    for _, row in papers_for_task.iterrows():\n",
    "        print(f\"- {row['title']} ({row['year']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce60bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of tasks per paper:\n",
      "num_tasks\n",
      "1     157\n",
      "2      11\n",
      "3       5\n",
      "4       3\n",
      "13      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple tasks: 20\n",
      "Percentage covering multiple tasks: 11.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at how many papers cover multiple tasks\n",
    "df_relevant['num_tasks'] = df_relevant['task_list'].apply(len)\n",
    "\n",
    "print(\"Distribution of number of tasks per paper:\")\n",
    "print(df_relevant['num_tasks'].value_counts().sort_index())\n",
    "print(f\"\\nPapers covering multiple tasks: {(df_relevant['num_tasks'] > 1).sum()}\")\n",
    "print(f\"Percentage covering multiple tasks: {(df_relevant['num_tasks'] > 1).mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef964a5",
   "metadata": {},
   "source": [
    "## Comparison to Non-LLM Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9992a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_llm_approaches_bool\n",
      "True     114\n",
      "False     63\n",
      "Name: count, dtype: int64\n",
      "Overall: Non-LLM techniques in 114 out of 177 papers (64.4%)\n"
     ]
    }
   ],
   "source": [
    "# Consolidating to just Boolean values\n",
    "df_relevant['non_llm_approaches_bool'] = df_relevant['non_llm_approaches'].apply(lambda x: 'true' in str(x).strip().lower())\n",
    "print(df_relevant['non_llm_approaches_bool'].value_counts())\n",
    "number_non_llm_present = df_relevant['non_llm_approaches_bool'].sum()\n",
    "total = df_relevant.shape[0]\n",
    "\n",
    "print(f\"Overall: Non-LLM techniques in {number_non_llm_present} out of {total} papers ({(number_non_llm_present/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17017377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Non-LLM techniques in 27 out of 32 papers (84.4%)\n",
      "2024: Non-LLM techniques in 36 out of 55 papers (65.5%)\n",
      "2025: Non-LLM techniques in 51 out of 90 papers (56.7%)\n"
     ]
    }
   ],
   "source": [
    "number_non_llm_present_2023 = df_relevant[df_relevant['year'] == 2023]['non_llm_approaches_bool'].sum()\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "print(f\"2023: Non-LLM techniques in {number_non_llm_present_2023} out of {total_2023} papers ({(number_non_llm_present_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "number_non_llm_present_2024 = df_relevant[df_relevant['year'] == 2024]['non_llm_approaches_bool'].sum()\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "print(f\"2024: Non-LLM techniques in {number_non_llm_present_2024} out of {total_2024} papers ({(number_non_llm_present_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "number_non_llm_present_2025 = df_relevant[df_relevant['year'] == 2025]['non_llm_approaches_bool'].sum()\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "print(f\"2025: Non-LLM techniques in {number_non_llm_present_2025} out of {total_2025} papers ({(number_non_llm_present_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3154521b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer</th>\n",
       "      <th>relevant</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>artifact_available</th>\n",
       "      <th>artifact_reusable</th>\n",
       "      <th>artifact_functional</th>\n",
       "      <th>ai</th>\n",
       "      <th>task</th>\n",
       "      <th>non_llm_approaches</th>\n",
       "      <th>models_open_closed</th>\n",
       "      <th>num_models</th>\n",
       "      <th>model_families</th>\n",
       "      <th>model_scale</th>\n",
       "      <th>model_size_free_text</th>\n",
       "      <th>model_sizes_reported</th>\n",
       "      <th>model_config</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>programming_language</th>\n",
       "      <th>cost</th>\n",
       "      <th>cost_free_text</th>\n",
       "      <th>artefact_manual</th>\n",
       "      <th>contamination</th>\n",
       "      <th>contamination_free_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>task_list</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>num_tasks</th>\n",
       "      <th>non_llm_approaches_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>DW</td>\n",
       "      <td>True</td>\n",
       "      <td>2025</td>\n",
       "      <td>Decoding Secret Memorization in Code LLMs Thro...</td>\n",
       "      <td>Nie, Yuqing, Wang, Chong, Wang, Kailong, Xu, G...</td>\n",
       "      <td>https://doi.org/10.1109/ICSE55347.2025.00229</td>\n",
       "      <td>Code Large Language Models (LLMs) have demonst...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>5</td>\n",
       "      <td>StableCode; CodeGen; DeepSeekCoder; CodeLlama;...</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>StableCode-3B,3B;CodeGen2.5-7B-multi,7B;DeepSe...</td>\n",
       "      <td>full</td>\n",
       "      <td>inference</td>\n",
       "      <td>code</td>\n",
       "      <td>HTML; Java; JavaScript; Python</td>\n",
       "      <td>gpu</td>\n",
       "      <td>All the experiments are conducted on a server ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Training Data Decontamination. Data cleaning s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>[emerging]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>DW</td>\n",
       "      <td>True</td>\n",
       "      <td>2024</td>\n",
       "      <td>Traces of Memorisation in Large Language Model...</td>\n",
       "      <td>Al-Kaswan, Ali, Izadi, Maliheh, van Deursen, Arie</td>\n",
       "      <td>https://doi.org/10.1145/3597503.3639133</td>\n",
       "      <td>Large language models have gained significant ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>37</td>\n",
       "      <td>GPT-NEO;GPT-2;Pythia;CodeGen-NL;CodeGen-Mono;C...</td>\n",
       "      <td>SMALL/MEDIUM</td>\n",
       "      <td>GPT-NEO-125M,GPT-NEO-1.3B,GPT-NEO-2.7B,GPT-2-1...</td>\n",
       "      <td>full</td>\n",
       "      <td>inference</td>\n",
       "      <td>code, text</td>\n",
       "      <td>Python</td>\n",
       "      <td>gpu</td>\n",
       "      <td>we allocated 8 CPU cores with 32GB of RAM and ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>The paper is about contamination/memorisation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>[emerging]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>MK</td>\n",
       "      <td>True</td>\n",
       "      <td>2024</td>\n",
       "      <td>Unveiling Memorization in Code Models</td>\n",
       "      <td>Yang, Zhou, Zhao, Zhipeng, Wang, Chenyu, Shi, ...</td>\n",
       "      <td>https://doi.org/10.1145/3597503.3639074</td>\n",
       "      <td>The availability of large-scale datasets, adva...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>code memorisation detection</td>\n",
       "      <td>False</td>\n",
       "      <td>open</td>\n",
       "      <td>6</td>\n",
       "      <td>CodeParrot; PolyCoder; GPT-NEO; InCoder; StarC...</td>\n",
       "      <td>SMALL/MEDIUM</td>\n",
       "      <td>CodeParrot is a GPT-2 model with 1.5 billion p...</td>\n",
       "      <td>some</td>\n",
       "      <td>inference</td>\n",
       "      <td>code, documentation (text), configuration (tex...</td>\n",
       "      <td>Python</td>\n",
       "      <td>time;gpu</td>\n",
       "      <td>... we run them an NVIDIA GeForce\\nA5000 GPU w...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>To mitigate this threat, we choose a state-of-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[code memorisation detection]</td>\n",
       "      <td>[emerging]</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reviewer  relevant  year  \\\n",
       "171       DW      True  2025   \n",
       "172       DW      True  2024   \n",
       "173       MK      True  2024   \n",
       "\n",
       "                                                 title  \\\n",
       "171  Decoding Secret Memorization in Code LLMs Thro...   \n",
       "172  Traces of Memorisation in Large Language Model...   \n",
       "173              Unveiling Memorization in Code Models   \n",
       "\n",
       "                                               authors  \\\n",
       "171  Nie, Yuqing, Wang, Chong, Wang, Kailong, Xu, G...   \n",
       "172  Al-Kaswan, Ali, Izadi, Maliheh, van Deursen, Arie   \n",
       "173  Yang, Zhou, Zhao, Zhipeng, Wang, Chenyu, Shi, ...   \n",
       "\n",
       "                                              url  \\\n",
       "171  https://doi.org/10.1109/ICSE55347.2025.00229   \n",
       "172       https://doi.org/10.1145/3597503.3639133   \n",
       "173       https://doi.org/10.1145/3597503.3639074   \n",
       "\n",
       "                                              abstract  artifact_available  \\\n",
       "171  Code Large Language Models (LLMs) have demonst...               False   \n",
       "172  Large language models have gained significant ...               False   \n",
       "173  The availability of large-scale datasets, adva...               False   \n",
       "\n",
       "     artifact_reusable  artifact_functional    ai  \\\n",
       "171              False                False  True   \n",
       "172              False                False  True   \n",
       "173              False                False  True   \n",
       "\n",
       "                            task non_llm_approaches models_open_closed  \\\n",
       "171  code memorisation detection              False               open   \n",
       "172  code memorisation detection              False               open   \n",
       "173  code memorisation detection              False               open   \n",
       "\n",
       "     num_models                                     model_families  \\\n",
       "171           5  StableCode; CodeGen; DeepSeekCoder; CodeLlama;...   \n",
       "172          37  GPT-NEO;GPT-2;Pythia;CodeGen-NL;CodeGen-Mono;C...   \n",
       "173           6  CodeParrot; PolyCoder; GPT-NEO; InCoder; StarC...   \n",
       "\n",
       "      model_scale                               model_size_free_text  \\\n",
       "171        MEDIUM  StableCode-3B,3B;CodeGen2.5-7B-multi,7B;DeepSe...   \n",
       "172  SMALL/MEDIUM  GPT-NEO-125M,GPT-NEO-1.3B,GPT-NEO-2.7B,GPT-2-1...   \n",
       "173  SMALL/MEDIUM  CodeParrot is a GPT-2 model with 1.5 billion p...   \n",
       "\n",
       "    model_sizes_reported model_config  \\\n",
       "171                 full    inference   \n",
       "172                 full    inference   \n",
       "173                 some    inference   \n",
       "\n",
       "                                          dataset_type  \\\n",
       "171                                               code   \n",
       "172                                         code, text   \n",
       "173  code, documentation (text), configuration (tex...   \n",
       "\n",
       "               programming_language      cost  \\\n",
       "171  HTML; Java; JavaScript; Python       gpu   \n",
       "172                          Python       gpu   \n",
       "173                          Python  time;gpu   \n",
       "\n",
       "                                        cost_free_text artefact_manual  \\\n",
       "171  All the experiments are conducted on a server ...            True   \n",
       "172  we allocated 8 CPU cores with 32GB of RAM and ...            True   \n",
       "173  ... we run them an NVIDIA GeForce\\nA5000 GPU w...            True   \n",
       "\n",
       "     contamination                            contamination_free_text  topic  \\\n",
       "171           True  Training Data Decontamination. Data cleaning s...    NaN   \n",
       "172           True      The paper is about contamination/memorisation    NaN   \n",
       "173           True  To mitigate this threat, we choose a state-of-...    NaN   \n",
       "\n",
       "                         task_list  topic_list  num_tasks  \\\n",
       "171  [code memorisation detection]  [emerging]          1   \n",
       "172  [code memorisation detection]  [emerging]          1   \n",
       "173  [code memorisation detection]  [emerging]          1   \n",
       "\n",
       "     non_llm_approaches_bool  \n",
       "171                    False  \n",
       "172                    False  \n",
       "173                    False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['task_list'].apply(lambda x: 'code memorisation detection' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832329f",
   "metadata": {},
   "source": [
    "## Programming Languages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "778900ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Programming Language Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "programming_languages_list\n",
       "Java           67\n",
       "Python         56\n",
       "C              30\n",
       "C++            23\n",
       "JavaScript     13\n",
       "PHP            10\n",
       "Go              9\n",
       "NM              8\n",
       "Rust            7\n",
       "Ruby            7\n",
       "C#              5\n",
       "Kotlin          3\n",
       "SQL             3\n",
       "R               2\n",
       "TypeScript      2\n",
       "Haskell         2\n",
       "Objective-C     2\n",
       "Scala           2\n",
       "Swift           2\n",
       "Prolog          1\n",
       "Erlang          1\n",
       "Solidity        1\n",
       "Bash            1\n",
       "CSharp          1\n",
       "Perl            1\n",
       "SCRATCH         1\n",
       "HTML            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[\"programming_languages_list\"] = df_relevant['programming_language'].apply(\n",
    "    lambda x: [lang.strip() for lang in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(\"Overall Programming Language Stats:\")\n",
    "df_relevant[\"programming_languages_list\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7652b1c",
   "metadata": {},
   "source": [
    "### Top 5 Programming Languages Per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2b51ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Programming Language Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "programming_languages_list\n",
       "Java          14\n",
       "Python         9\n",
       "C              7\n",
       "JavaScript     4\n",
       "PHP            4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2023 Programming Language Stats:\")\n",
    "df_relevant[df_relevant['year'] == 2023][\"programming_languages_list\"].explode().value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae5a6a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 Programming Language Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "programming_languages_list\n",
       "Java      22\n",
       "Python    18\n",
       "C         11\n",
       "C++        9\n",
       "Go         4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2024 Programming Language Stats:\")\n",
    "df_relevant[df_relevant['year'] == 2024][\"programming_languages_list\"].explode().value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1238f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 Programming Language Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "programming_languages_list\n",
       "Java          31\n",
       "Python        29\n",
       "C             12\n",
       "C++           12\n",
       "JavaScript     6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2025 Programming Language Stats:\")\n",
    "df_relevant[df_relevant['year'] == 2025][\"programming_languages_list\"].explode().value_counts().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def5e47",
   "metadata": {},
   "source": [
    "### Papers that Evaluate with Multiple Programming Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abccefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Programming Language Stats:\n",
      "Distribution of number of programming languages per paper:\n",
      "programming_languages_list\n",
      "1     81\n",
      "0     48\n",
      "2     24\n",
      "3     10\n",
      "6      4\n",
      "4      3\n",
      "10     2\n",
      "5      2\n",
      "16     1\n",
      "7      1\n",
      "13     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple programming languages: 48\n",
      "Percentage covering multiple programming languages: 27.1%\n"
     ]
    }
   ],
   "source": [
    "def programming_language_distribution_and_multi_language_stats(df):\n",
    "    print(\"Distribution of number of programming languages per paper:\")\n",
    "    print(df[\"programming_languages_list\"].apply(len).value_counts())\n",
    "    print(f\"\\nPapers covering multiple programming languages: {(df['programming_languages_list'].apply(len) > 1).sum()}\")\n",
    "    print(f\"Percentage covering multiple programming languages: {(df['programming_languages_list'].apply(len) > 1).mean()*100:.1f}%\")\n",
    "\n",
    "print(\"Overall Programming Language Stats:\")\n",
    "programming_language_distribution_and_multi_language_stats(df_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731ab2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023 Programming Language Stats:\n",
      "Distribution of number of programming languages per paper:\n",
      "programming_languages_list\n",
      "1    13\n",
      "0     9\n",
      "2     5\n",
      "4     1\n",
      "6     1\n",
      "7     1\n",
      "5     1\n",
      "3     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple programming languages: 10\n",
      "Percentage covering multiple programming languages: 31.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2023 Programming Language Stats:\")\n",
    "programming_language_distribution_and_multi_language_stats(df_relevant[df_relevant['year'] == 2023])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5d5a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024 Programming Language Stats:\n",
      "Distribution of number of programming languages per paper:\n",
      "programming_languages_list\n",
      "1     22\n",
      "0     15\n",
      "2     10\n",
      "3      3\n",
      "5      1\n",
      "6      1\n",
      "4      1\n",
      "16     1\n",
      "13     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple programming languages: 18\n",
      "Percentage covering multiple programming languages: 32.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2024 Programming Language Stats:\")\n",
    "programming_language_distribution_and_multi_language_stats(df_relevant[df_relevant['year'] == 2024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6036c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025 Programming Language Stats:\n",
      "Distribution of number of programming languages per paper:\n",
      "programming_languages_list\n",
      "1     46\n",
      "0     24\n",
      "2      9\n",
      "3      6\n",
      "6      2\n",
      "10     2\n",
      "4      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple programming languages: 20\n",
      "Percentage covering multiple programming languages: 22.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2025 Programming Language Stats:\")\n",
    "programming_language_distribution_and_multi_language_stats(df_relevant[df_relevant['year'] == 2025])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b40c9",
   "metadata": {},
   "source": [
    "# RQ2 - What models are being used?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'models_open_closed' (open/closed/both)\n",
    "- 'num_models' (int)\n",
    "- 'model_families' (list of short text)\n",
    "- 'model_sizes_reported' (NA/none/some/full - currently unfinished)\n",
    "- 'model_scale' (currently unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07775a87",
   "metadata": {},
   "source": [
    "### Open vs. Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27b5c9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models_open_closed\n",
       "open      71\n",
       "both      65\n",
       "closed    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant['models_open_closed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80376d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open models in 136 out of 177 papers (76.8%)\n",
      "Closed models in 106 out of 177 papers (59.9%)\n"
     ]
    }
   ],
   "source": [
    "total = df_relevant.shape[0]\n",
    "number_open = df_relevant[~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_closed = df_relevant[~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "\n",
    "print(f\"Open models in {number_open} out of {total} papers ({(number_open/total)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed} out of {total} papers ({(number_closed/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda04f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Papers:\n",
      "Only open models in 22 out of 32 papers (68.8%)\n",
      "Only closed models in 4 out of 32 papers (12.5%)\n",
      "Open models in 28 out of 32 papers (87.5%)\n",
      "Closed models in 10 out of 32 papers (31.2%)\n",
      "Both model types in 6 out of 32 papers (18.8%)\n",
      "\n",
      "2024 Papers:\n",
      "Only open models in 25 out of 55 papers (45.5%)\n",
      "Only closed models in 13 out of 55 papers (23.6%)\n",
      "Open models in 42 out of 55 papers (76.4%)\n",
      "Closed models in 30 out of 55 papers (54.5%)\n",
      "Both model types in 17 out of 55 papers (30.9%)\n",
      "\n",
      "2025 Papers:\n",
      "Only open models in 24 out of 90 papers (26.7%)\n",
      "Only closed models in 24 out of 90 papers (26.7%)\n",
      "Open models in 66 out of 90 papers (73.3%)\n",
      "Closed models in 66 out of 90 papers (73.3%)\n",
      "Both model types in 42 out of 90 papers (46.7%)\n",
      "Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\n"
     ]
    }
   ],
   "source": [
    "total_2023 = df_relevant[df_relevant[\"year\"] == 2023].shape[0]\n",
    "number_open_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"2023 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2023} out of {total_2023} papers ({(number_open_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2023} out of {total_2023} papers ({(number_closed_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2023} out of {total_2023} papers ({(number_open_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2023} out of {total_2023} papers ({(number_closed_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2023} out of {total_2023} papers ({(number_both_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant[\"year\"] == 2024].shape[0]\n",
    "number_open_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2024 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2024} out of {total_2024} papers ({(number_open_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2024} out of {total_2024} papers ({(number_closed_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2024} out of {total_2024} papers ({(number_open_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2024} out of {total_2024} papers ({(number_closed_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2024} out of {total_2024} papers ({(number_both_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant[\"year\"] == 2025].shape[0]\n",
    "number_open_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2025 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2025} out of {total_2025} papers ({(number_open_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2025} out of {total_2025} papers ({(number_closed_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2025} out of {total_2025} papers ({(number_open_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2025} out of {total_2025} papers ({(number_closed_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2025} out of {total_2025} papers ({(number_both_2025/total_2025)*100:.1f}%)\")\n",
    "\n",
    "print(\"Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ba1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with X models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_models\n",
       "1     48\n",
       "3     24\n",
       "2     23\n",
       "5     17\n",
       "4     17\n",
       "6     13\n",
       "7      9\n",
       "8      7\n",
       "9      5\n",
       "10     4\n",
       "11     3\n",
       "16     2\n",
       "19     2\n",
       "15     1\n",
       "12     1\n",
       "37     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of papers with X models:\")\n",
    "df_relevant['num_models'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5e9b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Paper counts per model family:\n",
      "model_families_list\n",
      "GPT-4                           47\n",
      "GPT-3.5                         44\n",
      "CodeBERT                        34\n",
      "CodeLlama                       26\n",
      "CodeT5                          22\n",
      "CodeGen                         19\n",
      "StarCoder                       18\n",
      "GraphCodeBERT                   18\n",
      "Llama                           17\n",
      "RoBERTa                         15\n",
      "ChatGPT                         14\n",
      "BERT                            13\n",
      "DeepSeekCoder                   12\n",
      "UniXcoder                       11\n",
      "Codex                           10\n",
      "InCoder                          9\n",
      "T5                               8\n",
      "Claude                           7\n",
      "Gemini                           7\n",
      "ChatGLM                          6\n",
      "DeepSeek                         6\n",
      "CodeQwen                         6\n",
      "GPT-3                            6\n",
      "UnixCoder                        6\n",
      "PLBART                           6\n",
      "CodeGPT                          5\n",
      "CodeParrot                       4\n",
      "Copilot                          4\n",
      "WizardCoder                      4\n",
      "GPT-2                            4\n",
      "PolyCoder                        4\n",
      "Mistral                          3\n",
      "Gemma                            3\n",
      "text-davinci                     3\n",
      "Pythia                           3\n",
      "Vicuna                           3\n",
      "DistilBERT                       3\n",
      "LineVul                          3\n",
      "CodeT5+                          3\n",
      "text-embedding                   3\n",
      "TransCoder                       2\n",
      "Codestral                        2\n",
      "Qwen                             2\n",
      "GPT3.5                           2\n",
      "CodeGemma                        2\n",
      "Incoder                          2\n",
      "CodeGeeX                         2\n",
      "VulBERTa                         2\n",
      "CuBERT                           2\n",
      "UniLog                           2\n",
      "BART                             2\n",
      "GPT-J                            2\n",
      "GPT-Neo                          2\n",
      "GPT-NEO                          2\n",
      "GPT-C                            2\n",
      "SynCoBERT                        2\n",
      "CoTexT                           2\n",
      "Phi                              2\n",
      "TFix                             2\n",
      "ALBERT                           2\n",
      "seBERT                           2\n",
      "Code-davinci                     2\n",
      "OpenDevin                        2\n",
      "SantaCoder                       2\n",
      "Longformer                       1\n",
      "CoditT5                          1\n",
      "Unixcoder                        1\n",
      "CodeBert                         1\n",
      "GPT4                             1\n",
      "SVulD                            1\n",
      "Poro                             1\n",
      "ChatDev                          1\n",
      "Self-collaboration               1\n",
      "MetaGPT                          1\n",
      "AutoGPT                          1\n",
      "Multi-Turn Program Synthesis     1\n",
      "AgentCoder                       1\n",
      "DetectGPT                        1\n",
      "GPT-2 Output Detector            1\n",
      "GPTZero                          1\n",
      "GPTSniffer                       1\n",
      "Starcoder                        1\n",
      "LLM-Parser                       1\n",
      "LILAC                            1\n",
      "Repilot                          1\n",
      "RAP-Gen                          1\n",
      "ChatRepair                       1\n",
      "FitRepair                        1\n",
      "AlphaRe-pair                     1\n",
      "Mixtral                          1\n",
      "VGX                              1\n",
      "ReVeal                           1\n",
      "Devign                           1\n",
      "VULGEN                           1\n",
      "COME                             1\n",
      "CCT5                             1\n",
      "NNGen                            1\n",
      "ALL-MINILM-L6-V210               1\n",
      "UniTrans                         1\n",
      "Shipwright                       1\n",
      "MagiCoder                        1\n",
      "Magicoder                        1\n",
      "Parfum                           1\n",
      "TOGA                             1\n",
      "AthenTest                        1\n",
      "SEQ Graph& HYBRID                1\n",
      "AppMap Naive                     1\n",
      "OpenCodeInterpreter              1\n",
      "AutoCodeRover                    1\n",
      "CodeShell                        1\n",
      "LLama                            1\n",
      "CoCoSoDa                         1\n",
      "CodeRetriever                    1\n",
      "HedgeCode                        1\n",
      "SYNCOBERT                        1\n",
      "Vercel                           1\n",
      "GPT-4, CodeBERT                  1\n",
      "Moatless Tools                   1\n",
      "Agentless                        1\n",
      "FuzzGPT                          1\n",
      "TitanFuzz                        1\n",
      "Aider                            1\n",
      "SWE-Agent                        1\n",
      "DeBERTa                          1\n",
      "OPT                              1\n",
      "GPT3-5                           1\n",
      "Tulu                             1\n",
      "Guanaco                          1\n",
      "PaLM                             1\n",
      "StarChat                         1\n",
      "CAT-LM                           1\n",
      "Exlong                           1\n",
      "GLTR                             1\n",
      "ContraBERT                       1\n",
      "ChatUniTest                      1\n",
      "TestGen-LLM                      1\n",
      "RustAssistant                    1\n",
      "Sonnet                           1\n",
      "Stable-Code                      1\n",
      "BigBird                          1\n",
      "Sapling                          1\n",
      "KeyBERT                          1\n",
      "DISCO                            1\n",
      "PDBERT                           1\n",
      "GPTBigCode                       1\n",
      "Sentence-BERT                    1\n",
      "Airboros                         1\n",
      "CodeBERTa                        1\n",
      "Flan                             1\n",
      "code-davinci                     1\n",
      "UnifiedQA                        1\n",
      "VRepair                          1\n",
      "CodeReviewer                     1\n",
      "GrammarT5                        1\n",
      "Transformer                      1\n",
      "LSTM                             1\n",
      "GIN                              1\n",
      "TypeFix                          1\n",
      "PyTER                            1\n",
      "CoCoNuT                          1\n",
      "AlphaRepair                      1\n",
      "LANCE                            1\n",
      "XLNet                            1\n",
      "RepresentThemAll                 1\n",
      "Curie                            1\n",
      "Davinci                          1\n",
      "ELECTRA                          1\n",
      "MiniLM                           1\n",
      "DOBF                             1\n",
      "VulRepair                        1\n",
      "VulMaster                        1\n",
      "PanguCoder                       1\n",
      "flan-alpaca                      1\n",
      "SPT-Code                         1\n",
      "ProphetNet-Code                  1\n",
      "T5-learning                      1\n",
      "JavaBERT                         1\n",
      "DeepDebug                        1\n",
      "C-BERT                           1\n",
      "CugLM                            1\n",
      "TreeBERT                         1\n",
      "PLBart                           1\n",
      "ATLAS                            1\n",
      "CoCoNut                          1\n",
      "Hoppity                          1\n",
      "Sequencer                        1\n",
      "CEDAR                            1\n",
      "Transformers                     1\n",
      "GPT-NeoX                         1\n",
      "FAIR                             1\n",
      "OSCAR                            1\n",
      "Transcoder*                      1\n",
      "IRGen                            1\n",
      "Deep-SE                          1\n",
      "GPT2SP                           1\n",
      "sentenceBERT                     1\n",
      "SDA-Trans                        1\n",
      "StableCode                       1\n",
      "CodeGen-NL                       1\n",
      "CodeGen-Mono                     1\n",
      "CodeGen-Multi                    1\n",
      "CodeGen2                         1\n",
      "PyCodeGPT                        1\n",
      "GPT-Code-Clippy                  1\n",
      "BERTOverflow                     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['model_families_list'] = df_relevant['model_families'].apply(\n",
    "    lambda x: [model_family.strip() for model_family in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(\"Overall Paper counts per model family:\")\n",
    "print(df_relevant['model_families_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7389b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "CodeBERT            11\n",
       "RoBERTa              8\n",
       "BERT                 7\n",
       "CodeT5               7\n",
       "Codex                5\n",
       "T5                   5\n",
       "GraphCodeBERT        4\n",
       "DistilBERT           3\n",
       "PLBART               2\n",
       "GPT-J                2\n",
       "CodeGen              2\n",
       "BART                 2\n",
       "InCoder              2\n",
       "GPT-Neo              2\n",
       "XLNet                1\n",
       "MiniLM               1\n",
       "ELECTRA              1\n",
       "ALBERT               1\n",
       "RepresentThemAll     1\n",
       "seBERT               1\n",
       "Code-davinci         1\n",
       "Curie                1\n",
       "Davinci              1\n",
       "T5-learning          1\n",
       "CodeGPT              1\n",
       "JavaBERT             1\n",
       "DOBF                 1\n",
       "CuBERT               1\n",
       "ProphetNet-Code      1\n",
       "SPT-Code             1\n",
       "CoTexT               1\n",
       "C-BERT               1\n",
       "GPT-C                1\n",
       "CugLM                1\n",
       "TreeBERT             1\n",
       "GPT-2                1\n",
       "SynCoBERT            1\n",
       "DeepDebug            1\n",
       "UniXcoder            1\n",
       "GPT-NeoX             1\n",
       "PLBart               1\n",
       "GPT-3                1\n",
       "CodeParrot           1\n",
       "Copilot              1\n",
       "ATLAS                1\n",
       "CoCoNut              1\n",
       "Hoppity              1\n",
       "Sequencer            1\n",
       "TFix                 1\n",
       "CEDAR                1\n",
       "Transformers         1\n",
       "TransCoder           1\n",
       "Transcoder*          1\n",
       "SDA-Trans            1\n",
       "BERTOverflow         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2023]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24cb0987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "CodeBERT           11\n",
       "GPT-3.5             9\n",
       "GPT-4               8\n",
       "CodeT5              8\n",
       "ChatGPT             8\n",
       "CodeGen             8\n",
       "GraphCodeBERT       7\n",
       "UniXcoder           5\n",
       "BERT                5\n",
       "Codex               4\n",
       "RoBERTa             4\n",
       "InCoder             4\n",
       "StarCoder           4\n",
       "Llama               3\n",
       "ChatGLM             3\n",
       "UnixCoder           3\n",
       "PLBART              3\n",
       "T5                  3\n",
       "GPT-3               3\n",
       "GPT-2               3\n",
       "text-davinci        3\n",
       "PolyCoder           3\n",
       "CodeGeeX            2\n",
       "CodeParrot          2\n",
       "UniLog              2\n",
       "GPT-NEO             2\n",
       "Copilot             2\n",
       "Vicuna              2\n",
       "Pythia              2\n",
       "CodeGPT             2\n",
       "Airboros            1\n",
       "code-davinci        1\n",
       "SantaCoder          1\n",
       "WizardCoder         1\n",
       "Sentence-BERT       1\n",
       "GPTBigCode          1\n",
       "seBERT              1\n",
       "VulBERTa            1\n",
       "PDBERT              1\n",
       "ALBERT              1\n",
       "KeyBERT             1\n",
       "LSTM                1\n",
       "TFix                1\n",
       "DISCO               1\n",
       "text-embedding      1\n",
       "Flan                1\n",
       "CodeBERTa           1\n",
       "SynCoBERT           1\n",
       "CodeLlama           1\n",
       "Transformer         1\n",
       "GIN                 1\n",
       "flan-alpaca         1\n",
       "UnifiedQA           1\n",
       "GrammarT5           1\n",
       "GPT-C               1\n",
       "VRepair             1\n",
       "CodeReviewer        1\n",
       "VulMaster           1\n",
       "VulRepair           1\n",
       "PanguCoder          1\n",
       "TypeFix             1\n",
       "CoTexT              1\n",
       "CodeT5+             1\n",
       "AlphaRepair         1\n",
       "PyTER               1\n",
       "CoCoNuT             1\n",
       "LANCE               1\n",
       "IRGen               1\n",
       "Deep-SE             1\n",
       "FAIR                1\n",
       "OSCAR               1\n",
       "sentenceBERT        1\n",
       "GPT2SP              1\n",
       "CodeGen-Mono        1\n",
       "CodeGen-NL          1\n",
       "CodeGen-Multi       1\n",
       "CodeGen2            1\n",
       "PyCodeGPT           1\n",
       "GPT-Code-Clippy     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2024]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a8ec5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "GPT-4                           39\n",
       "GPT-3.5                         35\n",
       "CodeLlama                       25\n",
       "Llama                           14\n",
       "StarCoder                       14\n",
       "CodeBERT                        12\n",
       "DeepSeekCoder                   12\n",
       "CodeGen                          9\n",
       "Claude                           7\n",
       "Gemini                           7\n",
       "CodeT5                           7\n",
       "GraphCodeBERT                    7\n",
       "ChatGPT                          6\n",
       "CodeQwen                         6\n",
       "DeepSeek                         6\n",
       "UniXcoder                        5\n",
       "UnixCoder                        3\n",
       "LineVul                          3\n",
       "WizardCoder                      3\n",
       "ChatGLM                          3\n",
       "Mistral                          3\n",
       "Gemma                            3\n",
       "RoBERTa                          3\n",
       "InCoder                          3\n",
       "Codestral                        2\n",
       "Qwen                             2\n",
       "CodeT5+                          2\n",
       "CodeGemma                        2\n",
       "GPT3.5                           2\n",
       "OpenDevin                        2\n",
       "CodeGPT                          2\n",
       "Incoder                          2\n",
       "text-embedding                   2\n",
       "Phi                              2\n",
       "GPT-3                            2\n",
       "AthenTest                        1\n",
       "ChatDev                          1\n",
       "SVulD                            1\n",
       "Code-davinci                     1\n",
       "Multi-Turn Program Synthesis     1\n",
       "GPT4                             1\n",
       "AgentCoder                       1\n",
       "Self-collaboration               1\n",
       "Copilot                          1\n",
       "AutoGPT                          1\n",
       "MetaGPT                          1\n",
       "Magicoder                        1\n",
       "CoditT5                          1\n",
       "Unixcoder                        1\n",
       "Longformer                       1\n",
       "Shipwright                       1\n",
       "MagiCoder                        1\n",
       "TransCoder                       1\n",
       "Parfum                           1\n",
       "UniTrans                         1\n",
       "SantaCoder                       1\n",
       "ALL-MINILM-L6-V210               1\n",
       "CodeBert                         1\n",
       "TOGA                             1\n",
       "Poro                             1\n",
       "BERT                             1\n",
       "SEQ Graph& HYBRID                1\n",
       "Devign                           1\n",
       "VULGEN                           1\n",
       "COME                             1\n",
       "CCT5                             1\n",
       "NNGen                            1\n",
       "ReVeal                           1\n",
       "Mixtral                          1\n",
       "VGX                              1\n",
       "LILAC                            1\n",
       "LLM-Parser                       1\n",
       "Starcoder                        1\n",
       "GPTSniffer                       1\n",
       "FitRepair                        1\n",
       "ChatRepair                       1\n",
       "RAP-Gen                          1\n",
       "AlphaRe-pair                     1\n",
       "GLTR                             1\n",
       "Sapling                          1\n",
       "OpenCodeInterpreter              1\n",
       "Stable-Code                      1\n",
       "BigBird                          1\n",
       "ContraBERT                       1\n",
       "CuBERT                           1\n",
       "VulBERTa                         1\n",
       "ChatUniTest                      1\n",
       "TestGen-LLM                      1\n",
       "RustAssistant                    1\n",
       "Sonnet                           1\n",
       "GPTZero                          1\n",
       "GPT-2 Output Detector            1\n",
       "DetectGPT                        1\n",
       "Repilot                          1\n",
       "Aider                            1\n",
       "AppMap Naive                     1\n",
       "AutoCodeRover                    1\n",
       "SWE-Agent                        1\n",
       "Vercel                           1\n",
       "Agentless                        1\n",
       "TitanFuzz                        1\n",
       "Moatless Tools                   1\n",
       "HedgeCode                        1\n",
       "GPT-4, CodeBERT                  1\n",
       "Codex                            1\n",
       "SYNCOBERT                        1\n",
       "CoCoSoDa                         1\n",
       "CodeRetriever                    1\n",
       "LLama                            1\n",
       "FuzzGPT                          1\n",
       "CodeShell                        1\n",
       "PLBART                           1\n",
       "Pythia                           1\n",
       "OPT                              1\n",
       "Exlong                           1\n",
       "DeBERTa                          1\n",
       "CodeParrot                       1\n",
       "CAT-LM                           1\n",
       "PolyCoder                        1\n",
       "StarChat                         1\n",
       "PaLM                             1\n",
       "Vicuna                           1\n",
       "Guanaco                          1\n",
       "Tulu                             1\n",
       "GPT3-5                           1\n",
       "StableCode                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2025]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94f1a0",
   "metadata": {},
   "source": [
    "# RQ3 - How well do authors tackle the problem of data leakage/contamination?\n",
    "\n",
    "DF Columns to use:\n",
    "- contamination (bool)\n",
    "- contamination_free_text (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "547d1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All years: Contamination reported in 58 out of 177 papers (32.8%)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "total = df_relevant.shape[0]\n",
    "contamination_reported = df_relevant['contamination'].sum()\n",
    "\n",
    "print(f\"All years: Contamination reported in {contamination_reported} out of {total} papers ({(contamination_reported/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb11e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Contamination reported in 6 out of 32 papers (18.8%)\n",
      "2024: Contamination reported in 14 out of 55 papers (25.5%)\n",
      "2025: Contamination reported in 38 out of 90 papers (42.2%)\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "contamination_2023 = df_relevant[df_relevant['year'] == 2023]['contamination'].sum()\n",
    "print(f\"2023: Contamination reported in {contamination_2023} out of {total_2023} papers ({(contamination_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "contamination_2024 = df_relevant[df_relevant['year'] == 2024]['contamination'].sum()\n",
    "print(f\"2024: Contamination reported in {contamination_2024} out of {total_2024} papers ({(contamination_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "contamination_2025 = df_relevant[df_relevant['year'] == 2025]['contamination'].sum()\n",
    "print(f\"2025: Contamination reported in {contamination_2025} out of {total_2025} papers ({(contamination_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfc14a",
   "metadata": {},
   "source": [
    "# RQ4 - How replicable are LLM-based studies?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'model_config' (short-text list)\n",
    "- 'artifact_available' (bool)\n",
    "- 'artifact_reusable' (bool)\n",
    "- 'artifact_functional' (bool)\n",
    "- 'artefact_manual' (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d775ea",
   "metadata": {},
   "source": [
    "## ACM Badge Artifact Availability - Relevant vs. Non-Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96259443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of relevant papers with artifact available: 18.64% (33/177)\n",
      "Proportion of other papers with artifact available: 41.36% (213/515)\n"
     ]
    }
   ],
   "source": [
    "# Calculate proportions in the combined dataset\n",
    "total_relevant = df_relevant.shape[0]\n",
    "total_non_relevant = df_non_relevant.shape[0]\n",
    "\n",
    "relevant_with_artifact = df_relevant['artifact_available'].sum()\n",
    "non_relevant_with_artifact = df_non_relevant['artifact_available'].sum()\n",
    "\n",
    "prop_relevant = relevant_with_artifact / total_relevant\n",
    "prop_non_relevant = non_relevant_with_artifact / total_non_relevant\n",
    "\n",
    "print(f\"Proportion of relevant papers with artifact available: {prop_relevant:.2%} ({relevant_with_artifact}/{total_relevant})\")\n",
    "print(f\"Proportion of other papers with artifact available: {prop_non_relevant:.2%} ({non_relevant_with_artifact}/{total_non_relevant})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "637d335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 - Proportion of relevant papers with artifact available: 18.75% (6/32)\n",
      "2024 - Proportion of relevant papers with artifact available: 16.36% (9/55)\n",
      "2025 - Proportion of relevant papers with artifact available: 20.00% (18/90)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of relevant papers with artifacts available per year\n",
    "total_relevant_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "artifact_avail_2023 = df_relevant[(df_relevant['year'] == 2023) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2023 = artifact_avail_2023 / total_relevant_2023\n",
    "print(f\"2023 - Proportion of relevant papers with artifact available: {prop_relevant_2023:.2%} ({artifact_avail_2023}/{total_relevant_2023})\")\n",
    "\n",
    "total_relevant_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "artifact_avail_2024 = df_relevant[(df_relevant['year'] == 2024) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2024 = artifact_avail_2024 / total_relevant_2024\n",
    "print(f\"2024 - Proportion of relevant papers with artifact available: {prop_relevant_2024:.2%} ({artifact_avail_2024}/{total_relevant_2024})\")\n",
    "\n",
    "total_relevant_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "artifact_avail_2025 = df_relevant[(df_relevant['year'] == 2025) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2025 = artifact_avail_2025 / total_relevant_2025\n",
    "print(f\"2025 - Proportion of relevant papers with artifact available: {prop_relevant_2025:.2%} ({artifact_avail_2025}/{total_relevant_2025})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1528756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\n",
      "artifact_available\n",
      "False    144\n",
      "True      33\n",
      "Name: count, dtype: int64\n",
      "artifact_reusable\n",
      "False    156\n",
      "True      21\n",
      "Name: count, dtype: int64\n",
      "artifact_functional\n",
      "False    164\n",
      "True      13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\")\n",
    "print(df_relevant['artifact_available'].value_counts())\n",
    "print(df_relevant['artifact_reusable'].value_counts())\n",
    "print(df_relevant['artifact_functional'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4be2217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\n",
      "artifact_available\n",
      "False    26\n",
      "True      6\n",
      "Name: count, dtype: int64\n",
      "artifact_reusable\n",
      "False    29\n",
      "True      3\n",
      "Name: count, dtype: int64\n",
      "artifact_functional\n",
      "False    31\n",
      "True      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "2024 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\n",
      "artifact_available\n",
      "False    46\n",
      "True      9\n",
      "Name: count, dtype: int64\n",
      "artifact_reusable\n",
      "False    47\n",
      "True      8\n",
      "Name: count, dtype: int64\n",
      "artifact_functional\n",
      "False    55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "2025 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\n",
      "artifact_available\n",
      "False    72\n",
      "True     18\n",
      "Name: count, dtype: int64\n",
      "artifact_reusable\n",
      "False    80\n",
      "True     10\n",
      "Name: count, dtype: int64\n",
      "artifact_functional\n",
      "False    78\n",
      "True     12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"2023 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\")\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artifact_available'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artifact_reusable'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artifact_functional'].value_counts())\n",
    "\n",
    "print(\"\\n\\n2024 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\")\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artifact_available'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artifact_reusable'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artifact_functional'].value_counts())\n",
    "\n",
    "print(\"\\n\\n2025 Relevant Papers - Artifact Availability, Reusability, and Functionality Counts:\")\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artifact_available'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artifact_reusable'].value_counts())\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artifact_functional'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e39d00",
   "metadata": {},
   "source": [
    "## Manual Artefact Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62cde1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact_manual\n",
      "True     144\n",
      "False     24\n",
      "DEAD       9\n",
      "Name: count, dtype: int64\n",
      "9 dead links in the 177 relevant papers\n",
      "2023 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     24\n",
      "False     5\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2024 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     44\n",
      "False     8\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2025 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     76\n",
      "False    11\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_relevant[\"artefact_manual\"].value_counts())\n",
    "print(f\"{df_relevant[df_relevant['artefact_manual'] == 'DEAD'].shape[0]} dead links in the {df_relevant.shape[0]} relevant papers\")\n",
    "\n",
    "# Per year\n",
    "print(\"2023 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artefact_manual'].value_counts())\n",
    "print(\"\\n2024 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artefact_manual'].value_counts())\n",
    "print(\"\\n2025 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artefact_manual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa8bd7",
   "metadata": {},
   "source": [
    "## How many papers have manually identified artefacts but no ACM badge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb9938c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with manually identified artefacts (including DEAD links): 153 out of 177 relevant papers (86.4%)\n",
      "Number of papers with ACM artifact available badge: 33 out of 177 relevant papers (18.6%)\n",
      "Number of papers with manually identified artefacts but no ACM artifact available badge: 124 out of 177 relevant papers (70.1%)\n",
      "Number of papers with no manually identified artefacts and no ACM artifact available badge: 20 out of 177 relevant papers (11.3%)\n"
     ]
    }
   ],
   "source": [
    "num_manually_identified = df_relevant[df_relevant['artefact_manual'].isin([True, 'DEAD'])].shape[0]\n",
    "print(f\"Number of papers with manually identified artefacts (including DEAD links): {num_manually_identified} out of {df_relevant.shape[0]} relevant papers ({(num_manually_identified/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_artifact_available = df_relevant[df_relevant['artifact_available']].shape[0]\n",
    "print(f\"Number of papers with ACM artifact available badge: {num_artifact_available} out of {df_relevant.shape[0]} relevant papers ({(num_artifact_available/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_manually_identified_no_badge = df_relevant[(df_relevant['artefact_manual'].isin([True, 'DEAD'])) & (~df_relevant['artifact_available'])].shape[0]\n",
    "print(f\"Number of papers with manually identified artefacts but no ACM artifact available badge: {num_manually_identified_no_badge} out of {df_relevant.shape[0]} relevant papers ({(num_manually_identified_no_badge/df_relevant.shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_no_manual_no_badge = df_relevant[(~df_relevant['artefact_manual'].isin([True, 'DEAD'])) & (~df_relevant['artifact_available'])].shape[0]\n",
    "print(f\"Number of papers with no manually identified artefacts and no ACM artifact available badge: {num_no_manual_no_badge} out of {df_relevant.shape[0]} relevant papers ({(num_no_manual_no_badge/df_relevant.shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336064ed",
   "metadata": {},
   "source": [
    "## How many papers with ACM badges have dead links?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8523134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with artifacts available but DEAD links: 2\n",
      "Number of papers with artifacts reusable but DEAD links: 1\n",
      "Number of papers with artifacts functional but DEAD links: 0\n",
      "Two total papers have ACM badges with dead links. These papers both have the artifact available badge, and one of them even has the artifact reusable badge.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artefact_manual</th>\n",
       "      <th>artifact_available</th>\n",
       "      <th>artifact_reusable</th>\n",
       "      <th>artifact_functional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLM Assistance for Memory Safety</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ChatGPT-Based Test Generation for Refactoring ...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Automated Generation of Accessibility Test Rep...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Make LLM a Testing Expert: Bringing Human-like...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Demystifying and Detecting Misuses of Deep Lea...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Large Language Models are Edge-Case Generators...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Context-Aware Bug Reproduction for Mobile Apps</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>AI-Based Question Answering Assistance for Ana...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Large Language Models are Few-Shot Testers: Ex...</td>\n",
       "      <td>DEAD</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title artefact_manual  \\\n",
       "13                    LLM Assistance for Memory Safety            DEAD   \n",
       "57   ChatGPT-Based Test Generation for Refactoring ...            DEAD   \n",
       "75   Automated Generation of Accessibility Test Rep...            DEAD   \n",
       "89   Make LLM a Testing Expert: Bringing Human-like...            DEAD   \n",
       "96   Demystifying and Detecting Misuses of Deep Lea...            DEAD   \n",
       "131  Large Language Models are Edge-Case Generators...            DEAD   \n",
       "134     Context-Aware Bug Reproduction for Mobile Apps            DEAD   \n",
       "140  AI-Based Question Answering Assistance for Ana...            DEAD   \n",
       "149  Large Language Models are Few-Shot Testers: Ex...            DEAD   \n",
       "\n",
       "     artifact_available  artifact_reusable  artifact_functional  \n",
       "13                False              False                False  \n",
       "57                False              False                False  \n",
       "75                False              False                False  \n",
       "89                False              False                False  \n",
       "96                False              False                False  \n",
       "131               False              False                False  \n",
       "134               False              False                False  \n",
       "140                True               True                False  \n",
       "149                True              False                False  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of papers with artifacts available but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_available'])].shape[0])\n",
    "print(\"Number of papers with artifacts reusable but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_reusable'])].shape[0])\n",
    "print(\"Number of papers with artifacts functional but DEAD links:\", df_relevant[(df_relevant['artefact_manual'] == 'DEAD') & (df_relevant['artifact_functional'])].shape[0])\n",
    "\n",
    "print(\"Two total papers have ACM badges with dead links. These papers both have the artifact available badge, and one of them even has the artifact reusable badge.\")\n",
    "df_relevant[df_relevant['artefact_manual'] == 'DEAD'][['title', 'artefact_manual', 'artifact_available', 'artifact_reusable', 'artifact_functional']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e44364",
   "metadata": {},
   "source": [
    "## Prevalence of Reporting on Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "731bae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Model Configuration Stats:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_config_list\n",
       "inference     89\n",
       "-             46\n",
       "finetuning    43\n",
       "training      24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[\"model_config\"].value_counts()\n",
    "df_relevant[\"model_config_list\"] = df_relevant['model_config'].apply(\n",
    "    lambda x: [config.strip() for config in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(\"Overall Model Configuration Stats:\")\n",
    "df_relevant[\"model_config_list\"].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b01652",
   "metadata": {},
   "source": [
    "### Reporting on Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb7154ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall - Number of papers reporting on inference (generation) configuration:\n",
      "89 out of 177 papers (50.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall - Number of papers reporting on inference (generation) configuration:\")\n",
    "num_reporting_inference_config = df_relevant[df_relevant['model_config_list'].apply(lambda x: 'inference' in [cfg.lower() for cfg in x])].shape[0]\n",
    "print(f\"{num_reporting_inference_config} out of {df_relevant.shape[0]} papers ({(num_reporting_inference_config/df_relevant.shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89e0eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Number of papers reporting on inference (generation) configuration: 10 out of 32 papers (31.2%)\n",
      "2024: Number of papers reporting on inference (generation) configuration: 27 out of 55 papers (49.1%)\n",
      "2025: Number of papers reporting on inference (generation) configuration: 52 out of 90 papers (57.8%)\n"
     ]
    }
   ],
   "source": [
    "num_reporting_inference_config_2023 = df_relevant[df_relevant['model_config_list'].apply(lambda x: 'inference' in [cfg.lower() for cfg in x]) & (df_relevant['year'] == 2023)].shape[0]\n",
    "print(f\"2023: Number of papers reporting on inference (generation) configuration: {num_reporting_inference_config_2023} out of {df_relevant[df_relevant['year'] == 2023].shape[0]} papers ({(num_reporting_inference_config_2023/df_relevant[df_relevant['year'] == 2023].shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_reporting_inference_config_2024 = df_relevant[df_relevant['model_config_list'].apply(lambda x: 'inference' in [cfg.lower() for cfg in x]) & (df_relevant['year'] == 2024)].shape[0]\n",
    "print(f\"2024: Number of papers reporting on inference (generation) configuration: {num_reporting_inference_config_2024} out of {df_relevant[df_relevant['year'] == 2024].shape[0]} papers ({(num_reporting_inference_config_2024/df_relevant[df_relevant['year'] == 2024].shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_reporting_inference_config_2025 = df_relevant[df_relevant['model_config_list'].apply(lambda x: 'inference' in [cfg.lower() for cfg in x]) & (df_relevant['year'] == 2025)].shape[0]\n",
    "print(f\"2025: Number of papers reporting on inference (generation) configuration: {num_reporting_inference_config_2025} out of {df_relevant[df_relevant['year'] == 2025].shape[0]} papers ({(num_reporting_inference_config_2025/df_relevant[df_relevant['year'] == 2025].shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece6106",
   "metadata": {},
   "source": [
    "### Reporting on Training/Fine-Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9aeb95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers reporting training OR fine-tuning configuration: 61 out of 177 papers (34.5%)\n"
     ]
    }
   ],
   "source": [
    "num_reporting_training_or_finetuning = df_relevant['model_config_list'].apply(\n",
    "    lambda x: any(cfg.lower() in ['training', 'finetuning'] for cfg in x)\n",
    ").sum()\n",
    "\n",
    "print(f\"Number of papers reporting training OR fine-tuning configuration: {num_reporting_training_or_finetuning} out of {df_relevant.shape[0]} papers ({(num_reporting_training_or_finetuning/df_relevant.shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6620792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Number of papers reporting training OR fine-tuning configuration: 20 out of 32 papers (62.5%)\n",
      "2024: Number of papers reporting training OR fine-tuning configuration: 20 out of 55 papers (36.4%)\n",
      "2025: Number of papers reporting training OR fine-tuning configuration: 21 out of 90 papers (23.3%)\n"
     ]
    }
   ],
   "source": [
    "num_reporting_training_or_finetuning_2023 = df_relevant[df_relevant['year'] == 2023]['model_config_list'].apply(\n",
    "    lambda x: any(cfg.lower() in ['training', 'finetuning'] for cfg in x)\n",
    ").sum()\n",
    "print(f\"2023: Number of papers reporting training OR fine-tuning configuration: {num_reporting_training_or_finetuning_2023} out of {df_relevant[df_relevant['year'] == 2023].shape[0]} papers ({(num_reporting_training_or_finetuning_2023/df_relevant[df_relevant['year'] == 2023].shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_reporting_training_or_finetuning_2024 = df_relevant[df_relevant['year'] == 2024]['model_config_list'].apply(\n",
    "    lambda x: any(cfg.lower() in ['training', 'finetuning'] for cfg in x)\n",
    ").sum()\n",
    "print(f\"2024: Number of papers reporting training OR fine-tuning configuration: {num_reporting_training_or_finetuning_2024} out of {df_relevant[df_relevant['year'] == 2024].shape[0]} papers ({(num_reporting_training_or_finetuning_2024/df_relevant[df_relevant['year'] == 2024].shape[0])*100:.1f}%)\")\n",
    "\n",
    "num_reporting_training_or_finetuning_2025 = df_relevant[df_relevant['year'] == 2025]['model_config_list'].apply(\n",
    "    lambda x: any(cfg.lower() in ['training', 'finetuning'] for cfg in x)\n",
    ").sum()\n",
    "print(f\"2025: Number of papers reporting training OR fine-tuning configuration: {num_reporting_training_or_finetuning_2025} out of {df_relevant[df_relevant['year'] == 2025].shape[0]} papers ({(num_reporting_training_or_finetuning_2025/df_relevant[df_relevant['year'] == 2025].shape[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbed05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c518a064",
   "metadata": {},
   "source": [
    "# RQ5 - How sustainable is LLM-based SE research?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'cost' (short-text list)\n",
    "- 'cost_free_text' (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb184c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
