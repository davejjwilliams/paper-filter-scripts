{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4934af8f",
   "metadata": {},
   "source": [
    "# Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fadbe",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab58f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Shape: (177, 27)\n",
      "All Papers Shape: (692, 11)\n"
     ]
    }
   ],
   "source": [
    "df_relevant = pd.read_excel('results/final/final_results.xlsx', sheet_name='relevant_papers')\n",
    "df_all_papers = pd.read_csv('results/ICSE_all_papers.csv')\n",
    "\n",
    "print(\"Relevant Shape:\", df_relevant.shape)\n",
    "print(\"All Papers Shape:\", df_all_papers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676387c9",
   "metadata": {},
   "source": [
    "### Creating Combined and Non-Relevant Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a29c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Paper List\n",
    "relevant_papers = df_relevant['title'].tolist()\n",
    "\n",
    "# Remove relevant papers from all papers to create final non-relevant set\n",
    "df_non_relevant = df_all_papers[~df_all_papers['title'].isin(relevant_papers)].copy()\n",
    "\n",
    "# Make sure all non-relevant papers are marked as such\n",
    "df_non_relevant['relevant'] = False\n",
    "\n",
    "# Re-order columns to match relevant dataframe\n",
    "common_columns = ['reviewer', 'relevant', 'year', 'title', 'authors', 'url', 'abstract', 'artifact_available', 'artifact_reusable', 'artifact_functional', 'ai']\n",
    "df_non_relevant = df_non_relevant[common_columns]\n",
    "\n",
    "# Add extra columns: 'task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text'\n",
    "extra_columns = ['task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text']\n",
    "for col in extra_columns:\n",
    "    df_non_relevant[col] = None\n",
    "\n",
    "# Combine relevant and non-relevant dataframes\n",
    "df_combined = pd.concat([df_relevant, df_non_relevant], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4613e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape: (692, 27)\n",
      "Num Unique Papers: 692\n",
      "Num Relevant Papers in Combined DF: 177\n",
      "Num Unique Papers in Relevant DF: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Shape:\", df_combined.shape)\n",
    "print(\"Num Unique Papers:\", df_combined['title'].nunique())\n",
    "print(\"Num Relevant Papers in Combined DF:\", df_combined[df_combined['relevant'] == True].shape[0])\n",
    "print(\"Num Unique Papers in Relevant DF:\", df_relevant['title'].nunique()) # Should match number above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100580d7",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We now have access to four dataframes for analysis:\n",
    "\n",
    "- `df_combined`: contains all papers and final columns from our spreadsheet (non-relevant papers just have None values in the fields we completed for the relevant papers)\n",
    "- `df_relevant`: contains all relevant papers as rows and the final columns we intend to use for analysis\n",
    "- `df_non_relevant`: contains all non-relevant papers. Our finals columns are present but all filled with None values as we didn't perform data extraction for these papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64a7b",
   "metadata": {},
   "source": [
    "## Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719ef68",
   "metadata": {},
   "source": [
    "### Number of Papers at Each Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Papers in Combined DF: 692\n",
      "\n",
      "Total Papers from AI Keywords:\n",
      "ai\n",
      "False    388\n",
      "True     304\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Relevant Papers:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total numbers\n",
    "print(\"Total Papers in Combined DF:\", df_combined.shape[0])\n",
    "\n",
    "print(\"\\nTotal Papers from AI Keywords:\")\n",
    "print(df_combined['ai'].value_counts())\n",
    "\n",
    "print(\"\\nTotal Relevant Papers:\")\n",
    "print(df_combined['relevant'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers Per Year:\n",
      "year\n",
      "2023    210\n",
      "2024    236\n",
      "2025    246\n",
      "Name: title, dtype: int64\n",
      "\n",
      "AI Papers Per Year:\n",
      "year\n",
      "2023     59\n",
      "2024     99\n",
      "2025    146\n",
      "Name: title, dtype: int64\n",
      "\n",
      "Relevant Papers Per Year:\n",
      "year\n",
      "2023    32\n",
      "2024    55\n",
      "2025    90\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "print(\"Papers Per Year:\")\n",
    "print(df_combined.groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nAI Papers Per Year:\")\n",
    "print(df_combined[df_combined['ai'] == True].groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nRelevant Papers Per Year:\")\n",
    "print(df_combined[df_combined['relevant'] == True].groupby('year')['title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029cd15",
   "metadata": {},
   "source": [
    "### Numbers of Relevant Papers (Papers with LLM-based Empirical Studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfef7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevant vs Non-Relevant Counts:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Relevant Papers by Year:\n",
      "year\n",
      "2025    90\n",
      "2024    55\n",
      "2023    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Relevant vs Non-Relevant Counts:\")\n",
    "print(df_combined[\"relevant\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nRelevant Papers by Year:\")\n",
    "print(df_relevant[\"year\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe95f7",
   "metadata": {},
   "source": [
    "### Geo-location of SE Research Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e76ac",
   "metadata": {},
   "source": [
    "# RQ1 - What Tasks are being tackled in LLM SE studies, and are they fairly evaluated against existing non-LLM techniques?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'task' (short-text)\n",
    "- 'non_llm_approaches' (bool)\n",
    "- 'dataset_type' (short-text)\n",
    "- programming_language (short-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a225d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list\n",
      "code generation                                   26\n",
      "program repair                                    24\n",
      "test generation                                   23\n",
      "vulnerability detection                           16\n",
      "bug detection                                     12\n",
      "code translation                                   8\n",
      "clone detection                                    6\n",
      "code completion                                    6\n",
      "code summarisation                                 6\n",
      "code comprehension                                 5\n",
      "type detection                                     5\n",
      "log parsing                                        5\n",
      "code search                                        5\n",
      "fuzzing                                            4\n",
      "bug reproduction                                   3\n",
      "commit message generation                          3\n",
      "fault localisation                                 3\n",
      "code memorisation detection                        3\n",
      "test repair                                        2\n",
      "formal verification                                2\n",
      "program analysis                                   2\n",
      "traceability link recovery                         2\n",
      "code retrieval                                     2\n",
      "code refinement                                    1\n",
      "regression testing                                 1\n",
      "SO post editing                                    1\n",
      "AI generated code detection                        1\n",
      "security patch detection                           1\n",
      "model completion                                   1\n",
      "configuration validation                           1\n",
      "smart contract auditing                            1\n",
      "UI design repair                                   1\n",
      "detection of code design issues                    1\n",
      "code adaptation                                    1\n",
      "comment repair                                     1\n",
      "bug report comprehension                           1\n",
      "inconsistency prediction in decentralised apps     1\n",
      "machine-generated code detection                   1\n",
      "security injection                                 1\n",
      "code optimisation                                  1\n",
      "root cause analysis                                1\n",
      "privacy inconsistencies detection                  1\n",
      "figurative language detection                      1\n",
      "binary software composition analysis               1\n",
      "API misuse detection                               1\n",
      "program comprehension                              1\n",
      "exception handling recommender                     1\n",
      "static analysis                                    1\n",
      "comment generation                                 1\n",
      "code review                                        1\n",
      "log understanding                                  1\n",
      "emotion-cause extraction                           1\n",
      "code idioms detection                              1\n",
      "log generation                                     1\n",
      "CI/CD workflow generation                          1\n",
      "requirements analysis                              1\n",
      "model extraction                                   1\n",
      "mutant generation                                  1\n",
      "assert generation                                  1\n",
      "SO posts summarisation                             1\n",
      "question answering                                 1\n",
      "patch correctness assessment                       1\n",
      "algorithm classification                           1\n",
      "optimal thread coarsening factor                   1\n",
      "heterogeneous device mapping                       1\n",
      "vulnerability alert prediction                     1\n",
      "vulnerability repair                               1\n",
      "GUI test case migration                            1\n",
      "software effort estimation                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['task_list'] = df_relevant['task'].apply(\n",
    "    lambda x: [task.strip() for task in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(df_relevant['task_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce60bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of tasks per paper:\n",
      "num_tasks\n",
      "1     157\n",
      "2      11\n",
      "3       5\n",
      "4       3\n",
      "13      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple tasks: 20\n",
      "Percentage covering multiple tasks: 11.3%\n",
      "Empty DataFrame\n",
      "Columns: [title, task_list]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Looking at how many papers cover multiple tasks\n",
    "df_relevant['num_tasks'] = df_relevant['task_list'].apply(len)\n",
    "\n",
    "print(\"Distribution of number of tasks per paper:\")\n",
    "print(df_relevant['num_tasks'].value_counts().sort_index())\n",
    "print(f\"\\nPapers covering multiple tasks: {(df_relevant['num_tasks'] > 1).sum()}\")\n",
    "print(f\"Percentage covering multiple tasks: {(df_relevant['num_tasks'] > 1).mean()*100:.1f}%\")\n",
    "\n",
    "print(df_relevant[df_relevant['num_tasks'] == 0][['title', 'task_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9992a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_llm_approaches\n",
      "True                                                                                                                                                                                                              92\n",
      "False                                                                                                                                                                                                             59\n",
      "FALSE (says TransCoder is not llm but a language modeling appraoch using transformers and seq2seq)                                                                                                                 1\n",
      "TRUE (FTLR, COMET, VSM, LSI,ArDoCode)                                                                                                                                                                              1\n",
      "TRUE (AEL, Drain)                                                                                                                                                                                                  1\n",
      "TRUE (Devign, IVDetect both using gated graph NNs)                                                                                                                                                                 1\n",
      "FALSE (because they compare with older transformers)                                                                                                                                                               1\n",
      "TRUE (see Table I: Apollo, REDriver, FixDriver)                                                                                                                                                                    1\n",
      "TRUE (comparison with other 4 fuzzing approaches)                                                                                                                                                                  1\n",
      "TRUE (code generation approaches: RawGPT, CodeT, Reflexion, and FlowGen - their approach. For the evaluations of all approaches, they use ChatGPT.)                                                                1\n",
      "TRUE (we also compare four state-of-the-art (SOTA)\\ncode generation approaches that operate during the decoding process)                                                                                           1\n",
      "TRUE (API-level fuzzers such as Free-\\nFuzz [13], DeepREL [64], NablaFuzz [14], TensorScope [15],\\nand TitanFuzz [21], while also considering model-level fuzzers\\nlike Muffin [18] and FUTURE their approach)     1\n",
      "TRUE (baselines: CodeExecutor, GPT-3.5, A variant of ORCA)                                                                                                                                                         1\n",
      "TRUE (deep code models use in three approaches ALERT, BeamAttack, ITGen for comparison. See Table II.)                                                                                                             1\n",
      "TRUE                                                                                                                                                                                                               1\n",
      "TRUE (QAQA; metamorphic testing for QA software)                                                                                                                                                                   1\n",
      "TRUE (e.g., the KNOD DL-based approach)                                                                                                                                                                            1\n",
      "TRUE (We choose seven baseline techniques. One of them is the Rust\\ncompiler (rustc), since it sometimes offers direct suggestions for\\nmodifying the program to pass compiler checks...)                          1\n",
      "TRUE (RQ4)                                                                                                                                                                                                         1\n",
      "TRUE (Table 3)                                                                                                                                                                                                     1\n",
      "YES (Table 2)                                                                                                                                                                                                      1\n",
      "TRUE (EvoCrash and Copy&Paste -> crash reproduction baselines)                                                                                                                                                     1\n",
      "TRUE (DynaMosa, CODEXONLY)                                                                                                                                                                                         1\n",
      "TRUE (Pyflakes -> static analysis tool, CommitGen -> RNN-based, NNGen -> Information Retrival-based, Fine-tuned CodeT5, Bugsplainer based on CodeT5-60M)                                                           1\n",
      "TRUE (see Table III)                                                                                                                                                                                               1\n",
      "TRUE (Transformers in RQ3)                                                                                                                                                                                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_combined['non_llm_approaches'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b40c9",
   "metadata": {},
   "source": [
    "# RQ2 - What models are being used?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'models_open_closed' (open/closed/both)\n",
    "- 'num_models' (int)\n",
    "- 'model_families' (list of short text)\n",
    "- 'model_sizes_reported' (NA/none/some/full - currently unfinished)\n",
    "- 'model_scale' (currently unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07775a87",
   "metadata": {},
   "source": [
    "### Open vs. Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56c8c3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewer', 'relevant', 'year', 'title', 'authors', 'url', 'abstract',\n",
       "       'artifact_available', 'artifact_reusable', 'artifact_functional', 'ai',\n",
       "       'task', 'non_llm_approaches', 'models_open_closed', 'num_models',\n",
       "       'model_families', 'model_scale', 'model_size_free_text',\n",
       "       'model_sizes_reported', 'model_config', 'dataset_type',\n",
       "       'programming_language', 'cost', 'cost_free_text', 'artefact_manual',\n",
       "       'contamination', 'contamination_free_text', 'task_list', 'num_tasks'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b5c9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models_open_closed\n",
       "open      71\n",
       "both      65\n",
       "closed    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant['models_open_closed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94f1a0",
   "metadata": {},
   "source": [
    "# RQ3 - How well do authors tackle the problem of data leakage/contamination?\n",
    "\n",
    "DF Columns to use:\n",
    "- contamination (bool)\n",
    "- contamination_free_text (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All years: Contamination reported in 58 out of 177 papers (32.8%)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "total = df_relevant.shape[0]\n",
    "contamination_reported = df_relevant['contamination'].sum()\n",
    "\n",
    "print(f\"All years: Contamination reported in {contamination_reported} out of {total} papers ({(contamination_reported/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb11e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Contamination reported in 6 out of 32 papers (18.8%)\n",
      "2024: Contamination reported in 14 out of 55 papers (25.5%)\n",
      "2025: Contamination reported in 38 out of 90 papers (42.2%)\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "contamination_2023 = df_relevant[df_relevant['year'] == 2023]['contamination'].sum()\n",
    "print(f\"2023: Contamination reported in {contamination_2023} out of {total_2023} papers ({(contamination_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "contamination_2024 = df_relevant[df_relevant['year'] == 2024]['contamination'].sum()\n",
    "print(f\"2024: Contamination reported in {contamination_2024} out of {total_2024} papers ({(contamination_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "contamination_2025 = df_relevant[df_relevant['year'] == 2025]['contamination'].sum()\n",
    "print(f\"2025: Contamination reported in {contamination_2025} out of {total_2025} papers ({(contamination_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfc14a",
   "metadata": {},
   "source": [
    "# RQ4 - How replicable are LLM-based studies?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'model_config' (short-text list)\n",
    "- 'artifact_available' (bool)\n",
    "- 'artifact_reusable' (bool)\n",
    "- 'artifact_functional' (bool)\n",
    "- 'artefact_manual' (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d775ea",
   "metadata": {},
   "source": [
    "## ACM Badge Artifact Availability - Relevant vs. Non-Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96259443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of relevant papers with artifact available: 18.64% (33/177)\n",
      "Proportion of other papers with artifact available: 41.36% (213/515)\n"
     ]
    }
   ],
   "source": [
    "# Calculate proportions in the combined dataset\n",
    "total_relevant = df_relevant.shape[0]\n",
    "total_non_relevant = df_non_relevant.shape[0]\n",
    "\n",
    "relevant_with_artifact = df_relevant['artifact_available'].sum()\n",
    "non_relevant_with_artifact = df_non_relevant['artifact_available'].sum()\n",
    "\n",
    "prop_relevant = relevant_with_artifact / total_relevant\n",
    "prop_non_relevant = non_relevant_with_artifact / total_non_relevant\n",
    "\n",
    "print(f\"Proportion of relevant papers with artifact available: {prop_relevant:.2%} ({relevant_with_artifact}/{total_relevant})\")\n",
    "print(f\"Proportion of other papers with artifact available: {prop_non_relevant:.2%} ({non_relevant_with_artifact}/{total_non_relevant})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "637d335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 - Proportion of relevant papers with artifact available: 18.75% (6/32)\n",
      "2024 - Proportion of relevant papers with artifact available: 16.36% (9/55)\n",
      "2025 - Proportion of relevant papers with artifact available: 20.00% (18/90)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of relevant papers with artifacts available per year\n",
    "total_relevant_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "artifact_avail_2023 = df_relevant[(df_relevant['year'] == 2023) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2023 = artifact_avail_2023 / total_relevant_2023\n",
    "print(f\"2023 - Proportion of relevant papers with artifact available: {prop_relevant_2023:.2%} ({artifact_avail_2023}/{total_relevant_2023})\")\n",
    "\n",
    "total_relevant_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "artifact_avail_2024 = df_relevant[(df_relevant['year'] == 2024) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2024 = artifact_avail_2024 / total_relevant_2024\n",
    "print(f\"2024 - Proportion of relevant papers with artifact available: {prop_relevant_2024:.2%} ({artifact_avail_2024}/{total_relevant_2024})\")\n",
    "\n",
    "total_relevant_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "artifact_avail_2025 = df_relevant[(df_relevant['year'] == 2025) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2025 = artifact_avail_2025 / total_relevant_2025\n",
    "print(f\"2025 - Proportion of relevant papers with artifact available: {prop_relevant_2025:.2%} ({artifact_avail_2025}/{total_relevant_2025})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e39d00",
   "metadata": {},
   "source": [
    "## Manual Artefact Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62cde1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact_manual\n",
      "True     144\n",
      "False     24\n",
      "DEAD       9\n",
      "Name: count, dtype: int64\n",
      "10 instances of dead links in the 173 relevant papers\n",
      "2023 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     24\n",
      "False     5\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2024 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     44\n",
      "False     8\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2025 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     76\n",
      "False    11\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_relevant[\"artefact_manual\"].value_counts())\n",
    "print(\"10 instances of dead links in the 173 relevant papers\")\n",
    "\n",
    "# Per year\n",
    "print(\"2023 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artefact_manual'].value_counts())\n",
    "print(\"\\n2024 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artefact_manual'].value_counts())\n",
    "print(\"\\n2025 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artefact_manual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518a064",
   "metadata": {},
   "source": [
    "# RQ5 - How sustainable is LLM-based SE research?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'cost' (short-text list)\n",
    "- 'cost_free_text' (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb184c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
