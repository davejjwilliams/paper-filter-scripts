{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4934af8f",
   "metadata": {},
   "source": [
    "# Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fadbe",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab58f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Shape: (177, 27)\n",
      "All Papers Shape: (692, 11)\n"
     ]
    }
   ],
   "source": [
    "df_relevant = pd.read_excel('results/final/final_results.xlsx', sheet_name='relevant_papers')\n",
    "df_all_papers = pd.read_csv('results/ICSE_all_papers.csv')\n",
    "\n",
    "print(\"Relevant Shape:\", df_relevant.shape)\n",
    "print(\"All Papers Shape:\", df_all_papers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676387c9",
   "metadata": {},
   "source": [
    "### Creating Combined and Non-Relevant Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a29c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Paper List\n",
    "relevant_papers = df_relevant['title'].tolist()\n",
    "\n",
    "# Remove relevant papers from all papers to create final non-relevant set\n",
    "df_non_relevant = df_all_papers[~df_all_papers['title'].isin(relevant_papers)].copy()\n",
    "\n",
    "# Make sure all non-relevant papers are marked as such\n",
    "df_non_relevant['relevant'] = False\n",
    "\n",
    "# Re-order columns to match relevant dataframe\n",
    "common_columns = ['reviewer', 'relevant', 'year', 'title', 'authors', 'url', 'abstract', 'artifact_available', 'artifact_reusable', 'artifact_functional', 'ai']\n",
    "df_non_relevant = df_non_relevant[common_columns]\n",
    "\n",
    "# Add extra columns: 'task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text'\n",
    "extra_columns = ['task', 'non_llm_approaches', 'models_open_closed', 'num_models', 'model_families', 'model_scale', 'model_size_free_text', 'model_sizes_reported', 'model_config', 'dataset_type', 'programming_language', 'cost', 'cost_free_text', 'artefact_manual', 'contamination', 'contamination_free_text']\n",
    "for col in extra_columns:\n",
    "    df_non_relevant[col] = None\n",
    "\n",
    "# Combine relevant and non-relevant dataframes\n",
    "df_combined = pd.concat([df_relevant, df_non_relevant], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4613e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Shape: (692, 27)\n",
      "Num Unique Papers: 692\n",
      "Num Relevant Papers in Combined DF: 177\n",
      "Num Unique Papers in Relevant DF: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Shape:\", df_combined.shape)\n",
    "print(\"Num Unique Papers:\", df_combined['title'].nunique())\n",
    "print(\"Num Relevant Papers in Combined DF:\", df_combined[df_combined['relevant'] == True].shape[0])\n",
    "print(\"Num Unique Papers in Relevant DF:\", df_relevant['title'].nunique()) # Should match number above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100580d7",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "We now have access to four dataframes for analysis:\n",
    "\n",
    "- `df_combined`: contains all papers and final columns from our spreadsheet (non-relevant papers just have None values in the fields we completed for the relevant papers)\n",
    "- `df_relevant`: contains all relevant papers as rows and the final columns we intend to use for analysis\n",
    "- `df_non_relevant`: contains all non-relevant papers. Our finals columns are present but all filled with None values as we didn't perform data extraction for these papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64a7b",
   "metadata": {},
   "source": [
    "## Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719ef68",
   "metadata": {},
   "source": [
    "### Number of Papers at Each Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Papers in Combined DF: 692\n",
      "\n",
      "Total Papers from AI Keywords:\n",
      "ai\n",
      "False    388\n",
      "True     304\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Relevant Papers:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Total numbers\n",
    "print(\"Total Papers in Combined DF:\", df_combined.shape[0])\n",
    "\n",
    "print(\"\\nTotal Papers from AI Keywords:\")\n",
    "print(df_combined['ai'].value_counts())\n",
    "\n",
    "print(\"\\nTotal Relevant Papers:\")\n",
    "print(df_combined['relevant'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers Per Year:\n",
      "year\n",
      "2023    210\n",
      "2024    236\n",
      "2025    246\n",
      "Name: title, dtype: int64\n",
      "\n",
      "AI Papers Per Year:\n",
      "year\n",
      "2023     59\n",
      "2024     99\n",
      "2025    146\n",
      "Name: title, dtype: int64\n",
      "\n",
      "Relevant Papers Per Year:\n",
      "year\n",
      "2023    32\n",
      "2024    55\n",
      "2025    90\n",
      "Name: title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "print(\"Papers Per Year:\")\n",
    "print(df_combined.groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nAI Papers Per Year:\")\n",
    "print(df_combined[df_combined['ai'] == True].groupby('year')['title'].nunique())\n",
    "\n",
    "print(\"\\nRelevant Papers Per Year:\")\n",
    "print(df_combined[df_combined['relevant'] == True].groupby('year')['title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029cd15",
   "metadata": {},
   "source": [
    "### Numbers of Relevant Papers (Papers with LLM-based Empirical Studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfef7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Relevant vs Non-Relevant Counts:\n",
      "relevant\n",
      "False    515\n",
      "True     177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Relevant Papers by Year:\n",
      "year\n",
      "2025    90\n",
      "2024    55\n",
      "2023    32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Relevant vs Non-Relevant Counts:\")\n",
    "print(df_combined[\"relevant\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nRelevant Papers by Year:\")\n",
    "print(df_relevant[\"year\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe95f7",
   "metadata": {},
   "source": [
    "### Geo-location of SE Research Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e76ac",
   "metadata": {},
   "source": [
    "# RQ1 - What Tasks are being tackled in LLM SE studies, and are they fairly evaluated against existing non-LLM techniques?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'task' (short-text)\n",
    "- 'non_llm_approaches' (bool)\n",
    "- 'dataset_type' (short-text)\n",
    "- programming_language (short-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a225d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list\n",
      "code generation                                   26\n",
      "program repair                                    24\n",
      "test generation                                   23\n",
      "vulnerability detection                           16\n",
      "bug detection                                     12\n",
      "code translation                                   8\n",
      "clone detection                                    6\n",
      "code completion                                    6\n",
      "code summarisation                                 6\n",
      "code comprehension                                 5\n",
      "type detection                                     5\n",
      "log parsing                                        5\n",
      "code search                                        5\n",
      "fuzzing                                            4\n",
      "bug reproduction                                   3\n",
      "commit message generation                          3\n",
      "fault localisation                                 3\n",
      "code memorisation detection                        3\n",
      "test repair                                        2\n",
      "formal verification                                2\n",
      "program analysis                                   2\n",
      "traceability link recovery                         2\n",
      "code retrieval                                     2\n",
      "code refinement                                    1\n",
      "regression testing                                 1\n",
      "SO post editing                                    1\n",
      "AI generated code detection                        1\n",
      "security patch detection                           1\n",
      "model completion                                   1\n",
      "configuration validation                           1\n",
      "smart contract auditing                            1\n",
      "UI design repair                                   1\n",
      "detection of code design issues                    1\n",
      "code adaptation                                    1\n",
      "comment repair                                     1\n",
      "bug report comprehension                           1\n",
      "inconsistency prediction in decentralised apps     1\n",
      "machine-generated code detection                   1\n",
      "security injection                                 1\n",
      "code optimisation                                  1\n",
      "root cause analysis                                1\n",
      "privacy inconsistencies detection                  1\n",
      "figurative language detection                      1\n",
      "binary software composition analysis               1\n",
      "API misuse detection                               1\n",
      "program comprehension                              1\n",
      "exception handling recommender                     1\n",
      "static analysis                                    1\n",
      "comment generation                                 1\n",
      "code review                                        1\n",
      "log understanding                                  1\n",
      "emotion-cause extraction                           1\n",
      "code idioms detection                              1\n",
      "log generation                                     1\n",
      "CI/CD workflow generation                          1\n",
      "requirements analysis                              1\n",
      "model extraction                                   1\n",
      "mutant generation                                  1\n",
      "assert generation                                  1\n",
      "SO posts summarisation                             1\n",
      "question answering                                 1\n",
      "patch correctness assessment                       1\n",
      "algorithm classification                           1\n",
      "optimal thread coarsening factor                   1\n",
      "heterogeneous device mapping                       1\n",
      "vulnerability alert prediction                     1\n",
      "vulnerability repair                               1\n",
      "GUI test case migration                            1\n",
      "software effort estimation                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['task_list'] = df_relevant['task'].apply(\n",
    "    lambda x: [task.strip() for task in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(df_relevant['task_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce60bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of number of tasks per paper:\n",
      "num_tasks\n",
      "1     157\n",
      "2      11\n",
      "3       5\n",
      "4       3\n",
      "13      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Papers covering multiple tasks: 20\n",
      "Percentage covering multiple tasks: 11.3%\n"
     ]
    }
   ],
   "source": [
    "# Looking at how many papers cover multiple tasks\n",
    "df_relevant['num_tasks'] = df_relevant['task_list'].apply(len)\n",
    "\n",
    "print(\"Distribution of number of tasks per paper:\")\n",
    "print(df_relevant['num_tasks'].value_counts().sort_index())\n",
    "print(f\"\\nPapers covering multiple tasks: {(df_relevant['num_tasks'] > 1).sum()}\")\n",
    "print(f\"Percentage covering multiple tasks: {(df_relevant['num_tasks'] > 1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9992a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_llm_approaches\n",
      "True                                                                                                                                                                                                              92\n",
      "False                                                                                                                                                                                                             59\n",
      "FALSE (says TransCoder is not llm but a language modeling appraoch using transformers and seq2seq)                                                                                                                 1\n",
      "TRUE (FTLR, COMET, VSM, LSI,ArDoCode)                                                                                                                                                                              1\n",
      "TRUE (AEL, Drain)                                                                                                                                                                                                  1\n",
      "TRUE (Devign, IVDetect both using gated graph NNs)                                                                                                                                                                 1\n",
      "FALSE (because they compare with older transformers)                                                                                                                                                               1\n",
      "TRUE (see Table I: Apollo, REDriver, FixDriver)                                                                                                                                                                    1\n",
      "TRUE (comparison with other 4 fuzzing approaches)                                                                                                                                                                  1\n",
      "TRUE (code generation approaches: RawGPT, CodeT, Reflexion, and FlowGen - their approach. For the evaluations of all approaches, they use ChatGPT.)                                                                1\n",
      "TRUE (we also compare four state-of-the-art (SOTA)\\ncode generation approaches that operate during the decoding process)                                                                                           1\n",
      "TRUE (API-level fuzzers such as Free-\\nFuzz [13], DeepREL [64], NablaFuzz [14], TensorScope [15],\\nand TitanFuzz [21], while also considering model-level fuzzers\\nlike Muffin [18] and FUTURE their approach)     1\n",
      "TRUE (baselines: CodeExecutor, GPT-3.5, A variant of ORCA)                                                                                                                                                         1\n",
      "TRUE (deep code models use in three approaches ALERT, BeamAttack, ITGen for comparison. See Table II.)                                                                                                             1\n",
      "TRUE                                                                                                                                                                                                               1\n",
      "TRUE (QAQA; metamorphic testing for QA software)                                                                                                                                                                   1\n",
      "TRUE (e.g., the KNOD DL-based approach)                                                                                                                                                                            1\n",
      "TRUE (We choose seven baseline techniques. One of them is the Rust\\ncompiler (rustc), since it sometimes offers direct suggestions for\\nmodifying the program to pass compiler checks...)                          1\n",
      "TRUE (RQ4)                                                                                                                                                                                                         1\n",
      "TRUE (Table 3)                                                                                                                                                                                                     1\n",
      "YES (Table 2)                                                                                                                                                                                                      1\n",
      "TRUE (EvoCrash and Copy&Paste -> crash reproduction baselines)                                                                                                                                                     1\n",
      "TRUE (DynaMosa, CODEXONLY)                                                                                                                                                                                         1\n",
      "TRUE (Pyflakes -> static analysis tool, CommitGen -> RNN-based, NNGen -> Information Retrival-based, Fine-tuned CodeT5, Bugsplainer based on CodeT5-60M)                                                           1\n",
      "TRUE (see Table III)                                                                                                                                                                                               1\n",
      "TRUE (Transformers in RQ3)                                                                                                                                                                                         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_combined['non_llm_approaches'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b40c9",
   "metadata": {},
   "source": [
    "# RQ2 - What models are being used?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'models_open_closed' (open/closed/both)\n",
    "- 'num_models' (int)\n",
    "- 'model_families' (list of short text)\n",
    "- 'model_sizes_reported' (NA/none/some/full - currently unfinished)\n",
    "- 'model_scale' (currently unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07775a87",
   "metadata": {},
   "source": [
    "### Open vs. Closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27b5c9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models_open_closed\n",
       "open      71\n",
       "both      65\n",
       "closed    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant['models_open_closed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80376d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open models in 136 out of 177 papers (76.8%)\n",
      "Closed models in 106 out of 177 papers (59.9%)\n"
     ]
    }
   ],
   "source": [
    "total = df_relevant.shape[0]\n",
    "number_open = df_relevant[~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_closed = df_relevant[~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "\n",
    "print(f\"Open models in {number_open} out of {total} papers ({(number_open/total)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed} out of {total} papers ({(number_closed/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda04f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 Papers:\n",
      "Only open models in 22 out of 32 papers (68.8%)\n",
      "Only closed models in 4 out of 32 papers (12.5%)\n",
      "Open models in 28 out of 32 papers (87.5%)\n",
      "Closed models in 10 out of 32 papers (31.2%)\n",
      "Both model types in 6 out of 32 papers (18.8%)\n",
      "\n",
      "2024 Papers:\n",
      "Only open models in 25 out of 55 papers (45.5%)\n",
      "Only closed models in 13 out of 55 papers (23.6%)\n",
      "Open models in 42 out of 55 papers (76.4%)\n",
      "Closed models in 30 out of 55 papers (54.5%)\n",
      "Both model types in 17 out of 55 papers (30.9%)\n",
      "\n",
      "2025 Papers:\n",
      "Only open models in 24 out of 90 papers (26.7%)\n",
      "Only closed models in 24 out of 90 papers (26.7%)\n",
      "Open models in 66 out of 90 papers (73.3%)\n",
      "Closed models in 66 out of 90 papers (73.3%)\n",
      "Both model types in 42 out of 90 papers (46.7%)\n",
      "Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\n"
     ]
    }
   ],
   "source": [
    "total_2023 = df_relevant[df_relevant[\"year\"] == 2023].shape[0]\n",
    "number_open_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2023 = df_relevant[(df_relevant[\"year\"] == 2023) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"2023 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2023} out of {total_2023} papers ({(number_open_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2023} out of {total_2023} papers ({(number_closed_only_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2023} out of {total_2023} papers ({(number_open_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2023} out of {total_2023} papers ({(number_closed_2023/total_2023)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2023} out of {total_2023} papers ({(number_both_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant[\"year\"] == 2024].shape[0]\n",
    "number_open_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2024 = df_relevant[(df_relevant[\"year\"] == 2024) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2024 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2024} out of {total_2024} papers ({(number_open_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2024} out of {total_2024} papers ({(number_closed_only_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2024} out of {total_2024} papers ({(number_open_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2024} out of {total_2024} papers ({(number_closed_2024/total_2024)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2024} out of {total_2024} papers ({(number_both_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant[\"year\"] == 2025].shape[0]\n",
    "number_open_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_open_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & ~(df_relevant['models_open_closed'] == 'open')].shape[0]\n",
    "number_closed_only_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'closed')].shape[0]\n",
    "number_both_2025 = df_relevant[(df_relevant[\"year\"] == 2025) & (df_relevant['models_open_closed'] == 'both')].shape[0]\n",
    "\n",
    "print(f\"\\n2025 Papers:\")\n",
    "print(f\"Only open models in {number_open_only_2025} out of {total_2025} papers ({(number_open_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Only closed models in {number_closed_only_2025} out of {total_2025} papers ({(number_closed_only_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Open models in {number_open_2025} out of {total_2025} papers ({(number_open_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Closed models in {number_closed_2025} out of {total_2025} papers ({(number_closed_2025/total_2025)*100:.1f}%)\")\n",
    "print(f\"Both model types in {number_both_2025} out of {total_2025} papers ({(number_both_2025/total_2025)*100:.1f}%)\")\n",
    "\n",
    "print(\"Concerning Trend: Increasing use of closed models over time! Particularly papers that feature only closed/commercial models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1ba1877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with X models:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_models\n",
       "1     48\n",
       "3     24\n",
       "2     23\n",
       "5     17\n",
       "4     17\n",
       "6     13\n",
       "7      9\n",
       "8      7\n",
       "9      5\n",
       "10     4\n",
       "11     3\n",
       "16     2\n",
       "19     2\n",
       "15     1\n",
       "12     1\n",
       "37     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of papers with X models:\")\n",
    "df_relevant['num_models'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e9b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Paper counts per model family:\n",
      "model_families_list\n",
      "GPT-4                           47\n",
      "GPT-3.5                         44\n",
      "CodeBERT                        34\n",
      "CodeLlama                       26\n",
      "CodeT5                          22\n",
      "CodeGen                         19\n",
      "StarCoder                       18\n",
      "GraphCodeBERT                   18\n",
      "Llama                           17\n",
      "RoBERTa                         15\n",
      "ChatGPT                         14\n",
      "BERT                            13\n",
      "DeepSeekCoder                   12\n",
      "UniXcoder                       11\n",
      "Codex                           10\n",
      "InCoder                          9\n",
      "T5                               8\n",
      "Claude                           7\n",
      "Gemini                           7\n",
      "ChatGLM                          6\n",
      "DeepSeek                         6\n",
      "CodeQwen                         6\n",
      "GPT-3                            6\n",
      "UnixCoder                        6\n",
      "PLBART                           6\n",
      "CodeGPT                          5\n",
      "CodeParrot                       4\n",
      "Copilot                          4\n",
      "WizardCoder                      4\n",
      "GPT-2                            4\n",
      "PolyCoder                        4\n",
      "Mistral                          3\n",
      "Gemma                            3\n",
      "text-davinci                     3\n",
      "Pythia                           3\n",
      "Vicuna                           3\n",
      "DistilBERT                       3\n",
      "LineVul                          3\n",
      "CodeT5+                          3\n",
      "text-embedding                   3\n",
      "TransCoder                       2\n",
      "Codestral                        2\n",
      "Qwen                             2\n",
      "GPT3.5                           2\n",
      "CodeGemma                        2\n",
      "Incoder                          2\n",
      "CodeGeeX                         2\n",
      "VulBERTa                         2\n",
      "CuBERT                           2\n",
      "UniLog                           2\n",
      "BART                             2\n",
      "GPT-J                            2\n",
      "GPT-Neo                          2\n",
      "GPT-NEO                          2\n",
      "GPT-C                            2\n",
      "SynCoBERT                        2\n",
      "CoTexT                           2\n",
      "Phi                              2\n",
      "TFix                             2\n",
      "ALBERT                           2\n",
      "seBERT                           2\n",
      "Code-davinci                     2\n",
      "OpenDevin                        2\n",
      "SantaCoder                       2\n",
      "Longformer                       1\n",
      "CoditT5                          1\n",
      "Unixcoder                        1\n",
      "CodeBert                         1\n",
      "GPT4                             1\n",
      "SVulD                            1\n",
      "Poro                             1\n",
      "ChatDev                          1\n",
      "Self-collaboration               1\n",
      "MetaGPT                          1\n",
      "AutoGPT                          1\n",
      "Multi-Turn Program Synthesis     1\n",
      "AgentCoder                       1\n",
      "DetectGPT                        1\n",
      "GPT-2 Output Detector            1\n",
      "GPTZero                          1\n",
      "GPTSniffer                       1\n",
      "Starcoder                        1\n",
      "LLM-Parser                       1\n",
      "LILAC                            1\n",
      "Repilot                          1\n",
      "RAP-Gen                          1\n",
      "ChatRepair                       1\n",
      "FitRepair                        1\n",
      "AlphaRe-pair                     1\n",
      "Mixtral                          1\n",
      "VGX                              1\n",
      "ReVeal                           1\n",
      "Devign                           1\n",
      "VULGEN                           1\n",
      "COME                             1\n",
      "CCT5                             1\n",
      "NNGen                            1\n",
      "ALL-MINILM-L6-V210               1\n",
      "UniTrans                         1\n",
      "Shipwright                       1\n",
      "MagiCoder                        1\n",
      "Magicoder                        1\n",
      "Parfum                           1\n",
      "TOGA                             1\n",
      "AthenTest                        1\n",
      "SEQ Graph& HYBRID                1\n",
      "AppMap Naive                     1\n",
      "OpenCodeInterpreter              1\n",
      "AutoCodeRover                    1\n",
      "CodeShell                        1\n",
      "LLama                            1\n",
      "CoCoSoDa                         1\n",
      "CodeRetriever                    1\n",
      "HedgeCode                        1\n",
      "SYNCOBERT                        1\n",
      "Vercel                           1\n",
      "GPT-4, CodeBERT                  1\n",
      "Moatless Tools                   1\n",
      "Agentless                        1\n",
      "FuzzGPT                          1\n",
      "TitanFuzz                        1\n",
      "Aider                            1\n",
      "SWE-Agent                        1\n",
      "DeBERTa                          1\n",
      "OPT                              1\n",
      "GPT3-5                           1\n",
      "Tulu                             1\n",
      "Guanaco                          1\n",
      "PaLM                             1\n",
      "StarChat                         1\n",
      "CAT-LM                           1\n",
      "Exlong                           1\n",
      "GLTR                             1\n",
      "ContraBERT                       1\n",
      "ChatUniTest                      1\n",
      "TestGen-LLM                      1\n",
      "RustAssistant                    1\n",
      "Sonnet                           1\n",
      "Stable-Code                      1\n",
      "BigBird                          1\n",
      "Sapling                          1\n",
      "KeyBERT                          1\n",
      "DISCO                            1\n",
      "PDBERT                           1\n",
      "GPTBigCode                       1\n",
      "Sentence-BERT                    1\n",
      "Airboros                         1\n",
      "CodeBERTa                        1\n",
      "Flan                             1\n",
      "code-davinci                     1\n",
      "UnifiedQA                        1\n",
      "VRepair                          1\n",
      "CodeReviewer                     1\n",
      "GrammarT5                        1\n",
      "Transformer                      1\n",
      "LSTM                             1\n",
      "GIN                              1\n",
      "TypeFix                          1\n",
      "PyTER                            1\n",
      "CoCoNuT                          1\n",
      "AlphaRepair                      1\n",
      "LANCE                            1\n",
      "XLNet                            1\n",
      "RepresentThemAll                 1\n",
      "Curie                            1\n",
      "Davinci                          1\n",
      "ELECTRA                          1\n",
      "MiniLM                           1\n",
      "DOBF                             1\n",
      "VulRepair                        1\n",
      "VulMaster                        1\n",
      "PanguCoder                       1\n",
      "flan-alpaca                      1\n",
      "SPT-Code                         1\n",
      "ProphetNet-Code                  1\n",
      "T5-learning                      1\n",
      "JavaBERT                         1\n",
      "DeepDebug                        1\n",
      "C-BERT                           1\n",
      "CugLM                            1\n",
      "TreeBERT                         1\n",
      "PLBart                           1\n",
      "ATLAS                            1\n",
      "CoCoNut                          1\n",
      "Hoppity                          1\n",
      "Sequencer                        1\n",
      "CEDAR                            1\n",
      "Transformers                     1\n",
      "GPT-NeoX                         1\n",
      "FAIR                             1\n",
      "OSCAR                            1\n",
      "Transcoder*                      1\n",
      "IRGen                            1\n",
      "Deep-SE                          1\n",
      "GPT2SP                           1\n",
      "sentenceBERT                     1\n",
      "SDA-Trans                        1\n",
      "StableCode                       1\n",
      "CodeGen-NL                       1\n",
      "CodeGen-Mono                     1\n",
      "CodeGen-Multi                    1\n",
      "CodeGen2                         1\n",
      "PyCodeGPT                        1\n",
      "GPT-Code-Clippy                  1\n",
      "BERTOverflow                     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_relevant['model_families_list'] = df_relevant['model_families'].apply(\n",
    "    lambda x: [model_family.strip() for model_family in str(x).split(';')] if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "print(\"Overall Paper counts per model family:\")\n",
    "print(df_relevant['model_families_list'].explode().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7389b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_families_list\n",
       "CodeBERT            11\n",
       "RoBERTa              8\n",
       "BERT                 7\n",
       "CodeT5               7\n",
       "Codex                5\n",
       "T5                   5\n",
       "GraphCodeBERT        4\n",
       "DistilBERT           3\n",
       "PLBART               2\n",
       "GPT-J                2\n",
       "CodeGen              2\n",
       "BART                 2\n",
       "InCoder              2\n",
       "GPT-Neo              2\n",
       "XLNet                1\n",
       "MiniLM               1\n",
       "ELECTRA              1\n",
       "ALBERT               1\n",
       "RepresentThemAll     1\n",
       "seBERT               1\n",
       "Code-davinci         1\n",
       "Curie                1\n",
       "Davinci              1\n",
       "T5-learning          1\n",
       "CodeGPT              1\n",
       "JavaBERT             1\n",
       "DOBF                 1\n",
       "CuBERT               1\n",
       "ProphetNet-Code      1\n",
       "SPT-Code             1\n",
       "CoTexT               1\n",
       "C-BERT               1\n",
       "GPT-C                1\n",
       "CugLM                1\n",
       "TreeBERT             1\n",
       "GPT-2                1\n",
       "SynCoBERT            1\n",
       "DeepDebug            1\n",
       "UniXcoder            1\n",
       "GPT-NeoX             1\n",
       "PLBart               1\n",
       "GPT-3                1\n",
       "CodeParrot           1\n",
       "Copilot              1\n",
       "ATLAS                1\n",
       "CoCoNut              1\n",
       "Hoppity              1\n",
       "Sequencer            1\n",
       "TFix                 1\n",
       "CEDAR                1\n",
       "Transformers         1\n",
       "TransCoder           1\n",
       "Transcoder*          1\n",
       "SDA-Trans            1\n",
       "BERTOverflow         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relevant[df_relevant['year'] == 2023]['model_families_list'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94f1a0",
   "metadata": {},
   "source": [
    "# RQ3 - How well do authors tackle the problem of data leakage/contamination?\n",
    "\n",
    "DF Columns to use:\n",
    "- contamination (bool)\n",
    "- contamination_free_text (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "547d1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All years: Contamination reported in 58 out of 177 papers (32.8%)\n"
     ]
    }
   ],
   "source": [
    "# Overall\n",
    "total = df_relevant.shape[0]\n",
    "contamination_reported = df_relevant['contamination'].sum()\n",
    "\n",
    "print(f\"All years: Contamination reported in {contamination_reported} out of {total} papers ({(contamination_reported/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb11e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: Contamination reported in 6 out of 32 papers (18.8%)\n",
      "2024: Contamination reported in 14 out of 55 papers (25.5%)\n",
      "2025: Contamination reported in 38 out of 90 papers (42.2%)\n"
     ]
    }
   ],
   "source": [
    "# Per Year\n",
    "total_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "contamination_2023 = df_relevant[df_relevant['year'] == 2023]['contamination'].sum()\n",
    "print(f\"2023: Contamination reported in {contamination_2023} out of {total_2023} papers ({(contamination_2023/total_2023)*100:.1f}%)\")\n",
    "\n",
    "total_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "contamination_2024 = df_relevant[df_relevant['year'] == 2024]['contamination'].sum()\n",
    "print(f\"2024: Contamination reported in {contamination_2024} out of {total_2024} papers ({(contamination_2024/total_2024)*100:.1f}%)\")\n",
    "\n",
    "total_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "contamination_2025 = df_relevant[df_relevant['year'] == 2025]['contamination'].sum()\n",
    "print(f\"2025: Contamination reported in {contamination_2025} out of {total_2025} papers ({(contamination_2025/total_2025)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfc14a",
   "metadata": {},
   "source": [
    "# RQ4 - How replicable are LLM-based studies?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'model_config' (short-text list)\n",
    "- 'artifact_available' (bool)\n",
    "- 'artifact_reusable' (bool)\n",
    "- 'artifact_functional' (bool)\n",
    "- 'artefact_manual' (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d775ea",
   "metadata": {},
   "source": [
    "## ACM Badge Artifact Availability - Relevant vs. Non-Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96259443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of relevant papers with artifact available: 18.64% (33/177)\n",
      "Proportion of other papers with artifact available: 41.36% (213/515)\n"
     ]
    }
   ],
   "source": [
    "# Calculate proportions in the combined dataset\n",
    "total_relevant = df_relevant.shape[0]\n",
    "total_non_relevant = df_non_relevant.shape[0]\n",
    "\n",
    "relevant_with_artifact = df_relevant['artifact_available'].sum()\n",
    "non_relevant_with_artifact = df_non_relevant['artifact_available'].sum()\n",
    "\n",
    "prop_relevant = relevant_with_artifact / total_relevant\n",
    "prop_non_relevant = non_relevant_with_artifact / total_non_relevant\n",
    "\n",
    "print(f\"Proportion of relevant papers with artifact available: {prop_relevant:.2%} ({relevant_with_artifact}/{total_relevant})\")\n",
    "print(f\"Proportion of other papers with artifact available: {prop_non_relevant:.2%} ({non_relevant_with_artifact}/{total_non_relevant})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637d335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 - Proportion of relevant papers with artifact available: 18.75% (6/32)\n",
      "2024 - Proportion of relevant papers with artifact available: 16.36% (9/55)\n",
      "2025 - Proportion of relevant papers with artifact available: 20.00% (18/90)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of relevant papers with artifacts available per year\n",
    "total_relevant_2023 = df_relevant[df_relevant['year'] == 2023].shape[0]\n",
    "artifact_avail_2023 = df_relevant[(df_relevant['year'] == 2023) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2023 = artifact_avail_2023 / total_relevant_2023\n",
    "print(f\"2023 - Proportion of relevant papers with artifact available: {prop_relevant_2023:.2%} ({artifact_avail_2023}/{total_relevant_2023})\")\n",
    "\n",
    "total_relevant_2024 = df_relevant[df_relevant['year'] == 2024].shape[0]\n",
    "artifact_avail_2024 = df_relevant[(df_relevant['year'] == 2024) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2024 = artifact_avail_2024 / total_relevant_2024\n",
    "print(f\"2024 - Proportion of relevant papers with artifact available: {prop_relevant_2024:.2%} ({artifact_avail_2024}/{total_relevant_2024})\")\n",
    "\n",
    "total_relevant_2025 = df_relevant[df_relevant['year'] == 2025].shape[0]\n",
    "artifact_avail_2025 = df_relevant[(df_relevant['year'] == 2025) & (df_relevant['artifact_available'])].shape[0]\n",
    "prop_relevant_2025 = artifact_avail_2025 / total_relevant_2025\n",
    "print(f\"2025 - Proportion of relevant papers with artifact available: {prop_relevant_2025:.2%} ({artifact_avail_2025}/{total_relevant_2025})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e39d00",
   "metadata": {},
   "source": [
    "## Manual Artefact Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62cde1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artefact_manual\n",
      "True     144\n",
      "False     24\n",
      "DEAD       9\n",
      "Name: count, dtype: int64\n",
      "10 instances of dead links in the 173 relevant papers\n",
      "2023 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     24\n",
      "False     5\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2024 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     44\n",
      "False     8\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2025 Artifacts (Manual Checking)\n",
      "artefact_manual\n",
      "True     76\n",
      "False    11\n",
      "DEAD      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_relevant[\"artefact_manual\"].value_counts())\n",
    "print(\"10 instances of dead links in the 173 relevant papers\")\n",
    "\n",
    "# Per year\n",
    "print(\"2023 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2023]['artefact_manual'].value_counts())\n",
    "print(\"\\n2024 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2024]['artefact_manual'].value_counts())\n",
    "print(\"\\n2025 Artifacts (Manual Checking)\")\n",
    "print(df_relevant[df_relevant['year'] == 2025]['artefact_manual'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518a064",
   "metadata": {},
   "source": [
    "# RQ5 - How sustainable is LLM-based SE research?\n",
    "\n",
    "DF Columns to use:\n",
    "- 'cost' (short-text list)\n",
    "- 'cost_free_text' (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb184c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
