@inproceedings{10.1109/ICSE48619.2023.00094,
author = {Lyu, Yunbo and Le-Cong, Thanh and Kang, Hong Jin and Widyasari, Ratnadira and Zhao, Zhipeng and Le, Xuan-Bach D. and Li, Ming and Lo, David},
title = {Chronos: Time-Aware Zero-Shot Identification of Libraries from Vulnerability Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00094},
doi = {10.1109/ICSE48619.2023.00094},
abstract = {Tools that alert developers about library vulnerabilities depend on accurate, up-to-date vulnerability databases which are maintained by security researchers. These databases record the libraries related to each vulnerability. However, the vulnerability reports may not explicitly list every library and human analysis is required to determine all the relevant libraries. Human analysis may be slow and expensive, which motivates the need for automated approaches. Researchers and practitioners have proposed to automatically identify libraries from vulnerability reports using extreme multi-label learning (XML).While state-of-the-art XML techniques showed promising performance, their experimental settings do not practically fit what happens in reality. Previous studies randomly split the vulnerability reports data for training and testing their models without considering the chronological order of the reports. This may unduly train the models on chronologically newer reports while testing the models on chronologically older ones. However, in practice, one often receives chronologically new reports, which may be related to previously unseen libraries. Under this practical setting, we observe that the performance of current XML techniques declines substantially, e.g., F1 decreased from 0.7 to 0.24 under experiments without and with consideration of chronological order of vulnerability reports.We propose a practical library identification approach, namely Chronos, based on zero-shot learning. The novelty of Chronos is three-fold. First, Chronos fits into the practical pipeline by considering the chronological order of vulnerability reports. Second, Chronos enriches the data of the vulnerability descriptions and labels using a carefully designed data enhancement step. Third, Chronos exploits the temporal ordering of the vulnerability reports using a cache to prioritize prediction of versions of libraries that recently had reports of vulnerabilities.In our experiments, Chronos achieves an average F1-score of 0.75, 3x better than the best XML-based approach. Data enhancement and the time-aware adjustment improve Chronos over the vanilla zero-shot learning model by 27% in average F1.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1033–1045},
numpages = {13},
keywords = {zero-shot learning, library identification, unseen labels, extreme multi-label classification, vulnerability reports},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00217,
author = {Christian, Garrett and Woodlief, Trey and Elbaum, Sebastian},
title = {Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00217},
doi = {10.1109/ICSE48619.2023.00217},
abstract = {Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2604–2616},
numpages = {13},
keywords = {software testing and validation, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00166,
author = {Mansur, S M Hasan and Salma, Sabiha and Awofisayo, Damilola and Moran, Kevin},
title = {AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00166},
doi = {10.1109/ICSE48619.2023.00166},
abstract = {Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability.In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1958–1970},
numpages = {13},
keywords = {dark pattern, UI analysis, UI design},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00095,
author = {Wu, Yulun and Yu, Zeliang and Wen, Ming and Li, Qiang and Zou, Deqing and Jin, Hai},
title = {Understanding the Threats of Upstream Vulnerabilities to Downstream Projects in the Maven Ecosystem},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00095},
doi = {10.1109/ICSE48619.2023.00095},
abstract = {Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales.This study aims to bridge such gaps. In particular, we collect 44,450 instances of 〈CVE, upstream, downstream〉 relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reachability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1046–1058},
numpages = {13},
keywords = {maven, ecosystem security, vulnerability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00051,
author = {Yan, Yutian and Zheng, Yunhui and Liu, Xinyue and Medvidovic, Nenad and Wang, Weihang},
title = {AdHere: Automated Detection and Repair of Intrusive Ads},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00051},
doi = {10.1109/ICSE48619.2023.00051},
abstract = {Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose AdHere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. AdHere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, AdHere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate AdHere on Alexa Top 1 Million Websites. The results show that AdHere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, AdHere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {486–498},
numpages = {13},
keywords = {ad experience, advertising practice, better ads standards},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00170,
author = {Yan, Yanyan and Feng, Yang and Fan, Hongcheng and Xu, Baowen},
title = {DLInfer: Deep Learning with Static Slicing for Python Type Inference},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00170},
doi = {10.1109/ICSE48619.2023.00170},
abstract = {Python programming language has gained enormous popularity in the past decades. While its flexibility significantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors.In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type information for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2009–2021},
numpages = {13},
keywords = {type inference, Python, static slicing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00031,
author = {Dann, Andreas and Hermann, Ben and Bodden, Eric},
title = {UpCy: Safely Updating Outdated Dependencies},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00031},
doi = {10.1109/ICSE48619.2023.00031},
abstract = {Recent research has shown that developers hesitate to update dependencies and mistrust automated approaches such as Dependabot, since they are afraid of introducing incompatibilities that break their project. In fact, such approaches only suggest na\"{\i}ve updates for a single outdated library but do not ensure compatibility with other dependent libraries in the project. To alleviate this situation and support developers in finding updates with minimal incompatibilities, we present UpCy. UpCy applies the min-(s,t)-cut algorithm and leverages a graph database of Maven Central to identify a list of valid update steps to update a dependency to a target version while minimizing incompatibilities with other libraries. By executing 29,698 updates in 380 projects, we compare the effectiveness of UpCy with the na\"{\i}ve updates applied by state-of-the-art tools. We find that in 41.1% of the cases where the na\"{\i}ve approach fails UpCy generates updates with fewer incompatibilities, and even 70.1% of the generated updates have zero incompatibilities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {233–244},
numpages = {12},
keywords = {semantic versioning, library updates, package management, dependency management, software maintenance},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00040,
author = {Liu, Jiahao and Zeng, Jun and Wang, Xiang and Liang, Zhenkai},
title = {Learning Graph-Based Code Representations for Source-Level Functional Similarity Detection},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00040},
doi = {10.1109/ICSE48619.2023.00040},
abstract = {Detecting code functional similarity forms the basis of various software engineering tasks. However, the detection is challenging as functionally similar code fragments can be implemented differently, e.g., with irrelevant syntax. Recent studies incorporate program dependencies as semantics to identify syntactically different yet semantically similar programs, but they often focus only on local neighborhoods (e.g., one-hop dependencies), limiting the expressiveness of program semantics in modeling functionalities. In this paper, we present Tailor that explicitly exploits deep graph-structured code features for functional similarity detection. Given source-level programs, Tailor first represents them into code property graphs (CPGs) --- which combine abstract syntax trees, control flow graphs, and data flow graphs --- to collectively reason about program syntax and semantics. Then, Tailor learns representations of CPGs by applying a CPG-based neural network (CPGNN) to iteratively propagate information on them. It improves over prior work on code representation learning through a new graph neural network (GNN) tailored to CPG structures instead of the off-the-shelf GNNs used previously. We systematically evaluate Tailor on C and Java programs using two public benchmarks. Experimental results show that Tailor outperforms the state-of-the-art approaches, achieving 99.8% and 99.9% F-scores in code clone detection and 98.3% accuracy in source code classification.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {345–357},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00175,
author = {Song, Jiansen and Dou, Wensheng and Cui, Ziyu and Dai, Qianwang and Wang, Wei and Wei, Jun and Zhong, Hua and Huang, Tao},
title = {Testing Database Systems via Differential Query Execution},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00175},
doi = {10.1109/ICSE48619.2023.00175},
abstract = {Database Management Systems (DBMSs) provide efficient data retrieval and manipulation for many applications through Structured Query Language (SQL). Incorrect implementations of DBMSs can result in logic bugs, which cause SELECT queries to fetch incorrect results, or UPDATE and DELETE queries to generate incorrect database states. Existing approaches mainly focus on detecting logic bugs in SELECT queries. However, logic bugs in UPDATE and DELETE queries have not been tackled.In this paper, we propose a novel and general approach, which we have termed Differential Query Execution (DQE), to detect logic bugs in SELECT, UPDATE and DELETE queries of DBMSs. The core idea of DQE is that different SQL queries with the same predicate usually access the same rows in a database. For example, a row updated by an UPDATE query with a predicate ϕ should also be fetched by a SELECT query with the same predicate ϕ. If not, a logic bug is revealed in the target DBMS. To evaluate the effectiveness and generality of DQE, we apply DQE on five production-level DBMSs, i.e., MySQL, MariaDB, TiDB, CockroachDB and SQLite. In total, we have detected 50 unique bugs in these DBMSs, 41 of which have been confirmed, and 11 have been fixed. We expect that the simplicity and generality of DQE can greatly improve the reliability of DBMSs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2072–2084},
numpages = {13},
keywords = {database system, DBMS testing, logic bug},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00161,
author = {Nam, Daye and Myers, Brad and Vasilescu, Bogdan and Hellendoorn, Vincent},
title = {Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00161},
doi = {10.1109/ICSE48619.2023.00161},
abstract = {Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-learning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1890–1906},
numpages = {17},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00144,
author = {Jin, Wuxia and Dai, Yitong and Zheng, Jianguo and Qu, Yu and Fan, Ming and Huang, Zhenyu and Huang, Dezhi and Liu, Ting},
title = {Dependency Facade: The Coupling and Conflicts between Android Framework and Its Customization},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00144},
doi = {10.1109/ICSE48619.2023.00144},
abstract = {Mobile device vendors develop their customized Android OS (termed downstream) based on Google Android (termed upstream) to support new features. During daily independent development, the downstream also periodically merges changes of a new release from the upstream into its development branches, keeping in sync with the upstream. Due to a large number of commits to be merged, heavy code conflicts would be reported if auto-merge operations failed. Prior work has studied conflicts in this scenario. However, it is still unclear about the coupling between the downstream and the upstream (We term this coupling as the dependency facade), as well as how merge conflicts are related to this coupling. To address this issue, we first propose the DepFCD to reveal the dependency facade from three aspects, including interface-level dependencies that indicate a clear design boundary, intrusion-level dependencies which blur the boundary, and dependency constraints imposed by the upstream non-SDK restrictions. We then empirically investigate these three aspects (RQ1, RQ2, RQ3) and merge conflicts (RQ4) on the dependency facade. To support the study, we collect four open-source downstream projects and one industrial project, with 15 downstream and 15 corresponding upstream versions. Our study reveals interesting observations and suggests earlier mitigation of merge conflicts through a well-managed dependency facade. Our study will benefit the research about the coupling between upstream and downstream as well as the downstream maintenance practice.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1674–1686},
numpages = {13},
keywords = {android, downstream, dependencies, merge conflict},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00133,
author = {Gohar, Usman and Biswas, Sumon and Rajan, Hridesh},
title = {Towards Understanding Fairness and its Composition in Ensemble Machine Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00133},
doi = {10.1109/ICSE48619.2023.00133},
abstract = {Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1533–1545},
numpages = {13},
keywords = {fairness, ensemble, machine learning, models},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00125,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%--1,267% improvement to CLMs and enables them to fix 46%--164% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430–1442},
numpages = {13},
keywords = {automated program repair, code language model, fine-tuning, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

