@inproceedings{10.1145/3650105.3652299,
author = {Li, Junjie and Sangalay, Aseem and Cheng, Cheng and Tian, Yuan and Yang, Jinqiu},
title = {Fine Tuning Large Language Model for Secure Code Generation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652299},
doi = {10.1145/3650105.3652299},
abstract = {AI pair programmers, such as GitHub's Copilot, have shown great success in automatic code generation. However, such large language model-based code generation techniques face the risk of introducing security vulnerabilities to codebases. In this work, we explore the direction of fine-tuning large language models for generating more secure code. We use real-world vulnerability fixes as our fine-tuning dataset. We craft a code-generation scenario dataset (C/C++) for evaluating and comparing the pre-trained and fine-tuned models. Our experiments on GPT-J show that the fine-tuned GPT-J achieved 70.4% and 64.5% ratios of non-vulnerable code generation for C and C++, respectively, which has a 10% increase for C and a slight increase for C++ compared with the pre-trained large language model.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {86–90},
numpages = {5},
keywords = {code generation, cybersecurity, artificial intelligence (AI), common weakness enumerations (CWEs)},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1109/NSE66660.2025.00010,
author = {Voboril, Florentina and Ramaswamy, Vaidyanathan Peruvemba and Szeider, Stefan},
title = {StreamLLM: Enhancing Constraint Programming with Large Language Model-Generated Streamliners},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NSE66660.2025.00010},
doi = {10.1109/NSE66660.2025.00010},
abstract = {This paper introduces StreamLLM, a method that uses Large Language Models (LLMs) to generate streamliners for constraint programming. Streamliners narrow the search space to improve the efficiency of solving complex problems but typically require extensive manual design or exhaustive testing. StreamLLM instead leverages LLMs to propose effective streamliners dynamically, incorporating realtime feedback and empirical tests within the MiniZinc modeling language. Evaluated across six diverse constraint satisfaction problems, StreamLLM demonstrates substantial runtime reductions, up to 99% improvement in some cases. This work highlights the potential of combining symbolic reasoning with machine learning techniques to enhance constraint-solving speed and adaptability.},
booktitle = {2025 IEEE/ACM 1st International Workshop on Neuro-Symbolic Software Engineering (NSE)},
pages = {17–22},
numpages = {6},
location = {Ottawa, ON, Canada}
}

@inproceedings{10.1145/3639476.3639773,
author = {Wang, Jiabo and Chu, Guojun and Wang, Jingyu and Sun, Haifeng and Qi, Qi and Wang, Yuanyi and Qi, Ji and Liao, Jianxin},
title = {LogExpert: Log-based Recommended Resolutions Generation using Large Language Model},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639773},
doi = {10.1145/3639476.3639773},
abstract = {Software logs play a vital role in ensuring the reliability and availability of large-scale software systems. In recent years, researchers have made significant efforts to build log analysis approaches to manage software systems. However, these approaches focus on log compression, log parsing and log anomaly detection. In the current context, engineers continue to spend substantial time and effort on resolving errors once anomalous logs have been detected. To achieve truly automated software system management and high-level Artificial Intelligence for IT Operations (AIOps), it's necessary to bridge the gap between anomalous logs and their resolutions.In this paper, we propose a novel framework LogExpert to automatically generate recommended resolutions for anomalous logs. Specifically, we build a log recognizer to utilize the wealth of software knowledge in technical forums such as Stack Overflow (SO). In addition, LogExpert combines the great power of a Large Language Model (LLM) with domain-specific knowledge to generate the resolution. We conducted a preliminary evaluation of our framework on datasets from SO. Our log recognizer achieves the F1 score of 0.936. Our lexical metrics and human evaluation show the overall LogExpert framework achieves excellent performance in log-based resolution generation.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {42–46},
numpages = {5},
keywords = {log-based resolution generation, log anomaly detection, large language models, Stack Overflow},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3643916.3644411,
author = {Ciniselli, Matteo and Martin-Lopez, Alberto and Bavota, Gabriele},
title = {On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644411},
doi = {10.1145/3643916.3644411},
abstract = {Code completion is a key feature of Integrated Development Environments (IDEs), aimed at predicting the next tokens a developer is likely to write, helping them write code faster and with less effort. Modern code completion approaches are often powered by deep learning (DL) models. However, the swift evolution of programming languages poses a critical challenge to the performance of DL-based code completion models: Can these models generalize across different language versions? This paper delves into such a question. In particular, we assess the capabilities of a state-of-the-art model, CodeT5, to generalize across nine different Java versions, ranging from Java 2 to Java 17, while being exclusively trained on Java 8 code. Our evaluation spans three completion scenarios, namely, predicting tokens, constructs (e.g., the condition of an if statement) and entire code blocks. The results of our study reveal a noticeable disparity among language versions, with the worst performance being obtained in Java 2 and 17---the most far apart versions compared to Java 8. We investigate possible causes for the performance degradation and show that the adoption of a limited version-specific fine-tuning can partially alleviate the problem. Our work raises awareness on the importance of continuous model refinement, and it can inform the design of alternatives to make code completion models more robust to language evolution.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {99–111},
numpages = {13},
keywords = {code completion, empirical software engineering},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639477.3639719,
author = {Di, Peng and Li, Jianguo and Yu, Hang and Jiang, Wei and Cai, Wenting and Cao, Yang and Chen, Chaoyu and Chen, Dajun and Chen, Hongwei and Chen, Liang and Fan, Gang and Gong, Jie and Gong, Zi and Hu, Wen and Guo, Tingting and Lei, Zhichao and Li, Ting and Li, Zheng and Liang, Ming and Liao, Cong and Liu, Bingchang and Liu, Jiachen and Liu, Zhiwei and Lu, Shaojun and Shen, Min and Wang, Guangpei and Wang, Huan and Wang, Zhi and Xu, Zhaogui and Yang, Jiawei and Ye, Qing and Zhang, Gehao and Zhang, Yu and Zhao, Zelin and Zheng, Xunjin and Zhou, Hailian and Zhu, Lifu and Zhu, Xianying},
title = {CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639719},
doi = {10.1145/3639477.3639719},
abstract = {Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM 2. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high-quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {418–429},
numpages = {12},
keywords = {code large language models, multi-lingual, chinese prompts},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639476.3639762,
author = {Zhou, Xin and Zhang, Ting and Lo, David},
title = {Large Language Model for Vulnerability Detection: Emerging Results and Future Directions},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639762},
doi = {10.1145/3639476.3639762},
abstract = {Previous learning-based vulnerability detection methods relied on either medium-sized pre-trained models or smaller neural networks from scratch. Recent advancements in Large Pre-Trained Language Models (LLMs) have showcased remarkable few-shot learning capabilities in various tasks. However, the effectiveness of LLMs in detecting software vulnerabilities is largely unexplored. This paper aims to bridge this gap by exploring how LLMs perform with various prompts, particularly focusing on two state-of-the-art LLMs: GPT-3.5 and GPT-4. Our experimental results showed that GPT-3.5 achieves competitive performance with the prior state-of-the-art vulnerability detection approach and GPT-4 consistently outperformed the state-of-the-art.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {47–51},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639157,
author = {Su, Yanqi and Liao, Dianshu and Xing, Zhenchang and Huang, Qing and Xie, Mulong and Lu, Qinghua and Xu, Xiwei},
title = {Enhancing Exploratory Testing by Large Language Model and Knowledge Graph},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639157},
doi = {10.1145/3597503.3639157},
abstract = {Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {98},
numpages = {12},
keywords = {exploratory testing, knowledge graph, AI chain, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644933,
author = {Oishwee, Sahrima Jannat and Stakhanova, Natalia and Codabux, Zadia},
title = {Large Language Model vs. Stack Overflow in Addressing Android Permission Related Challenges},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644933},
doi = {10.1145/3643991.3644933},
abstract = {The Android permission system regulates access to sensitive mobile device resources such as camera and location. To access these resources, third-party developers need to request permissions. However, the Android permission system is complex and fast-evolving, presenting developers with numerous challenges surrounding compatibility issues, misuse of permissions, and vulnerabilities related to permissions. Our study aims to explore whether Large Language Models (LLMs) can serve as a reliable tool to assist developers in using Android permissions correctly and securely, thereby reducing the risks of misuse and security vulnerabilities in apps. In our study, we analyzed 1,008 Stack Overflow questions related to Android permissions and their accepted answers. In parallel, we generate answers to these questions using a popular LLM tool, ChatGPT. We focused on how well the ChatGPT's responses align with the accepted answers on Stack Overflow. Our findings show that above 50% of ChatGPT's answers align with Stack Overflow's accepted answers. ChatGPT offers better-aligned responses for challenges related to Documentation and Conceptual Understanding, while it provides less aligned answers for Debugging-related issues. In addition, we found that ChatGPT provides more consistent answers for 73.27% questions. Our study demonstrates the potential for using LLMs such as ChatGPT as a supporting tool to help developers navigate Android permission-related problems.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {373–383},
numpages = {11},
keywords = {Android permissions, stack overflow, large language model (LLM)},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643788.3648020,
author = {Jiang, Nan and Wu, Yi},
title = {RepairCAT: Applying Large Language Model to Fix Bugs in AI-Generated Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648020},
doi = {10.1145/3643788.3648020},
abstract = {Automated program repair has been a crucial and popular domain for years, and with the development of large language models (LLMs) and the trend of using LLMs for code generation, there comes the new challenge of fixing bugs in LLM-generated (AI-generated) programs. In this work, we introduce RepairCAT, a simple and neat framework for fine-tuning large language models for automated repairing Python programs. Our experiments built on StarCoder-1B successfully generated patches fixing the failed test cases for 14 out of 100 bugs in the Python programs, 2 of which passed all the public test cases and were considered plausible.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {58–60},
numpages = {3},
keywords = {automated program repair, large language model},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3650105.3652298,
author = {Katzy, Jonathan and Popescu, Razvan and Van Deursen, Arie and Izadi, Maliheh},
title = {An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652298},
doi = {10.1145/3650105.3652298},
abstract = {Does the training of large language models potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of large language models. Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 large language models trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code.Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in large language models trained on code, our recommendation for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {74–85},
numpages = {12},
keywords = {large language models, foundation models, code licensing, software engineering, ML4SE, machine learning, datasets},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3597503.3639118,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Tian, Zhilin and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639118},
doi = {10.1145/3597503.3639118},
abstract = {Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% bug detection rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {137},
numpages = {12},
keywords = {Android GUI testing, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643796.3648446,
author = {Semenkin, Anton and Sokolov, Yaroslav and Vu, Evgeniia},
title = {Context Composing for Full Line Code Completion},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648446},
doi = {10.1145/3643796.3648446},
abstract = {Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the Transformer model that is a core of the feature's implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {15–17},
numpages = {3},
keywords = {code completion, transformers, context composing, prompt engineering, integrated development environment, programming, artificial intelligence},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643916.3644421,
author = {Zhu, Tingwei and Liu, Zhongxin and Xu, Tongtong and Tang, Ze and Zhang, Tian and Pan, Minxue and Xia, Xin},
title = {Exploring and Improving Code Completion for Test Code},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644421},
doi = {10.1145/3643916.3644421},
abstract = {Code completion is an important feature in Integrated Development Environments (IDEs). These years, researchers have been making efforts for intelligent code completion. However, existing work on intelligent code completion either only considered production code, or did not distinguish between production code and test code. It is unclear how effective existing completion models are for test code completion, nor whether we can further improve it. In this work, we focus on the completion of test code. We first find through experiments that completion models for production code are suboptimal for test code completion. Then we analyze the specific characteristics of test code, and observe that test code has inter- and intra-project similarities, and a strong relationship with its focal class and other production classes depending on the focal class (i.e., focal-related code). By incorporating test code from other projects to fine-tune existing models, we leverage the inter-project similarity of test code to improve the completion of tokens specific to test code. By introducing a local component and constructing existing test code as well as the focal-related code in the project as references, we enhance existing code completion models with the intra-project similarity and the focal-related code of test code. Experiments show that each characteristic of test code we exploit can bring substantial improvement to test code completion and our integrated framework outperforms other baseline frameworks. Compared to the base completion model, on token-level completion, our optimal model for test code completion relatively improves all-token and identifier completion accuracy by 7.68% and 19.96%, respectively; on line-level completion, it relatively improves edit-distance similarity and exact-match metrics by 8.89% and 22.82%, respectively. Moreover, we perform error analysis and point out potential directions for future work.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {137–148},
numpages = {12},
keywords = {code completion, test code, retrieval augmentation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1109/ICSE48619.2023.00110,
author = {Li, Zongjie and Wang, Chaozheng and Liu, Zhibo and Wang, Haoxuan and Chen, Dong and Wang, Shuai and Gao, Cuiyun},
title = {CCTest: Testing and Repairing Code Completion Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00110},
doi = {10.1109/ICSE48619.2023.00110},
abstract = {Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems.In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTest features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTest repairs the code completion outputs by selecting the output that mostly reflects the "average" appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1238–1250},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643691.3648588,
author = {Sikand, Samarth and Mehra, Rohit and Sharma, Vibhu Saujanya and Kaulgud, Vikrant and Podder, Sanjay and Burden, Adam P.},
title = {Do Generative AI Tools Ensure Green Code? An Investigative Study},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648588},
doi = {10.1145/3643691.3648588},
abstract = {Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or 'greenness' of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools --- ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {52–55},
numpages = {4},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1145/3597503.3639111,
author = {Dolata, Mateusz and Lange, Norbert and Schwabe, Gerhard},
title = {Development in times of hype: How freelancers explore Generative AI?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639111},
doi = {10.1145/3597503.3639111},
abstract = {The rise of generative AI has led many companies to hire freelancers to harness its potential. However, this technology presents unique challenges to developers who have not previously engaged with it. Freelancers may find these challenges daunting due to the absence of organizational support and their reliance on positive client feedback. In a study involving 52 freelance developers, we identified multiple challenges associated with developing solutions based on generative AI. Freelancers often struggle with aspects they perceive as unique to generative AI such as unpredictability of its output, the occurrence of hallucinations, and the inconsistent effort required due to trial-and-error prompting cycles. Further, the limitations of specific frameworks, such as token limits and long response times, add to the complexity. Hype-related issues, such as inflated client expectations and a rapidly evolving technological ecosystem, further exacerbate the difficulties. To address these issues, we propose Software Engineering for Generative AI (SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the software engineering community can provide effective guidance. This support is essential for freelancers working with generative AI and other emerging technologies.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {183},
numpages = {13},
keywords = {generative AI, AI-based systems, challenges, freelancers, hype, SE for generative AI, SE4GenAI, hype-induced SE, hype-SE, fashion, product, paradigm, novelty, qualitative research},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00089,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Ni, Mingze and Li, Li},
title = {Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00089},
doi = {10.1109/ICSE-Companion58688.2023.00089},
abstract = {Currently, large pre-trained language models are widely applied in neural code completion systems. Though large code models significantly outperform their smaller counterparts, around 70% of displayed code completions from Copilot are not accepted by developers. Being reviewed but not accepted, their help to developer productivity is considerably limited. Even worse, considering the high cost of the large code models, it is a huge waste of computing resources and energy. To fill this significant gap, we propose an early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities without sending them to the code completion system. Furthermore, we propose a lightweight Transformer-based estimator to demonstrate the feasibility of the mechanism. The experimental results show that the proposed estimator helps save 23.3% of computational cost measured in floating-point operations for the code completion systems, and 80.2% of rejected prompts lead to unhelpful completion.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {324–325},
numpages = {2},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639138,
author = {Izadi, Maliheh and Katzy, Jonathan and Van Dam, Tim and Otten, Marc and Popescu, Razvan Mihai and Van Deursen, Arie},
title = {Language Models for Code Completion: A Practical Evaluation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639138},
doi = {10.1145/3597503.3639138},
abstract = {Transformer-based language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models' performance in online and offline settings was also performed, using benchmark synthetic datasets and two masking strategies.Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the models' predictions, we found that 66.3% of failures were due to models' limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {79},
numpages = {13},
keywords = {automatic code completion, transformers, language models, IDE, evaluation, open source, InCoder, UniXcoder, CodeGPT},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643113,
author = {Du, Xiaoting and Li, Chenglong and Ma, Xiangyue and Zheng, Zheng},
title = {How Does Pre-trained Language Model Perform on Deep Learning Framework Bug Prediction?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643113},
doi = {10.1145/3639478.3643113},
abstract = {Understanding and predicting bugs is crucial for developers seeking to enhance testing efficiency and mitigate issues in software releases. Bug reports, though semi-structured texts, contain a wealth of semantic information, rendering their comprehension a critical aspect of bug prediction. In light of the recent success of pre-trained language models (PLMs) in the domain of natural language processing, numerous studies have leveraged these models to grasp various forms of textual information. However, the capability of PLMs to understand bug reports remains uncertain. To tackle this challenge, we introduce KnowBug, a framework with a bug report knowledge-enhanced PLM. In this framework, utilizing bug reports obtained from open-source deep learning frameworks as input, prompts are designed and the PLM is fine-tuned for evaluating KnowBug's ability to comprehend bug reports and predict bug types.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {346–347},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644420,
author = {Xia, Yu and Liang, Tian and Min, Weihuan and Kuang, Li},
title = {Improving AST-Level Code Completion with Graph Retrieval and Multi-Field Attention},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644420},
doi = {10.1145/3643916.3644420},
abstract = {Code completion, which provides code suggestions by generating code snippets or structures, has become an essential feature of integrated development environments (IDEs). Recently, some studies have begun to use graph neural networks to complete AST-level code, and shown that it is promising to introduce GNNs into ASTlevel completion. However, these methods do not fully exploit the potential of reference codes with similar structures nor solve out-of-vocabulary (OOV). We propose Retrieval-Assisted Graph Code Completion (ReGCC) to enhance AST-level code completion further. ReGCC integrates a retrieval model that searches for similar code graphs to generate graph nodes and a completion model that leverages information from multiple domains. The key component of both the retrieval and completion models is the Multi-field Graph Attention Block, which consists of three layers of stacked attention: (1) Neighborhood Attention: preserves the heterogeneity and local dependency of the graph, enabling nodes to exchange information within their neighborhood. (2) Global &amp; Memory Attention: addresses the long-distance dependency problem by providing nodes with a global view and the ability to extract information from the memory domain. (3) Reference Attention: lets nodes obtain valuable information from structurally similar reference code graphs. Furthermore, we tackle the OOV issue by employing feature matching and copying values from existing nodes. Specifically, we predict edges between nodes beyond the vocabulary, enabling effective information transfer. Experimental results demonstrate the superiority of our approach over state-of-the-art AST-level completion methods and generative language models.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {125–136},
numpages = {12},
keywords = {code completion, graph retrieval, graph neural network},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3597503.3639201,
author = {Choudhuri, Rudrajit and Liu, Dylan and Steinmacher, Igor and Gerosa, Marco and Sarma, Anita},
title = {How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639201},
doi = {10.1145/3597503.3639201},
abstract = {Conversational Generative AI (convo-genAI) is revolutionizing Software Engineering (SE) as engineers and academics embrace this technology in their work. However, there is a gap in understanding the current potential and pitfalls of this technology, specifically in supporting students in SE tasks. In this work, we evaluate through a between-subjects study (N=22) the effectiveness of ChatGPT, a convo-genAI platform, in assisting students in SE tasks. Our study did not find statistical differences in participants' productivity or self-efficacy when using ChatGPT as compared to traditional resources, but we found significantly increased frustration levels. Our study also revealed 5 distinct faults arising from violations of Human-AI interaction guidelines, which led to 7 different (negative) consequences on participants.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {184},
numpages = {13},
keywords = {empirical study, software engineering, generative AI, ChatGPT},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3645079,
author = {Zhang, Yue and Meredith, Rachel and Reeves, Wilson and Coriolano, J\'{u}lia and Babar, Muhammad Ali and Rahman, Akond},
title = {Does Generative AI Generate Smells Related to Container Orchestration?: An Exploratory Study with Kubernetes Manifests},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645079},
doi = {10.1145/3643991.3645079},
abstract = {Generative artificial intelligence (AI) technologies, such as ChatGPT have shown promise in solving software engineering problems. However, these technologies have also shown to be susceptible to generating software artifacts that contain quality issues. A systematic characterization of quality issues, such as smells in ChatGPT-generated artifacts can help in providing recommendations for practitioners who use generative AI for container orchestration.We conduct an empirical study with 98 Kubernetes manifests to quantify smells in manifests generated by ChatGPT. Our empirical study shows: (i) 35.8% of the 98 Kubernetes manifests generated include at least one instance of smell; (ii) two types of objects Kubernetes namely, Deployment and Service are impacted by identified smells; and (iii) the most frequently occurring smell is unset CPU and memory requirements. Based on our findings, we recommend practitioners to apply quality assurance activities for ChatGPT-generated Kubernetes manifests prior to using these manifests for container orchestration.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {192–196},
numpages = {5},
keywords = {container orchestration, empirical study, kubernetes, quality, smell},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643795.3648396,
author = {Bhatia, Shreya and Gandhi, Tarushi and Kumar, Dhruv and Jalote, Pankaj},
title = {Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648396},
doi = {10.1145/3643795.3648396},
abstract = {Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT's performance, achieving a much higher coverage.*These authors contributed equally.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {54–61},
numpages = {8},
keywords = {large language models, unit test generation, ChatGPT, generative AI},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643795.3648377,
author = {Ramler, Rudolf and Moser, Michael and Fischer, Lukas and Nissl, Markus and Heinzl, Rene},
title = {Industrial Experience Report on AI-Assisted Coding in Professional Software Development},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648377},
doi = {10.1145/3643795.3648377},
abstract = {AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {1–7},
numpages = {7},
keywords = {AI-assisted development, code generation, ChatGPT, copilot},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3597503.3639120,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Wang, Shangwen and Li, Li},
title = {When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639120},
doi = {10.1145/3597503.3639120},
abstract = {Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from them are rarely considered helpful, with only a 4.2% Acceptance Rate. These findings motivate our exploration of dynamic inference in code completion and inspire us to enhance it with a decision-making mechanism that stops the generation of incorrect code. We thus propose a novel dynamic inference method specifically tailored for code completion models. This method aims not only to produce correct predictions with largely reduced computation but also to prevent incorrect predictions proactively. Our extensive evaluation shows that it can averagely skip 1.7 layers out of 16 layers in the models, leading to an 11.2% speedup with only a marginal 1.1% reduction in ROUGE-L.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {75},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643665.3648567,
author = {Hoda, Rashina},
title = {Keynote on Augmented Agile: Human-centred AI-assisted Software Project Management at FinanSE Workshop (ICSE 2024)},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643665.3648567},
doi = {10.1145/3643665.3648567},
abstract = {Software engineering teams face a number of challenges on a regular basis. Software practitioners have employed agile methods for over two decades to work around challenges such as inadequate customer collaboration, poor management practices, teamwork issues, and coordination challenges in large-scale teams. While they have served us well for over two decades, agile methods are not without limitations. Transitioning into agile methods as well as adapting agile practices to suit different domains such as Finance, can be challenging for software teams, management, and firms at large. This keynote shares experiences from industrial agile research to shed light on what's been working, what's missing, and what can be done better, including a vision of a new future of software project management - augmented agile - that combines a deeply human-centric approach ('the heart') with AI-assisted techniques ('the mind') to augment and boost current agile practice.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
pages = {17–18},
numpages = {2},
keywords = {augmented agile, agile methods, software engineering, finance},
location = {Lisbon, Portugal},
series = {FinanSE '24}
}

@inproceedings{10.1145/3597503.3623345,
author = {Steenhoek, Benjamin and Gao, Hongyang and Le, Wei},
title = {Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623345},
doi = {10.1145/3597503.3623345},
abstract = {Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ vulnerable and several hundreds of total examples as training data, the model retained the same performance as 100% of the dataset. DeepDFA also generalized to real-world vulnerabilities in DbgBench; it detected 8.7 out of 17 vulnerabilities on average across folds and was able to distinguish between patched and buggy versions, while the highest-performing baseline models did not detect any vulnerabilities. By combining DeepDFA with a large language model, we surpassed the state-of-the-art vulnerability detection performance on the Big-Vul dataset with 96.46 F1 score, 97.82 precision, and 95.14 recall. Our replication package is located at https://doi.org/10.6084/m9.figshare.21225413.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {16},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639170,
author = {Rahman, Md Mahbubur and Ceka, Ira and Mao, Chengzhi and Chakraborty, Saikat and Ray, Baishakhi and Le, Wei},
title = {Towards Causal Deep Learning for Vulnerability Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639170},
doi = {10.1145/3597503.3639170},
abstract = {Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it's indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {153},
numpages = {11},
keywords = {vulnerability detection, causality, spurious features},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639177,
author = {Wei, Moshi and Harzevili, Nima Shiri and Huang, Yuekai and Yang, Jinqiu and Wang, Junjie and Wang, Song},
title = {Demystifying and Detecting Misuses of Deep Learning APIs},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639177},
doi = {10.1145/3597503.3639177},
abstract = {Deep Learning (DL) libraries have significantly impacted various domains in computer science over the last decade. However, developers often face challenges when using the DL APIs, as the development paradigm of DL applications differs greatly from traditional software development. Existing studies on API misuse mainly focus on traditional software, leaving a gap in understanding API misuse within DL APIs. To address this gap, we present the first comprehensive study of DL API misuse in TensorFlow and PyTorch. Specifically, we first collected a dataset of 4,224 commits from the top 200 most-starred projects using these two libraries and manually identified 891 API misuses. We then investigated the characteristics of these misuses from three perspectives, i.e., types, root causes, and symptoms. We have also conducted an evaluation to assess the effectiveness of the current state-of-the-art API misuse detector on our 891 confirmed API misuses. Our results confirmed that the state-of-the-art API misuse detector is ineffective in detecting DL API misuses. To address the limitations of existing API misuse detection for DL APIs, we propose LLMAPIDet, which leverages Large Language Models (LLMs) for DL API misuse detection and repair. We build LLMAPIDet by prompt-tuning a chain of ChatGPT prompts on 600 out of 891 confirmed API misuses and reserve the rest 291 API misuses as the testing dataset. Our evaluation shows that LLMAPIDet can detect 48 out of the 291 DL API misuses while none of them can be detected by the existing API misuse detector. We further evaluate LLMAPIDet on the latest versions of 10 GitHub projects. The evaluation shows that LLMAPIDet can identify 119 previously unknown API misuses and successfully fix 46 of them.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {201},
numpages = {12},
keywords = {API misuse, deep learning APIs, empirical study, detection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3608141,
author = {Sejfia, Adriana and Das, Satyaki and Shafiq, Saad and Medvidovi\'{c}, Nenad},
title = {Toward Improved Deep Learning-based Vulnerability Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608141},
doi = {10.1145/3597503.3608141},
abstract = {Deep learning (DL) has been a common thread across several recent techniques for vulnerability detection. The rise of large, publicly available datasets of vulnerabilities has fueled the learning process underpinning these techniques. While these datasets help the DL-based vulnerability detectors, they also constrain these detectors' predictive abilities. Vulnerabilities in these datasets have to be represented in a certain way, e.g., code lines, functions, or program slices within which the vulnerabilities exist. We refer to this representation as a base unit. The detectors learn how base units can be vulnerable and then predict whether other base units are vulnerable. We have hypothesized that this focus on individual base units harms the ability of the detectors to properly detect those vulnerabilities that span multiple base units (or MBU vulnerabilities). For vulnerabilities such as these, a correct detection occurs when all comprising base units are detected as vulnerable. Verifying how existing techniques perform in detecting all parts of a vulnerability is important to establish their effectiveness for other downstream tasks. To evaluate our hypothesis, we conducted a study focusing on three prominent DL-based detectors: ReVeal, DeepWukong, and LineVul. Our study shows that all three detectors contain MBU vulnerabilities in their respective datasets. Further, we observed significant accuracy drops when detecting these types of vulnerabilities. We present our study and a framework that can be used to help DL-based detectors toward the proper inclusion of MBU vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {62},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00031,
author = {Zhang, Yang and Chang, Xiaosong and Fang, Lining and Lu, Yifan},
title = {DeepLog: Deep-Learning-Based Log Recommendation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00031},
doi = {10.1109/ICSE-Companion58688.2023.00031},
abstract = {Log recommendation plays a vital role in analyzing run-time issues including anomaly detection, performance monitoring, and security evaluation. However, existing deep-learning-based approaches for log recommendation suffer from insufficient features and low F1. To this end, this paper proposes a prototype called DeepLog to recommend log location based on a deep learning model. DeepLog parses the source code into an abstract syntax tree and then converts each method into a block hierarchical tree in which DeepLog extracts both semantic and syntactic features. By doing this, we construct a dataset with more than 110K samples. DeepLog employs a double-branched neural network model to recommend log locations. We evaluate the effectiveness of DeepLog by answering four research questions. The experimental results demonstrate that it can recommend 8,725 logs for 23 projects and the F1 of DeepLog is 28.17% higher than that of the existing approaches, which improves state-of-the-art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {88–92},
numpages = {5},
keywords = {log location, deep learning, recommendation, similarity analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639232,
author = {Gao, Yanjie and He, Yichen and Li, Xinze and Zhao, Bo and Lin, Haoxiang and Liang, Yoyo and Zhong, Jing and Zhang, Hongyu and Wang, Jingzhou and Zeng, Yonghua and Gui, Keli and Tong, Jie and Yang, Mao},
title = {An Empirical Study on Low GPU Utilization of Deep Learning Jobs},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639232},
doi = {10.1145/3597503.3639232},
abstract = {Deep learning plays a critical role in numerous intelligent software applications. Enterprise developers submit and run deep learning jobs on shared, multi-tenant platforms to efficiently train and test models. These platforms are typically equipped with a large number of graphics processing units (GPUs) to expedite deep learning computations. However, certain jobs exhibit rather low utilization of the allocated GPUs, resulting in substantial resource waste and reduced development productivity. This paper presents a comprehensive empirical study on low GPU utilization of deep learning jobs, based on 400 real jobs (with an average GPU utilization of 50% or less) collected from Microsoft's internal deep learning platform. We discover 706 low-GPU-utilization issues through meticulous examination of job metadata, execution logs, runtime metrics, scripts, and programs. Furthermore, we identify the common root causes and propose corresponding fixes. Our main findings include: (1) Low GPU utilization of deep learning jobs stems from insufficient GPU computations and interruptions caused by non-GPU tasks; (2) Approximately half (46.03%) of the issues are attributed to data operations; (3) 45.18% of the issues are related to deep learning models and manifest during both model training and evaluation stages; (4) Most (84.99%) low-GPU-utilization issues could be fixed with a small number of code/script modifications. Based on the study results, we propose potential research directions that could help developers utilize GPUs better in cloud-based platforms.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {96},
numpages = {13},
keywords = {deep learning jobs, GPU utilization, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639183,
author = {Ahmed, Toufique and Pai, Kunal Suresh and Devanbu, Premkumar and Barr, Earl},
title = {Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639183},
doi = {10.1145/3597503.3639183},
abstract = {Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. Researchers are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously collect semantics facts, from the code, while working. Mostly these are shallow, simple facts arising from a quick read. For a function, such facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them implicitly capable of doing this simple level of "code analysis" and extracting such information, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.Prior work shows that LLM performance on code summarization benefits from embedding a few code &amp; summary exemplars in the prompt, before the code to be summarized. While summarization performance has steadily progressed since the early days, there is still room for improvement: LLM performance on code summarization still lags its performance on natural-language tasks like translation and text summarization.We find that adding semantic facts to the code in the prompt actually does help! This approach improves performance in several different settings suggested by prior work, including for three different Large Language Models. In most cases, we see improvements, as measured by a range of commonly-used metrics; for the PHP language in the challenging CodeSearchNet dataset, this augmentation actually yields performance surpassing 30 BLEU1. In addition, we have also found that including semantic facts yields a substantial enhancement in LLMs' line completion performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {220},
numpages = {13},
keywords = {LLM, code summarization, program analysis, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00019,
author = {You, Hanmo and Wang, Zan and Chen, Junjie and Liu, Shuang and Li, Shuochuan},
title = {Regression Fuzzing for Deep Learning Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00019},
doi = {10.1109/ICSE48619.2023.00019},
abstract = {Deep learning (DL) Systems have been widely used in various domains. Similar to traditional software, DL system evolution may also incur regression faults. To find the regression faults between versions of a DL system, we propose a novel regression fuzzing technique called DRFuzz, which facilitates generating inputs that trigger diverse regression faults and have high fidelity. To enhance the diversity of the found regression faults, DRFuzz proposes a diversity-oriented test criterion to explore as many faulty behaviors as possible. Then, DRFuzz incorporates the GAN model to guarantee the fidelity of generated test inputs. We conduct an extensive study on four subjects in four regression scenarios of DL systems. The experimental results demonstrate the superiority of DRFuzz over the two compared state-of-the-art approaches, with an average improvement of 1,177% and 539% in terms of the number of detected regression faults.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {82–94},
numpages = {13},
keywords = {regression, fuzzing, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644396,
author = {Guan, Xueting and Treude, Christoph},
title = {Enhancing Source Code Representations for Deep Learning with Static Analysis},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644396},
doi = {10.1145/3643916.3644396},
abstract = {Deep learning techniques applied to program analysis tasks such as code classification, summarization, and bug detection have seen widespread interest. Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details. Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context. This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection. Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {64–68},
numpages = {5},
keywords = {source code representation, deep learning, static analysis, bug reports, design patterns},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639478.3643080,
author = {Li, Yi and Nguyen, Tien N. and Wang, Shaohua and Yadavally, Aashish},
title = {Poirot: Deep Learning for API Misuse Detection},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643080},
doi = {10.1145/3639478.3643080},
abstract = {API misuses refer to incorrect usages that violate the usage constraints of API elements, potentially leading to issues such as runtime errors, exceptions, program crashes, and security vulnerabilities. Existing mining-based approaches for API misuse detection face challenges in accuracy, particularly in distinguishing infrequent from invalid usage. This limitation stems from the necessity to set predefined thresholds for frequent API usage patterns, resulting in potential misclassification of alternative usages. This paper introduces Poirot, a learning-based approach that mitigates the need for predefined thresholds. Leveraging Labeled, Graph-based Convolutional Networks, Poirot learns embeddings for API usages, capturing key features and enhancing API misuse detection. Preliminary evaluation on an API misuse benchmark demonstrates that Poirot achieves a relative improvement of 1.37--10.36X in F-score compared to state-of-the-art API misuse detection techniques.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {302–303},
numpages = {2},
keywords = {AI4SE, API misuse detection, deep learning},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@proceedings{10.1145/3643786,
title = {DeepTest '24: Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {DeepTest is a high-quality workshop for research at the intersection of Machine Learning (ML) and Software Engineering (SE). ML is widely adopted in modern software systems, including safetycritical systems such as autonomous vehicles, and aircraft collision avoidance. Thus, it is crucial to rigorously test ML-based applications to ensure high dependability. However, standard notions of software quality and reliability become irrelevant when considering ML systems due to their non-deterministic nature and the lack of a transparent understanding of the ML models' semantics. On the other hand, software engineering researchers and practitioners increasingly resort to ML approaches to devise novel solutions to address existing problems in the software development life-cycle.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3623304,
author = {Ma, Lipeng and Yang, Weidong and Xu, Bo and Jiang, Sihang and Fei, Ben and Liang, Jiaqing and Zhou, Mingjie and Xiao, Yanghua},
title = {KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623304},
doi = {10.1145/3597503.3623304},
abstract = {Logs as semi-structured text are rich in semantic information, making their comprehensive understanding crucial for automated log analysis. With the recent success of pre-trained language models in natural language processing, many studies have leveraged these models to understand logs. Despite their successes, existing pre-trained language models still suffer from three weaknesses. Firstly, these models fail to understand domain-specific terminology, especially abbreviations. Secondly, these models struggle to adequately capture the complete log context information. Thirdly, these models have difficulty in obtaining universal representations of different styles of the same logs. To address these challenges, we introduce KnowLog, a knowledge-enhanced pre-trained language model for log understanding. Specifically, to solve the previous two challenges, we exploit abbreviations and natural language descriptions of logs from public documentation as local and global knowledge, respectively, and leverage this knowledge by designing novel pre-training tasks for enhancing the model. To solve the last challenge, we design a contrastive learning-based pre-training task to obtain universal representations. We evaluate KnowLog by fine-tuning it on six different log understanding tasks. Extensive experiments demonstrate that KnowLog significantly enhances log understanding and achieves state-of-the-art results compared to existing pre-trained language models without knowledge enhancement. Moreover, we conduct additional experiments in transfer learning and low-resource scenarios, showcasing the substantial advantages of KnowLog. Our source code and detailed experimental data are available at https://github.com/LeaperOvO/KnowLog.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {32},
numpages = {13},
keywords = {pre-trained language model, knowledge enhancement, log understanding},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00022,
author = {Vaithilingam, Priyan and Glassman, Elena L. and Groenwegen, Peter and Gulwani, Sumit and Henley, Austin Z. and Malpani, Rohan and Pugh, David and Radhakrishna, Arjun and Soares, Gustavo and Wang, Joey and Yim, Aaron},
title = {Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCode's User Experience},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00022},
doi = {10.1109/ICSE-SEIP58684.2023.00022},
abstract = {AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AI-driven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {185–195},
numpages = {11},
keywords = {inline-suggestion, AI-suggestion, refactoring, iterative-refinement, code-completion},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00105,
author = {Yang, Chenyuan and Deng, Yinlin and Yao, Jiayi and Tu, Yuxing and Li, Hanchi and Zhang, Lingming},
title = {Fuzzing Automatic Differentiation in Deep-Learning Libraries},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00105},
doi = {10.1109/ICSE48619.2023.00105},
abstract = {Deep learning (DL) has attracted wide attention and has been widely deployed in recent years. As a result, more and more research efforts have been dedicated to testing DL libraries and frameworks. However, existing work largely overlooked one crucial component of any DL system, automatic differentiation (AD), which is the basis for the recent development of DL. To this end, we propose ∇Fuzz, the first general and practical approach specifically targeting the critical AD component in DL libraries. Our key insight is that each DL library API can be abstracted into a function processing tensors/vectors, which can be differentially tested under various execution scenarios (for computing outputs/gradients with different implementations). We have implemented ∇Fuzz as a fully automated API-level fuzzer targeting AD in DL libraries, which utilizes differential testing on different execution scenarios to test both first-order and high-order gradients, and also includes automated filtering strategies to remove false positives caused by numerical instability. We have performed an extensive study on four of the most popular and actively-maintained DL libraries, PyTorch, TensorFlow, JAX, and OneFlow. The result shows that ∇Fuzz substantially outperforms state-of-the-art fuzzers in terms of both code coverage and bug detection. To date, ∇Fuzz has detected 173 bugs for the studied DL libraries, with 144 already confirmed by developers (117 of which are previously unknown bugs and 107 are related to AD). Remarkably, ∇Fuzz contributed 58.3% (7/12) of all high-priority AD bugs for PyTorch and JAX during a two-month period. None of the confirmed AD bugs were detected by existing fuzzers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1174–1186},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644032.3644448,
author = {Marchetto, Alessandro},
title = {Can explainability and deep-learning be used for localizing vulnerabilities in source code?},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644448},
doi = {10.1145/3644032.3644448},
abstract = {Security vulnerabilities are weaknesses of software due for instance to design flaws or implementation bugs that can be exploited and lead to potentially devastating security breaches. Traditionally, static code analysis is recognized as effective in the detection of software security vulnerabilities but at the expense of a high human effort required for checking a large number of produced false positive cases. Deep-learning methods have been recently proposed to overcome such a limitation of static code analysis and detect the vulnerable code by using vulnerability-related patterns learned from large source code datasets. However, the use of these methods for localizing the causes of the vulnerability in the source code, i.e., localize the statements that contain the bugs, has not been extensively explored.In this work, we experiment the use of deep-learning and explainability methods for detecting and localizing vulnerability-related statements in code fragments (named snippets). We aim at understanding if the code features adopted by deep-learning methods to identify vulnerable code snippets can also support the developers in debugging the code, thus localizing the vulnerability's cause. Our work shows that deep-learning methods can be effective in detecting the vulnerable code snippets, under certain conditions, but the code features that such methods use can only partially face the actual causes of the vulnerabilities in the code.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {110–119},
numpages = {10},
keywords = {cybersecurity, vulnerability detection, vulnerability localization},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1109/ICSE48619.2023.00170,
author = {Yan, Yanyan and Feng, Yang and Fan, Hongcheng and Xu, Baowen},
title = {DLInfer: Deep Learning with Static Slicing for Python Type Inference},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00170},
doi = {10.1109/ICSE48619.2023.00170},
abstract = {Python programming language has gained enormous popularity in the past decades. While its flexibility significantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors.In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type information for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2009–2021},
numpages = {13},
keywords = {type inference, Python, static slicing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00101,
author = {Ye, Ming and Chen, Yuanfan and Zhang, Xin and He, Jinning and Cao, Jicheng and Liu, Dong and Gao, Jing and Dai, Hailiang and Cheng, Shengyu},
title = {Automated Feature Document Review via Interpretable Deep Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00101},
doi = {10.1109/ICSE-Companion58688.2023.00101},
abstract = {A feature in the agile methodology is a function of a product that delivers business value and meets stakeholders' requirements. Developers compile and store the content of features in a structured feature document. Feature documents play a critical role in controlling software development at a macro level. It is therefore important to ensure the quality of feature documents so that defects are not introduced at the outset. Manual review is an effective activity to ensure quality, but it is human-intensive and challenging. In this paper, we propose a feature document review tool to automate the process of manual review (quality classification, and suggestion generation) based on neural networks and interpretable deep learning. Our goal is to reduce human effort in reviewing feature documents and to prompt authors to craft better feature documents. We have evaluated our tool on a real industrial project from ZTE Corporation. The results show that our quality classification model achieved 75.6% precision and 94.4% recall for poor quality feature documents. For the suggestion generation model, about 70% of the poor quality feature documents could be improved to the qualified level in three rounds of revision based on the suggestions. User feedback shows that our tool helps users save an average of 15.9% of their time.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {351–354},
numpages = {4},
keywords = {feature documents, agile methodology, neural networks, interpretable deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00052,
author = {Gao, Yanjie and Shi, Xiaoxiang and Lin, Haoxiang and Zhang, Hongyu and Wu, Hao and Li, Rui and Yang, Mao},
title = {An Empirical Study on Quality Issues of Deep Learning Platform},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00052},
doi = {10.1109/ICSE-SEIP58684.2023.00052},
abstract = {In recent years, deep learning (DL) has been increasingly adopted in many application areas. To help deep learning developers better train and test their models, enterprises have built dedicated, multi-tenant platforms equipped with a mass of computing devices like GPUs. The service quality of these platforms plays a critical role in system efficiency and user experience. Nevertheless, there indeed exist diverse types of quality issues that not only waste computing resources significantly but also slow down development productivity severely. In this paper, we present a comprehensive empirical study on quality issues of Platform-X in Microsoft. Platform-X is an internal production deep learning platform that serves hundreds of developers and researchers. We have manually examined 360 real issues and investigated their common symptoms, root causes, and mitigation actions. Our major findings include: (1) 28.33% of the quality issues are caused by hardware (the GPU, network, and compute node) faults; (2) 28.33% of them result from system-side faults (e.g., system defects and service outages); (3) User-side faults (e.g., user bugs and policy violation) account for more than two-fifths (43.34%) of all the common causes; (4) More than three-fifths of all the quality issues can be mitigated by simply resubmitting jobs (34.72%) and improving user code (24.72%). Our study results provide valuable guidance on promoting the service quality of deep learning platforms from both the development and maintenance aspects. The results further motivate possible research directions and tooling support.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {455–466},
numpages = {12},
keywords = {deep learning, deep learning platform, quality issue, empirical study},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3647632.3647990,
author = {Jacques, Vitor Maciel Fontes and Alizadeh, Negar and Castor, Fernando},
title = {A Study on the Battery Usage of Deep Learning Frameworks on iOS Devices},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3647990},
doi = {10.1145/3647632.3647990},
abstract = {As machine learning continues to establish its presence on mobile platforms, there arises a need to evaluate model resource usage across a variety of devices and frameworks. In this paper, we measure the performance and battery usage of inference for three significant machine learning models, MobileNetV2, ResNet50, and BERT QA, when employing different deep learning frameworks (CoreML, Tensorflow, and PyTorch), iOS devices (iPhone 8 Plus, iPhone 11 Pro, and iPad Air 4), and threading configurations. Throughout our study, we systematically assessed key metrics: battery usage, inference duration, and accuracy rates. Our findings challenge some conventional beliefs; for instance, an increase in thread count did not always guarantee faster model execution, even when there are physical cores available. Similarly, a quick inference time was not always synonymous with higher energy efficiency. In addition, our study shows no single best framework for all cases. CoreML is more energy-efficient for MobileNetV2 and ResNet50 but sometimes slower, especially on older devices. TensorFlow Lite excels in energy and performance for BERT QA, even on newer hardware. While multithreading often helps, its benefits are limited, especially for CoreML beyond two threads. These results emphasize the need to tailor machine learning implementations to specific hardware and model characteristics, indicating room for improvement in existing frameworks.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {1–11},
numpages = {11},
keywords = {energy efficiency, performance, deep learning, CoreML, Tensorflow, Pytorch, iOS},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00039,
author = {Gao, Yanjie and Gu, Xianyu and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Runtime Performance Prediction for Deep Learning Models with Graph Neural Network},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00039},
doi = {10.1109/ICSE-SEIP58684.2023.00039},
abstract = {Deep learning models have been widely adopted in many application domains. Predicting the runtime performance of deep learning models, such as GPU memory consumption and training time, is important for boosting development productivity and reducing resource waste. The reason is that improper configurations of hyperparameters and neural architectures can result in many failed training jobs or unsatisfactory models. However, the runtime performance prediction of deep learning models is challenging because of the hybrid programming paradigm, complicated hidden factors within the framework runtime, enormous model configuration space, and broad differences among models. In this paper, we propose DNNPerf, a novel ML-based tool for predicting the runtime performance of deep learning models using Graph Neural Network. DNNPerf represents a model as a directed acyclic computation graph and incorporates a rich set of performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder for the node and edge features. DNNPerf is evaluated on thousands of configurations of real-world and synthetic deep learning models to predict their GPU memory consumption and training time. The experimental results show that DNNPerf achieves accurate predictions, with an overall error of 7.4% for the training time prediction and an overall error of 13.7% for the GPU memory consumption prediction, confirming its effectiveness.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {368–380},
numpages = {13},
keywords = {deep learning, AutoML, graph neural network, runtime performance, performance prediction},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00188,
author = {Steenhoek, Benjamin and Rahman, Md Mahbubur and Jiles, Richard and Le, Wei},
title = {An Empirical Study of Deep Learning Models for Vulnerability Detection},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00188},
doi = {10.1109/ICSE48619.2023.00188},
abstract = {Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider "hard" to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at https://doi.org/10.6084/m9.figshare.20791240.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2237–2248},
numpages = {12},
keywords = {deep learning, vulnerability detection, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623333,
author = {Ahmed, Shibbir and Gao, Hongyang and Rajan, Hridesh},
title = {Inferring Data Preconditions from Deep Learning Models for Trustworthy Prediction in Deployment},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623333},
doi = {10.1145/3597503.3623333},
abstract = {Deep learning models are trained with certain assumptions about the data during the development stage and then used for prediction in the deployment stage. It is important to reason about the trustworthiness of the model's predictions with unseen data during deployment. Existing methods for specifying and verifying traditional software are insufficient for this task, as they cannot handle the complexity of DNN model architecture and expected outcomes. In this work, we propose a novel technique that uses rules derived from neural network computations to infer data preconditions for a DNN model to determine the trustworthiness of its predictions. Our approach, DeepInfer involves introducing a novel abstraction for a trained DNN model that enables weakest precondition reasoning using Dijkstra's Predicate Transformer Semantics. By deriving rules over the inductive type of neural network abstract representation, we can overcome the matrix dimensionality issues that arise from the backward non-linear computation from the output layer to the input layer. We utilize the weakest precondition computation using rules of each kind of activation function to compute layer-wise precondition from the given postcondition on the final output of a deep neural network. We extensively evaluated DeepInfer on 29 real-world DNN models using four different datasets collected from five different sources and demonstrated the utility, effectiveness, and performance improvement over closely related work. DeepInfer efficiently detects correct and incorrect predictions of high-accuracy models with high recall (0.98) and high F-1 score (0.84) and has significantly improved over the prior technique, SelfChecker. The average runtime overhead of DeepInfer is low, 0.22 sec for all the unseen datasets. We also compared runtime overhead using the same hardware settings and found that DeepInfer is 3.27 times faster than SelfChecker, the state-of-the-art in this area.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {38},
numpages = {13},
keywords = {deep neural networks, weakest precondition, trustworthiness},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623343,
author = {Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming},
title = {Large Language Models are Edge-Case Generators: Crafting Unusual Programs for Fuzzing Deep Learning Libraries},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623343},
doi = {10.1145/3597503.3623343},
abstract = {Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries (and beyond). However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive pre-training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced.To fill this gap, this paper proposes FuzzGPT, the first approach to priming LLMs to synthesize unusual programs for fuzzing. FuzzGPT is mainly built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruction-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {70},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00211,
author = {Nong, Yu and Ou, Yuzhe and Pradel, Michael and Chen, Feng and Cai, Haipeng},
title = {VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00211},
doi = {10.1109/ICSE48619.2023.00211},
abstract = {Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0--430.1% and 16.3--158.2%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2527–2539},
numpages = {13},
keywords = {software vulnerability, data generation, bug injection, pattern mining, deep learning, vulnerability detection},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00027,
author = {Humbatova, Nargiz and Jabangirova, Gunel and Tonella, Paolo},
title = {DeepCrime: From Real Faults to Mutation Testing Tool for Deep Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00027},
doi = {10.1109/ICSE-Companion58688.2023.00027},
abstract = {The recent advance of Deep Learning (DL) due to its human-competitive performance in complex and often safety-critical tasks, reveals many gaps in their testing. There exist a number of DL-specific testing approaches, and yet none has presented the possibility of simulating the occurrence of real DL faults for the mutation testing of DL systems. We propose 35 and implement 24 mutation operators that were systematically extracted from the existing studies on real DL faults. Our evaluation shows that the proposed operators produce non-redundant, killable, and non-trivial mutations while being more sensitive to the change in the quality of test data than the existing mutation testing approaches. Video demonstration is available at: https://youtu.be/WOvuPaXH6Jk},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {68–72},
numpages = {5},
keywords = {deep learning, mutation testing, real faults},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528588.3528658,
author = {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya and Saake, Gunter and Leich, Thomas},
title = {Supporting systematic literature reviews using deep-learning-based language models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528658},
doi = {10.1145/3528588.3528658},
abstract = {Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {67–74},
numpages = {8},
keywords = {BERT, deep learning, language models, systematic literature review},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00026,
author = {Chen, Jialuo and Sun, Youcheng and Wang, Jingyi and Cheng, Peng and Ma, Xingjun},
title = {DeepJudge: A Testing Framework for Copyright Protection of Deep Learning Models},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00026},
doi = {10.1109/ICSE-Companion58688.2023.00026},
abstract = {Deep learning (DL) models have become one of the most valuable assets in modern society, and those most complex ones require millions of dollars for the model development. As a result, unauthorized duplication or reproduction of DL models can lead to copyright infringement and cause huge economic losses to model owners.In this work, we present DeepJudge, a testing framework for DL copyright protection. DeepJudge quantitatively tests the similarities between two DL models: a victim model and a suspect model. It leverages a diverse set of testing metrics and efficient test case generation algorithms to produce a chain of supporting evidence to help determine whether a suspect model is a copy of the victim model. Our experiments confirm the effectiveness of DeepJudge under typical model copyright infringement scenarios. The tool has been made publicly available at https://github.com/Testing4AI/DeepJudge. A demo video can be found at https://www.youtube.com/watch?v=LhNeo615YOE.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {64–67},
numpages = {4},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00190,
author = {Yuan, Bin and Lu, Yifan and Fang, Yilin and Wu, Yueming and Zou, Deqing and Li, Zhen and Li, Zhi and Jin, Hai},
title = {Enhancing Deep Learning-Based Vulnerability Detection by Building Behavior Graph Model},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00190},
doi = {10.1109/ICSE48619.2023.00190},
abstract = {Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced FFMpeg+Qemu dataset and the unbalanced Chrome+Debian dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2262–2274},
numpages = {13},
keywords = {vulnerability detection, behavior graph, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623308,
author = {Yu, Boxi and Yao, Jiayi and Fu, Qiuai and Zhong, Zhiqing and Xie, Haotian and Wu, Yaoliang and Ma, Yuchi and He, Pinjia},
title = {Deep Learning or Classical Machine Learning? An Empirical Study on Log-Based Anomaly Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623308},
doi = {10.1145/3597503.3623308},
abstract = {While deep learning (DL) has emerged as a powerful technique, its benefits must be carefully considered in relation to computational costs. Specifically, although DL methods have achieved strong performance in log anomaly detection, they often require extended time for log preprocessing, model training, and model inference, hindering their adoption in online distributed cloud systems that require rapid deployment of log anomaly detection service.This paper investigates the superiority of DL methods compared to simpler techniques in log anomaly detection. We evaluate basic algorithms (e.g., KNN, SLFN) and DL approaches (e.g., CNN) on five public log anomaly detection datasets (e.g., HDFS). Our findings demonstrate that simple algorithms outperform DL methods in both time efficiency and accuracy. For instance, on the Thunderbird dataset, the K-nearest neighbor algorithm trains 1,000 times faster than NeuralLog while achieving a higher F1-Score by 0.0625. We also identify three factors contributing to this phenomenon, which are: (1) redundant log preprocessing strategies, (2) dataset simplicity, and (3) the nature of binary classification in log anomaly detection. To assess the necessity of DL, we propose LightAD, an architecture that optimizes training time, inference time, and performance score. With automated hyper-parameter tuning, LightAD allows fair comparisons among log anomaly detection models, enabling engineers to evaluate the suitability of complex DL methods.Our findings serve as a cautionary tale for the log anomaly detection community, highlighting the need to critically analyze datasets and research tasks before adopting DL approaches. Researchers proposing computationally expensive models should benchmark their work against lightweight algorithms to ensure a comprehensive evaluation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {35},
numpages = {13},
keywords = {log analysis, anomaly detection, dataset, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00091,
author = {Zhu, Tingwei and Li, Zhong and Pan, Minxue and Shi, Chaoxuan and Zhang, Tian and Pe, Yu and Li, Xuandong},
title = {Revisiting Information Retrieval and Deep Learning Approaches for Code Summarization},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00091},
doi = {10.1109/ICSE-Companion58688.2023.00091},
abstract = {Code summarization refers to the procedure of creating short descriptions that outline the semantics of source code snippets. Existing code summarization approaches can be broadly classified into Information Retrieval (IR)-based and Deep Learning (DL)-based approaches. However, their effectiveness, and especially their strengths and weaknesses, remain largely understudied. Existing evaluations use different benchmarks and metrics, making performance comparisons of these approaches susceptible to bias and potentially yielding misleading results. For example, the DL-based approaches typically show better code summarization performance in their original papers [1], [2]. However, Gros et al. [3] report that a naive IR approach could achieve comparable (or even better) performance to the DL-based ones. In addition, some recent work [4], [5] suggests that incorporating IR techniques can improve the DL-based approaches. To further advance code summarization techniques, it is critical that we have a good understanding of how IR-based and DL-based approaches perform on different datasets and in terms of different metrics.Prior works have studied some aspects of code summarization, such as the factors affecting performance evaluation [6] and the importance of data preprocessing [7], etc. In this paper, we focus on the study of the IR-based and DL-based code summarization approaches to enhance the understanding and design of more advanced techniques. We first compare the IR-based and DL-based approaches under the same experimental settings and benchmarks, then study their strengths and limitations through quantitative and qualitative analyses. Finally, we propose a simpler but effective strategy to combine IR and DL to further improve code summarization.Four IR-based approaches and two DL-based approaches are investigated with regard to representativeness and diversity. For IR-based approaches, we select three BM25-based approaches (i.e., BM25-spl, BM25-ast, BM25-alpha) and one nearest neighbor-based approach NNGen [8], which are often compared as baselines in prior works. They retrieve the most similar code from the database and directly output the corresponding summary. BM25-based approaches are implemented by Lucene [9]. Taking code forms as input, BM25-spl splits the CamelCase and snake_case in original source code tokens, BM25-ast obtains sequence representations using pre-order Abstract Syntax Tree (AST) traversal, and BM25-alpha keeps only the alpha tokens in the code. For DL-based approaches, we choose the state-of-the-art pre-trained model PLBART [2] and the trained-from-scratch model SiT[1].We adopt four widely used Java datasets, namely TLC [10], CSN [11], HDC [12], and FCM [13] as our subject datasets. TLC and HDC are method-split datasets, where methods in the same project are randomly split into training/validation/test sets. CSN and FCM are project-split datasets, where examples from the same project exist in only one partition. We further process the four datasets to build cleaner datasets by removing examples that have syntax errors, empty method bodies, and too long or too short sequence lengths, etc. We also remove the duplicate examples in the validation and test sets.To comprehensively and systematically evaluate the performance of the code summarization approaches, we adopt three widely used metrics, i.e., BLEU (both C-BLEU and S-BLEU are included), ROUGE, and METEOR, in our experiments. Effectiveness. We conduct a comprehensive comparison of the six studied approaches under exactly the same settings and datasets. Table I shows the experimental results obtained on four subject datasets in terms of four metrics.Comparing by metrics from Table I, we can observe that overall there are large variations in the approach rankings and score gaps when using different metrics for evaluation. DL-based approaches generally achieve better performance than IR-based approaches in terms of METEOR and ROUGE-L. IR-based approaches achieve comparable or even better C-BLEU scores, but have lower S-BLEU scores than the DL-based approaches. This shows that different metrics have a large impact on the results of the approach evaluation. Different metrics are needed to evaluate the code summarization approaches.Considering different datasets, the pre-trained DL-based approach PLBART performs best among the six approaches studied. On the other hand, we notice that the IR-based approaches, despite their simplicity, also achieve comparable or even better performance, especially on method-split datasets. For example, the C-BLEU scores of BM25-spl on TLC and HDC are the highest among all approaches. Therefore, although DL-based methods usually show better performance for code summarization, we should not overlook the capabilities of IR-based methods.Strengths. To evaluate how the similarity between the training and test codes affects the performance of the approaches, we use the Retrieval-Similarity metric, as Rencos [4] did, to measure the token-level similarity between a test code and its most similar training code. Based on this, we examine how the BLEU score of each approach varies as the Retrieval-Similarity value changes on the four subject datasets.Figure 1 shows the results, from which we observe that IR-based approaches perform better than DL-based ones when the Retrieval-Similarity values are higher. Through qualitative analysis of examples with high retrieval similarity, we find that due to the cloning phenomenon, similar codes have similar summaries, so IR-based approaches tend to perform better on examples with high Retrieval-Similarity values.Integration. Based on previous findings, we are motivated to design a simpler integration approach. We propose to take advantage of Retrieval-Similarity to decide whether to use the IR or DL approach to generate a summary for the input code. Specifically, we first use Lucene to retrieve a similar code for the input and compute a Retrieval-Similarity value between them. If the value is higher than a similarity threshold, we directly use the IR summary. Otherwise, we choose the DL model to get the output. To determine the similarity threshold, we conduct grid search on the validation set. The similarity achieving the highest metric score on validation set is set as the final threshold.We choose the best DL model PLBART and the best IR approach BM25-spl for integration and evaluate our approach on all four cleaned datasets. The effectiveness results are shown in the 'Ours' row of Table I. From the table, we can see that our integration is effective and achieves state-of-the-art results. Not only does it outperform a single approach, but the scores it achieves are higher than all the previous highest scores in our experiments on all metrics across all datasets.In summary, our study shows that the IR and DL approaches have their own strengths in terms of performance using different metrics on different datasets. Although IR-based approaches are simpler, they can still achieve comparable or even better performance in some cases, especially in the presence of high-similarity code. Based on the results, we propose a simple integration approach that achieves state-of-the-art results. Our study shows that it is not enough to focus on the DL model alone. Taking advantage of IR approaches is a promising direction for improving code summarization. Future work should explore the incorporation of more types of information and more advanced integration methods.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {328–329},
numpages = {2},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3648400,
author = {Xiao, Tao and Treude, Christoph and Hata, Hideaki and Matsumoto, Kenichi},
title = {DevGPT: Studying Developer-ChatGPT Conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3648400},
doi = {10.1145/3643991.3648400},
abstract = {This paper introduces DevGPT, a dataset curated to explore how software developers interact with ChatGPT, a prominent large language model (LLM). The dataset encompasses 29,778 prompts and responses from ChatGPT, including 19,106 code snippets, and is linked to corresponding software development artifacts such as source code, commits, issues, pull requests, discussions, and Hacker News threads. This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work. DevGPT enables the study of developer queries, the effectiveness of ChatGPT in code generation and problem solving, and the broader implications of AI-assisted programming. By providing this dataset, the paper paves the way for novel research avenues in software engineering, particularly in understanding and improving the use of LLMs like ChatGPT by developers.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {227–230},
numpages = {4},
keywords = {ChatGPT, LLM, generative AI, dataset},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00192,
author = {Yang, Xu and Wang, Shaowei and Li, Yi and Wang, Shaohua},
title = {Does Data Sampling Improve Deep Learning-Based Vulnerability Detection? Yeas! and Nays!},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00192},
doi = {10.1109/ICSE48619.2023.00192},
abstract = {Recent progress in Deep Learning (DL) has sparked interest in using DL to detect software vulnerabilities automatically and it has been demonstrated promising results at detecting vulnerabilities. However, one prominent and practical issue for vulnerability detection is data imbalance. Prior study observed that the performance of state-of-the-art (SOTA) DL-based vulnerability detection (DLVD) approaches drops precipitously in real world imbalanced data and a 73% drop of F1-score on average across studied approaches. Such a significant performance drop can disable the practical usage of any DLVD approaches. Data sampling is effective in alleviating data imbalance for machine learning models and has been demonstrated in various software engineering tasks. Therefore, in this study, we conducted a systematical and extensive study to assess the impact of data sampling for data imbalance problem in DLVD from two aspects: i) the effectiveness of DLVD, and ii) the ability of DLVD to reason correctly (making a decision based on real vulnerable statements). We found that in general, oversampling outperforms undersampling, and sampling on raw data outperforms sampling on latent space, typically random oversampling on raw data performs the best among all studied ones (including advanced one SMOTE and OSS). Surprisingly, OSS does not help alleviate the data imbalance issue in DLVD. If the recall is pursued, random undersampling is the best choice. Random oversampling on raw data also improves the ability of DLVD approaches for learning real vulnerable patterns. However, for a significant portion of cases (at least 33% in our datasets), DVLD approach cannot reason their prediction based on real vulnerable statements. We provide actionable suggestions and a roadmap to practitioners and researchers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2287–2298},
numpages = {12},
keywords = {vulnerability detection, deep learning, data sampling, interpretable AI},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00057,
author = {Fu, Michael},
title = {Toward More Effective Deep Learning-Based Automated Software Vulnerability Prediction, Classification, and Repair},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00057},
doi = {10.1109/ICSE-Companion58688.2023.00057},
abstract = {Software vulnerabilities are prevalent in software systems and the unresolved vulnerable code may cause system failures or serious data breaches. To enhance security and prevent potential cyberattacks on software systems, it is critical to (1) early detect vulnerable code, (2) identify its vulnerability type, and (3) suggest corresponding repairs. Recently, deep learning-based approaches have been proposed to predict those tasks based on source code. In particular, software vulnerability prediction (SVP) detects vulnerable source code; software vulnerability classification (SVC) identifies vulnerability types to explain detected vulnerable programs; neural machine translation (NMT)-based automated vulnerability repair (AVR) generates patches to repair detected vulnerable programs. However, existing SVPs require much effort to inspect their coarse-grained predictions; SVCs encounter an unresolved data imbalance issue; AVRs are still inaccurate. I hypothesize that by addressing the limitations of existing SVPs, SVCs and AVRs, we can improve the accuracy and effectiveness of DL-based approaches for the aforementioned three prediction tasks. To test this hypothesis, I will propose (1) a finer-grained SVP approach that can point out vulnerabilities at the line level; (2) an SVC approach that mitigates the data imbalance issue; (3) NMT-based AVR approaches to address limitations of previous NMT-based approaches. Finally, I propose integrating these novel approaches into an open-source software security framework to promote the adoption of the DL-powered security tool in the industry.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {208–212},
numpages = {5},
keywords = {cybersecurity, software vulnerability, software security},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00104,
author = {Riccio, Vincenzo and Tonella, Paolo},
title = {When and Why Test Generators for Deep Learning Produce Invalid Inputs: An Empirical Study},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00104},
doi = {10.1109/ICSE48619.2023.00104},
abstract = {Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate.In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1161–1173},
numpages = {13},
keywords = {software testing, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00206,
author = {Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R. and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
title = {An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00206},
doi = {10.1109/ICSE48619.2023.00206},
abstract = {Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems.In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2463–2475},
numpages = {13},
keywords = {software reuse, empirical software engineering, machine learning, deep learning, software supply chain, engineering decision making, cybersecurity, trust},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639194,
author = {Tanzil, Minaoar Hossain and Khan, Junaed Younus and Uddin, Gias},
title = {ChatGPT Incorrectness Detection in Software Reviews},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639194},
doi = {10.1145/3597503.3639194},
abstract = {We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with an F1-score of 0.74 -- 0.75.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {180},
numpages = {12},
keywords = {large language model, chatGPT, hallucination, testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3645078,
author = {Mohamed, Suad and Parvin, Abdullah and Parra, Esteban},
title = {Chatting with AI: Deciphering Developer Conversations with ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645078},
doi = {10.1145/3643991.3645078},
abstract = {Large Language Models (LLMs) have been widely adopted and are becoming ubiquitous and integral to software development. However, we have little knowledge as to how these tools are being used by software developers beyond anecdotal evidence and word-of-mouth reports. In this work, we present a study toward understanding how developers engage with and utilize LLMs by reporting the results of an empirical study identifying patterns in the conversation that developers have with LLMs. We identified a total of 19 topics describing the purpose of the developers in their conversations with LLMs. Our findings reveal that developers use LLMs to facilitate various aspects of their software development processes (e.g., information-seeking about programming languages and frameworks and soliciting high-level design recommendations) to a similar extent to which they use them for non-development purposes such as writing assistance, general purpose queries, and conducting Turing tests to assess the intrinsic capabilities of the models. This work not only sheds light on the diverse applications of LLMs in software development but also underscores their emerging role as critical tools in enhancing developer productivity and creativity as we move closer to widespread AI-assisted software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {187–191},
numpages = {5},
keywords = {large language models, LLM, ChatGPT, software development, empirical study, developer conversations},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639474.3640076,
author = {Xue, Yuankai and Chen, Hanlin and Bai, Gina R. and Tairas, Robert and Huang, Yu},
title = {Does ChatGPT Help With Introductory Programming?An Experiment of Students Using ChatGPT in CS1},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640076},
doi = {10.1145/3639474.3640076},
abstract = {Generative AI, notably ChatGPT, has garnered attention in computer science education. This paper presents a controlled experiment that explores ChatGPT's role in CS1 in a classroom setting. Specifically, we aim to investigate the impact of ChatGPT on student learning outcomes and their behaviors when working on programming assignments. Participants were tasked with creating a UML diagram and subsequently implementing its design through programming, followed by a closed-book post-evaluation and a post-survey. All the participants were required to screen-record the whole process. In total, 56 participants were recruited, with 48 successful screen recordings. Participants in the Experimental Group can access ChatGPT 3.5 and other online resources, such as Google and Stack Overflow when creating the UML diagram and programming; however, participants in the Control Group can access all online resources except for ChatGPT (i.e., the only design variable is the access to ChatGPT). Finally, we measured and analyzed participants' learning outcomes through their UML diagram, programming, and post-evaluation scores. We also analyzed the time participants took to complete the tasks and their interactions with ChatGPT and other resources from the screen recordings. After finishing the tasks, student participants also provided their perceptions of using ChatGPT in CS1 through a post-survey.With rigorous quantitative and qualitative analysis, we found that (1) using ChatGPT does not present a significant impact on students' learning performance in the CS1 assignment-style tasks; (2) once using ChatGPT, students' tendency to explore other traditional educational resources is largely reduced (though available) and they tend to rely solely on ChatGPT, and this reliance on ChatGPT did not guarantee enhanced learning performance; (3) the majority of students hold neutral views on ChatGPT's role in CS1 programming but most of them raised concerns about its potential ethical issues and inconsistent performance across different tasks. We hope this study can help educators and students better understand the impact of ChatGPT in CS1 and inspire future work to provide proper guidelines for using ChatGPT in introductory programming classes.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {331–341},
numpages = {11},
keywords = {CS education, CS1, generative AI, ChatGPT, OOP},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3597503.3639075,
author = {Serafini, Raphael and Otto, Clemens and Horstmann, Stefan Albert and Naiakshina, Alena},
title = {ChatGPT-Resistant Screening Instrument for Identifying Non-Programmers},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639075},
doi = {10.1145/3597503.3639075},
abstract = {To ensure the validity of software engineering and IT security studies with professional programmers, it is essential to identify participants without programming skills. Existing screening questions are efficient, cheating robust, and effectively differentiate programmers from non-programmers. However, the release of ChatGPT raises concerns about their continued effectiveness in identifying non-programmers. In a simulated attack, we showed that Chat-GPT can easily solve existing screening questions. Therefore, we designed new ChatGPT-resistant screening questions using visual concepts and code comprehension tasks. We evaluated 28 screening questions in an online study with 121 participants involving programmers and non-programmers. Our results showed that questions using visualizations of well-known programming concepts performed best in differentiating between programmers and non-programmers. Participants prompted to use ChatGPT struggled to solve the tasks. They considered ChatGPT ineffective and changed their strategy after a few screening questions. In total, we present six ChatGPT-resistant screening questions that effectively identify non-programmers. We provide recommendations on setting up a ChatGPT-resistant screening instrument that takes less than three minutes to complete by excluding 99.47% of non-programmers while including 94.83% of programmers.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {181},
numpages = {13},
keywords = {chatgpt, programmer screening, developer study, study protection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3645072,
author = {Grewal, Balreet and Lu, Wentao and Nadi, Sarah and Bezemer, Cor-Paul},
title = {Analyzing Developer Use of ChatGPT Generated Code in Open Source GitHub Projects},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645072},
doi = {10.1145/3643991.3645072},
abstract = {The rapid development of large language models such as ChatGPT have made them particularly useful to developers in generating code snippets for their projects. To understand how ChatGPT's generated code is leveraged by developers, we conducted an empirical study of 3,044 ChatGPT-generated code snippets integrated within GitHub projects. A median of 54% of the generated lines of code is found in the project's code and this code typically remains unchanged once added. The modifications of the 76 code snippets that changed in a subsequent commit, consisted of minor functionality changes and code reorganizations that were made within a day. Our findings offer insights that help drive the development of AI-assisted programming tools. We highlight the importance of making changes in ChatGPT code before integrating it into a project.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {157–161},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643667.3648222,
author = {Aragon\'{e}s-Soria, Yaiza and Oriol, Manuel},
title = {C4Q: A Chatbot for Quantum},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643667.3648222},
doi = {10.1145/3643667.3648222},
abstract = {Quantum computing is a growing field that promises many real-world applications such as quantum cryptography or quantum finance. The number of people able to use quantum computing is however still very small. This limitation comes from the difficulty to understand the concepts and to know how to start coding. Therefore, there is a need for tools that can assist non-expert in overcoming this complexity. One possibility would be to use existing conversational agents. Unfortunately ChatGPT and other Large-Language Models produce inaccurate results.This article presents C4Q, a chatbot that answers accurately basic questions and guides users when trying to code quantum programs. Contrary to other approaches C4Q uses a pre-trained large language model only to discover and classify user requests. It then generates an accurate answer using an own engine. Thanks to this architectural design, C4Q's answers are always correct, and thus C4Q can become a support tool that makes quantum computing more available to non-experts.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
pages = {29–36},
numpages = {8},
keywords = {chatbot, quantum computing, software engineering},
location = {Lisbon, Portugal},
series = {Q-SE 2024}
}

@inproceedings{10.1145/3643916.3644409,
author = {Fagadau, Ionut Daniel and Mariani, Leonardo and Micucci, Daniela and Riganelli, Oliviero},
title = {Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644409},
doi = {10.1145/3643916.3644409},
abstract = {Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering.In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {24–34},
numpages = {11},
keywords = {prompt engineering, code generation, copilot},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643795.3648392,
author = {Li, Yichen and Peng, Yun and Huo, Yintong and Lyu, Michael R.},
title = {Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648392},
doi = {10.1145/3643795.3648392},
abstract = {Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of LLMs of repository-level code completion. We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {70–74},
numpages = {5},
keywords = {large language model, code generation},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3639478.3643112,
author = {Chen, Xiaolei and Shi, Jie and Chen, Jia and Wang, Peng and Wang, Wei},
title = {High-precision Online Log Parsing with Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643112},
doi = {10.1145/3639478.3643112},
abstract = {System logs are vital for diagnosing system failures, with log parsing converting unstructured logs into structured data. Existing methods fall into two categories: non-deep-learning approaches cluster logs based on stats but often miss semantic information, resulting in poor performance. Deep-learning approaches excel at identifying variables and constants but often lack generalizability beyond training data. And they always suffer from low efficiency. This paper proposes a novel LLM-based log parsing approach, named Hooglle, to address these challenges. Leveraging a large language model, Hooglle extracts templates for precise and generalized parsing. To overcome the efficiency issue, we propose a prefix-tree-based full-matching strategy which significantly improves parsing efficiency. Extensive evaluation across real-world datasets showcases Hooglle's superior performance on 16 public benchmark datasets.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {354–355},
numpages = {2},
keywords = {log parsing, large language model, prefix tree},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639085,
author = {Chen, Junkai and Hu, Xing and Li, Zhenhao and Gao, Cuiyun and Xia, Xin and Lo, David},
title = {Code Search is All You Need? Improving Code Suggestions with Code Search},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639085},
doi = {10.1145/3597503.3639085},
abstract = {Modern integrated development environments (IDEs) provide various automated code suggestion techniques (e.g., code completion and code generation) to help developers improve their efficiency. Such techniques may retrieve similar code snippets from the code base or leverage deep learning models to provide code suggestions. However, how to effectively enhance the code suggestions using code retrieval has not been systematically investigated. In this paper, we study and explore a retrieval-augmented framework for code suggestions. Specifically, our framework leverages different retrieval approaches and search strategies to search similar code snippets. Then the retrieved code is used to further enhance the performance of language models on code suggestions. We conduct experiments by integrating different language models into our framework and compare the results with their original models. We find that our framework noticeably improves the performance of both code completion and code generation by up to 53.8% and 130.8% in terms of BLEU-4, respectively. Our study highlights that integrating the retrieval process into code suggestions can improve the performance of code suggestions by a large margin.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {73},
numpages = {13},
keywords = {code suggestion, code search, language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639176,
author = {OBrien, David and Biswas, Sumon and Imtiaz, Sayem Mohammad and Abdalkareem, Rabe and Shihab, Emad and Rajan, Hridesh},
title = {Are Prompt Engineering and TODO Comments Friends or Foes? An Evaluation on GitHub Copilot},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639176},
doi = {10.1145/3597503.3639176},
abstract = {Code intelligence tools such as GitHub Copilot have begun to bridge the gap between natural language and programming language. A frequent software development task is the management of technical debts, which are suboptimal solutions or unaddressed issues which hinder future software development. Developers have been found to "self-admit" technical debts (SATD) in software artifacts such as source code comments. Thus, is it possible that the information present in these comments can enhance code generative prompts to repay the described SATD? Or, does the inclusion of such comments instead cause code generative tools to reproduce the harmful symptoms of described technical debt? Does the modification of SATD impact this reaction? Despite the heavy maintenance costs caused by technical debt and the recent improvements of code intelligence tools, no prior works have sought to incorporate SATD towards prompt engineering. Inspired by this, this paper contributes and analyzes a dataset consisting of 36,381 TODO comments in the latest available revisions of their respective 102,424 repositories, from which we sample and manually generate 1,140 code bodies using GitHub Copilot. Our experiments show that GitHub Copilot can generate code with the symptoms of SATD, both prompted and unprompted. Moreover, we demonstrate the tool's ability to automatically repay SATD under different circumstances and qualitatively investigate the characteristics of successful and unsuccessful comments. Finally, we discuss gaps in which GitHub Copilot's successors and future researchers can improve upon code intelligence tasks to facilitate AI-assisted software maintenance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {219},
numpages = {13},
keywords = {technical debt, GitHub copilot, LLM, code generation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643795.3648380,
author = {S Kumar, Smitha and Adam Lones, Michael and Maarek, Manuel and Zantout, Hind},
title = {Investigating the Proficiency of Large Language Models in Formative Feedback Generation for Student Programmers},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648380},
doi = {10.1145/3643795.3648380},
abstract = {Generative AI has considerably altered traditional workplace practice across numerous industries. Ever since the emergence of large language models (LLMs), their potential to generate formative feedback for introductory programming courses has been extensively researched. However, most of these studies have focused on Python. In this work, we examine the bug-fixing and feedback-generation abilities of Code Llama and ChatGPT for Java programming assignments using our new Java benchmark called CodeWBugs. The results indicate that ChatGPT performs reasonably well, and was able to fix 94.33% programs. By comparison, we observed high variability in the results from Code Llama. We further analyzed the impact of different types of prompts and observed that prompts that included task descriptions and test inputs yielded better results. In most cases, the LLMs precisely localized the bugs and also offered guidance on how to proceed. Nevertheless, we also noticed incorrect responses generated by the LLMs, emphasizing the need to validate responses before disseminating feedback to learners.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {88–93},
numpages = {6},
keywords = {large language models (LLM), GPT-4, feedback, java programming, program repair},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643787.3648033,
author = {Balfroid, Martin and Vanderose, Beno\^{\i}t and Devroey, Xavier},
title = {Towards LLM-Generated Code Tours for Onboarding},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648033},
doi = {10.1145/3643787.3648033},
abstract = {Onboarding new developers is a challenge for any software project. Addressing this challenge relies on human resources (e.g., having a senior developer write documentation or mentor the new developer). One promising solution is using annotated code tours. While this approach partially lifts the need for mentorship, it still requires a senior developer to write this interactive form of documentation. This paper argues that a Large Language Model (LLM) might help with this documentation process. Our approach is to record the stack trace between a failed test and a faulty method. We then extract code snippets from the methods in this stack trace using CodeQL, a static analysis tool and have them explained by gpt-3.5-turbo-1106, the LLM behind ChatGPT. Finally, we evaluate the quality of a sample of these generated tours using a checklist. We show that the automatic generation of code tours is feasible but has limitations like redundant and low-level explanations.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {65–68},
numpages = {4},
keywords = {large language models, code tour, developer onboarding},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00181,
author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
title = {On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00181},
doi = {10.1109/ICSE48619.2023.00181},
abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in ~46% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (±28%).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2149–2160},
numpages = {12},
keywords = {empirical study, recommender systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643795.3648393,
author = {Chusap, Krerkkiat and Liu, Chang},
title = {Gauging Tech Community Acceptance of Rapid Prototyping in Unfamiliar Programming Languages using LLM Chatbots},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648393},
doi = {10.1145/3643795.3648393},
abstract = {Large Language Model (LLM) chatbots such as ChatGPT possess information not only about human languages but also computer languages. It is now possible to perform programming and software design tasks with assistance from ChatGPT. We are particularly interested in how the software development community views the use of LLM chatbots in rapid prototyping using unfamiliar programming languages. In four different tech events, several example scenarios of how a tech-savvy engineer could use ChatGPT to prototype apps in unfamiliar programming languages were demonstrated, including a health education app. The four events include an IEEE chapter workshop, an IEEE WIE (Woman In Engineering) meeting, an IEEE joint chapter talk, and a university-level Computer Science class. The responses from the tech audience showed that the majority perceived value in the use of LLM chatbots in these contexts, even though there were subtle differences among different groups. This shows the need for further research on how to effectively incorporate LLM chatbots into traditional software design workflow to better serve the software development community.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {8–13},
numpages = {6},
keywords = {software engineering, software design, rapid prototyping, LLMs, ChatGPT},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1109/ICSE48619.2023.00199,
author = {Griebl, Elisabeth and Fein, Benedikt and Oberm\"{u}ller, Florian and Fraser, Gordon and Just, Ren\'{e}},
title = {On the Applicability of Language Models to Block-Based Programs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00199},
doi = {10.1109/ICSE48619.2023.00199},
abstract = {Block-based programming languages like SCRATCH are increasingly popular for programming education and end-user programming. Recent program analyses build on the insight that source code can be modelled using techniques from natural language processing. Many of the regularities of source code that support this approach are due to the syntactic overhead imposed by textual programming languages. This syntactic overhead, however, is precisely what block-based languages remove in order to simplify programming. Consequently, it is unclear how well this modelling approach performs on block-based programming languages. In this paper, we investigate the applicability of language models for the popular block-based programming language SCRATCH. We model SCRATCH programs using n-gram models, the most essential type of language model, and transformers, a popular deep learning model. Evaluation on the example tasks of code completion and bug finding confirm that blocks inhibit predictability, but the use of language models is nevertheless feasible. Our findings serve as foundation for improving tooling and analyses for block-based languages.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2374–2386},
numpages = {13},
keywords = {block-based programs, scratch, natural language model, code completion, bugram},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639154,
author = {Asare, Owura and Nagappan, Meiyappan and Asokan, N.},
title = {A User-centered Security Evaluation of Copilot},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639154},
doi = {10.1145/3597503.3639154},
abstract = {Code generation tools driven by artificial intelligence have recently become more popular due to advancements in deep learning and natural language processing that have increased their capabilities. The proliferation of these tools may be a double-edged sword because while they can increase developer productivity by making it easier to write code, research has shown that they can also generate insecure code. In this paper, we perform a user-centered evaluation GitHub's Copilot to better understand its strengths and weaknesses with respect to code security. We conduct a user study where participants solve programming problems (with and without Copilot assistance) that have potentially vulnerable solutions. The main goal of the user study is to determine how the use of Copilot affects participants' security performance. In our set of participants (n=25), we find that access to Copilot accompanies a more secure solution when tackling harder problems. For the easier problem, we observe no effect of Copilot access on the security of solutions. We also observe no disproportionate impact of Copilot use on particular kinds of vulnerabilities. Our results indicate that there are potential security benefits to using Copilot, but more research is warranted on the effects of the use of code generation tools on technically complex problems with security requirements.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {158},
numpages = {11},
keywords = {user study, code generation, copilot, security, software engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3644032,
title = {AST '24: Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {AST continues to be a venue for researchers and practitioners where they can discuss high quality research contributions on methods for software test automation, and various case studies reporting practices in this field. Indeed, software test automation is a discipline that has produced noteworthy research in the last decade.The special theme of AST 2024 is "Test automation for and with Generative AI". This innovative and promising research direction deals with the application of test automation technologies to the testing of Generative AI applications, as well as the adoption of generative AI to facilitate test automation.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3650105.3652289,
author = {van Dam, Tim and van der Heijden, Frank and de Bekker, Philippe and Nieuwschepen, Berend and Otten, Marc and Izadi, Maliheh},
title = {Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652289},
doi = {10.1145/3650105.3652289},
abstract = {Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may not transfer well to functional languages, but that code completion on functional languages is feasible. Consequently, this shows the need for more high-quality Haskell datasets. A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions. Finally, we release HumanEval-Haskell, along with the fine-tuned models and all code required to reproduce our experiments on GitHub [41].},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {91–102},
numpages = {12},
keywords = {language models, automatic code completion, line completion, programming languages, functional programming, haskell, CodeGPT, UniXcoder},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3643795.3648382,
author = {Piya, Sanyogita and Sullivan, Allison},
title = {LLM4TDD: Best Practices for Test Driven Development Using Large Language Models},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648382},
doi = {10.1145/3643795.3648382},
abstract = {In today's society, we are becoming increasingly dependent on software systems. However, we also constantly witness the negative impacts of buggy software. Program synthesis aims to improve software correctness by automatically generating the program given an outline of the expected behavior. For decades, program synthesis has been an active research field, with recent approaches looking to incorporate Large Language Model. This paper explores the concept of LLM4TDD, where we guide Large Language Models to generate code iteratively using a test-driven development methodology. We conduct an empirical evaluation using ChatGPT and coding problems from LeetCode to investigate the impact of different test, prompt and problem attributes on the efficacy of LLM4TDD.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3650105.3652297,
author = {Wang, Guanyu and Li, Yuekang and Liu, Yi and Deng, Gelei and Li, Tianlin and Xu, Guosheng and Liu, Yang and Wang, Haoyu and Wang, Kailong},
title = {MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652297},
doi = {10.1145/3650105.3652297},
abstract = {Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation.This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM-augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissimilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, emphasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM-augmented applications.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {12–23},
numpages = {12},
keywords = {metamorphic testing, vector matching, augmented generation},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3650105.3652292,
author = {Dhulipala, Hridya and Yadavally, Aashish and Nguyen, Tien N.},
title = {Planning to Guide LLM for Code Coverage Prediction},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652292},
doi = {10.1145/3650105.3652292},
abstract = {Code coverage serves as a crucial metric to assess testing effectiveness, measuring the degree to which a test suite exercises different facets of the code, such as statements, branches, or paths. Despite its significance, coverage profilers necessitate access to the entire codebase, constraining their usefulness in situations where the code is incomplete or execution is not feasible, and even cost-prohibitive. In this paper, we present CodePilot, a plan-based prompting approach grounded in program semantics, which collaborates with a Large Language Model (LLM) to enhance code coverage prediction. To address the intricacies of predicting code coverage, CodePilot employs planning by discerning various types of statements in an execution flow. Planning empowers GPT to autonomously generate plans based on guided examples, and then CodePilot prompts the GPT model to predict code coverage (Action) based on the plan it generated (Reasoning). Our experiments evaluating CodePilot demonstrate high accuracy, achieving up to 55% in exact-match and 89% in statement-match. It performs relatively better than the baselines, achieving up to 33% and 19% relatively higher in those metrics. We also showed that due to highly accurate plans (90%), GPT model predicts better code coverage. Moreover, we show CodePilot's utility in correctly predicting the least covered statements.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {24–34},
numpages = {11},
keywords = {AI4SE, large language models, planning, code coverage analysis},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3650105.3652291,
author = {Khakzad Shahandashti, Kimya and Sivakumar, Mithila and Mohajer, Mohammad Mahdi and Boaye Belle, Alvine and Wang, Song and Lethbridge, Timothy},
title = {Assessing the Impact of GPT-4 Turbo in Generating Defeaters for Assurance Cases},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652291},
doi = {10.1145/3650105.3652291},
abstract = {Assurance cases (ACs) are structured arguments that allow verifying the correct implementation of the created systems' non-functional requirements (e.g., safety, security). This allows for preventing system failure. The latter may result in catastrophic outcomes (e.g., loss of lives). ACs support the certification of systems in compliance with industrial standards, e.g., DO-178C and ISO 26262. Identifying defeaters ---arguments that challenge these ACs --- is crucial for enhancing ACs' robustness and confidence. To automatically support that task, we propose a novel approach that explores the potential of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, in identifying defeaters within ACs formalized using the Eliminative Argumentation (EA) notation. Our preliminary evaluation assesses the model's ability to comprehend and generate arguments in this context and the results show that GPT-4 turbo is very proficient in EA notation and can generate different types of defeaters.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {52–56},
numpages = {5},
keywords = {large language models, assurance cases, assurance defeaters, system certification, FM for Requirement Engineering},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3643796.3648457,
author = {Gonzalez-Barahona, Jesus M.},
title = {IDEs in the age of LLMs and XR},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648457},
doi = {10.1145/3643796.3648457},
abstract = {Let's imagine that in a few years generative AI has changed software development dramatically, taking charge of most of the programming tasks. Let's also assume that extended reality devices became ubiquitous, being the preferred interface for interacting with computers. This paper proposes how this situation would impact IDEs, by exploring how the development process would be affected, and analyzing which tools would be needed for supporting developers.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {66–69},
numpages = {4},
keywords = {XR, VR, AR, extended reality, LLM, generative AI, IDE, software development},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643991.3645077,
author = {Champa, Arifa Islam and Rabbi, Md Fazle and Nachuma, Costain and Zibran, Minhaz F.},
title = {ChatGPT in Action: Analyzing Its Use in Software Development},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645077},
doi = {10.1145/3643991.3645077},
abstract = {The emergence of AI tools such as ChatGPT is being used to assist with software development, but little is known of how developers utilize these tools as well as the capabilities of these tools in software engineering tasks. Using the DevGPT dataset, we conduct quantitative analyses of the tasks developers seek assistance from ChatGPT and how effectively ChatGPT addresses them. We also examine the impact of initial prompt quality on conversation length. The findings reveal where ChatGPT is most and least suited to assist in the identified 12 software development tasks. The insights from this research would guide the software developers, researchers, and AI tool providers in optimizing these tools for more effective programming aid.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {182–186},
numpages = {5},
keywords = {ChatGPT conversation, software development tasks, task efficiency, prompt quality},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639476.3639764,
author = {Sallou, June and Durieux, Thomas and Panichella, Annibale},
title = {Breaking the Silence: the Threats of Using LLMs in Software Engineering},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639764},
doi = {10.1145/3639476.3639764},
abstract = {Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {102–106},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3643795.3648376,
author = {Fei, Haoxiang and Zhang, Yu and Zhang, Hongbo and Wang, Yanlin and Liu, Qing},
title = {MoonBit: Explore the Design of an AI-Friendly Programming Language},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648376},
doi = {10.1145/3643795.3648376},
abstract = {MoonBit, a new general-purpose programming language designed for cloud and edge computing, was initiated in late 2022, coinciding with the announcement of ChatGPT. Language models like GPT, capable of producing practical programs, are revolutionizing the way we write programs and interact with computers. However, significant challenges persist, such as the models' inability to understand the global context of a whole project with its dependencies, the need for human verification and correction of generated code, and the lack of assurance in meeting basic requirements like syntactic correctness.In this paper, we explore the design of the MoonBit language highlighting its AI integration, emphasizing the synergy between traditional code intelligence and large language model capabilities. We also introduce a real-time, semantics-based sampler to guide the inference process of language models. This approach ensures the generated programs are both syntactically correct and free from obvious semantic flaws, such as type errors. Crucially, this has been achieved with minimal impact on overall performance. Our evaluation demonstrates a notable improvement in code quality, achieved without sacrificing the models' responsiveness.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {79–83},
numpages = {5},
keywords = {large language model, program synthesize, static analysis},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3643991.3644918,
author = {Tufano, Rosalia and Mastropaolo, Antonio and Pepe, Federica and Dabic, Ozren and Di Penta, Massimiliano and Bavota, Gabriele},
title = {Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644918},
doi = {10.1145/3643991.3644918},
abstract = {Large Language Models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as OpenAI's ChatGPT. While the potential of LLMs in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of LLMs in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of ChatGPT to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which ChatGPT was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via ChatGPT. The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit LLMs in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {571–583},
numpages = {13},
keywords = {ChatGPT, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3623306,
author = {Guo, Qi and Cao, Junming and Xie, Xiaofei and Liu, Shangqing and Li, Xiaohong and Chen, Bihuan and Peng, Xin},
title = {Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623306},
doi = {10.1145/3597503.3623306},
abstract = {Code review is an essential activity for ensuring the quality and maintainability of software projects. However, it is a time-consuming and often error-prone task that can significantly impact the development process. Recently, ChatGPT, a cutting-edge language model, has demonstrated impressive performance in various natural language processing tasks, suggesting its potential to automate code review processes. However, it is still unclear how well ChatGPT performs in code review tasks. To fill this gap, in this paper, we conduct the first empirical study to understand the capabilities of ChatGPT in code review tasks, specifically focusing on automated code refinement based on given code reviews. To conduct the study, we select the existing benchmark CodeReview and construct a new code review dataset with high quality. We use CodeReviewer, a state-of-the-art code review tool, as a baseline for comparison with ChatGPT. Our results show that ChatGPT outperforms CodeReviewer in code refinement tasks. Specifically, our results show that ChatGPT achieves higher EM and BLEU scores of 22.78 and 76.44 respectively, while the state-of-the-art method achieves only 15.50 and 62.88 on a high-quality code review dataset. We further identify the root causes for ChatGPT's underperformance and propose several strategies to mitigate these challenges. Our study provides insights into the potential of ChatGPT in automating the code review process, and highlights the potential research directions.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {34},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644907,
author = {Jiang, Wenxin and Yasmin, Jerin and Jones, Jason and Synovic, Nicholas and Kuo, Jiashen and Bielanski, Nathaniel and Tian, Yuan and Thiruvathukal, George K. and Davis, James C.},
title = {PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644907},
doi = {10.1145/3643991.3644907},
abstract = {The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.Our artifact is available at https://github.com/PurdueDualityLab/PeaTMOSS-Artifact. Our dataset is available at https://transfer.rcac.purdue.edu/file-manager?origin_id=ff978999-16c2-4b50-ac7a-947ffdc3eb1d&amp;origin_path=%2F.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {431–443},
numpages = {13},
keywords = {datasets, machine learning, deep neural networks, model zoos, package registries, open-source, empirical software engineering},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/GREENS66463.2025.00013,
author = {Sikand, Samarth and Mehra, Rohit and Pathania, Priyavanshi and Bamby, Nikhil and Sharma, Vibhu Saujanya and Kaulgud, Vikrant and Podder, Sanjay and Burden, Adam P.},
title = {Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon &amp; Energy estimation for LLMs},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00013},
doi = {10.1109/GREENS66463.2025.00013},
abstract = {While Generative AI stands to be one of the fastest adopted technologies ever, studies have made evident that the usage of Large Language Models (LLMs) puts significant burden on energy grids and our environment. It may prove a hindrance to the Sustainability goals of any organization. A crucial step in any Sustainability strategy is monitoring or estimating the energy consumption of various components. While there exist multiple tools for monitoring energy consumption, there is a dearth of tools/frameworks for estimating the consumption or carbon emissions. Current drawbacks of both monitoring and estimation tools include high input data points, intrusive nature, high error margin, etc. We posit that leveraging emerging LLM benchmarks and related data points can help overcome aforementioned challenges while balancing accuracy of the emission estimations. To that extent, we discuss the challenges of current approaches and present our evolving framework, R-ICE, which estimates prompt level inference carbon emissions by leveraging existing state-of-the-art(SOTA) benchmark. This direction provides a more practical and non-intrusive way to enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our promising validation results suggest that benchmark-based modelling holds great potential for inference emission estimation and warrants further exploration from the scientific community.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {55–59},
numpages = {5},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3644032.3644443,
author = {El Haji, Khalid and Brandt, Carolin and Zaidman, Andy},
title = {Using GitHub Copilot for Test Generation in Python: An Empirical Study},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644443},
doi = {10.1145/3644032.3644443},
abstract = {Writing unit tests is a crucial task in software development, but it is also recognized as a time-consuming and tedious task. As such, numerous test generation approaches have been proposed and investigated. However, most of these test generation tools produce tests that are typically difficult to understand. Recently, Large Language Models (LLMs) have shown promising results in generating source code and supporting software engineering tasks. As such, we investigate the usability of tests generated by GitHub Copilot, a proprietary closed-source code generation tool that uses an LLM. We evaluate GitHub Copilot's test generation abilities both within and without an existing test suite, and we study the impact of different code commenting strategies on test generations.Our investigation evaluates the usability of 290 tests generated by GitHub Copilot for 53 sampled tests from open source projects. Our findings highlight that within an existing test suite, approximately 45.28% of the tests generated by Copilot are passing tests; 54.72% of generated tests are failing, broken, or empty tests. Furthermore, if we generate tests using Copilot without an existing test suite in place, we observe that 92.45% of the tests are failing, broken, or empty tests. Additionally, we study how test method comments influence the usability of test generations.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {45–55},
numpages = {11},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3643667.3648223,
author = {Guo, Xiaoyu and Zhao, Jianjun and Zhao, Pengzhan},
title = {On Repairing Quantum Programs Using ChatGPT},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643667.3648223},
doi = {10.1145/3643667.3648223},
abstract = {Automated Program Repair (APR) is a vital area in software engineering that generates automatic patches for vulnerable programs. While numerous techniques have been proposed for repairing classical programs, quantum programming lacks a comparable automated repair technique. In this initial exploration, we investigate the use of ChatGPT for quantum program repair and evaluate its performance on Bugs4Q, a benchmark suite of quantum program bugs. Our findings demonstrate the feasibility of employing ChatGPT for quantum program repair. Specifically, we assess ChatGPT's ability to address bugs within the Bugs4Q benchmark, revealing its success in repairing 29 out of 38 bugs. This research represents a promising step towards automating the repair process for quantum programs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
pages = {9–16},
numpages = {8},
keywords = {automatic program repair, quantum programming, debugging},
location = {Lisbon, Portugal},
series = {Q-SE 2024}
}

@inproceedings{10.1145/3643991.3645074,
author = {Jin, Kailun and Wang, Chung-Yu and Pham, Hung Viet and Hemmati, Hadi},
title = {Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645074},
doi = {10.1145/3643991.3645074},
abstract = {Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts of modern software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {167–171},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3650105.3652300,
author = {Wu, Yifan and Li, Ying and Yu, Siyu},
title = {Commit Message Generation via ChatGPT: How Far Are We?},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652300},
doi = {10.1145/3650105.3652300},
abstract = {Commit messages concisely describe code changes in natural language and are important for software maintenance. Various automatic commit message generation approaches have been proposed, such as retrieval-based, learning-based, and hybrid approaches. Recently, large language models have shown impressive performance in many natural language processing tasks. Among them, ChatGPT is the most popular one and has attracted wide attention from the software engineering community. ChatGPT demonstrates the ability of in-context learning (ICL), which allows ChatGPT to perform downstream tasks by learning from just a few demonstrations without explicit model tuning. However, it remains unclear how well ChatGPT performs in the commit message generation task via ICL. Therefore, in this paper, we conduct a preliminary evaluation of ChatGPT with ICL on commit message generation. Specifically, we first explore the impact of two key settings on the performance of ICL on commit message generation. Then, based on the best settings, we compare ChatGPT with several state-of-the-art approaches. The results show that a carefully-designed demonstration can lead to substantial improvements for ChatGPT on commit message generation. Furthermore, ChatGPT outperforms all the retrieval-based and learning-based approaches in terms of BLEU, METEOR, ROUGE-L, and Cider, and is comparable to hybrid approaches. Based on our findings, we outline several open challenges and opportunities for ChatGPT-based commit message generation.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {124–129},
numpages = {6},
keywords = {commit message generation, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3643991.3644895,
author = {Storey, Margaret Anne D},
title = {Questioning the questions we ask about the impact of AI on software engineering},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644895},
doi = {10.1145/3643991.3644895},
abstract = {The recent advent and wide diffusion of generative AI has initiated a fundamental change in how software is developed. This technology is just one innovation along a long arc of disruptions in software engineering that include the internet, high-level programming languages, integrated development environments, open source, agile development, and social coding environments. Disruptive technologies such as these show the potential to augment and accelerate development activities along many socio-technical dimensions, while altering fundamental business processes and paradigms. Yet paradoxically, these innovations have the potential to eventually undermine the very advancements they seek to promote, rendering technologies and methods obsolete [1].When any new disruptive technology emerges, successful software companies that traditionally respond well to incremental innovations often fail when they suffer from inertia to change or don't anticipate how people will interact with the new technology. Similarly, researchers constrained by rigid research discipline can be slow to react, and may fail to recognize important and urgent societal and industrial needs. Researchers and companies alike may struggle in knowing which metrics to use and even how to measure the impact of change, further misleading their efforts to adapt.In this talk, I question the way we select research questions in software engineering and how we study them, particularly in the face of innovations such as generative AI. To provoke a change in our research, I introduce a disruptive playbook to steer us towards broader and more novel research directions. This step-by-step playbook is first illustrated by applying it to a prior disruptive technology, Stack Overflow. I will discuss how the playbook provides a new lens for reflecting on this body of research and how doing so reveals new insights. I then use the playbook, assisted with a customized research playbook GPT, to brainstorm and frame new research directions about the emerging disruptive innovations in software engineering that are being built on top of generative AI.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {530},
numpages = {1},
keywords = {software engineering, disruptive innovations, playbook, research questions},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3639188,
author = {Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis and Nguyen, Tien},
title = {Programming Assistant for Exception Handling with CodeBERT},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639188},
doi = {10.1145/3597503.3639188},
abstract = {With practical code reuse, the code fragments from developers' forums often migrate to applications. Owing to the incomplete nature of such fragments, they often lack the details on exception handling. The adaptation for exception handling to the codebase is not trivial as developers must learn and memorize what API methods could cause exceptions and what exceptions need to be handled. We propose Neurex, an exception handling recommender that learns from complete code, and accepts a given Java code snippet and recommends 1) if a try-catch block is needed, 2) what statements need to be placed in a try block, and 3) what exception types need to be caught in the catch clause. Inspired by the sequence chunking techniques in natural language processing, we design Neurex via a multi-tasking model with the fine-tuning of the large language model CodeBERT for these three exception handling recommendation tasks. Via the large language model, Neurex can learn the surrounding context, leading to better learning the dependencies among the API elements, and the relations between the statements and the corresponding exception types needed to be handled.Our empirical evaluation shows that Neurex correctly performs all three exception handling recommendation tasks in 71.5% of the cases with a F1-score of 70.2%, which is a relative improvement of 166% over the baseline. It achieves high F1-score from 98.2%-99.7% in try-catch block necessity checking (a relative improvement of up to 55.9% over the baselines). It also correctly decides both the need for try-catch block(s) and the statements to be placed in try blocks with the F1-scores of 74.7% and 87.1% at the instance and statement levels, an improvement of 129.1% and 44.9% over the baseline, respectively. Our extrinsic evaluation shows that Neurex relatively improves over the baseline by 56.5% in F1-score for detecting exception-related bugs in incomplete Android code snippets.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {94},
numpages = {13},
keywords = {AI4SE, large language models, automated exception handling},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00178,
author = {Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
title = {Learning Deep Semantics for Test Completion},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00178},
doi = {10.1109/ICSE48619.2023.00178},
abstract = {Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TECO---a deep learning model using code semantics for test completion. The key insight underlying TECO is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. TECO extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TECO, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that TECO achieves an exact-match accuracy of 18, which is 29% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, TECO can generate runnable code in 29% of the cases compared to 18% obtained by the best baseline. Moreover, TECO is significantly better than prior work on test oracle generation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2111–2123},
numpages = {13},
keywords = {test completion, deep neural networks, programming language semantics},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3645075,
author = {Raj, Rachna and Costa, Diego Elias},
title = {The role of library versions in Developer-ChatGPT conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645075},
doi = {10.1145/3643991.3645075},
abstract = {The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {172–176},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643788.3648012,
author = {Santos, Sofia and Saraiva, Jo\~{a}o and Ribeiro, Francisco},
title = {Large Language Models in Automated Repair of Haskell Type Errors},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648012},
doi = {10.1145/3643788.3648012},
abstract = {This paper introduces a new method of Automated Program Repair that relies on a combination of the GPT-4 Large Language Model and automatic type checking of Haskell programs. This method identifies the source of a type error and asks GPT-4 to fix that specific portion of the program. Then, QuickCheck is used to automatically generate a large set of test cases to validate whether the generated repair behaves as the correct solution. Our publicly available experiments revealed a success rate of 88.5% in normal conditions. However, more detailed testing should be performed to more accurately evaluate this form of APR.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {42–45},
numpages = {4},
keywords = {automated program repair, large language model, fault localization, code generation, type checking, automatic testing},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643991.3645071,
author = {Siddiq, Mohammed Latif and Roney, Lindsay and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Quality Assessment of ChatGPT Generated Code and their Use by Developers},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645071},
doi = {10.1145/3643991.3645071},
abstract = {The release of large language models (LLMs) like ChatGPT has revolutionized software development. Prior works explored ChatGPT's generated response quality, the effectiveness of different prompting techniques, its performance in programming contests, etc. However, there is limited information regarding the practical usage of ChatGPT by software developers. This data mining challenge focuses on DevGPT, a curated dataset of developer-ChatGPT conversations encompassing prompts with ChatGPT's responses, including code snippets. Our paper leverages this dataset to investigate (RQ1) whether ChatGPT generates Python &amp; Java code with quality issues; (RQ2) whether ChatGPT-generated code is merged into a repository, and, if it does, to what extent developers change them; and (RQ3) what are the main use cases for ChatGPT besides code generation. We found that ChatGPT-generated code suffers from using undefined/unused variables and improper documentation. They also have security issues related to improper resources and exception management. Our results show that ChatGPT-generated codes are hardly merged, and they are significantly modified before merging. Based on an analysis of developers' discussions and the developer-ChatGPT chats, we found that developers use ChatGPT for every stage of software development and leverage it to learn about new frameworks and development kits.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {152–156},
numpages = {5},
keywords = {datasets, ChatGPT, security, quality, pull-request, open-coding},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639476.3639768,
author = {Velasco, Alejandro and Palacio, David N. and Rodriguez-Cardenas, Daniel and Poshyvanyk, Denys},
title = {Which Syntactic Capabilities Are Statistically Learned by Masked Language Models for Code?},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639768},
doi = {10.1145/3639476.3639768},
abstract = {This paper discusses the limitations of evaluating Masked Language Models (MLMs) in code completion tasks. We highlight that relying on accuracy-based measurements may lead to an overestimation of models' capabilities by neglecting the syntax rules of programming languages. To address these issues, we introduce a technique called SyntaxEval in which Syntactic Capabilities are used to enhance the evaluation of MLMs. SyntaxEval automates the process of masking elements in the model input based on their Abstract Syntax Trees (ASTs). We conducted a case study on two popular MLMs using data from GitHub repositories. Our results showed negative causal effects between the node types and MLMs' accuracy. We conclude that MLMs under study fail to predict some syntactic capabilities.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {72–76},
numpages = {5},
keywords = {deep learning, code generation, interpretability, transformers, dl4se},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3643916.3644402,
author = {Corso, Vincenzo and Mariani, Leonardo and Micucci, Daniela and Riganelli, Oliviero},
title = {Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644402},
doi = {10.1145/3643916.3644402},
abstract = {AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers.In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {13–23},
numpages = {11},
keywords = {AI-based code assistants, code completion, copilot, ChatGPT, tabnine, bard, empirical study},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1109/ICSE48619.2023.00203,
author = {Tufano, Rosalia and Pascarella, Luca and Bavota, Gabriele},
title = {Automating Code-Related Tasks Through Transformers: The Impact of Pre-Training},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00203},
doi = {10.1109/ICSE48619.2023.00203},
abstract = {Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of transformers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2425–2437},
numpages = {13},
keywords = {pre-training, code recommenders},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623342,
author = {Yang, Aidan Z. H. and Le Goues, Claire and Martins, Ruben and Hellendoorn, Vincent},
title = {Large Language Models for Test-Free Fault Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623342},
doi = {10.1145/3597503.3623342},
abstract = {Fault Localization (FL) aims to automatically localize buggy lines of code, a key first step in many manual and automatic debugging tasks. Previous FL techniques assume the provision of input tests, and often require extensive program analysis, program instrumentation, or data preprocessing. Prior work on deep learning for APR struggles to learn from small datasets and produces limited results on real-world programs. Inspired by the ability of large language models (LLMs) of code to adapt to new tasks based on very few examples, we investigate the applicability of LLMs to line level fault localization. Specifically, we propose to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs to produce LLMAO, the first language model based fault localization approach that locates buggy lines of code without any test coverage information. We fine-tune LLMs with 350 million, 6 billion, and 16 billion parameters on small, manually curated corpora of buggy programs such as the Defects4J corpus. We observe that our technique achieves substantially more confidence in fault localization when built on the larger models, with bug localization performance scaling consistently with the LLM size. Our empirical evaluation shows that LLMAO improves the Top-1 results over the state-of-the-art machine learning fault localization (MLFL) baselines by 2.3%--54.4%, and Top-5 results by 14.4%-35.6%. LLMAO is also the first FL technique trained using a language model architecture that can detect security vulnerabilities down to the code line level.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {17},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639789,
author = {Deljouyi, Amirhossein},
title = {Understandable Test Generation Through Capture/Replay and LLMs},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639789},
doi = {10.1145/3639478.3639789},
abstract = {Automatic unit test generators, particularly search-based software testing (SBST) tools such as EvoSuite, efficiently generate unit test suites with acceptable coverage. Although this removes the burden of writing unit tests from developers, these generated tests often pose challenges in terms of comprehension for developers. In my doctoral research, I aim to investigate strategies to address the issue of comprehensibility in generated test cases and improve the test suite in terms of effectiveness. To achieve this, I introduce four projects leveraging Capture/Replay and Large Language Model (LLM) techniques.Capture/Replay carves information from End-to-End (E2E) tests, enabling the generation of unit tests containing meaningful test scenarios and actual test data. Moreover, the growing capabilities of large language models (LLMs) in language analysis and transformation play a significant role in improving readability in general. Our proposed approach involves leveraging E2E test scenario extraction alongside an LLM-guided approach to enhance test case understandability, augment coverage, and establish comprehensive mock and test oracles.In this research, we endeavor to conduct both a quantitative analysis and a user evaluation of the quality of the generated tests in terms of executability, coverage, and understandability.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {261–263},
numpages = {3},
keywords = {automatic test generation, carving and replaying, large language models, readability, understandability, unit testing},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3645073,
author = {Przymus, Piotr and Fejzer, Miko\l{}aj and Narundefinedbski, Jakub and Stencel, Krzysztof},
title = {How I Learned to Stop Worrying and Love ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645073},
doi = {10.1145/3643991.3645073},
abstract = {In the dynamic landscape of software engineering, the emergence of ChatGPT-generated code signifies a distinctive and evolving paradigm in development practices. We delve into the impact of interactions with ChatGPT on the software development process, specifically analysing its influence on source code changes. Our emphasis lies in aligning code with ChatGPT conversations, separately analysing the user-provided context of the code and the extent to which the resulting code has been influenced by ChatGPT. Additionally, employing survival analysis techniques, we examine the longevity of ChatGPT-generated code segments in comparison to lines written traditionally. The goal is to provide valuable insights into the transformative role of ChatGPT in software development, illuminating its implications for code evolution and sustainability within the ecosystem.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {162–166},
numpages = {5},
keywords = {ChatGPT, DevGPT, MSR, code survival analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644917,
author = {Koyanagi, Kei and Wang, Dong and Noguchi, Kotaro and Kondo, Masanari and Serebrenik, Alexander and Kamei, Yasutaka and Ubayashi, Naoyasu},
title = {Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644917},
doi = {10.1145/3643991.3644917},
abstract = {GitHub Copilot is an AI-enabled tool that automates program synthesis. It has gained significant attention since its launch in 2021. Recent studies have extensively examined Copilot's capabilities in various programming tasks, as well as its security issues. However, little is known about the effect of different natural languages on code suggestion. Natural language is considered a social bias in the field of NLP, and this bias could impact the diversity of software engineering. To address this gap, we conducted an empirical study to investigate the effect of three popular natural languages (English, Japanese, and Chinese) on Copilot. We used 756 questions of varying difficulty levels from AtCoder contests for evaluation purposes. The results highlight that the capability varies across natural languages, with Chinese achieving the worst performance. Furthermore, regardless of the type of natural language, the performance decreases significantly as the difficulty of questions increases. Our work represents the initial step in comprehending the significance of natural languages in Copilot's capability and introduces promising opportunities for future endeavors.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {481–486},
numpages = {6},
keywords = {code suggestion, GitHub copilot, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00182,
author = {Ciniselli, Matteo and Pascarella, Luca and Aghajani, Emad and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
title = {Source Code Recommender Systems: The Practitioners' Perspective},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00182},
doi = {10.1109/ICSE48619.2023.00182},
abstract = {The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 "requirements" that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being "aware" of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2161–2172},
numpages = {12},
keywords = {code recommender systems, empirical study, practitioners' survey},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3647632.3647986,
author = {Shrestha, Shristi and Mahmoud, Anas},
title = {Generating Rate Features for Mobile Applications},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3647986},
doi = {10.1145/3647632.3647986},
abstract = {Mobile application (app) stores employ standardized mechanisms for rating hosted apps, typically in the form of free text reviews and numerical rating scales. App users use these mechanisms to express their opinions about their apps and discover apps that fit their specific needs. However, existing app rating systems do not take into account the operational characteristics of application domains. Thus, generated user reviews are often short, subjective, and one-dimensional. To overcome these limitations, in this paper, we propose a multi-dimensional rating system for mobile apps. Our assumption is that an adaptive goal-based app rating system can prompt users to generate higher-quality reviews. To achieve our research objectives, we initially apply extractive summarization to generate short and concise summaries of salient themes in app reviews. Extracted summaries are then fed to a language model to generate Rate Features for apps. Our results show that the language model GPT-3.5 can be prompted to generate abstract, neutral, and domain-specific Rate Features that are aligned to a large extent with user goals in different application domains.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {54–64},
numpages = {11},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1145/3641822.3641870,
author = {Sera, Rie and Washizaki, Hironori and Chen, Junyan and Fukazawa, Yoshiaki and Taga, Masahiro and Nakagawa, Kazuyuki and Sakai, Yusuke and Honda, Kiyoshi},
title = {Development of Data-driven Persona Including User Behavior and Pain Point through Clustering with User Log of B2B Software},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641870},
doi = {10.1145/3641822.3641870},
abstract = {Persona --- fictional user profiles --- are used to identify user requirements in software engineering. However, methods targeting revisions, especially for existing B2B services, remain sparse. This paper proposes a method that integrates several models, including k-means clustering, term frequency-inverse document frequency (TF-IDF), and generative AI. Users' behavior tendencies, pain points, and other attributes are output solely from clickstream log data, bypassing the traditional survey-based approaches of previous studies. Clickstreams are vectorized and categorized, whereas users are further analyzed on the basis of time and content of their clickstreams. A case study was conducted with evaluations carried out both quantitatively and qualitatively. The results suggest that, although some parameters still need improvement, fairly rated persona outcomes were attained.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {85–90},
numpages = {6},
keywords = {persona, data-driven design, pain point, clustering, user experience, user behavior, user analytics, machine learning, data science},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00119,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Che, Xing and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Fill in the Blank: Context-Aware Automated Text Input Generation for Mobile GUI Testing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00119},
doi = {10.1109/ICSE48619.2023.00119},
abstract = {Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1355–1367},
numpages = {13},
keywords = {text input generation, GUI testing, android app, large language model, prompt-tuning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643661.3643953,
author = {Alshahwan, Nadia and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy},
title = {Assured Offline LLM-Based Software Engineering},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643953},
doi = {10.1145/3643661.3643953},
abstract = {In this paper we address the following question: How can we use Large Language Models (LLMs) to improve code independently of a human, while ensuring that the improved code(1) does not regress the properties of the original code ?(2) improves the original in a verifiable and measurable way ?To address this question, we advocate Assured LLM-Based Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of LLM's propensity to hallucinate. It allows us to generate code using LLMs, independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers.This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {7–12},
numpages = {6},
keywords = {large language models (LLMs), genetic improvement (GI), search based software engineering (SBSE), llama, codellama, automated code generation},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1145/3643991.3645084,
author = {Chouchen, Moataz and Bessghaier, Narjes and Begoug, Mahi and Ouni, Ali and Alomar, Eman and Mkaouer, Mohamed Wiem},
title = {How Do Software Developers Use ChatGPT? An Exploratory Study on GitHub Pull Requests},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645084},
doi = {10.1145/3643991.3645084},
abstract = {Nowadays, Large Language Models (LLMs) play a pivotal role in software engineering. Developers can use LLMs to address software development-related tasks such as documentation, code refactoring, debugging, and testing. ChatGPT, released by OpenAI, has become the most prominent LLM. In particular, ChatGPT is a cutting-edge tool for providing recommendations and solutions for developers in their pull requests (PRs). However, little is known about the characteristics of PRs that incorporate ChatGPT compared to those without it and what developers usually use it for. To this end, we quantitatively analyzed 243 PRs that listed at least one ChatGPT prompt against a representative sample of 384 PRs without any ChatGPT prompts. Our findings show that developers use ChatGPT in larger, time-consuming pull requests that are five times slower to be closed than PRs that do not use ChatGPT. Furthermore, we perform a qualitative analysis to build a taxonomy of the topics developers primarily address in their prompts. Our analysis results in a taxonomy comprising 8 topics and 32 sub-topics. Our findings highlight that ChatGPT is often used in review-intensive pull requests. Moreover, our taxonomy enriches our understanding of the developer's current applications of ChatGPT.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {212–216},
numpages = {5},
keywords = {large language models, ChatGPT, manual analysis, mining software repositories, pull requests},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3639222,
author = {Zhou, Xin and Kim, Kisub and Xu, Bowen and Han, Donggyun and Lo, David},
title = {Out of Sight, Out of Mind: Better Automatic Vulnerability Repair by Broadening Input Ranges and Sources},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639222},
doi = {10.1145/3597503.3639222},
abstract = {The advances of deep learning (DL) have paved the way for automatic software vulnerability repair approaches, which effectively learn the mapping from the vulnerable code to the fixed code. Nevertheless, existing DL-based vulnerability repair methods face notable limitations: 1) they struggle to handle lengthy vulnerable code, 2) they treat code as natural language texts, neglecting its inherent structure, and 3) they do not tap into the valuable expert knowledge present in the expert system. To address this, we propose VulMaster, a Transformer-based neural network model that excels at generating vulnerability repairs by comprehensively understanding the entire vulnerable code, irrespective of its length. This model also integrates diverse information, encompassing vulnerable code structures and expert knowledge from the CWE system. We evaluated VulMaster on a real-world C/C++ vulnerability repair dataset comprising 1,754 projects with 5,800 vulnerable functions. The experimental results demonstrated that VulMaster exhibits substantial improvements compared to the learning-based state-of-the-art vulnerability repair approach. Specifically, VulMaster improves the EM, BLEU, and CodeBLEU scores from 10.2% to 20.0%, 21.3% to 29.3%, and 32.5% to 40.9%, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {88},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643787.3648032,
author = {Shome, Arumoy and Cruz, Luis and Van Deursen, Arie},
title = {Towards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648032},
doi = {10.1145/3643787.3648032},
abstract = {We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, we mined 54, 070 Jupyter notebooks from Github and created a catalogue of 269 semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of information mined from the Jupyter notebooks---visualisations, Python source code, and associated markdown text. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool's effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {29–32},
numpages = {4},
keywords = {SE4AI, NLP4Code, ML testing, visualisations, assertions, computational notebooks, automated tool},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3643991.3645080,
author = {Sagdic, Ertugrul and Bayram, Arda and Islam, Md Rakibul},
title = {On the Taxonomy of Developers' Discussion Topics with ChatGPT},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645080},
doi = {10.1145/3643991.3645080},
abstract = {Large language models (LLMs) like ChatGPT can generate text for various prompts. With exceptional reasoning capabilities, ChatGPT (particularly the GPT-4 model) has achieved widespread adoption across many tasks - from creative writing to domain-specific inquiries, code generation, and more. This research analyzed the DevGPT dataset to determine common topics posed by developers interacting with ChatGPT. The DevGPT dataset comprises ChatGPT interactions from GitHub issues, pull requests and discussions. By employing a mixed-methods approach combining unsupervised semantic modeling and expert qualitative analysis we categorize the topics developers discuss when interacting with ChatGPT.Our approach reveals 17 topics within seven categories, with over 25% of prompts focused on advanced programming guidance. Additional areas of significant query volume include DevOps workflows, SQL, databases, and specialized domains, such as localization, streaming media, and image processing. This research effectively illuminates core topics and dependencies that motivate developers to leverage ChatGPT. The taxonomy classification further clarifies critical areas to better customize AI tools for aligning with workflows and needs within software engineering contexts.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {197–201},
numpages = {5},
keywords = {DevGPT, ChatGPT, software engineering, topic taxonomy},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644926,
author = {Idialu, Oseremen Joy and Mathews, Noble Saji and Maipradit, Rungroj and Atlee, Joanne M. and Nagappan, Mei},
title = {Whodunit: Classifying Code as Human Authored or GPT-4 Generated - A case study on CodeChef problems},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644926},
doi = {10.1145/3643991.3644926},
abstract = {Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI-generated code as their work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier on the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {394–406},
numpages = {13},
keywords = {code stylometry, ChatGPT, AI code, GPT-4 generated code, authorship profiling, software engineering},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00125,
author = {Jiang, Nan and Liu, Kevin and Lutellier, Thibaud and Tan, Lin},
title = {Impact of Code Language Models on Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00125},
doi = {10.1109/ICSE48619.2023.00125},
abstract = {Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task.Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%--1,267% improvement to CLMs and enables them to fix 46%--164% more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs.This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1430–1442},
numpages = {13},
keywords = {automated program repair, code language model, fine-tuning, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3645085,
author = {Mondal, Saikat and Bappon, Suborno Deb and Roy, Chanchal K.},
title = {Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645085},
doi = {10.1145/3643991.3645085},
abstract = {Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses. Thus, optimal prompt construction is essential for maximizing the utility and performance of ChatGPT. However, sub-optimal prompt design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from ChatGPT. Existing studies explore several prompt patterns and strategies to improve the relevance of responses generated by ChatGPT. However, the exploration of constraints that necessitate the submission of multiple prompts is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in prompt design that demand multiple iterations. In particular, we manually analyze 686 prompts that were submitted to resolve issues related to Java and Python programming languages and identify eleven prompt design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single prompts in ChatGPT. Second, we attempt to reproduce the ChatGPT response by consolidating multiple prompts into a single one. We can completely consolidate prompts with four gaps (e.g., missing context) and partially consolidate prompts with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal prompts mitigating these gaps. Our study findings and evidence can - (a) save users time, (b) reduce costs, and (c) increase user satisfaction.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {222–226},
numpages = {5},
keywords = {prompt design, ChatGPT, prompt consolidation, qualitative analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643691.3648587,
author = {Bano, Muneera and Zowghi, Didar and Gervasi, Vincenzo},
title = {A Vision for Operationalising Diversity and Inclusion in AI},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648587},
doi = {10.1145/3643691.3648587},
abstract = {The growing presence of Artificial Intelligence (AI) in various sectors necessitates systems that accurately reflect societal diversity. This study seeks to envision the operationalization of the ethical imperatives of diversity and inclusion (D&amp;I) within AI ecosystems, addressing the current disconnect between ethical guidelines and their practical implementation. A significant challenge in AI development is the effective operationalization of D&amp;I principles, which is critical to prevent the reinforcement of existing biases and ensure equity across AI applications. This paper proposes a vision of a framework for developing a tool utilizing persona-based simulation by Generative AI (GenAI). The approach aims to facilitate the representation of the needs of diverse users in the requirements analysis process for AI software. The proposed framework is expected to lead to a comprehensive persona repository with diverse attributes that inform the development process with detailed user narratives. This research contributes to the development of an inclusive AI paradigm that ensures future technological advances are designed with a commitment to the diverse fabric of humanity.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {36–45},
numpages = {10},
keywords = {artificial intelligence, diversity and inclusion, requirements engineering, persona},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1145/3639477.3639745,
author = {Kuang, Jinxi and Liu, Jinyang and Huang, Junjie and Zhong, Renyi and Gu, Jiazhen and Yu, Lan and Tan, Rui and Yang, Zengyin and Lyu, Michael R.},
title = {Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639745},
doi = {10.1145/3639477.3639745},
abstract = {Due to the scale and complexity of cloud systems, a system failure would trigger an "alert storm", i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts.To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the LLM reasoning module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning capabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform. The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud X1.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {369–380},
numpages = {12},
keywords = {alert aggregation, cloud systems, software reliability},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639478.3639792,
author = {Rodriguez-Cardenas, Daniel},
title = {Beyond Accuracy and Robustness Metrics for Large Language Models for Code},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639792},
doi = {10.1145/3639478.3639792},
abstract = {In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot [31] and Google's CodeBot [21] exemplify how LLMc contributes to substantial time and effort savings in software development. However, despite their widespread use, there is a growing need to thoroughly assess LLMc, as current evaluation processes heavily rely on accuracy and robustness metrics, lacking consensus on additional influential factors in code generation. This gap hinders a holistic understanding of LLMc performance, impacting interpretability, efficiency, bias, fairness, and robustness. The challenges in benchmarking and data maintenance compound this issue, underscoring the necessity for a comprehensive evaluation approach. To address these issues, this dissertation proposes the development of a benchmarking infrastructure, named HolBench, aimed at overcoming gaps in evaluating LLMc quality. The goal is to standardize testing scenarios, facilitate meaningful comparisons across LLMc, and provide multi-metric measurements beyond a sole focus on accuracy. This approach aims to decrease the costs associated with advancing LLMc research, enhancing their reliability for adoption in academia and industry.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {159–161},
numpages = {3},
keywords = {deep learning, code generation, interpretability, transformers},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3645083,
author = {Das, Joy Krishan and Mondal, Saikat and Roy, Chanchal},
title = {Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645083},
doi = {10.1145/3643991.3645083},
abstract = {Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users' requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like ChatGPT to enhance problem-solving efficiency. While previous studies have demonstrated the potential of ChatGPT in areas such as automatic program repair, debugging, and code generation, there is a lack of study on how developers explicitly utilize ChatGPT to resolve issues in their tracking system. Hence, this study aims to examine the interaction between ChatGPT and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the code reliability by confirming if the code produced by ChatGPT was integrated into the project's codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use ChatGPT for brainstorming solutions but often opt to write their code instead of using ChatGPT-generated code, possibly due to concerns over the generation of "hallucinated" code, as highlighted in the literature.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {217–221},
numpages = {5},
keywords = {ChatGPT, issue tracking, NiCad, code clone},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@proceedings{10.1145/3643795,
title = {LLM4Code '24: Proceedings of the 1st International Workshop on Large Language Models for Code},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the first edition of the InternationalWorkshop on Large Language Models for Code (LLM4Code). Large Language Models (LLMs), which are large-scale models being trained on massive textual corpora, have achieved significant advances in various domains, including Software Engineering (SE). Recently, there has been a growing interest in applying LLMs to assist software development and maintenance, such as code generation and comprehension, test generation, and program repair. Although the application of LLMs on code-relevant tasks has shown very promising performance, there is a huge potential to explore this growing domain further. The motivation of the LLM4Code workshop is to provide a platform for academics and practitioners to discuss and share their ideas on applying and developing LLMs to solve code-relevant problems in SE activities.The LLM4Code workshop is concerned with the research on how to better apply LLMs to solve code-relevant tasks, how to design better LLMs for code-relevant tasks, and how to better benchmark LLMs on code-relevant tasks. The workshop aims to achieve multiple goals as follows. Firstly, the workshop aims to provide an opportunity for participants to discuss novel ideas and preliminary results on LLMs for solving code-relevant SE problems, to exchange the latest progress in this domain. Secondly, the workshop aims to encourage participants to discuss the open challenges and problems of LLM4code, to identify important future directions in this domain. Finally, the workshop aims to encourage participants to share infrastructures and benchmarks that are foundational and beneficial for future research in this domain.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643991.3645081,
author = {AlOmar, Eman Abdullah and Venkatakrishnan, Anushkrishna and Mkaouer, Mohamed Wiem and Newman, Christian and Ouni, Ali},
title = {How to refactor this code? An exploratory study on developer-ChatGPT refactoring conversations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645081},
doi = {10.1145/3643991.3645081},
abstract = {Large Language Models (LLMs), like ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with ChatGPT. In this paper, our goal is to explore conversations between developers and ChatGPT related to refactoring to better understand how developers identify areas for improvement in code and how ChatGPT addresses developers' needs. Our approach relies on text mining refactoring-related conversations from 17,913 ChatGPT prompts and responses, and investigating developers' explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while ChatGPT typically includes the refactoring intention; and (3) various learning settings when prompting ChatGPT in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {202–206},
numpages = {5},
keywords = {refactoring documentation, ChatGPT, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639478.3643065,
author = {Zhang, Chenyuan and Liu, Hao and Zeng, Jiutian and Yang, Kejing and Li, Yuhong and Li, Hui},
title = {Prompt-Enhanced Software Vulnerability Detection Using ChatGPT},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643065},
doi = {10.1145/3639478.3643065},
abstract = {With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Moreover, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {276–277},
numpages = {2},
keywords = {software vulnerability detection, prompt engineering, large language model, chatgpt},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643787.3648037,
author = {Qiao, Yining and Rojas, Jos\'{e} Miguel},
title = {What’s in a Display Name? An Empirical Study on the Use of Display Names in Open-Source JUnit Tests},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648037},
doi = {10.1145/3643787.3648037},
abstract = {Readability is an important aspect of any production or test code artefact. During development, testing and maintenance, the readability of a unit test can be a key contributor to its usefulness for a range of tasks, including refactoring, debugging, and test augmentation. Several strategies have been proposed both in academia and in industry to build readability into unit tests, e.g., test code summarisation and test naming conventions. Display names constitute an industry-led effort to incorporate natural language descriptions into unit tests. In this paper, we investigate the use of display names in a large dataset of open-source Java projects. Our study reveals that despite being a stable feature of the JUnit framework for at least five years, display names are not widely used in open-source projects yet. We analyse existing display names in terms of length, language and grammatical structure, explore the use of a large language model to generate display names similar to those open-source developers write, and develop a taxonomy of display name smells aimed at fostering a more cohesive and coherent use of the feature by developers.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {17–24},
numpages = {8},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3643915.3644082,
author = {Boltz, Nicolas and Getir Yaman, Sinem and Inverardi, Paola and De Lemos, Rog\'{e}rio and Van Landuyt, Dimitri and Zisman, Andrea},
title = {Human empowerment in self-adaptive socio-technical systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644082},
doi = {10.1145/3643915.3644082},
abstract = {Recent advances in generative AI and machine learning have stirred up fears about the unbridled adoption of autonomous, self-adaptive decision mechanisms in socio-technical systems. This vision paper explores the critical relationship between software-intensive systems and the empowerment of humans as individuals and society. We highlight the need for human empowerment within the context of self-adaptive socio-technical systems (SASTSs), which require mechanisms for balancing of diverse needs, values, and ethics on the individual, community, and societal levels. We propose an architecture comprised of Connector and Mediator elements, and third-party auditing, to support interactions and ensure preservation of human needs, values, and ethics. We use an example of Robot-Assisted A&amp;E Triage system to motivate and illustrate our work and discuss some open challenges for future research.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {200–206},
numpages = {7},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3643916.3644422,
author = {Kumar, Jahnavi and Chimalakonda, Sridhar},
title = {What Do Developers Feel About Fast-Growing Programming Languages? An Exploratory Study},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644422},
doi = {10.1145/3643916.3644422},
abstract = {The developer community has witnessed an unprecedented surge in recent years, with over 100 million active developers on the GitHub platform in 2023. Along with it, there is a significant rise and adoption of new programming languages, frameworks and tools. The study aims to comprehend how developers perceive these fast-growing programming languages by performing emotion analysis of developer's comments posted in various software artifacts such as pull requests, issues and commits of GitHub repositories. In this regard, we employed a fine-tuned small 'Large Language Model' (sLLM) to detect emotions, leveraging a balanced dataset from existing literature complemented with additional manual annotations from our collected data. We have analyzed 10 fast-growing programming languages, examining 1.8 million comments from 4.1 million non-code artifacts. To further validate our findings, we have performed a qualitative survey and analysis with 28 developers. Our study reveals insights into the developers emotion associated with these fast-growing languages. Notably, "Surprise" is the predominant emotion associated with these languages.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {178–189},
numpages = {12},
keywords = {emotion analysis, developer emotions, programming languages},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639475.3640097,
author = {Shi, Jieke and Yang, Zhou and Kang, Hong Jin and Xu, Bowen and He, Junda and Lo, David},
title = {Greening Large Language Models of Code},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640097},
doi = {10.1145/3639475.3640097},
abstract = {Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers' devices becomes essential.To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness (e.g., prediction accuracy on downstream tasks). The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160\texttimes{} smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184\texttimes{} less), carbon footprint (up to 157\texttimes{} less), and inference latency (up to 76\texttimes{} faster), with only a negligible loss in effectiveness (1.67%).},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {142–153},
numpages = {12},
keywords = {language models of code, configuration tuning, multi-objective optimization},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@inproceedings{10.1109/ICSE48619.2023.00060,
author = {Fang, Sen and Zhang, Tao and Tan, Youshuai and Jiang, He and Xia, Xin and Sun, Xiaobing},
title = {RepresentThemAll: A Universal Learning Representation of Bug Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00060},
doi = {10.1109/ICSE48619.2023.00060},
abstract = {Deep learning techniques have shown promising performance in automated software maintenance tasks associated with bug reports. Currently, all existing studies learn the customized representation of bug reports for a specific downstream task. Despite early success, training multiple models for multiple downstream tasks faces three issues: complexity, cost, and compatibility, due to the customization, disparity, and uniqueness of these automated approaches. To resolve the above challenges, we propose RepresentThemAll, a pre-trained approach that can learn the universal representation of bug reports and handle multiple downstream tasks. Specifically, RepresentThemAll is a universal bug report framework that is pre-trained with two carefully designed learning objectives: one is the dynamic masked language model and another one is a contrastive learning objective, "find yourself". We evaluate the performance of RepresentThemAll on four downstream tasks, including duplicate bug report detection, bug report summarization, bug priority prediction, and bug severity prediction. Our experimental results show that RepresentThemAll outperforms all baseline approaches on all considered downstream tasks after well-designed fine-tuning.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {602–614},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3645076,
author = {Rabbi, Md Fazle and Champa, Arifa Islam and Zibran, Minhaz F. and Islam, Md Rakibul},
title = {AI Writes, We Analyze: The ChatGPT Python Code Saga},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645076},
doi = {10.1145/3643991.3645076},
abstract = {In this study, we quantitatively analyze 1,756 AI-written Python code snippets in the DevGPT dataset and evaluate them for quality and security issues. We systematically distinguish the code snippets as either generated by ChatGPT from scratch (ChatGPT-generated) or modified user-provided code (ChatGPT-modified). The results reveal that ChatGPT-modified code more frequently displays quality issues compared to ChatGPT-generated code. The findings provide insights into the inherent limitations of AI-written code and emphasize the need for scrutiny before integrating such pieces of code into software systems.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {177–181},
numpages = {5},
keywords = {code quality, code security, ChatGPT, Python, CWE, analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643795.3648385,
author = {Koziolek, Heiko and Koziolek, Anne},
title = {LLM-based Control Code Generation using Image Recognition},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648385},
doi = {10.1145/3643795.3648385},
abstract = {LLM-based code generation could save significant manual efforts in industrial automation, where control engineers manually produce control logic for sophisticated production processes. Previous attempts in control logic code generation lacked methods to interpret schematic drawings from process engineers. Recent LLMs now combine image recognition, trained domain knowledge, and coding skills. We propose a novel LLM-based code generation method that generates IEC 61131-3 Structure Text control logic source code from Piping-and-Instrumentation Diagrams (P&amp;IDs) using image recognition. We have evaluated the method in three case study with industrial P&amp;IDs and provide first evidence on the feasibility of such a code generation besides experiences on image recognition glitches.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {38–45},
numpages = {8},
keywords = {large language models, code generation, P&amp;IDs, IEC 61131-3, image recognition, industrial case study, industrial automation, PLC, DCS, ChatGPT, GPT4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3597503.3623298,
author = {Huang, Yuchao and Wang, Junjie and Liu, Zhe and Wang, Yawen and Wang, Song and Chen, Chunyang and Hu, Yuanzhe and Wang, Qing},
title = {CrashTranslator: Automatically Reproducing Mobile Application Crashes Directly from Stack Trace},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623298},
doi = {10.1145/3597503.3623298},
abstract = {Crash reports are vital for software maintenance since they allow the developers to be informed of the problems encountered in the mobile application. Before fixing, developers need to reproduce the crash, which is an extremely time-consuming and tedious task. Existing studies conducted the automatic crash reproduction with the natural language described reproducing steps. Yet we find a non-neglectable portion of crash reports only contain the stack trace when the crash occurs. Such stack-trace-only crashes merely reveal the last GUI page when the crash occurs, and lack step-by-step guidance. Developers tend to spend more effort in understanding the problem and reproducing the crash, and existing techniques cannot work on this, thus calling for a greater need for automatic support. This paper proposes an approach named CrashTranslator to automatically reproduce mobile application crashes directly from the stack trace. It accomplishes this by leveraging a pre-trained Large Language Model to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically. We evaluate CrashTranslator on 75 crash reports involving 58 popular Android apps, and it successfully reproduces 61.3% of the crashes, outperforming the state-of-the-art baselines by 109% to 206%. Besides, the average reproducing time is 68.7 seconds, outperforming the baselines by 302% to 1611%. We also evaluate the usefulness of CrashTranslator with promising results.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {18},
numpages = {13},
keywords = {bug reproduction, stack trace, mobile application testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639476.3639777,
author = {Mishra, Shyamal and Chatterjee, Preetha},
title = {Exploring ChatGPT for Toxicity Detection in GitHub},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639777},
doi = {10.1145/3639476.3639777},
abstract = {Fostering a collaborative and inclusive environment is crucial for the sustained progress of open source development. However, the prevalence of negative discourse, often manifested as toxic comments, poses significant challenges to developer well-being and productivity. To identify such negativity in project communications, especially within large projects, automated toxicity detection models are necessary. To train these models effectively, we need large software engineering-specific toxicity datasets. However, such datasets are limited in availability and often exhibit imbalance (e.g., only 6 in 1000 GitHub issues are toxic) [1], posing challenges for training effective toxicity detection models. To address this problem, we explore a zero-shot LLM (ChatGPT) that is pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting toxicity in software-related text. Our preliminary evaluation indicates that ChatGPT shows promise in detecting toxicity in GitHub, and warrants further investigation. We experimented with various prompts, including those designed for justifying model outputs, thereby enhancing model interpretability and paving the way for potential integration of ChatGPT-enabled toxicity detection into developer communication channels.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {6–10},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639187,
author = {Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
title = {Using an LLM to Help With Code Understanding},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639187},
doi = {10.1145/3597503.3639187},
abstract = {Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {97},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3645070,
author = {Moratis, Konstantinos and Diamantopoulos, Themistoklis and Nastos, Dimitrios-Nikitas and Symeonidis, Andreas},
title = {Write me this Code: An Analysis of ChatGPT Quality for Producing Source Code},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645070},
doi = {10.1145/3643991.3645070},
abstract = {Developers nowadays are increasingly turning to large language models (LLMs) like ChatGPT to assist them with coding tasks, inspired by the promise of efficiency and the advanced capabilities they offer. However, this raises important questions about the ease of integration and the safety of incorporating these tools into the development process. To investigate these questions, this paper examines a set of ChatGPT conversations. Upon annotating the conversations according to the intent of the developer, we focus on two critical aspects: firstly, the ease with which developers can produce suitable source code using ChatGPT, and, secondly, the quality aspects of the generated source code, determined by the compliance to standards and best practices. We research both the quality of the generated code itself and its impact on the project of the developer. Our results indicate that ChatGPT can be a useful tool for software development when used with discretion.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {147–151},
numpages = {5},
keywords = {code generation, code quality, ChatGPT, large language models},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3645069,
author = {Wu, Liangxuan and Zhao, Yanjie and Hou, Xinyi and Liu, Tianming and Wang, Haoyu},
title = {ChatGPT Chats Decoded: Uncovering Prompt Patterns for Superior Solutions in Software Development Lifecycle},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645069},
doi = {10.1145/3643991.3645069},
abstract = {The advent of Large Language Models (LLMs) like ChatGPT has markedly transformed software development, aiding tasks from code generation to issue resolution with their human-like text generation. Nevertheless, the effectiveness of these models greatly depends on the nature of the prompts given by developers. Therefore, this study delves into the DevGPT dataset, a rich collection of developer-ChatGPT dialogues, to unearth the patterns in prompts that lead to effective problem resolutions. The underlying motivation for this research is to enhance the collaboration between human developers and AI tools, thereby improving productivity and problem-solving efficacy in software development. Utilizing a combination of textual analysis and data-driven approaches, this paper seeks to identify the attributes of prompts that are associated with successful interactions, providing crucial insights for the strategic employment of ChatGPT in software engineering environments.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {142–146},
numpages = {5},
keywords = {data mining, large language model, LLM, ChatGPT},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3623326,
author = {Xu, Junjielong and Cui, Ziang and Zhao, Yuan and Zhang, Xu and He, Shilin and He, Pinjia and Li, Liqun and Kang, Yu and Lin, Qingwei and Dang, Yingnong and Rajmohan, Saravan and Zhang, Dongmei},
title = {UniLog: Automatic Logging via LLM and In-Context Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623326},
doi = {10.1145/3597503.3623326},
abstract = {Logging, which aims to determine the position of logging statements, the verbosity levels, and the log messages, is a crucial process for software reliability enhancement. In recent years, numerous automatic logging tools have been designed to assist developers in one of the logging tasks (e.g., providing suggestions on whether to log in try-catch blocks). These tools are useful in certain situations yet cannot provide a comprehensive logging solution in general. Moreover, although recent research has started to explore end-to-end logging, it is still largely constrained by the high cost of fine-tuning, hindering its practical usefulness in software development. To address these problems, this paper proposes UniLog, an automatic logging framework based on the in-context learning (ICL) paradigm of large language models (LLMs). Specifically, UniLog can generate an appropriate logging statement with only a prompt containing five demonstration examples without any model tuning. In addition, UniLog can further enhance its logging ability after warmup with only a few hundred random samples. We evaluated UniLog on a large dataset containing 12,012 code snippets extracted from 1,465 GitHub repositories. The results show that UniLog achieved the state-of-the-art performance in automatic logging: (1) 76.9% accuracy in selecting logging positions, (2) 72.3% accuracy in predicting verbosity levels, and (3) 27.1 BLEU-4 score in generating log messages. Meanwhile, UniLog requires less than 4% of the parameter tuning time needed by fine-tuning the same LLM.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {14},
numpages = {12},
keywords = {logging, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/NSE66660.2025.00007,
author = {Teuber, Samuel and Beckert, Bernhard},
title = {Next Steps in LLM-Supported Java Verification},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NSE66660.2025.00007},
doi = {10.1109/NSE66660.2025.00007},
abstract = {Recent work has shown that Large Language Models (LLMs) are not only a suitable tool for code generation but also capable of generating annotation-based code specifications. Scaling these methodologies may allow us to deduce provably correctness guarantees for large-scale software systems. In comparison to other LLM tasks, the application field of deductive verification has the notable advantage of providing a rigorous toolset to check LLM-generated solutions. This short paper provides early results on how this rigorous toolset can be used to reliably elicit correct specification annotations from an unreliable LLM oracle.},
booktitle = {2025 IEEE/ACM 1st International Workshop on Neuro-Symbolic Software Engineering (NSE)},
pages = {1–4},
numpages = {4},
location = {Ottawa, ON, Canada}
}

@inproceedings{10.1145/3643796.3648452,
author = {Marron, Mark},
title = {A New Generation of Intelligent Development Environments},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648452},
doi = {10.1145/3643796.3648452},
abstract = {The practice of programming is undergoing a revolution with the introduction of AI assisted development (copilots) and the creation of new programming languages that are designed explicitly for tooling, analysis, and automation. Integrated Development Environments (IDEs) as they are currently conceptualized have not yet responded to these changes. They are still designed around the idea of a human programmer typing textual code into an editor window with the IDE providing assistance via the integration of various tools for syntax highlighting, compilation, debugging, and (maybe) code version control. This paper presents a vision for transforming the IDE from an Integrated Development Environment to an Intelligent Development Environment. The new IDE will be designed around the idea of a human programmer as the manager or curator of a software project who, rather than manually typing in code to implement a solution, will instead use the IDE to direct AI programming agents and/or automated tools to combine existing APIs, packages, and new code to implement the needed features. In this new model, the fundamental roles of the IDE are to 1) facilitate the communication between the human programmer and the AI agents and automated tools and 2) organize the workflow tasks needed to go from requirements gathering to the final tested and validated deployed feature. This paper presents a vision for the new Intelligent Development Environment based on a range of proof-of-concept high-value scenarios we have experimented with and discusses the challenges that remain to realizing these in a cohesive intelligent development experience.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {43–46},
numpages = {4},
keywords = {interactivity, development environment, AI assisted programming},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1109/GREENS66463.2025.00010,
author = {Cappendijk, Tom and de Reus, Pepijn and Oprescu, Ana},
title = {An exploration of prompting LLMs to generate energy-efficient code},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00010},
doi = {10.1109/GREENS66463.2025.00010},
abstract = {The increasing electricity demands of personal computers, communication networks, and data centers contribute to higher atmospheric greenhouse gas emissions, which in turn lead to global warming and climate change. Therefore the energy consumption of code must be minimised. Large language models can generate code, so we study the influence of prompting for energy-efficient code by examining the energy consumption of the generated code. We use three different Python code problems of varying difficulty levels. Prompt modification is done by adding the sentence "Give me an energy-optimised solution for this problem" or by providing two Python coding best practices. The large language models used are Code Llama-70b, Code Llama-70b-Instruct, Code Llama-70b-Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in energy consumption for a specific combination of prompt optimisation, LLM, and Python code problem. However, no single optimisation prompt consistently decreases energy consumption for the same LLM across the different Python code problems.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {31–38},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3643795.3648375,
author = {Grandel, Skyler and Schmidt, Douglas C. and Leach, Kevin},
title = {Applying Large Language Models to Enhance the Assessment of Parallel Functional Programming Assignments},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648375},
doi = {10.1145/3643795.3648375},
abstract = {Courses in computer science (CS) often assess student programming assignments manually, with the intent of providing in-depth feedback to each student regarding correctness, style, efficiency, and other quality attributes. As class sizes increase, however, it is hard to provide detailed feedback consistently, especially when multiple assessors are required to handle a larger number of assignment submissions. Large language models (LLMs), such as ChatGPT, offer a promising alternative to help automate this process in a consistent, scalable, and minimally-biased manner.This paper explores ChatGPT-4's scalablility and accuracy in assessing programming assignments based on predefined rubrics in the context of a case study we conducted in an upper-level undergraduate and graduate CS course at Vanderbilt University. In this case study, we employed a method that compared assessments generated by ChatGPT-4 against human graders to measure the accuracy, precision, and recall associated with identifying programming mistakes. Our results show that when ChatGPT-4 is used properly (e.g., with appropriate prompt engineering and feature selection) it can improve objectivity and grading efficiency, thereby acting as a complementary tool to human graders for advanced computer science graduate and undergraduate students.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {102–110},
numpages = {9},
keywords = {ChatGPT, education, generative AI, large language models, prompt engineering, automated grading},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3597503.3639215,
author = {Xu, Zhiwei and Qiang, Shaohua and Song, Dinghong and Zhou, Min and Wan, Hai and Zhao, Xibin and Luo, Ping and Zhang, Hongyu},
title = {DSFM: Enhancing Functional Code Clone Detection with Deep Subtree Interactions},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639215},
doi = {10.1145/3597503.3639215},
abstract = {Functional code clone detection is important for software maintenance. In recent years, deep learning techniques are introduced to improve the performance of functional code clone detectors. By representing each code snippet as a vector containing its program semantics, syntactically dissimilar functional clones are detected. However, existing deep learning-based approaches attach too much importance to code feature learning, hoping to project all recognizable knowledge of a code snippet into a single vector. We argue that these deep learning-based approaches can be enhanced by considering the characteristics of syntactic code clone detection, where we need to compare the contents of the source code (e.g., intersection of tokens, similar flow graphs, and similar subtrees) to obtain code clones. In this paper, we propose a novel deep learning-based approach named DSFM, which incorporates comparisons between code snippets for detecting functional code clones. Specifically, we improve the typical deep clone detectors with deep subtree interactions that compare every two subtrees extracted abstract syntax trees (ASTs) of two code snippets, thereby introducing more fine-grained semantic similarity. By conducting extensive experiments on three widely-used datasets, GCJ, OJClone, and BigCloneBench, we demonstrate the great potential of deep subtree interactions in code clone detection task. The proposed DSFM outperforms the state-of-the-art approaches, including two traditional approaches, two unsupervised and four supervised deep learning-based baselines.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {221},
numpages = {12},
keywords = {code clone detection, semantic clone, code similarity, factorization machine},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639815,
author = {Velasco, Alejandro},
title = {Beyond Accuracy: Evaluating Source Code Capabilities in Large Language Models for Software Engineering},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639815},
doi = {10.1145/3639478.3639815},
abstract = {This dissertation aims to introduce interpretability techniques to comprehensively evaluate the performance of Large Language Models (LLMs) in software engineering tasks, beyond canonical metrics. In software engineering, Deep Learning techniques are widely employed across various domains, automating tasks such as code comprehension, bug fixing, code summarization, machine translation, and code generation. However, the prevalent use of accuracy-based metrics for evaluating Language Models trained on code often leads to an overestimation of their performance. Our work seeks to propose novel and comprehensive interpretability techniques to evaluate source code capabilities and provide a more nuanced understanding of LLMs performance across downstream tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {162–164},
numpages = {3},
keywords = {large language models, interpretability, DL4SE, category theory, causal inference},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639217,
author = {Wang, Wenhan and Li, Yanzhou and Li, Anran and Zhang, Jian and Ma, Wei and Liu, Yang},
title = {An Empirical Study on Noisy Label Learning for Program Understanding},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639217},
doi = {10.1145/3597503.3639217},
abstract = {Recently, deep learning models have been widely applied in program understanding tasks, and these models achieve state-of-the-art results on many benchmark datasets. A major challenge of deep learning for program understanding is that the effectiveness of these approaches depends on the quality of their datasets, and these datasets often contain noisy data samples. A typical kind of noise in program understanding datasets is label noise, which means that the target outputs for some inputs are incorrect.Researchers have proposed various approaches to alleviate the negative impact of noisy labels, and formed a new research topic: noisy label learning (NLL). In this paper, we conduct an empirical study on the effectiveness of noisy label learning on deep learning for program understanding datasets. We evaluate various NLL approaches and deep learning models on three tasks: program classification, vulnerability detection, and code summarization. From the evaluation results, we come to the following findings: 1) small trained-from-scratch models are prone to label noises in program understanding, while large pre-trained models are highly robust against them. 2) NLL approaches significantly improve the program classification accuracies for small models on noisy training sets, but they only slightly benefit large pre-trained models in classification accuracies. 3) NLL can effectively detect synthetic noises in program understanding, but struggle in detecting real-world noises. We believe our findings can provide insights on the abilities of NLL in program understanding, and shed light on future works in tackling noises in software engineering datasets. We have released our code at https://github.com/jacobwwh/noise_SE.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {95},
numpages = {12},
keywords = {program understanding, deep learning, noisy label learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639125,
author = {Zhu, Qihao and Liang, Qingyuan and Sun, Zeyu and Xiong, Yingfei and Zhang, Lu and Cheng, Shengyu},
title = {GrammarT5: Grammar-Integrated Pretrained Encoder-Decoder Neural Model for Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639125},
doi = {10.1145/3597503.3639125},
abstract = {Pretrained models for code have exhibited promising performance across various code-related tasks, such as code summarization, code completion, code translation, and bug detection. However, despite their success, the majority of current models still represent code as a token sequence, which may not adequately capture the essence of the underlying code structure.In this work, we propose GrammarT5, a grammar-integrated encoder-decoder pretrained neural model for code. GrammarT5 employs a novel grammar-integrated representation, Tokenized Grammar Rule Sequence (TGRS), for code. TGRS is constructed based on the grammar rule sequence utilized in syntax-guided code generation and integrates syntax information with code tokens within an appropriate input length. Furthermore, we suggest attaching language flags to help GrammarT5 differentiate between grammar rules of various programming languages. Finally, we introduce two novel pretraining tasks---Edge Prediction (EP), and Sub-Tree Prediction (STP) to learn syntactic information.Experiments were conducted on five code-related tasks using eleven datasets, demonstrating that GrammarT5 achieves state-of-the-art (SOTA) performance on most tasks in comparison to models of the same scale. Additionally, the paper illustrates that the proposed pretraining tasks and language flags can enhance GrammarT5 to better capture the syntax and semantics of code.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {76},
numpages = {13},
keywords = {neural networks, pretrained model, text tagging},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643656.3643900,
author = {Chen, Yang and Jabbarvand, Reyhaneh},
title = {Can ChatGPT Repair Non-Order-Dependent Flaky Tests?},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643900},
doi = {10.1145/3643656.3643900},
abstract = {Regression testing helps developers check whether the latest code changes break software functionality. Flaky tests, which can non-deterministically pass or fail on the same code version, may mislead developers' concerns, resulting in missing some bugs or spending time pinpointing bugs that do not exist. Existing flakiness detection and mitigation techniques have primarily focused on general order-dependent (OD) and implementation-dependent (ID) flaky tests. There is also a dearth of research on repairing test flakiness, out of which, mostly have focused on repairing OD flaky tests, and a few have explored repairing a subcategory of non-order-dependent (NOD) flaky tests that are caused by asynchronous waits. As a result, there is a demand for devising techniques to reproduce, detect, and repair NOD flaky tests. Large language models (LLMs) have shown great effectiveness in several programming tasks. To explore the potential of LLMs in addressing NOD flakiness, this paper investigates the possibility of using ChatGPT to repair different categories of NOD flaky tests. Our comprehensive study on 118 from the IDoFT dataset shows that ChatGPT, despite as a leading LLM with notable success in multiple code generation tasks, is ineffective in repairing NOD test flakiness, even by following the best practices for prompt crafting. We investigated the reasons behind the failure of using ChatGPT in repairing NOD tests, which provided us valuable insights about the next step to advance the field of NOD test flakiness repair.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {22–29},
numpages = {8},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1145/3597503.3639133,
author = {Al-Kaswan, Ali and Izadi, Maliheh and van Deursen, Arie},
title = {Traces of Memorisation in Large Language Models for Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639133},
doi = {10.1145/3597503.3639133},
abstract = {Large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as Software Engineering. Large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. The content of these datasets is memorised and can be extracted by attackers with data extraction attacks. In this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. We adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. We run both benchmarks against a variety of models, and perform a data extraction attack. We find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. From the training data that was identified to be potentially extractable we were able to extract 47% from a CodeGen-Mono-16B code completion model. We also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. We also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. Data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {78},
numpages = {12},
keywords = {large language models, privacy, memorisation, data leakage},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643795.3648384,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3639477.3643648,
author = {Davila, Nicole and Wiese, Igor and Steinmacher, Igor and Lucio da Silva, Lucas and Kawamoto, Andre and Favaro, Gilson Jose Peres and Nunes, Ingrid},
title = {An Industry Case Study on Adoption of AI-based Programming Assistants},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3643648},
doi = {10.1145/3639477.3643648},
abstract = {Programming assistants based on artificial intelligence (AI), such as ChatGPT and GitHub Copilot, have gained worldwide popularity recently. Studies in software development have explored the adoption of these tools, investigating their characteristics and impacts and how practitioners interact and perceive them. To contribute to this growing body of knowledge, in this study, we aim to explore the adoption of AI-based programming assistants in the Brazilian industry. More specifically, we aim to understand how practitioners of a particular Brazilian agroindustry-related company perceive and use AI-based tools to develop software. Using an online survey, we collected and analyzed 72 responses from employees of the studied company. Our findings suggest that practitioners mainly adopt ChatGPT and GitHub Copilot, interacting with these tools to accelerate online searching, typing, and syntax recall. A recurrent difficulty is the lack of context in the suggestions provided by these tools, but participants work on detailed descriptions to contextualize and cope with this challenge. Among the reasons for not using AI-based tools, the most influential is that participants use a commercial programming language, i.e., Uniface, which these tools lack examples. Our results provide insights into the state of the practice related to AI-based programming assistants and discuss implications for practitioners and researchers.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {92–102},
numpages = {11},
keywords = {artificial intelligence, generative AI, ChatGPT, industry case study, software development},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639474.3640058,
author = {Lehtinen, Teemu and Koutcheme, Charles and Hellas, Arto},
title = {Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To Program Comprehension Questions},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640058},
doi = {10.1145/3639474.3640058},
abstract = {Recent research has explored the creation of questions from code submitted by students. These Questions about Learners' Code (QLCs) are created through program analysis, exploring execution paths, and then creating code comprehension questions from these paths and the broader code structure. Responding to the questions requires reading and tracing the code, which is known to support students' learning. At the same time, computing education researchers have witnessed the emergence of Large Language Models (LLMs) that have taken the community by storm. Researchers have demonstrated the applicability of these models especially in the introductory programming context, outlining their performance in solving introductory programming problems and their utility in creating new learning resources. In this work, we explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in answering QLCs that are generated from code that the LLMs have created. Our results show that although the state-of-the-art LLMs can create programs and trace program execution when prompted, they easily succumb to similar errors that have previously been recorded for novice programmers. These results demonstrate the fallibility of these models and perhaps dampen the expectations fueled by the recent LLM hype. At the same time, we also highlight future research possibilities such as using LLMs to mimic students as their behavior can indeed be similar for some specific tasks.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {221–232},
numpages = {12},
keywords = {QLCs, large language models, artificial intelligence, introductory programming, program comprehension},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3643916.3645030,
author = {Khajezade, Mohamad and Wu, Jie JW and Fard, Fatemeh Hendijani and Rodriguez-Perez, Gema and Shehata, Mohamed Sami},
title = {Investigating the Efficacy of Large Language Models for Code Clone Detection},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3645030},
doi = {10.1145/3643916.3645030},
abstract = {Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are 'generative' tasks. However, there is limited research on the usage of LLMs for 'non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We then conducted an analysis to understand the strengths and weaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language CCD attaining an F1-score of 0.877 and achieves comparable performance to fully fine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the prompt and the difficulty level of the problems has an impact on the performance of ChatGPT. Finally, we provide insights and future directions based on our initial analysis1.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {161–165},
numpages = {5},
keywords = {large language models, code clone detection, zero-shot learning, few-shot learning},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639476.3639779,
author = {Luo, Weilin and Fang, Weiyuan and Qiu, Junming and Wan, Hai and Liu, Yanan and Ye, Rongzhen},
title = {ITG: Trace Generation via Iterative Interaction between LLM Query and Trace Checking},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639779},
doi = {10.1145/3639476.3639779},
abstract = {Due to the complexity of linear temporal logic (LTL) trace generation (PSPACE-Complete), existing neural network-based approaches will fail as the formula sizes increase. Recently, large language models (LLMs) have demonstrated remarkable reasoning capabilities, benefiting from efficient training on hyper-scale data. Inspired by this, we propose an iterative interaction framework for applying LLMs, exemplified by ChatGPT, to generate a trace satisfying a given LTL formula. The key insight behind it is to transfer the powerful reasoning capabilities of LLM to LTL trace generation via iterative interaction between LLM reasoning and logical reasoning. Preliminary results show that compared with the state-of-the-art approach, the accuracy is relatively improved by 9.7%-23.4%. Besides, we show that our framework is able to produce heuristics for new tasks, which provides a reference for other reasoning-heavy tasks requiring heuristics.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {11–15},
numpages = {5},
keywords = {large language model, linear temporal logic, satisfiability checking, trace generation, trace checking},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639173,
author = {Chen, Yizhou and Sun, Zeyu and Gong, Zhihao and Hao, Dan},
title = {Improving Smart Contract Security with Contrastive Learning-based Vulnerability Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639173},
doi = {10.1145/3597503.3639173},
abstract = {Currently, smart contract vulnerabilities (SCVs) have emerged as a major factor threatening the transaction security of blockchain. Existing state-of-the-art methods rely on deep learning to mitigate this threat. They treat each input contract as an independent entity and feed it into a deep learning model to learn vulnerability patterns by fitting vulnerability labels. It is a pity that they disregard the correlation between contracts, failing to consider the commonalities between contracts of the same type and the differences among contracts of different types. As a result, the performance of these methods falls short of the desired level.To tackle this problem, we propose a novel Contrastive Learning Enhanced Automated Recognition Approach for Smart Contract Vulnerabilities, named Clear. In particular, Clear employs a contrastive learning (CL) model to capture the fine-grained correlation information among contracts and generates correlation labels based on the relationships between contracts to guide the training process of the CL model. Finally, it combines the correlation and the semantic information of the contract to detect SCVs. Through an empirical evaluation of a large-scale real-world dataset of over 40K smart contracts and compare 13 state-of-the-art baseline methods. We show that Clear achieves (1) optimal performance over all baseline methods; (2) 9.73%-39.99% higher F1-score than existing deep learning methods.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {156},
numpages = {11},
keywords = {smart contract, vulnerability detection, deep learning, contrastive learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3650105.3652294,
author = {Voria, Gianmario and Catolino, Gemma and Palomba, Fabio},
title = {Is Attention All You Need? Toward a Conceptual Model for Social Awareness in Large Language Models},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652294},
doi = {10.1145/3650105.3652294},
abstract = {Large Language Models (LLMs) are revolutionizing the landscape of Artificial Intelligence (AI) due to recent technological breakthroughs. Their remarkable success in aiding various Software Engineering (SE) tasks through AI-powered tools and assistants has led to the integration of LLMs as active contributors within development teams, ushering in novel modes of communication and collaboration. However, great power comes with great responsibility: ensuring that these models meet fundamental ethical principles such as fairness is still an open challenge. In this light, our vision paper analyzes the existing body of knowledge to propose a conceptual model designed to frame ethical, social, and cultural considerations that researchers and practitioners should consider when defining, employing, and validating LLM-based approaches for software engineering tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {69–73},
numpages = {5},
keywords = {social awareness, software engineering for artificial intelligence, large language models},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00027,
author = {Pasechnyuk, Dmitry and Prazdnichnykh, Anton and Evtikhiev, Mikhail and Bryksin, Timofey},
title = {Judging Adam: Studying the Performance of Optimization Methods on ML4SE Tasks},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00027},
doi = {10.1109/ICSE-NIER58687.2023.00027},
abstract = {Solving a problem with a deep learning model requires researchers to optimize the loss function with a certain optimization method. The research community has developed more than a hundred different optimizers, yet there is scarce data on optimizer performance in various tasks. In particular, none of the benchmarks test the performance of optimizers on source code-related problems. However, existing benchmark data indicates that certain optimizers may be more efficient for particular domains. In this work, we test the performance of various optimizers on deep learning models for source code and find that the choice of an optimizer can have a significant impact on the model quality, with up to two-fold score differences between some of the relatively well-performing optimizers. We also find that RAdam optimizer (and its modification with the Lookahead envelope) is the best optimizer that almost always performs well on the tasks we consider. Our findings show a need for a more extensive study of the optimizers in code-related tasks, and indicate that the ML4SE community should consider using RAdam instead of Adam as the default optimizer for code-related deep learning tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {117–122},
numpages = {6},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1145/3643991.3644880,
author = {Sutoyo, Edi and Capiluppi, Andrea},
title = {SATDAUG - A Balanced and Augmented Dataset for Detecting Self-Admitted Technical Debt},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644880},
doi = {10.1145/3643991.3644880},
abstract = {Self-admitted technical debt (SATD) refers to a form of technical debt in which developers explicitly acknowledge and document the existence of technical shortcuts, workarounds, or temporary solutions within the codebase. Over recent years, researchers have manually labeled datasets derived from various software development artifacts: source code comments, messages from the issue tracker and pull request sections, and commit messages. These datasets are designed for training, evaluation, performance validation, and improvement of machine learning and deep learning models to accurately identify SATD instances. However, class imbalance poses a serious challenge across all the existing datasets, particularly when researchers are interested in categorizing the specific types of SATD. In order to address the scarcity of labeled data for SATD identification (i.e., whether an instance is SATD or not) and categorization (i.e., which type of SATD is being classified) in existing datasets, we share the SATDAUG dataset, an augmented version of existing SATD datasets, including source code comments, issue tracker, pull requests, and commit messages. These augmented datasets have been balanced in relation to the available artifacts and provide a much richer source of labeled data for training machine learning or deep learning models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {289–293},
numpages = {5},
keywords = {self-admitted technical debt, data augmentation, class imbalance},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639474.3640059,
author = {Fwa, Hua Leong},
title = {Experience Report: Identifying common misconceptions and errors of novice programmers with ChatGPT},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640059},
doi = {10.1145/3639474.3640059},
abstract = {Identifying the misconceptions of novice programmers is pertinent for informing instructors of the challenges faced by their students in learning computer programming. In the current literature, custom tools, test scripts were developed and, in most cases, manual effort to go through the individual codes were required to identify and categorize the errors latent within the students' code submissions. This entails investment of substantial effort and time from the instructors. In this study, we thus propose the use of ChatGPT in identifying and categorizing the errors. Using prompts that were seeded only with the student's code and the model code solution for questions from two lab tests, we were able to leverage on ChatGPT's natural language processing and knowledge representation capabilities to automatically collate frequencies of occurrence of the errors by error types. We then clustered the generated error descriptions for further insights into the misconceptions of the students. The results showed that although ChatGPT was not able to identify the errors perfectly, the achieved accuracy of 93.3% is sufficiently high for instructors to have an aggregated picture of the common errors of their students. To conclude, we have proposed a method for instructors to automatically collate the errors latent within the students' code submissions using ChatGPT. Notably, with the novel use of generated error descriptions, the instructors were able to have a more granular view of the misconceptions of their students, without the onerous effort of manually going through the students' codes.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {233–241},
numpages = {9},
keywords = {LLM, ChatGPT, misconception, programming, errors, cluster, prompts},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3597503.3639180,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Che, Xing and Wang, Dandan and Wang, Qing},
title = {Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639180},
doi = {10.1145/3597503.3639180},
abstract = {Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&amp;A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {100},
numpages = {13},
keywords = {automated GUI testing, large language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639751,
author = {Pinto, Gustavo and De Souza, Cleidson and Neto, Joao Batista and Souza, Alberto and Gotto, Tarci­sio and Monteiro, Edward},
title = {Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639751},
doi = {10.1145/3639477.3639751},
abstract = {With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and CoPilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information.In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot AI. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {408–417},
numpages = {10},
keywords = {LLM, LLM-based applications, LLM for code, LLM4code, code LLMs, challenges},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00013,
author = {Huang, Qing and Zhu, Jiahui and Li, Zhilong and Xing, Zhenchang and Wang, Changjing and Xu, Xiwei},
title = {PCR-Chain: Partial Code Reuse Assisted by Hierarchical Chaining of Prompts on Frozen Copilot},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00013},
doi = {10.1109/ICSE-Companion58688.2023.00013},
abstract = {API documentation, technical blogs and programming Q&amp;A sites contain a large amount of partial code that can be reused in programming tasks. However, due to unresolved simple names and last-mile syntax errors, such partial code is frequently not compilable. To facilitate partial code reuse, we develop PCR-Chain for resolving FQNs and fixing last-mile syntax errors in partial code based on a giant pre-trained code model (e.g., Copilot). Methodologically, PCR-Chain is backed up by the underlying global-level prompt architecture (which combines three design ideas: hierarchical task breakdown, prompt composition including sequential and conditional structures, and a mix of prompt-based AI and non-AI units) and the local-level prompt design. Technically, we propose PCR-Chain, which employs in-context learning rather than supervised fine-tuning with gradient updates on downstream task data. This approach enables the frozen, giant pre-trained code model to learn the desired behavior for a specific task through behavior-describing prompts and imitate it to complete the task. Experimental results show that PCR-Chain automatically resolves the FQNs and fixes last-mile syntax errors in 50 partial code samples collected from Stack Overflow with high success rates, without requiring any program analysis. The correct execution of the unit, module, and PCR-Chain demonstrates the effectiveness of the prompt design, prompt composition, and prompt architecture.Website:https://github.com/SE-qinghuang/PCR-ChainDemo Video: https://youtu.be/6HGRNdc2_JE},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {in-context learning, pre-trained language model, frozen copilot, AI chain, hierarchical prompts},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00060,
author = {Xue, Qiaomu},
title = {Automating Code Generation for MDE Using Machine Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00060},
doi = {10.1109/ICSE-Companion58688.2023.00060},
abstract = {The overall aim of our research is to improve the techniques for synthesizing code generators in the Model-Driven Engineering (MDE) context. Code generation is one of the main elements of Model-Driven Engineering, involving transformation from specification models to produce executable code. A code generator is designed to reduce the manual program construction work used to implement a software system, but building a code generator itself still currently needs much manual effort. Meanwhile, existing code generators are typically not flexible to adjust for changing development requirements and are hard to reuse for different target languages.Therefore, we aim to provide techniques to improve the process of building code generators, and let them be more reusable.Currently, we researched the related new and traditional approaches for generating code and projects using AI for program translation, code completion or program generation. Based on this research we decided to focus on a symbolic machine learning method related to the programming-by-example concept to build code generators. We use this "Code Generation By Example" (CGBE) concept with tree-to-tree structure mappings as the information format. CGBE has good performance in terms of training dataset size and time when applied to learning a UML-to-Java code generator, but further work is needed to extend it to generate different programming languages and to evaluate these cases, and to handle the optimisation of generated code.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {221–223},
numpages = {3},
keywords = {code generation, model transformation by example, model-driven engineering, symbolic machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643795.3648394,
author = {Vartziotis, Tina and Dellatolas, Ippolyti and Dasoulas, George and Schmidt, Maximilian and Schneider, Florian and Hoffmann, Tim and Kotsopoulos, Sotirios and Keckeisen, Michael},
title = {Learn to Code Sustainably: An Empirical Study on Green Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648394},
doi = {10.1145/3643795.3648394},
abstract = {The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generated code considered in this study is produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's "green capacity", based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {30–37},
numpages = {8},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3639478.3647635,
author = {Fu, Ying and Wang, Teng and Li, Shanshan and Ding, Jinyan and Zhou, Shulin and Jia, Zhouyang and Li, Wang and Jiang, Yu and Liao, Xiangke},
title = {MissConf: LLM-Enhanced Reproduction of Configuration-Triggered Bugs},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647635},
doi = {10.1145/3639478.3647635},
abstract = {Bug reproduction stands as a pivotal phase in software development, but the absence of configuration information emerges as the main obstacle to effective bug reproduction. Since configuration options generally control critical branches of the software, many bugs can only be triggered under specific configuration settings. We refer to these bugs as configuration-triggered bugs or CTBugs for short. The reproduction of CTBugs consumes considerable time and manual efforts due to the challenges in deducing the missing configuration options within the vast search space of configurations. This complexity contributes to a form of technical debt in software development.To address these challenges, we first conducted an empirical study on 120 CTBugs from 4 widely used systems to understand the root causes and factors influencing the reproduction of CTBugs. Based on our study, we designed and implemented MissConf, the first LLM-enhanced automated tool for CTBug reproduction. Miss-Conf first leverages the LLM to infer whether crucial configuration options are missing in the bug report. Once a suspect CTBug is found, MissConf employs configuration taint analysis and dynamic monitoring methods to filter suspicious configuration options set. Furthermore, it adopts a heuristic strategy for identifying crucial configuration options and their corresponding values. We evaluated MissConf on 5 real-world software systems. The experimental results demonstrate that MissConf successfully infers the 84% (41/49) of the CTBugs and reproduces the 65% (32/49) CTBugs. In the reproduction phase, MissConf eliminates up to 76% of irrelevant configurations, offering significant time savings for developers.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {484–495},
numpages = {12},
keywords = {bug reproduction, software configuration, software maintenance},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639474.3640065,
author = {Tao, Yida and Chen, Wenyan and Ye, Qingyang and Zhao, Yao},
title = {Beyond Functional Correctness: An Exploratory Study on the Time Efficiency of Programming Assignments},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640065},
doi = {10.1145/3639474.3640065},
abstract = {Practical programming assignments are critical parts of programming courses in Computer Science education. Students are expected to translate programming concepts learned from lectures into executable implementations that solve the tasks outlined in the assignments. These implementations are primarily assessed based on their functional correctness, ensuring that students' code produces the expected output when provided with specific inputs.However, functional correctness is not the only metric that evaluates the quality of programs. Runtime efficiency is a metric that is less frequently evaluated in programming courses, yet it holds significant importance in the context of professional software development. To investigate this gap and its potential ramifications, we conducted a large-scale empirical study on the time efficiency of 250 programming assignments that are evaluated solely on functional correctness. The results demonstrate that students' programming assignments exhibit significant variance in terms of execution time. We further identified 27 recurring inefficient code patterns from these assignments, and observed that most of the inefficient patterns can be optimized by automated tools such as PMD, IntelliJ IDEA and ChatGPT. Our findings provide actionable guidelines for educators to enhance the organization and integration of code performance topics throughout the programming course curriculum.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {320–330},
numpages = {11},
keywords = {programming assignment, code performance, tool support},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3643788.3648014,
author = {Diaz-De-Arcaya, Josu and L\'{o}pez-De-Armentia, Juan and Z\'{a}rate, Gorka and Torre-Bastida, Ana I.},
title = {Towards the self-healing of Infrastructure as Code projects using constrained LLM technologies},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648014},
doi = {10.1145/3643788.3648014},
abstract = {The generalization of the use of cloud computing and edge computing solutions in industry requires innovative techniques to keep up with the complexity of these scenarios. In particular, the large heterogeneity of the infrastructural devices and the myriad of services offered by the various private and cloud providers represent a challenge. Infrastructure as Code (IaC) technologies have been adopted to reduce the complexity of these scenarios, but even IaC technologies have their drawbacks, as the errors resulting from their use often combine the complexities of the underlying layers and require a high level of expertise. In this regard, the recent upsurge of Large Language Models represents an opportunity as they are able to tackle different problems. In this article, we aspire to shed light on the automated patching of IaC projects with the help of LLMs. We evaluate the suitability of this hypothesis by using a well-known LLM that is able to solve all the scenarios we envisioned and assess the possibility of doing the same with smaller, offline LLMs, which could lead to the use of these technologies in resource-constrained environments, such as edge computing.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {22–25},
numpages = {4},
keywords = {infrastructure as code, IaC, large language models, LLMs, self-healing, automated patching},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643991.3645082,
author = {Chavan, Omkar Sandip and Hinge, Divya Dilip and Deo, Soham Sanjay and Wang, Yaxuan (Olivia) and Mkaouer, Mohamed Wiem},
title = {Analyzing Developer-ChatGPT Conversations for Software Refactoring: An Exploratory Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645082},
doi = {10.1145/3643991.3645082},
abstract = {In recent years, Large Language Models (LLMs) have witnessed a remarkable ascent, with OpenAI's ChatGPT, introduced in 2022, garnering substantial attention. ChatGPT's rapid adoption in the software development community has opened up new avenues for exploring its qualitative and quantitative impact on Developer-ChatGPT conversations. In this paper, we delve into a rich dataset from GitHub and Hacker News to perform a thorough analysis. Our objectives include characterizing the nature of these interactions and evaluating the use of ChatGPT in refactoring. To achieve these goals, we employ a combination of exploratory data analysis and data annotation, utilizing relevant keyword filters to extract pertinent information. Our examination encompasses the identification and analysis of code refactorings facilitated by ChatGPT. Through a meticulous exploration of these conversations, our goal is to illuminate the potential of ChatGPT to enhance software development practices. This research promises to provide valuable insights into the evolving role of ChatGPT in the world of software development.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {207–211},
numpages = {5},
keywords = {refactoring documentation, ChatGPT, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639474.3640068,
author = {Pan, Wei Hung and Chok, Ming Jie and Wong, Jonathan Leong Shan and Shin, Yung Xin and Poon, Yeong Shian and Yang, Zhou and Chong, Chun Yong and Lo, David and Lim, Mei Kuan},
title = {Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640068},
doi = {10.1145/3639474.3640068},
abstract = {Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct.In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from Leet-Code. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {1–11},
numpages = {11},
keywords = {software engineering education, AI-generated code, AI-generated code detection},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3605760.3623764,
author = {Chen, Bocheng and Paliwal, Advait and Yan, Qiben},
title = {Jailbreaker in Jail: Moving Target Defense for Large Language Models},
year = {2023},
isbn = {9798400702563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605760.3623764},
doi = {10.1145/3605760.3623764},
abstract = {Large language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks. Researchers have found that current commercial LLMs either fail to be "harmless" by presenting unethical answers, or fail to be "helpful" by refusing to offer meaningful answers when faced with adversarial queries. To strike a balance between being helpful and harmless, we design a moving target defense (MTD) enhanced LLM system. The system aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks. We design a query and output analysis model to filter out unsafe or non-responsive answers. %to achieve the two objectives of randomly selecting outputs from different LLMs. We evaluate over 8 most recent chatbot models with state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the attack success rate from 37.5% to 0%. Meanwhile, it decreases the response refusal rate from 50% to 0%.},
booktitle = {Proceedings of the 10th ACM Workshop on Moving Target Defense},
pages = {29–32},
numpages = {4},
keywords = {dialogue system, moving target defense, trustworthy machine learning},
location = {Copenhagen, Denmark},
series = {MTD '23}
}

@inproceedings{10.1145/3643991.3644929,
author = {Nguyen, Huy and Treude, Christoph and Thongtanunam, Patanamon},
title = {Encoding Version History Context for Better Code Representation},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644929},
doi = {10.1145/3643991.3644929},
abstract = {With the exponential growth of AI tools that generate source code, understanding software has become crucial. When developers comprehend a program, they may refer to additional contexts to look for information, e.g. program documentation or historical code versions. Therefore, we argue that encoding this additional contextual information could also benefit code representation for deep learning. Recent papers incorporate contextual data (e.g. call hierarchy) into vector representation to address program comprehension problems. This motivates further studies to explore additional contexts, such as version history, to enhance models' understanding of programs. That is, insights from version history enable recognition of patterns in code evolution over time, recurring issues, and the effectiveness of past solutions. Our paper presents preliminary evidence of the potential benefit of encoding contextual information from the version history to predict code clones and perform code classification. We experiment with two representative deep learning models, ASTNN and CodeBERT, to investigate whether combining additional contexts with different aggregations may benefit downstream activities. The experimental result affirms the positive impact of combining version history into source code representation in all scenarios; however, to ensure the technique performs consistently, we need to conduct a holistic investigation on a larger code base using different combinations of contexts, aggregation, and models. Therefore, we propose a research agenda aimed at exploring various aspects of encoding additional context to improve code representation and its optimal utilisation in specific situations.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {631–636},
numpages = {6},
keywords = {source code representation, additional context, version history},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643796.3648463,
author = {Sergeyuk, Agnia and Titov, Sergey and Izadi, Maliheh},
title = {In-IDE Human-AI Experience in the Era of Large Language Models; A Literature Review},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648463},
doi = {10.1145/3643796.3648463},
abstract = {Integrated Development Environments (IDEs) have become central to modern software development, especially with the integration of Artificial Intelligence (AI) to enhance programming efficiency and decision-making. The study of in-IDE Human-AI Experience is critical in understanding how these AI tools are transforming the software development process, impacting programmer productivity, and influencing code quality.We conducted a literature review to study the current state of in-IDE Human-AI Experience research, bridging a gap in understanding the nuanced interactions between programmers and AI assistants within IDEs. By analyzing 36 selected papers, our study illustrates three primary research branches: Design, Impact, and Quality of Interaction.The trends, challenges, and opportunities identified in this paper emphasize the evolving landscape of software development and inform future directions for research, and development in this dynamic field. Specifically, we invite the community to investigate three aspects of these interactions: designing task-specific user interface, building trust, and improving readability.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {95–100},
numpages = {6},
keywords = {human-computer interaction, artificial intelligence, integrated development environment, programming, user studies, user experience},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3597503.3639202,
author = {Sun, Jiamou and Chen, Jieshan and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
title = {Where is it? Tracing the Vulnerability-relevant Files from Vulnerability Reports},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639202},
doi = {10.1145/3597503.3639202},
abstract = {With the widely usage of open-source software, supply-chain-based vulnerability attacks, including SolarWind and Log4Shell, have posed significant risks to software security. Currently, people rely on vulnerability advisory databases or commercial software bill of materials (SBOM) to defend against potential risks. Unfortunately, these datasets do not provide finer-grained file-level vulnerability information, compromising their effectiveness. Previous works have not adequately addressed this issue, and mainstream vulnerability detection methods have their drawbacks that hinder resolving this gap. Driven by the real needs, we propose a framework that can trace the vulnerability-relevant file for each disclosed vulnerability. Our approach uses NVD descriptions with metadata as the inputs, and employs a series of strategies with a LLM model, search engine, heuristic-based text matching method and a deep learning classifier to recommend the most likely vulnerability-relevant file, effectively enhancing the completeness of existing NVD data. Our experiments confirm that the efficiency of the proposed framework, with CodeBERT achieving 0.92 AUC and 0.85 MAP, and our user study proves our approach can help with vulnerability-relevant file detection effectively. To the best of our knowledge, our work is the first one focusing on tracing vulnerability-relevant files, laying the groundwork of building finer-grained vulnerability-aware software bill of materials.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {200},
numpages = {13},
keywords = {vulnerability-relevant file, security, software supply chain},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3650105,
title = {FORGE '24: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {FORGE aims to bring researchers, practitioners, and educators from the AI and Software Engineering community to solve the new challenges we meet in the era of foundation models.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3641822.3641882,
author = {Mendes, Wendy and Souza, Samara and De Souza, Cleidson},
title = {"You're on a bicycle with a little motor": Benefits and Challenges of Using AI Code Assistants},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641882},
doi = {10.1145/3641822.3641882},
abstract = {AI code assistants, such as Tabnine, GitHub CoPilot, and ChatGPT, employ Large Language Models (LLMs) trained on extensive source code and other documents. They receive prompts and generate code suggestions aimed to facilitate programming tasks. Previous research in this field has explored the correctness, complexity, quality, and security of the code suggestions. Software developers' experiences have been studied in the context of controlled experiments. Based on 14 interviews with software developers, this paper describes the developers' daily and continuous experiences with AI code assistants, presenting benefits and challenges grounded in actual development work, along with strategies to address these challenges.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {144–152},
numpages = {9},
keywords = {AI code assistants, developer experiences, code generation},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1145/3597503.3639156,
author = {Babakol, Timur and Liu, Yu David},
title = {Tensor-Aware Energy Accounting},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639156},
doi = {10.1145/3597503.3639156},
abstract = {With the rapid growth of Artificial Intelligence (AI) applications supported by deep learning (DL), the energy efficiency of these applications has an increasingly large impact on sustainability. We introduce Smaragdine, a new energy accounting system for tensor-based DL programs implemented with TensorFlow. At the heart of Smaragdine is a novel white-box methodology of energy accounting: Smaragdine is aware of the internal structure of the DL program, which we call tensor-aware energy accounting. With Smaragdine, the energy consumption of a DL program can be broken down into units aligned with its logical hierarchical decomposition structure. We apply Smaragdine for understanding the energy behavior of BERT, one of the most widely used language models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of identifying the highest energy/power-consuming components of BERT. Furthermore, we conduct two case studies on how Smaragdine supports downstream toolchain building, one on the comparative energy impact of hyperparameter tuning of BERT, the other on the energy behavior evolution when BERT evolves to its next generation, ALBERT.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {93},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00048,
author = {Song, Yang and Mahmud, Junayed and De Silva, Nadeeshan and Zhou, Ying and Chaparro, Oscar and Moran, Kevin and Marcus, Andrian and Poshyvanyk, Denys},
title = {Burt: A Chatbot for Interactive Bug Reporting},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00048},
doi = {10.1109/ICSE-Companion58688.2023.00048},
abstract = {This paper introduces Burt, a web-based chatbot for interactive reporting of Android app bugs. Burt is designed to assist Android app end-users in reporting high-quality defect information using an interactive interface. Burt guides the users in reporting essential bug report elements, i.e., the observed behavior, expected behavior, and the steps to reproduce the bug. It verifies the quality of the text written by the user and provides instant feedback. In addition, Burt provides graphical suggestions that the users can choose as alternatives to textual descriptions.We empirically evaluated Burt, asking end-users to report bugs from six Android apps. The reporters found that Burt's guidance and automated suggestions and clarifications are useful and Burt is easy to use. Burt is an open-source tool, available at github.com/sea-lab-wm/burt/tree/tool-demo.A video showing the full capabilities of Burt can be found at https://youtu.be/SyfOXpHYGRo.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {170–174},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3643097,
author = {Zeyen, Olivier and Cordy, Maxime and Perrouin, Gilles and Acher, Mathieu},
title = {Exploring the Computational Complexity of SAT Counting and Uniform Sampling with Phase Transitions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643097},
doi = {10.1145/3639478.3643097},
abstract = {Uniform Random Sampling (URS) is the problem of selecting solutions (models) from a Boolean formula such that each solution gets the same probability of being selected. URS has many applications. In large configurable software systems, one wants an unbiased sample of configurations to look for bugs at an affordable cost [12, 13]. Other applications of URS include deep learning verification (to sample inputs from unknown distributions) [2] and evolutionary algorithms (to initialize the input population) [4].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {322–323},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3528588.3528660,
author = {Siddiq, Mohammed Latif and Santos, Joanna C. S.},
title = {BERT-based GitHub issue report classification},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528660},
doi = {10.1145/3528588.3528660},
abstract = {Issue tracking is one of the integral parts of software development, especially for open source projects. GitHub, a commonly used software management tool, provides its own issue tracking system. Each issue can have various tags, which are manually assigned by the project's developers. However, manually labeling software reports is a time-consuming and error-prone task. In this paper, we describe a BERT-based classification technique to automatically label issues as questions, bugs, or enhancements. We evaluate our approach using a dataset containing over 800,000 labeled issues from real open source projects available on GitHub. Our approach classified reported issues with an average F1-score of 0.8571. Our technique outperforms a previous machine learning technique based on FastText.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {33–36},
numpages = {4},
keywords = {issue type classification, multi-class classification, pre-trained model, software maintenance, text processing},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1109/GREENS66463.2025.00014,
author = {Rubei, Riccardo and Moussaid, Aicha and Di Sipio, Claudio and Di Ruscio, Davide},
title = {Prompt engineering and its implications on the energy consumption of Large Language Models},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00014},
doi = {10.1109/GREENS66463.2025.00014},
abstract = {Reducing the environmental impact of AI-based software systems has become critical. The intensive use of large language models (LLMs) in software engineering poses severe challenges regarding computational resources, data centers, and carbon emissions. In this paper, we investigate how prompt engineering techniques (PETs) can impact the carbon emission of the Llama 3 model for the code generation task. We experimented with the CodeXGLUE benchmark to evaluate both energy consumption and the accuracy of the generated code using an isolated testing environment. Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts. Even though a more in-depth evaluation is needed to confirm our findings, this work suggests that prompt engineering can reduce LLMs’ energy consumption during the inference phase without compromising performance, paving the way for further investigations.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {60–67},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3597503.3649399,
author = {Rinard, Martin},
title = {Software Engineering Research in a World with Generative Artificial Intelligence},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3649399},
doi = {10.1145/3597503.3649399},
abstract = {Generative artificial intelligence systems such as large language models (LLMs) exhibit powerful capabilities that many see as the kind of flexible and adaptive intelligence that previously only humans could exhibit. I address directions and implications of LLMs for software engineering research.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {2},
numpages = {5},
keywords = {software engineering, generative artificial intelligence, large language models},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623316,
author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623316},
doi = {10.1145/3597503.3623316},
abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {37},
numpages = {12},
keywords = {code generation, large language models, benchmark},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643068,
author = {Lee, Jonathan and Li, Mason and Hsu, Kuo-Hsun},
title = {Applying Transformer Models for Automatic Build Errors Classification of Java-Based Open Source Projects},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643068},
doi = {10.1145/3639478.3643068},
abstract = {In open-source development, encountering build failures is a common challenge. Addressing these issues requires analyzing the causes of errors and developing solutions for fixing them. In this work, we fine-tuned Google's BERT, a well-known language model excellent in transfer learning, to address build issues in Gradle Java projects. Our strategy utilizes this model to classify error logs and identify fixing solutions. This approach extends our previous work, Gradle ACFix, an automated build error fixing system, to explore the potential of using machine learning to classify error types and identify appropriate fixing strategies for software projects. We gathered a dataset of 11,483 open-source Gradle Java projects from GitHub for this research. The model's evaluation on the error logs of these projects demonstrated a high accuracy rate exceeding 98%.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {282–283},
numpages = {2},
keywords = {build error, build fixing, gradle, open source, deep learning model},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644424,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Santos, Joanna Cecilia Da Silva},
title = {Understanding Regular Expression Denial of Service (ReDoS): Insights from LLM-Generated Regexes and Developer Forums},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644424},
doi = {10.1145/3643916.3644424},
abstract = {Regular expression Denial of Service (ReDoS) represents an algorithmic complexity attack that exploits the processing of regular expressions (regexes) to produce a denial-of-service attack. This attack occurs when a regex's evaluation time scales polynomially or exponentially with input length, posing significant challenges for software developers. The advent of Large Language Models (LLMs) has revolutionized the generation of regexes from natural language prompts, but not without its risks. Prior works showed that LLMs can generate code with vulnerabilities and security smells. In this paper, we examined the correctness and security of regexes generated by LLMs as well as the characteristics of LLM-generated vulnerable regexes. Our study also examined ReDoS patterns in actual software projects, aligning them with corresponding regex equivalence classes and algorithmic complexity. Moreover, we analyzed developer discussions on GitHub and StackOverflow, constructing a taxonomy to investigate their experiences and perspectives on ReDoS. In this study, we found that GPT-3.5 was the best LLM to generate regexes that are both correct and secure. We also observed that LLM-generated regexes mainly have polynomial ReDoS vulnerability patterns, and it is consistent with vulnerable regexes found in open source projects. We also found that developers' main discussions around insecure regexes is related to mitigation strategies to remove vulnerable regexes.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {190–201},
numpages = {12},
keywords = {ReDoS, DoS attack, large language models, regex generation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3528588.3528663,
author = {Bharadwaj, Shikhar and Kadam, Tushar},
title = {GitHub issue classification using BERT-style models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528663},
doi = {10.1145/3528588.3528663},
abstract = {Recent innovations in natural language processing techniques have led to the development of various tools for assisting software developers. This paper provides a report of our proposed solution to the issue report classification task from the NL-Based Software Engineering workshop. We approach the task of classifying issues on GitHub repositories using BERT-style models [1, 2, 6, 8]. We propose a neural architecture for the problem that utilizes contextual embeddings for the text content in the GitHub issues. Besides, we design additional features for the classification task. We perform a thorough ablation analysis of the designed features and benchmark various BERT-style models for generating textual embeddings. Our proposed solution performs better than the competition organizer's method and achieves an F1 score of 0.8653. Our code and trained models are available at https://github.com/Kadam-Tushar/Issue-Classifier.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {40–43},
numpages = {4},
keywords = {BERT, NLP, text classification},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3597503.3639144,
author = {Zhou, Mingyi and Gao, Xiang and Wu, Jing and Liu, Kui and Sun, Hailong and Li, Li},
title = {Investigating White-Box Attacks for On-Device Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639144},
doi = {10.1145/3597503.3639144},
abstract = {Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Although the structure and parameters information of these models can be accessed, existing on-device attacking approaches only generate black-box attacks (i.e., indirect white-box attacks), which are less effective and efficient than white-box strategies. This is because mobile deep learning (DL) frameworks like TensorFlow Lite (TFLite) do not support gradient computing (referred to as non-debuggable models), which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harm-fulness of on-device attacks. To validate this, we systematically analyze the difficulties of transforming the on-device model to its debuggable version and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to its debuggable version, enabling attackers to launch white-box attacks. Our empirical results show that our approach is effective in achieving automated transformation (i.e., 92.6%) among 244 TFLite models. Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates (10.23%→89.03%) with a hundred times smaller attack perturbations (1.0→0.01). Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models. Our artifacts 1 are available.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {152},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643795.3648395,
author = {Pister, Kaiser and Paul, Dhruba Jyoti and Joshi, Ishan and Brophy, Patrick},
title = {PromptSet: A Programmer's Prompting Dataset},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648395},
doi = {10.1145/3643795.3648395},
abstract = {The rise of capabilities expressed by large language models has been quickly followed by the integration of the same complex systems into application level logic. Algorithms, programs, systems, and companies are built around structured prompting to black box models where the majority of the design and implementation lies in capturing and quantifying the `agent mode'. The standard way to shape a closed language model is to prime it for a specific task with a tailored prompt, often initially handwritten by a human. The textual prompts co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be. Unlike traditional code, we find that prompts do not receive effective static testing and linting to prevent runtime issues. In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer prompts used in open source Python programs. We perform analysis on this dataset and introduce the notion of a static linter for prompts. Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name pisterlabs/promptset.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {62–69},
numpages = {8},
keywords = {prompt management, large language models, dataset, information systems, ethnography, taxonomy},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3597503.3623341,
author = {Lill, Alexander and Meyer, Andr\'{e} N. and Fritz, Thomas},
title = {On the Helpfulness of Answering Developer Questions on Discord with Similar Conversations and Posts from the Past},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623341},
doi = {10.1145/3597503.3623341},
abstract = {A big part of software developers' time is spent finding answers to their coding-task-related questions. To answer their questions, developers usually perform web searches, ask questions on Q&amp;A websites, or, more recently, in chat communities. Yet, many of these questions have frequently already been answered in previous chat conversations or other online communities. Automatically identifying and then suggesting these previous answers to the askers could, thus, save time and effort. In an empirical analysis, we first explored the frequency of repeating questions on the Discord chat platform and assessed our approach to identify them automatically. The approach was then evaluated with real-world developers in a field experiment, through which we received 142 ratings on the helpfulness of the suggestions we provided to help answer 277 questions that developers posted in four Discord communities. We further collected qualitative feedback through 53 surveys and 10 follow-up interviews. We found that the suggestions were considered helpful in 40% of the cases, that suggesting Stack Overflow posts is more often considered helpful than past Discord conversations, and that developers have difficulties describing their problems as search queries and, thus, prefer describing them as natural language questions in online communities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {58},
numpages = {13},
keywords = {developer questions, chat community, semantic similarity},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643087,
author = {Xue, Zhiyi and Li, Liangguo and Tian, Senyue and Chen, Xiaohong and Li, Pingping and Chen, Liangyu and Jiang, Tingting and Zhang, Min},
title = {Domain Knowledge is All You Need: A Field Deployment of LLM-Powered Test Case Generation in FinTech Domain},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643087},
doi = {10.1145/3639478.3643087},
abstract = {Despite the promise of automation, general-purpose Large Language Models (LLMs) face difficulties in generating complete and accurate test cases from informal software requirements, primarily due to challenges in interpreting unstructured text and producing diverse, relevant scenarios. This paper argues that incorporating domain knowledge significantly improves LLM performance in test case generation. We report on the successful deployment of our LLM-powered tool, LLM4Fin, in the FinTech domain, showcasing the crucial role of domain knowledge in addressing the aforementioned challenges. We demonstrate two methods for integrating domain knowledge: implicit incorporation through model fine-tuning, and explicit incorporation with algorithm design. This combined approach delivers remarkable results, achieving up to 98.18% improvement in test scenario coverage and reducing generation time from 20 minutes to 7 seconds.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {314–315},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00072,
author = {Liu, Fang and Li, Jia and Zhang, Li},
title = {Syntax and Domain Aware Model for Unsupervised Program Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00072},
doi = {10.1109/ICSE48619.2023.00072},
abstract = {There is growing interest in software migration as the development of software and society. Manually migrating projects between languages is error-prone and expensive. In recent years, researchers have begun to explore automatic program translation using supervised deep learning techniques by learning from large-scale parallel code corpus. However, parallel resources are scarce in the programming language domain, and it is costly to collect bilingual data manually. To address this issue, several unsupervised programming translation systems are proposed. However, these systems still rely on huge monolingual source code to train, which is very expensive. Besides, these models cannot perform well for translating the languages that are not seen during the pre-training procedure. In this paper, we propose SDA-Trans, a syntax and domain-aware model for program translation, which leverages the syntax structure and domain knowledge to enhance the cross-lingual transfer ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus, including Python and Java monolingual programs. The experimental results on function translation tasks between Python, Java, and C++ show that SDA-Trans outperforms many large-scale pre-trained models, especially for unseen language translation.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {755–767},
numpages = {13},
keywords = {program translation, neural networks, syntax structure, unsupervised learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644032.3644454,
author = {Hoffmann, Jacob and Frister, Demian},
title = {Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644454},
doi = {10.1145/3644032.3644454},
abstract = {Motivation. Software tests are a necessity in the development of software to secure functionality, reliability, and usability [10]; however, these tests are costly and time-consuming [6]. Although tool support for software testing has advanced, there remains considerable potential for enhancement. Many software tests are still devised manually, with the creation of unit tests being particularly laborious. Automating the generation of test cases is promising for streamlining this aspect of software testing [6].Large Language Models (LLMs) have exhibited capabilities in code generation [11, 13--15], test case generation [17], and various other domains [11]. The advancement of model performance of transformer-based LLMs is mainly achieved by expanding the model size in line with an increase in training data size [7, 8]. However, this approach leads to high computational costs which can only be afforded by corporations with significant financial resources. This highlights the need for transformer-based LLMs that perform well on a specific downstream task and are also cost-efficient. Addressing this, we focused on supervised fine-tuning (SFT) of more resource-efficient transformer-based LLMs LLaMA 2 13B, Code Llama 13B, and Mistral 7B for the specific downstream task of generating test cases for mobile applications.Research questions. This work investigated: Does SFT enhance the capabilities of a transformer-based LLM in the specific downstream task of generating test cases for mobile applications while being cost-efficient and runnable on standard consumer hardware? Does the fine-tuned model outperform other state-of-the-art models in the task of test generation for mobile applications?Approach. Our approach is a modification of the ATHENATEST approach [16]. However, our approach focuses on supervised fine-tuning (SFT) on both pre-trained and already fine-tuned transformer-based LLMs for the task of test case generation for mobile applications in Dart.The approach involves three steps, as illustrated in Figure 1. Firstly, a labeled dataset of corresponding input-output pairs (X, Y) was obtained to model the conditional probability P(Y|X; θ) [9, 12]. Dart code and corresponding test files were extracted from open-source GitHub repositories using Google BigQuery. These files were then matched using regular expressions, ensuring that each code file was matched with its corresponding test file based on matching base filenames. The dataset underwent quality filtering and deduplication, resulting in 16,252 input-output pairs, which was then divided into training (90%) and validation (10%) sets. The training set of the dataset consists of a total of 88.5M tokens using the LLaMA tokenizer.Secondly, for SFT on the downstream task of test generation, models were selected based on their code generation capabilities, as indicated by the pass@1 score on the HumanEval [2] and MBPP [1] benchmark, their parameter sizes, and the extent to which they had been trained on Dart data. In model selection, open-source models capable of running on cost-efficient consumer hardware with code generation abilities were primarily chosen.Thirdly, in the SFT process, the test generation task was represented as translation task, in line with ATHENATEST [16]. This is achieved by employing the following structured prompt format for SFT [9]:"{prefix_prompt} ### Code: {code} ### Test: {test}"In this work, there was no prefix prompt used during SFT.Fine-tuning. The fine-tuning was conducted on a single GPU system using Flash Attention 2 [3] and the QLoRA method [4] to reduce memory size and the number of trainable parameters. The fine-tuning process varied in duration up to 32 hours, resulting in total emissions of 13.099 kgCO2eq [5].Experimental Results. The performance of TestGen-Dart models was evaluated for their unit testing capabilities in Dart, in comparison to base models LLaMA 2 13B, Code Llama 13B, and Mistral 7B. The models were loaded in both float16 and 4-bit quantization configurations, and the evaluation involved nine different Dart files, encompassing 42 test cases. The results were obtained in a zero-shot setting using a structured prompt format, as described in the approach section. This included a prefix prompt instructing the models to generate unit tests: "Generate unit tests in Dart for the following class. The unit test should be structured with the 'test' function, an appropriate description, and an assertion 'expect' within the function to validate the test case." The generated unit tests were classified into three categories: syntax errors (SE), syntactic correctness (SC), and functional correctness (FC). In a 4-bit quantization configuration, TestGen-Dart_v0.2 enhanced the generation of syntactically correct unit tests by 15.38% and functionally correct unit tests by 16.67%, compared to the underlying base model, Code Llama 13B. Additionally, TestGen-Dart_v0.2 demonstrated superior performance in the 16-bit configuration. This evidenced that supervised fine-tuning (SFT) increases the capability of transformer-based LLMs in a specific downstream task, in this instance, generating test cases for mobile applications, addressing the first research question posed in this work. Additionally, TestGen-Dart_v0.2 outperformed the other state-of-the-art models of interest LLaMA 2 13B and Mistral 7B in that task, addressing the second research question.Conclusion. This work demonstrates that SFT enhances the capability of transformer-based LLMs in generating test cases for mobile applications in Dart. Furthermore, the 13B parameter size of the TestGen-Dart enables it to run locally on standard consumer hardware, potentially making it a cost-efficient and privacy-friendly testing assistant for software developers by avoiding an external server connection to run the model.Outlook. Future work currently in progress may expand this approach to other programming languages and refine TestGen-Dart's performance by using higher-quality fine-tuning data either synthetic or human-annotated. Additionally, the evaluation method may be enhanced by using TestGen-Dart for generating test cases for dummy applications and measuring code coverage.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {76–77},
numpages = {2},
keywords = {software testing, mobile testing, machine learning, large language models},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3650105.3652293,
author = {Wang, Hailong and Xu, Tongtong and Wang, Bei},
title = {Deep Multiple Assertions Generation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652293},
doi = {10.1145/3650105.3652293},
abstract = {Software testing is one of the most crucial parts of the software development life cycle. Developers spend substantial amount of time and efforts on software testing. Recently, there has been a growing scholarly interest in the automation of software testing. However, recent studies have revealed significant limitations in the quality and efficacy of the generated assert statements. These limitations primarily arise due to: (i) the inherent complexity involved in generating assert statements that are both meaningful and effective; (ii) the challenge of capturing the relationship between multiple assertions in a single test case. In recent research, deep learning techniques have been employed to generate meaningful assertions. However, it is typical for a single assertion to be generated for each test case, which contradicts the current situation where over 40% of test cases contain multiple assertions.To address these open challenges, we propose a novel approach, called DeepAssert that exploits the pre-trained model GraphCode-BERT to automatically generate multiple assertions for test methods. It can recommend a sequence of assert statements effectively given a test method and a focal method (the method under test).To evaluate the effectiveness of our approach, we conduct extensive experiments on the dataset built on the top of Methods2Test dataset. Experimental results show that DeepAssert achieves scores of 54.16%, 18.36%, and 15.38% in terms of CodeBLEU, accuracy and perfect prediction and substantially outperforms the state-of-the-art baselines by a large margin. Furthermore, we evaluate the effectiveness of DeepAssert on the task of bug detection and the result indicates that the assert sequences generated by DeepAssert can assist in exposing 42 real-world bugs extracting from Defects4J while only considering the first compiled assert sequence, outperforming the SOTA approaches by a large margin as well.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {1–11},
numpages = {11},
keywords = {software testing, deep learning, assert statement generation},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00007,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Ma, Lei and Traon, Yves Le},
title = {CodeS: Towards Code Model Generalization Under Distribution Shift},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00007},
doi = {10.1109/ICSE-NIER58687.2023.00007},
abstract = {Distribution shift has been a longstanding challenge for the reliable deployment of deep learning (DL) models due to unexpected accuracy degradation. Although DL has been becoming a driving force for large-scale source code analysis in the big code era, limited progress has been made on distribution shift analysis and benchmarking for source code tasks. To fill this gap, this paper initiates to propose CodeS, a distribution shift benchmark dataset, for source code learning. Specifically, CodeS supports two programming languages (Java and Python) and five shift types (task, programmer, time-stamp, token, and concrete syntax tree). Extensive experiments based on CodeS reveal that 1) out-of-distribution detectors from other domains (e.g., computer vision) do not generalize to source code, 2) all code classification models suffer from distribution shifts, 3) representation-based shifts have a higher impact on the model than others, and 4) pre-trained bimodal models are relatively more resistant to distribution shifts.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {1–6},
numpages = {6},
keywords = {source code learning, distribution shift},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1145/3643787.3648028,
author = {Chakraborty, Partha and Arumugam, Venkatraman and Nagappan, Meiyappan},
title = {Aligning Programming Language and Natural Language: Exploring Design Choices in Multi-Modal Transformer-Based Embedding for Bug Localization},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648028},
doi = {10.1145/3643787.3648028},
abstract = {Bug localization refers to the identification of source code files which is in a programming language and also responsible for the unexpected behavior of software using the bug report, which is a natural language. As bug localization is labor-intensive, bug localization models are employed to assist software developers. Due to the domain difference between source code files and bug reports, modern bug-localization systems, based on deep learning models, rely heavily on embedding techniques that project bug reports and source code files into a shared vector space. The creation of an embedding involves several design choices, but the impact of these choices on the quality of embedding and the performance of bug localization models remains unexplained in current research.To address this gap, our study evaluated 14 distinct embedding models to gain insights into the effects of various design choices. Subsequently, we developed bug localization models utilizing these embedding models to assess the influence of these choices on the performance of the localization models. Our findings indicate that the pre-training strategies significantly affect the quality of the embedding. Moreover, we discovered that the familiarity of the embedding models with the data has a notable impact on the bug localization model's performance. Notably, when the training and testing data are collected from different projects, the performance of the bug localization models exhibits substantial fluctuations.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {1–8},
numpages = {8},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@proceedings{10.1145/3643796,
title = {IDE '24: Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Despite the research community's desire to improve the productivity of software developers, it is challenging for research to move beyond papers into the everyday practice of software development. Since IDEs are one of the most widely used tools in developers' toolkit, they remain a crucial venue for research to reach the practitioners. To close the gap between research and adoption in practice, we launched the first edition of the IDE Workshop.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639478.3643519,
author = {Mu, Wenchuan and Lim, Kwan Hui},
title = {Towards Precise Observations of Neural Model Robustness in Classification},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643519},
doi = {10.1145/3639478.3643519},
abstract = {In deep learning applications, robustness measures the ability of neural models that handle slight changes in input data, which could lead to potential safety hazards, especially in safety-critical applications. Pre-deployment assessment of model robustness is essential, but existing methods often suffer from either high costs or imprecise results. To enhance safety in real-world scenarios, metrics that effectively capture the model's robustness are needed. To address this issue, we compare the rigour and usage conditions of various assessment methods based on different definitions. Then, we propose a straightforward and practical metric utilizing hypothesis testing for probabilistic robustness and have integrated it into the TorchAttacks library. Through a comparative analysis of diverse robustness assessment methods, our approach contributes to a deeper understanding of model robustness in safety-critical applications.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {388–389},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644437,
author = {de Oliveira, Benedito and Castor, Fernando},
title = {AthenaLLM: Supporting Experiments with Large Language Models in Software Development},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644437},
doi = {10.1145/3643916.3644437},
abstract = {Existing studies on the use of Large Language Models (LLMs) in software development leverage methodologies that limit their scalability and require intensive manual data collection and analysis, for example, due to the use of video data or think-aloud protocols. We propose the use of a specialized tool capable of automatically collecting fine-grained, relevant data during experiments and case studies. It enables researchers to understand for example how often participants accept or reject suggestions made by LLMs and what kinds of prompts are more likely to trigger accepted suggestions, even in studies targeting a large number of participants. We implement this idea as a Visual Studio Code plugin named AthenaLLM 1. It mimics the functionalities of GitHub Copilot and offers seamless integration with OpenAI API models like GPT-4 and GPT-3.5, and compatibility with other models providing an OpenAI-compatible API, e.g., Vicuna [6]. It automatically collects data at a fine level of granularity and covers both the interactions of developers with their IDE, e.g., all changes made in the code, and the products of such interactions, e.g., the generated code, when accepted. Thus, the proposed approach also reduces bias that the experimental process itself may introduce, e.g., due to the need for participants to verbalize their thoughts. In this paper we discuss how AthenaLLM could enable researchers to go both broader (in terms of number of participants) and deeper (in terms of the kinds of research questions that can be tackled).},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {69–73},
numpages = {5},
keywords = {experimentation, large language models, software development, empirical software engineering},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643916.3644434,
author = {Li, Jiliang and Zhang, Yifan and Karas, Zachary and McMillan, Collin and Leach, Kevin and Huang, Yu},
title = {Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644434},
doi = {10.1145/3643916.3644434},
abstract = {Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability --- informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive exPlanations), to identify which code tokens influence that generation of summaries. Using these settings, we find no statistically significant relationship between language models' focus and human programmers' attention. Furthermore, alignment between model and human foci in this setting does not seem to dictate the quality of the LLM-generated summaries. Our study highlights an inability to align human focus with SHAP-based model focus measures. This result calls for future investigation of multiple open questions for explainable language models for code summarization and software engineering tasks in general, including the training mechanisms of language models for code, whether there is an alignment between human and model attention on code, whether human attention can improve the development of language models, and what other model focus measures are appropriate for improving explainability.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {47–51},
numpages = {5},
keywords = {neural code summarization, language models, explainable AI, SHAP, human attention, eye-tracking},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639477.3639743,
author = {Fakih, Mohamad and Dharmaji, Rahul and Moghaddas, Yasamin and Quiros, Gustavo and Ogundare, Oluwatosin and Al Faruque, Mohammad Abdullah},
title = {LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639743},
doi = {10.1145/3639477.3639743},
abstract = {Although Large Language Models (LLMs) have established predominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools - including grammar checkers, compilers and SMV verifiers - to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally-flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10.To promote open research, we share the complete experimental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage1.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {192–203},
numpages = {12},
keywords = {industrial control, verifiable synthesis, large language models, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@proceedings{10.1145/3643991,
title = {MSR '24: Proceedings of the 21st International Conference on Mining Software Repositories},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {MSR is a thriving research community that organizes a yearly conference with a solid reputation amongst software engineering researchers.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3643916,
title = {ICPC '24: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICPC is the premier (CORE A) venue for research on program comprehension. Research on program comprehension encompasses both human activities for comprehending the software and technologies for supporting such comprehension.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643916.3644431,
author = {Zhang, Zhang and Mao, Xinjun and Wang, Shangwen and Yang, Kang and Lu, Yao},
title = {CAREER: Context-Aware API Recognition with Data Augmentation for API Knowledge Extraction},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644431},
doi = {10.1145/3643916.3644431},
abstract = {The recognition of Application Programming Interface (API) mentions in the software-related texts is a prerequisite task for extracting API-related knowledge. Previous studies have demonstrated the superiority of deep learning-based methods in accomplishing this task. However, such techniques still meet their bottlenecks due to their inability to effectively handle the following three challenges: (1) differentiating APIs from common words; (2) identifying APIs in morphological variants of the standard APIs; and (3) the lack of high-quality labeled data for training. To overcome these challenges, this paper proposes a context-aware API recognition method named CAREER. This approach utilizes two key components, namely Bidirectional Encoder Representations from Transformers (BERT) and Bi-directional Long Short-Term Memory (BiLSTM), to extract context information at both the word-level and sequence-level. This strategic combination empowers the method to dynamically capture both syntactic and semantic information, effectively addressing the first challenge. To tackle the second challenge, CAREER introduces a character-level BiLSTM component, enriched with an attention mechanism. This enables the model to grasp character-level global context information, thereby enhancing the recognition of morphological attributes within API mentions. Furthermore, to address the third challenge, the paper introduces three data augmentation techniques aimed at generating new data samples. Accompanying these techniques is a novel sample selection algorithm designed to screen out high-quality instances. This dual-pronged approach effectively mitigates the requirement for data labeling. Experiments demonstrate that CAREER significantly improves F1-score by 11.0% compared with state-of-the-art methods. We also construct specific datasets to assess CAREER's capacity to tackle the aforementioned challenges. Results confirm that (1) CAREER significantly outperforms baseline methods in addressing the first and second challenges, and (2) with the aid of data augmentation techniques and sample selection algorithms, high-quality samples can be generated to improve the performance, and alleviate the third challenge.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {438–449},
numpages = {12},
keywords = {API mention, data augmentation, context-aware},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00033,
author = {Song, Xidan and Sun, Youcheng and Mustafa, Mustafa A. and Cordeiro, Lucas C.},
title = {AIRepair: A Repair Platform for Neural Networks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00033},
doi = {10.1109/ICSE-Companion58688.2023.00033},
abstract = {We present AIRepair, a platform for repairing neural networks. It features the integration of existing network repair tools. Based on AIRepair, one can run different repair methods on the same model, thus enabling the fair comparison of different repair techniques. In this paper, we evaluate AIRepair with five recent repair methods on popular deep-learning datasets and models. Our evaluation confirms the utility of AIRepair, by comparing and analyzing the results from different repair techniques. A demonstration is available at https://youtu.be/UkKw5neeWhw.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {98–101},
numpages = {4},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00179,
author = {Li, Jia and Li, Yongmin and Li, Ge and Jin, Zhi and Hao, Yiyang and Hu, Xing},
title = {SKCODER: A Sketch-Based Approach for Automatic Code Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00179},
doi = {10.1109/ICSE48619.2023.00179},
abstract = {Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results.In this paper, we propose a sketch-based code generation approach named SKCODER to mimic developers' code reuse behavior. Given a natural language requirement, SKCODER retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models "how to write". The post-editing further adds requirement-specific details into the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) SKCODER can generate more correct programs, and outperforms the state-of-the-art - CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@1. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our SKCODER in three aspects.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2124–2135},
numpages = {12},
keywords = {code generation, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643666.3648577,
author = {Rejithkumar, Gokul and Anish, Preethu Rose and Shukla, Jyoti and Ghaisas, Smita},
title = {Probing with Precision: Probing Question Generation for Architectural Information Elicitation},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643666.3648577},
doi = {10.1145/3643666.3648577},
abstract = {Software Requirements Specifications (SRS) often lack the necessary level of specificity required by software architects to make well-informed architectural decisions. This deficiency compels software architects to probe business analysts to collect more details pertinent to architectural requirements from the clients. In our previous work, we introduced Probing Question-flows (PQ-flows) that can assist business analysts to probe stakeholders and gather architecturally significant information for the creation of a more comprehensive SRS. Key limitations of our previous work were the manually created templatized PQ-flows and the mapping of PQ-flows to the software requirements based on standard Vector Space Model. In this study, we propose a Retrieval Augmented Generation (RAG) prompting framework to address these limitations. We conducted experiments using ChatGPT and Mistral-7B models. We present our findings utilizing human and automated evaluation metrics on a subset of the publicly available PUblic REquirements (PURE) dataset.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Multi-Disciplinary, Open, and RElevant Requirements Engineering},
pages = {8–14},
numpages = {7},
keywords = {requirements engineering, probing questions, large language models, prompting, retrieval augmented generation, ChatGPT, mistral},
location = {Lisbon, Portugal},
series = {MO2RE 2024}
}

@proceedings{10.1145/3643667,
title = {Q-SE 2024: Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th International Workshop on Quantum Software Engineering (Q-SE 2024), co-located with ICSE 2024, provides a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639150,
author = {Ma, Zeyang and Chen, An Ran and Kim, Dong Jae and Chen, Tse-Hsun and Wang, Shaowei},
title = {LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639150},
doi = {10.1145/3597503.3639150},
abstract = {Logs are important in modern software development with runtime information. Log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. Traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. In this paper, we explore the potential of using Large Language Models (LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on generative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16 open-source systems shows that LLMParser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96% average parsing accuracy). We further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training LLM on log parsing accuracy. We find that smaller LLMs may be more effective than more complex LLMs; for instance where Flan-T5-base achieves comparable results as LLaMA-7B with a shorter inference time. We also find that using LLMs pre-trained using logs from other systems does not always improve parsing accuracy. While using pre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA results in a decrease (decrease by almost 55% in group accuracy). In short, our study provides empirical evidence for using LLMs for log parsing and highlights the limitations and future research direction of LLM-based log parsers.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {99},
numpages = {13},
keywords = {log parsing, log analysis, large language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639810,
author = {Huang, Yutan},
title = {Generating User Experience Based on Personas with AI Assistants},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639810},
doi = {10.1145/3639478.3639810},
abstract = {Traditional UX development methodologies focus on developing "one size fits all" solutions and lack the flexibility to cater to diverse user needs. In response, a growing interest has arisen in developing more dynamic UX frameworks. However, existing approaches often cannot personalise user experiences and adapt to user feedback in real-time. Therefore, my research introduces a novel approach of combining Large Language Models and personas, to address these limitations. The research is structured around three areas: (1) a critical review of existing adaptive UX practices and the potential for their automation; (2) an investigation into the role and effectiveness of personas in enhancing UX adaptability; and (3) the proposal of a theoretical framework that leverages LLM capabilities to create more dynamic and responsive UX designs and guidelines.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {181–183},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639149,
author = {He, Junda and Yang, Zhou and Shi, Jieke and Yang, Chengran and Kim, Kisub and Xu, Bowen and Zhou, Xin and Lo, David},
title = {Curiosity-Driven Testing for Sequential Decision-Making Process},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639149},
doi = {10.1145/3597503.3639149},
abstract = {Sequential decision-making processes (SDPs) are fundamental for complex real-world challenges, such as autonomous driving, robotic control, and traffic management. While recent advances in Deep Learning (DL) have led to mature solutions for solving these complex problems, SDMs remain vulnerable to learning unsafe behaviors, posing significant risks in safety-critical applications. However, developing a testing framework for SDMs that can identify a diverse set of crash-triggering scenarios remains an open challenge. To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows a fuzzer to effectively explore novel and diverse scenarios, leading to improved detection of crash-triggering scenarios. Additionally, we introduce a multi-objective seed selection technique to balance the exploration of novel scenarios and the generation of crash-triggering scenarios, thereby optimizing the fuzzing process. We evaluate CureFuzz on various SDMs and experimental results demonstrate that CureFuzz outperforms the state-of-the-art method by a substantial margin in the total number of faults and distinct types of crash-triggering scenarios. We also demonstrate that the crash-triggering scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a valuable tool for testing SDMs and optimizing their performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {165},
numpages = {14},
keywords = {fuzz testing, sequential decision making, deep learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643659.3643926,
author = {Blattner, Timo and Birchler, Christian and Kehrer, Timo and Panichella, Sebastiano},
title = {Diversity-guided Search Exploration for Self-driving Cars Test Generation through Frenet Space Encoding},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643926},
doi = {10.1145/3643659.3643926},
abstract = {The rise of self-driving cars (SDCs) presents important safety challenges to address in dynamic environments. While field testing is essential, current methods lack diversity in assessing critical SDC scenarios. Prior research introduced simulation-based testing for SDCs, with Frenetic, a test generation approach based on Frenet space encoding, achieving a relatively high percentage of valid tests (approximately 50%) characterized by naturally smooth curves. The "minimal out-of-bound distance" is often taken as a fitness function, which we argue to be a sub-optimal metric. Instead, we show that the likelihood of leading to an out-of-bound condition can be learned by the deep-learning vanilla transformer model. We combine this "inherently learned metric" with a genetic algorithm, which has been shown to produce a high diversity of tests. To validate our approach, we conducted a large-scale empirical evaluation on a dataset comprising over 1,174 simulated test cases created to challenge the SDCs behavior. Our investigation revealed that our approach demonstrates a substantial reduction in generating non-valid test cases, increased diversity, and high accuracy in identifying safety violations during SDC test execution.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {9–12},
numpages = {4},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643795.3648391,
author = {Antal, G\'{a}bor and Voz\'{a}r, Rich\'{a}rd and Ferenc, Rudolf},
title = {Toward a New Era of Rapid Development: Assessing GPT-4-Vision's Capabilities in UML-Based Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648391},
doi = {10.1145/3643795.3648391},
abstract = {The emergence of advanced neural networks has opened up new ways in automated code generation from conceptual models, promising to enhance software development processes. This paper presents a preliminary evaluation of GPT-4-Vision, a state-of-the-art deep learning model, and its capabilities in transforming Unified Modeling Language (UML) class diagrams into fully operating Java class files. In our study, we used exported images of 18 class diagrams comprising 10 single-class and 8 multi-class diagrams. We used 3 different prompts for each input, and we manually evaluated the results. We created a scoring system in which we scored the occurrence of elements found in the diagram within the source code. On average, the model was able to generate source code for 88% of the elements shown in the diagrams. Our results indicate that GPT-4-Vision exhibits proficiency in handling single-class UML diagrams, successfully transforming them into syntactically correct class files. However, for multi-class UML diagrams, the model's performance is weaker compared to single-class diagrams. In summary, further investigations are necessary to exploit the model's potential completely.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {84–87},
numpages = {4},
keywords = {large language models, code generation, OOP, UML, AI in software engineering},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3650105.3652290,
author = {Abukhalaf, Seif and Hamdaqa, Mohammad and Khomh, Foutse},
title = {PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652290},
doi = {10.1145/3650105.3652290},
abstract = {The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {108–118},
numpages = {11},
keywords = {object constraint language (OCL), simple path, prompt engineering, large language model (LLM), generative pre-trained transformer (GPT), foundation model (FM)},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3639478.3643525,
author = {Fakhoury, Sarah and Naik, Aaditya and Sakkas, Georgios and Chakraborty, Saikat and Musuvathi, Madan and Lahiri, Shuvendu},
title = {Exploring the Effectiveness of LLM based Test-driven Interactive Code Generation: User Study and Empirical Evaluation},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643525},
doi = {10.1145/3639478.3643525},
abstract = {We introduce a novel workflow, TiCoder, designed to enhance the trust and accuracy of LLM-based code generation through interactive and guided intent formalization. TiCoder partially formalizes ambiguous intent in natural language prompts by generating a set of tests to distinguish common divergent behaviours in generated code suggestions. We evaluate the code generation accuracy improvements provided by TiCoder at scale across four competitive LLMs, and evaluate the cost-benefit trade off of evaluating tests surfaced by TiCoder through a user study with 15 participants.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {390–391},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639212,
author = {Wang, Huanting and Tang, Zhanyong and Tan, Shin Hwei and Wang, Jie and Liu, Yuzhe and Fang, Hejun and Xia, Chunwei and Wang, Zheng},
title = {Combining Structured Static Code Information and Dynamic Symbolic Traces for Software Vulnerability Prediction},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639212},
doi = {10.1145/3597503.3639212},
abstract = {Deep learning (DL) has emerged as a viable means for identifying software bugs and vulnerabilities. The success of DL relies on having a suitable representation of the problem domain. However, existing DL-based solutions for learning program representations have limitations - they either cannot capture the deep, precise program semantics or suffer from poor scalability. We present Concoction, the first DL system to learn program presentations by combining static source code information and dynamic program execution traces. Concoction employs unsupervised active learning techniques to determine a subset of important paths to collect dynamic symbolic execution traces. By implementing a focused symbolic execution solution, Concoction brings the benefits of static and dynamic code features while reducing the expensive symbolic execution overhead. We integrate Concoction with fuzzing techniques to detect function-level code vulnerabilities in C programs from 20 open-source projects. In 200 hours of automated concurrent test runs, Concoction has successfully uncovered vulnerabilities in all tested projects, identifying 54 unique vulnerabilities and yielding 37 new, unique CVE IDs. Concoction also significantly outperforms 16 prior methods by providing higher accuracy and lower false positive rates.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {169},
numpages = {13},
keywords = {software vulnerability detection, deep learning, symbolic execution},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639218,
author = {Li, Zhen and Wang, Ning and Zou, Deqing and Li, Yating and Zhang, Ruqian and Xu, Shouhuai and Zhang, Chao and Jin, Hai},
title = {On the Effectiveness of Function-Level Vulnerability Detectors for Inter-Procedural Vulnerabilities},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639218},
doi = {10.1145/3597503.3639218},
abstract = {Software vulnerabilities are a major cyber threat and it is important to detect them. One important approach to detecting vulnerabilities is to use deep learning while treating a program function as a whole, known as function-level vulnerability detectors. However, the limitation of this approach is not understood. In this paper, we investigate its limitation in detecting one class of vulnerabilities known as inter-procedural vulnerabilities, where the to-be-patched statements and the vulnerability-triggering statements belong to different functions. For this purpose, we create the first Inter-Procedural Vulnerability Dataset (InterPVD) based on C/C++ open-source software, and we propose a tool dubbed VulTrigger for identifying vulnerability-triggering statements across functions. Experimental results show that VulTrigger can effectively identify vulnerability-triggering statements and inter-procedural vulnerabilities. Our findings include: (i) inter-procedural vulnerabilities are prevalent with an average of 2.8 inter-procedural layers; and (ii) function-level vulnerability detectors are much less effective in detecting to-be-patched functions of inter-procedural vulnerabilities than detecting their counterparts of intra-procedural vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {157},
numpages = {12},
keywords = {vulnerability detection, inter-procedural vulnerability, vulnerability type, patch},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639748,
author = {Hu, Yongxiang and Jin, Hailiang and Wang, Xuan and Gu, Jiazhen and Guo, Shiyu and Chen, Chaoyi and Wang, Xin and Zhou, Yangfan},
title = {AutoConsis: Automatic GUI-driven Data Inconsistency Detection of Mobile Apps},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639748},
doi = {10.1145/3639477.3639748},
abstract = {In industrial practice, many bugs in commercial mobile apps manifest as self-conflicts of data presented in the GUI (Graphical User Interface). Such data inconsistency bugs can bring confusion to the users and deteriorate user experiences. They are a major target of industrial testing practice. However, due to the complication and diversity of GUI implementation and data presentation (e.g., the ways to present the data in natural language), detecting data inconsistency bugs is a very challenging task. It still largely relies on manual efforts. To reduce such human efforts, we proposed AutoConsis, an automated data inconsistency testing tool we designed for Meituan. one of the largest E-commerce providers with over 600 million transacting users. AutoConsis can automatically analyze GUI pages via a multi-modal deep-learning model and extract target data from textual phrases leveraging LLMs (Large Language Models). With these extracted data, their inconsistencies can then be detected. We evaluate the design of AutoConsis via a set of ablation experiments. Moreover, we demonstrate the effectiveness of AutoConsis when applying it to real-world commercial mobile apps with eight representative cases.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {137–146},
numpages = {10},
keywords = {automatic testing, mobile apps, functional bug, in-context learning},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3644032.3644453,
author = {Shrestha, Abhishek and Gro\ss{}mann, J\"{u}rgen},
title = {Properties that allow or prohibit transferability of adversarial attacks among quantized networks},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644453},
doi = {10.1145/3644032.3644453},
abstract = {Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Further, these adversarial examples are found to be transferable from the source network in which they are crafted to a black-box target network. As the trend of using deep learning on embedded devices grows, it becomes relevant to study the transferability properties of adversarial examples among compressed networks. In this paper, we consider quantization as a network compression technique and evaluate the performance of transfer-based attacks when the source and target networks are quantized at different bitwidths. We explore how algorithm specific properties affect transferability by considering various adversarial example generation algorithms. Furthermore, we examine transferability in a more realistic scenario where the source and target networks may differ in bitwidth and other model-related properties like capacity and architecture. We find that although quantization reduces transferability, certain attack types demonstrate an ability to enhance it. Additionally, the average transferability of adversarial examples among quantized versions of a network can be used to estimate the transferability to quantized target networks with varying capacity and architecture.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {99–109},
numpages = {11},
keywords = {deep neural networks, quantization, adversarial examples, transferability},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1109/ICSE48619.2023.00158,
author = {Kou, Bonan and Chen, Muhao and Zhang, Tianyi},
title = {Automated Summarization of Stack Overflow Posts},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00158},
doi = {10.1109/ICSE48619.2023.00158},
abstract = {Software developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language.This paper presents a deep learning based framework called Assort for SO post summarization. Assort includes two complementary learning methods, AssortS and AssortIS, to address the lack of labeled training data for SO post summarization. AssortS is designed to directly train a novel ensemble learning model with BERT embeddings and domain-specific features to account for the unique characteristics of SO posts. By contrast, AssortIS is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both AssortS and AssortIS outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by AssortS and AssortIS over the best baseline, while the preference difference between AssortS and AssortIS was small.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1853–1865},
numpages = {13},
keywords = {stack overflow, text summarization, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643787.3648041,
author = {Alam, Khubaib Amjad and Jumani, Ashish and Aamir, Harris and Uzair, Muhammad},
title = {ClassifAI: Automating Issue Reports Classification using Pre-Trained BERT (Bidirectional Encoder Representations from Transformers) Models},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648041},
doi = {10.1145/3643787.3648041},
abstract = {The utilization of Issue Tracking Systems by users to systematically manage and monitor issue reports within their repositories has become indispensable. An issue report encapsulates a wealth of software-related information, encompassing problem descriptions, requests for new features and inquiries about the software product, to name a few. As the volume of these issues escalates, manual management becomes increasingly challenging, prompting the exploration of automated approaches for more effective handling. This paper introduces ClassifAI 1, an automated Issue Report Categorization approach built on the foundation of the Transformer-based pre-trained RoBERTa-Large model. ClassifAI proficiently classifies issue reports into three primary categories: Bug report, Enhancement/feature request, and Question. The process involves cleaning and preprocessing data sets provided for the NLBSE'24 [7] tool competition, followed by fine-tuning the pre-trained RoBERTa model on the refined data set. The experimental evaluation of ClassifAI is performed on approximately 1500 issue reports belonging to five different projects. The results indicate that RoBERTa-Large fine tuned variant demonstrates an acceptable level of performance by achieving a 83.2% F1-score (micro average).},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {49–52},
numpages = {4},
keywords = {issue reports, natural language processing, text analysis, machine learning, language models, BERT},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3597503.3639106,
author = {Dola, Swaroopa and McDaniel, Rory and Dwyer, Matthew B. and Soffa, Mary Lou},
title = {CIT4DNN: Generating Diverse and Rare Inputs for Neural Networks Using Latent Space Combinatorial Testing},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639106},
doi = {10.1145/3597503.3639106},
abstract = {Deep neural networks (DNN) are being used in a wide range of applications including safety-critical systems. Several DNN test generation approaches have been proposed to generate fault-revealing test inputs. However, the existing test generation approaches do not systematically cover the input data distribution to test DNNs with diverse inputs, and none of the approaches investigate the relationship between rare inputs and faults. We propose cit4dnn, an automated black-box approach to generate DNN test sets that are feature-diverse and that comprise rare inputs. cit4dnn constructs diverse test sets by applying combinatorial interaction testing to the latent space of generative models and formulates constraints over the geometry of the latent space to generate rare and fault-revealing test inputs. Evaluation on a range of datasets and models shows that cit4dnn generated tests are more feature diverse than the state-of-the-art, and can target rare fault-revealing testing inputs more effectively than existing methods.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {118},
numpages = {13},
keywords = {deep neural networks, test generation, test coverage, combinatorial interaction testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644894,
author = {Rajput, Saurabhsingh and Kechagia, Maria and Sarro, Federica and Sharma, Tushar},
title = {Greenlight: Highlighting TensorFlow APIs Energy Footprint},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644894},
doi = {10.1145/3643991.3644894},
abstract = {Deep learning (DL) models are being widely deployed in real-world applications, but their usage remains computationally intensive and energy-hungry. While prior work has examined model-level energy usage, the energy footprint of the DL frameworks, such as TensorFlow and PyTorch, used to train and build these models, has not been thoroughly studied. We present Greenlight, a large-scale dataset containing fine-grained energy profiling information of 1284 TensorFlow API calls. We developed a command line tool called CodeGreen to curate such a dataset. CodeGreen is based on our previously proposed framework FECoM, which employs static analysis and code instrumentation to isolate invocations of Tensor-Flow operations and measure their energy consumption precisely. By executing API calls on representative workloads and measuring the consumed energy, we construct detailed energy profiles for the APIS. Several factors, such as input data size and the type of operation, significantly impact energy footprints. Greenlight provides a ground-truth dataset capturing energy consumption along with relevant factors such as input parameter size to take the first step towards optimization of energy-intensive TensorFlow code. The Greenlight dataset opens up new research directions such as predicting API energy consumption, automated optimization, modeling efficiency trade-offs, and empirical studies into energy-aware DL system design.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {304–308},
numpages = {5},
keywords = {energy measurement, green artificial intelligence, fine-grained energy measurement},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00194,
author = {Kang, Sungmin and Yoon, Juyeon and Yoo, Shin},
title = {Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00194},
doi = {10.1109/ICSE48619.2023.00194},
abstract = {Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2312–2323},
numpages = {12},
keywords = {test generation, natural language processing, software engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00197,
author = {Feng, Sidong and Xie, Mulong and Xue, Yinxing and Chen, Chunyang},
title = {Read It, Don't Watch It: Captioning Bug Recordings Automatically},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00197},
doi = {10.1109/ICSE48619.2023.00197},
abstract = {Screen recordings of mobile applications are easy to capture and include a wealth of information, making them a popular mechanism for users to inform developers of the problems encountered in the bug reports. However, watching the bug recordings and efficiently understanding the semantics of user actions can be time-consuming and tedious for developers. Inspired by the conception of the video subtitle in movie industry, we present a lightweight approach CAPdroid to caption bug recordings automatically. CAPdroid is a purely image-based and non-intrusive approach by using image processing and convolutional deep learning models to segment bug recordings, infer user action attributes, and generate subtitle descriptions. The automated experiments demonstrate the good performance of CAPdroid in inferring user actions from the recordings, and a user study confirms the usefulness of our generated step descriptions in assisting developers with bug replay.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2349–2361},
numpages = {13},
keywords = {bug recording, video captioning, android app},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00064,
author = {Melo, Glaucia},
title = {Designing Adaptive Developer-Chatbot Interactions: Context Integration, Experimental Studies, and Levels of Automation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00064},
doi = {10.1109/ICSE-Companion58688.2023.00064},
abstract = {The growing demand for software developers and the increasing development complexity have emphasized the need for support in software engineering projects. This is especially relevant in light of advancements in artificial intelligence, such as conversational systems. A significant contributor to the complexity of software development is the multitude of tools and methods used, creating various contexts in which software developers must operate. Moreover, there has been limited investigation into the interaction between context-based chatbots and software developers through experimental user studies. Assisting software developers in their work becomes essential. In particular, understanding the context surrounding software development and integrating this context into chatbots can lead to novel insight into what software developers expect concerning these human-chatbot interactions and their levels of automation. In my research, I study the design of context-based adaptive interactions between software developers and chatbots to foster solutions and knowledge to support software developers at work.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {235–239},
numpages = {5},
keywords = {software engineering, context, chatbot, levels of automation, autonomous systems, interactions},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644418,
author = {Huang, Tao and Sun, Zhihong and Jin, Zhi and Li, Ge and Lyu, Chen},
title = {Knowledge-Aware Code Generation with Large Language Models},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644418},
doi = {10.1145/3643916.3644418},
abstract = {Large Language Models (LLMs) perform well on basic programming problems. However, they encounter challenges when dealing with complex tasks involving the use of diverse algorithmic and data structure skills, particularly programming competition-level problems. Notably, ChatGPT exhibits proficient performance on problems it has encountered during its pre-training phase, but this performance deteriorates when faced with novel problems. Consequently, enhancing the ability of LLMs to address unfamiliar problems has emerged as a pivotal research focus. The problem-solving process of LLMs mirrors human programmers' approach to a certain extent. When confronted with new programming tasks, human programmers engage in task planning and code writing with the previously acquired knowledge about algorithms and data structures. Despite having learned such knowledge, LLMs struggle to effectively apply it when faced with specific new problems. To address this issue, we constructed a novel dataset, CodeF, which contains a portion of programming problems that ChatGPT has not previously encountered. Furthermore, we developed a Knowledge Library tailored for Python programming contest problems and introduced the concept of Knowledge-Aware Code Generation (KareCoder). KareCoder bolsters the models' understanding and problem-solving capabilities by integrating prompt and knowledge from the library into the LLMs' code generation reasoning process, especially on Pass@1 metrics. Upon testing on the CodeF and APPS datasets, KareCoder demonstrated outstanding performance in handling novel problems previously unencountered by LLMs. In contrast with the code directly generated by ChatGPT, KareCoder achieved a relative improvement of 23.3% on the Pass@1 metric on the CodeF post2021-9 dataset. Additionally, it performs well compared to other methods when dealing with problems that LLMs have previously encountered. Our dataset and experiment data are open-sourced and can be accessed at https://github.com/CodeGeneration3/KareCoder.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {52–63},
numpages = {12},
keywords = {code generation, large language models, knowledge library},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643991.3644885,
author = {Liu, Zhipeng and Yan, Meng and Gao, Zhipeng and Li, Dong and Zhang, Xiaohong and Yang, Dan},
title = {AW4C: A Commit-Aware C Dataset for Actionable Warning Identification},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644885},
doi = {10.1145/3643991.3644885},
abstract = {Excessive non-actionable warnings generated by static program analysis tools can hinder developers from utilizing these tools effectively. Leveraging learning-based approaches for actionable warning identification has demonstrated promise in boosting developer productivity, minimizing the risk of bugs, and reducing code smells. However, the small sizes of existing datasets have limited the model choices for machine learning researchers, and the lack of aligned fix commits limits the scope of the dataset for research. In this paper, we present AW4C, an actionable warning C dataset that contains 38,134 actionable warnings mined from more than 500 repositories on GitHub. These warnings are generated via Cppcheck, and most importantly, each warning is precisely mapped to the commit where the corrective action occurred. To the best of our knowledge, this is the largest publicly available actionable warning dataset for C programming language to date. The dataset is suited for use in machine/deep learning models and can support a wide range of tasks, such as actionable warning identification and vulnerability detection. Furthermore, we have released our dataset1 and a general framework for collecting actionable warnings on GitHub2 to facilitate other researchers to replicate our work and validate their innovative ideas.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {133–137},
numpages = {5},
keywords = {static program analysis, actionable warning identification},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00189,
author = {Wang, Wenbo and Nguyen, Tien N. and Wang, Shaohua and Li, Yi and Zhang, Jiyuan and Yadavally, Aashish},
title = {DeepVD: Toward Class-Separation Features for Neural Network Vulnerability Detection},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00189},
doi = {10.1109/ICSE48619.2023.00189},
abstract = {The advances of machine learning (ML) including deep learning (DL) have enabled several approaches to implicitly learn vulnerable code patterns to automatically detect software vulnerabilities. A recent study showed that despite successes, the existing ML/DL-based vulnerability detection (VD) models are limited in the ability to distinguish between the two classes of vulnerability and benign code. We propose DEEPVD, a graph-based neural network VD model that emphasizes on class-separation features between vulnerability and benign code. DEEPVD leverages three types of class-separation features at different levels of abstraction: statement types (similar to Part-of-Speech tagging), Post-Dominator Tree (covering regular flows of execution), and Exception Flow Graph (covering the exception and error-handling flows). We conducted several experiments to evaluate DEEPVD in a real-world vulnerability dataset of 303 projects with 13,130 vulnerable methods. Our results show that DEEPVD relatively improves over the state-of-the-art ML/DL-based VD approaches 13%--29.6% in precision, 15.6%--28.9% in recall, and 16.4%--25.8% in F-score. Our ablation study confirms that our designed features and components help DEEPVD achieve high class-separability for vulnerability and benign code.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2249–2261},
numpages = {13},
keywords = {neural vulnerability detection, graph neural network, class separation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3639476,
title = {ICSE-NIER'24: Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639478.3643526,
author = {Fakhoury, Sarah and Chakraborty, Saikat and Musuvathi, Madanlal and Lahiri, Shuvendu K.},
title = {NL2Fix: Generating Functionally Correct Code Edits from Bug Descriptions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643526},
doi = {10.1145/3639478.3643526},
abstract = {Despite the notable advancement of Large Language Models for Code Generation, there is a distinct gap in benchmark datasets and evaluation of LLMs' proficiency in generating functionally correct code edits based on natural language descriptions of intended changes. We address this void by presenting the challenge of translating natural language descriptions of code changes, particularly bug fixes outlined in Issue reports within repositories, into accurate code fixes. To tackle this issue, we introduce Defects4J-Nl2fix, a dataset comprising 283 Java programs from the widely-used Defects4J dataset, augmented with high-level descriptions of bug fixes. Subsequently, we empirically evaluate three state-of-the-art LLMs on this task, exploring the impact of different prompting strategies on their ability to generate functionally correct edits. Results show varied ability across models on this novel task. Collectively, the studied LLMs are able to produce plausible fixes for 64.6% of the bugs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {410–411},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639477.3639716,
author = {Moon, Jijoong and Lee, Hyeonseok and Chu, Jiho and Park, Donghak and Hong, Seungbaek and Seo, Hyungjun and Jeong, Donghyeon and Kong, Sungsik and Ham, Myungjoo},
title = {A New Frontier of AI: On-Device AI Training and Personalization},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639716},
doi = {10.1145/3639477.3639716},
abstract = {Modern consumer electronic devices have started executing deep learning-based intelligence services on devices, not cloud servers, to keep personal data on devices and to reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. However, the limited resources of devices incurs significant difficulties. We propose a light-weight on-device training framework, NNTrainer, which provides highly memory-efficient neural network training techniques and proactive swapping based on fine-grained execution order analysis for neural networks. Moreover, its optimizations do not sacrifice accuracy and are transparent to training algorithms; thus, prior algorithmic studies may be implemented on top of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption down to 1/20 (saving 95%!) and effectively personalizes intelligence services on devices. NNTrainer is cross-platform and practical open-source software, which is being deployed to millions of mobile devices.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {323–333},
numpages = {11},
keywords = {on-device AI, neural network, personalization, training, software framework},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643690.3648236,
author = {Hamza, Muhammad and Siemon, Dominik and Akbar, Muhammad Azeem and Rahman, Tahsinur},
title = {Human-AI Collaboration in Software Engineering: Lessons Learned from a Hands-On Workshop},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648236},
doi = {10.1145/3643690.3648236},
abstract = {This paper investigates the dynamics of human-AI collaboration in software engineering, focusing on the use of ChatGPT. Through a thematic analysis of a hands-on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner. The study identifies key themes such as the evolving nature of human-AI interaction, the capabilities of AI in software engineering tasks, and the challenges and limitations of integrating AI in this domain. The findings show that while AI, particularly ChatGPT, improves the efficiency of code generation and optimization, human oversight remains crucial, especially in areas requiring complex problem-solving and security considerations. This research contributes to the theoretical understanding of human-AI collaboration in software engineering and provides practical insights for effectively integrating AI tools into development processes. It highlights the need for clear role allocation, effective communication, and balanced AI-human collaboration to realize the full potential of AI in software engineering.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {7–14},
numpages = {8},
keywords = {generative AI, ChatGPT, software engineering, workshop, empirical investigation},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1109/ICSE48619.2023.00111,
author = {Jiang, Nan and Lutellier, Thibaud and Lou, Yiling and Tan, Lin and Goldwasser, Dan and Zhang, Xiangyu},
title = {KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00111},
doi = {10.1109/ICSE48619.2023.00111},
abstract = {Automated Program Repair (APR) improves software reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug.We propose a DL-based APR approach KNOD, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases.We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1251–1263},
numpages = {13},
keywords = {automated program repair, abstract syntax tree, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623349,
author = {Li, Yue and Ren, Zhong and Wang, Zhiqi and Yang, Lanxin and Dong, Liming and Zhong, Chenxing and Zhang, He},
title = {Fine-SE: Integrating Semantic Features and Expert Features for Software Effort Estimation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623349},
doi = {10.1145/3597503.3623349},
abstract = {Reliable effort estimation is of paramount importance to software planning and management, especially in industry that requires effective and on-time delivery. Although various estimation approaches have been proposed (e.g., planning poker and analogy), they may be manual and/or subjective, which are difficult to apply to other projects. In recent years, deep learning approaches for effort estimation that rely on learning expert features or semantic features respectively have been extensively studied and have been found to be promising. Semantic features and expert features describe software tasks from different perspectives, however, in the literature, the best combination of these two features has not been explored to enhance effort estimation. Additionally, there are a few studies that discuss which expert features are useful for estimating effort in the industry. To this end, we investigate the potential 13 expert features that can be used to estimate effort by interviewing 26 enterprise employees. Based on that, we propose a novel model, called Fine-SE, that leverages semantic features and expert features for effort estimation. To validate our model, a series of evaluations are conducted on more than 30,000 software tasks from 17 industrial projects of a global ICT enterprise and four open-source software (OSS) projects. The evaluation results indicate that Fine-SE provides higher performance than the baselines on evaluation measures (i.e., mean absolute error, mean magnitude of relative error, and performance indicator), particularly in industrial projects with large amounts of software tasks, which implies a significant improvement in effort estimation. In comparison with expert estimation, Fine-SE improves the performance of evaluation measures by 32.0%-45.2% in within-project estimation. In comparison with the state-of-the-art models, Deep-SE and GPT2SP, it also achieves an improvement of 8.9%-91.4% in industrial projects. The experimental results reveal the value of integrating expert features with semantic features in effort estimation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {27},
numpages = {12},
keywords = {effort estimation, AI for SE, deep learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3648505.3648507,
author = {Herbold, Lars and Sadeghi, Mersedeh and Vogelsang, Andreas},
title = {Generating Context-Aware Contrastive Explanations in Rule-based Systems},
year = {2024},
isbn = {9798400705960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648505.3648507},
doi = {10.1145/3648505.3648507},
abstract = {Human explanations are often contrastive, meaning that they do not answer the indeterminate "Why?" question, but instead "Why P, rather than Q?". Automatically generating contrastive explanations is challenging because the contrastive event (Q) represents the expectation of a user in contrast to what happened. We present an approach that predicts a potential contrastive event in situations where a user asks for an explanation in the context of rule-based systems. Our approach analyzes a situation that needs to be explained and then selects the most likely rule a user may have expected instead of what the user has observed. This contrastive event is then used to create a contrastive explanation that is presented to the user. We have implemented the approach as a plugin for a home automation system and demonstrate its feasibility in four test scenarios.},
booktitle = {Proceedings of the 2024 Workshop on Explainability Engineering},
pages = {8–14},
numpages = {7},
keywords = {explainability, software engineering, smart environments},
location = {Lisbon, Portugal},
series = {ExEn '24}
}

@inproceedings{10.1109/ICSE48619.2023.00054,
author = {Wu, Weibin and Zhang, Jianping and Wei, Victor Junqiu and Chen, Xixian and Zheng, Zibin and King, Irwin and Lyu, Michael R.},
title = {Practical and Efficient Model Extraction of Sentiment Analysis APIs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00054},
doi = {10.1109/ICSE48619.2023.00054},
abstract = {Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {524–536},
numpages = {13},
keywords = {model extraction, sentiment analysis APIS, active learning, architecture search},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3643661,
title = {InteNSE '24: Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {InteNSE is an interdisciplinary workshop for research at the intersection of Artificial Intelligence (AI) and Software Engineering (SE) and would be a pioneer in emphasizing the implicit properties and applications of neural software engineering and analysis. Due to recent computational advancements, AI has become an inseparable part of the SE research community, with Large Language Models (LLMs) showing a promising performance to automate SE tasks. However, most research in the AI and SE communities consider machine learning (ML) components as closed-box, i.e., only considering the final performance of the developed models as an evaluation metric. Ignoring the implicit properties of neural models, such as interpretability, robustness, and fairness, one cannot validate its actual performance, generalizability, and whether it is learning what it should do. Specifically, in the domain of SE, where the result of AI4SE tools is code synthesis, bug finding, or repair; interpretability and robustness are crucial to ensure the reliability of the products.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/GREENS66463.2025.00015,
author = {Ahuja, Naman and Feng, Yile and Li, Luming and Malik, Amisha and Sivayoganathan, Thuvaragan and Balani, Navveen and Rakhunathan, Srinivasan and Sarro, Federica},
title = {Automatically Assessing Software Architecture Compliance With Green Software Patterns},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00015},
doi = {10.1109/GREENS66463.2025.00015},
abstract = {With increasing awareness of climate change, there is a growing emphasis on the environmental impact of digital solutions. While numerous tools are available to assess software environmental footprint post-development, few focus on sustainability during the software design phase. To address this gap, we propose EcoDocSense, a framework that supports engineers to evaluate the sustainability of a software design at design time. Using Large Language Models fine-tuned on a catalog of green software patterns, EcoDocSense analyzes software architecture documents to generate sustainability reports, assessing alignment with green software practices to minimize carbon emissions and recommending improvements. As one of the first frameworks targeting sustainability at the design stage, EcoDocSense represents a significant advancement, though opportunities remain for further enhancement. In future, we plan to extend EcoDocSense’s applicability to a variety of architectural types and documents as well as to provide the capability to estimate carbon emissions.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {68–75},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3597503.3639185,
author = {Ferrara, Carmine and Casillo, Francesco and Gravino, Carmine and De Lucia, Andrea and Palomba, Fabio},
title = {ReFAIR: Toward a Context-Aware Recommender for Fairness Requirements Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639185},
doi = {10.1145/3597503.3639185},
abstract = {Machine learning (ML) is increasingly being used as a key component of most software systems, yet serious concerns have been raised about the fairness of ML predictions. Researchers have been proposing novel methods to support the development of fair machine learning solutions. Nonetheless, most of them can only be used in late development stages, e.g., during model training, while there is a lack of methods that may provide practitioners with early fairness analytics enabling the treatment of fairness throughout the development lifecycle. This paper proposes ReFair, a novel context-aware requirements engineering framework that allows to classify sensitive features from User Stories. By exploiting natural language processing and word embedding techniques, our framework first identifies both the use case domain and the machine learning task to be performed in the system being developed; afterward, it recommends which are the context-specific sensitive features to be considered during the implementation. We assess the capabilities of ReFair by experimenting it against a synthetic dataset---which we built as part of our research---composed of 12,401 User Stories related to 34 application domains. Our findings showcase the high accuracy of ReFair, other than highlighting its current limitations.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {213},
numpages = {12},
keywords = {software fairness, machine learning, requirements engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00011,
author = {Tonella, Paolo},
title = {The Road Toward Dependable AI Based Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00011},
doi = {10.1109/ICSE48619.2023.00011},
abstract = {With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2},
numpages = {1},
keywords = {software testing, deep learning, reliability and dependability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00084,
author = {Feng, Sidong and Xie, Mulong and Chen, Chunyang},
title = {Efficiency Matters: Speeding Up Automated Testing with GUI Rendering Inference},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00084},
doi = {10.1109/ICSE48619.2023.00084},
abstract = {Due to the importance of Android app quality assurance, many automated GUI testing tools have been developed. Although the test algorithms have been improved, the impact of GUI rendering has been overlooked. On the one hand, setting a long waiting time to execute events on fully rendered GUIs slows down the testing process. On the other hand, setting a short waiting time will cause the events to execute on partially rendered GUIs, which negatively affects the testing effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We propose AdaT, a lightweight image-based approach to dynamically adjust the inter-event time based on GUI rendering state. Given the real-time streaming on the GUI, AdaT presents a deep learning model to infer the rendering state, and synchronizes with the testing tool to schedule the next event when the GUI is fully rendered. The evaluations demonstrate the accuracy, efficiency, and effectiveness of our approach. We also integrate our approach with the existing automated testing tool to demonstrate the usefulness of AdaT in covering more activities and executing more events on fully rendered GUIs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {906–918},
numpages = {13},
keywords = {efficient android GUI testing, GUI rendering, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3608128,
author = {Liang, Jenny T. and Yang, Chenyang and Myers, Brad A.},
title = {A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608128},
doi = {10.1145/3597503.3608128},
abstract = {The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {52},
numpages = {13},
keywords = {AI programming assistants, usability study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639137,
author = {Huang, Ruikai and Motwani, Manish and Martinez, Idel and Orso, Alessandro},
title = {Generating REST API Specifications through Static Analysis},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639137},
doi = {10.1145/3597503.3639137},
abstract = {Web Application Programming Interfaces (APIs) allow services to be accessed over the network. RESTful (or REST) APIs, which use the REpresentation State Transfer (REST) protocol, are a popular type of web API. To use or test REST APIs, developers use specifications written in standards such as OpenAPI. However, creating and maintaining these specifications is time-consuming and error-prone, especially as software evolves, leading to incomplete or inconsistent specifications that negatively affect the use and testing of the APIs. To address this problem, we present Respector (REST API specification generator), the first technique to employ static and symbolic program analysis to generate specifications for REST APIs from their source code. We evaluated Respector on 15 real-world APIs with promising results in terms of precision and recall in inferring endpoint methods, endpoint parameters, method responses, and parameter attributes, including constraints leading to successful HTTP responses or errors. Furthermore, these results could be further improved with additional engineering. Comparing the Respector-generated specifications with the developer-provided ones shows that Respector was able to identify many missing end-point methods, parameters, constraints, and responses, along with some inconsistencies between developer-provided specifications and API implementations. Finally, Respector outperformed several techniques that infer specifications from annotations within API implementations or by invoking the APIs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {107},
numpages = {13},
keywords = {REST APIs, openapi specifications, documentation, static analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639086,
author = {Prenner, Julian Aron and Robbes, Romain},
title = {Out of Context: How important is Local Context in Neural Program Repair?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639086},
doi = {10.1145/3597503.3639086},
abstract = {Deep learning source code models have been applied very successfully to the problem of automated program repair. One of the standing issues is the small input window of current models which often cannot fully fit the context code required for a bug fix (e.g., method or class declarations of a project). Instead, input is often restricted to the local context, that is, the lines below and above the bug location. In this work we study the importance of this local context on repair success: how much local context is needed?; is context before or after the bug location more important? how is local context tied to the bug type? To answer these questions we train and evaluate Transformer models in many different local context configurations on three datasets and two programming languages. Our results indicate that overall repair success increases with the size of the local context (albeit not for all bug types) and confirm the common practice that roughly 50--60% of the input window should be used for context leading the bug. Our results are not only relevant for researchers working on Transformer-based APR tools but also for benchmark and dataset creators who must decide what and how much context to include in their datasets.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {83},
numpages = {13},
keywords = {automated program repair, data-driven software engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3608132,
author = {Peng, Yun and Gao, Shuzheng and Gao, Cuiyun and Huo, Yintong and Lyu, Michael},
title = {Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608132},
doi = {10.1145/3597503.3608132},
abstract = {As a dynamic programming language, Python has become increasingly popular in recent years. Although the dynamic type system of Python facilitates the developers in writing Python programs, it also brings type errors at run-time which are prevalent yet not easy to fix. There exist rule-based approaches for automatically repairing Python type errors. The approaches can generate accurate patches for the type errors covered by manually defined templates, but they require domain experts to design patch synthesis rules and suffer from low template coverage of real-world type errors. Learning-based approaches alleviate the manual efforts in designing patch synthesis rules and have become prevalent due to the recent advances in deep learning. Among the learning-based approaches, the prompt-based approach which leverages the knowledge base of code pre-trained models via pre-defined prompts, obtains state-of-the-art performance in general program repair tasks. However, such prompts are manually defined and do not involve any specific clues for repairing Python type errors, resulting in limited effectiveness. How to automatically improve prompts with the domain knowledge for type error repair is challenging yet under-explored.In this paper, we present TypeFix, a novel prompt-based approach with fix templates incorporated for repairing Python type errors. TypeFix first mines generalized fix templates via a novel hierarchical clustering algorithm. The identified fix templates indicate the common edit patterns and contexts of existing type error fixes. TypeFix then generates code prompts for code pre-trained models by employing the generalized fix templates as domain knowledge, in which the masks are adaptively located for each type error instead of being pre-determined. Experiments on two benchmarks, including BugsInPy and TypeBugs, show that TypeFix successfully repairs 26 and 55 type errors, outperforming the best baseline approach by 9 and 14, respectively. Besides, the proposed fix template mining approach can cover 75% of developers' patches in both benchmarks, increasing the best rule-based approach PyTER by more than 30%.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {4},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3643690,
title = {IWSiB '24: Proceedings of the 7th ACM/IEEE International Workshop on Software-intensive Business},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The workshop brings together research communities working on softwareintensive business and software engineering. It aims to bridge the gap between the research in these areas. This year's theme, "Software Business in the Era of Generative Artificial Intelligence," reflects our focus on exploring how generative artificial intelligence (GenAI) and the related large language models (LLMs) impact the established practices of software engineering and software business.},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3597503,
title = {ICSE '24: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00030,
author = {Talebipour, Saghar and Park, Hyojae and Baral, Kesina and Yee, Leon and Khan, Safwat Ali and Moran, Kevin and Brun, Yuriy and Medvidovic, Nenad and Zhao, Yixue},
title = {Avgust: A Tool for Generating Usage-Based Tests from Videos of App Executions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00030},
doi = {10.1109/ICSE-Companion58688.2023.00030},
abstract = {Creating UI tests for mobile applications is a difficult and time-consuming task. As such, there has been a considerable amount of work carried out to automate the generation of mobile tests---largely focused upon the goals of maximizing code coverage or finding crashes. However, comparatively fewer automated techniques have been proposed to generate a highly sought after type of test: usage-based tests. These tests exercise targeted app functionalities for activities such as regression testing. In this paper, we present the Avgust tool for automating the construction of usage-based tests for mobile apps. Avgust learns usage patterns from videos of app executions collected by beta testers or crowd-workers, translates these into an app-agnostic state-machine encoding, and then uses this encoding to generate new test cases for an unseen target app. We evaluated Avgust on 374 videos of use cases from 18 popular apps and found that it can successfully exercise the desired usage in 69% of the tests. Avgust is an open-source tool available at https://github.com/felicitia/UsageTesting-Repo/tree/demo. A video illustrating the capabilities of Avgust can be found at: https://youtu.be/LPICxVd0YAg.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {83–87},
numpages = {5},
keywords = {mobile application, UI understanding, mobile testing, test generation, AI/ML},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3644916,
author = {Alsayed, Ahmed Saeed and Dam, Hoa Khanh and Nguyen, Chau},
title = {MicroRec: Leveraging Large Language Models for Microservice Recommendation},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644916},
doi = {10.1145/3643991.3644916},
abstract = {The increasing adoption of microservices in software development requires effective recommendation systems that guide developers to relevant microservices. In this paper, we introduce MicroRec, a novel microservice recommender framework which leverages insights from Stack Overflow posts and the power of Large Language Models (LLMs). MicroRec utilizes a dual-encoder architecture that combines contrastive learning and semantic similarity learning, allowing us to achieve robust and accurate retrieval and ranking of relevant posts based on user queries. Using LLMs, MicroRec builds up a deep understanding of both user queries and microservices through the information they provide (e.g., README files and Dockerfiles). Our empirical evaluations demonstrate significant improvements brought by MicroRec over the existing methods across a variety of performance metrics including MRR, MAP, and precision@k. In addition, the results returned by MicroRec were fourteen times more accurate than those provided by the existing recommendation tool on the widely-used Docker Hub platform.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {419–430},
numpages = {12},
keywords = {microservices, recommendation system, semantic search, large language models, docker hub},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643916.3644400,
author = {Mastropaolo, Antonio and Ciniselli, Matteo and Pascarella, Luca and Tufano, Rosalia and Aghajani, Emad and Bavota, Gabriele},
title = {Towards Summarizing Code Snippets Using Pre-Trained Transformers},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644400},
doi = {10.1145/3643916.3644400},
abstract = {When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there. To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code. Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement). Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs &lt;method, javadoc&gt; that can be fed to DL models to teach them how to summarize a method. Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements. In this work, we take all the steps needed to train a DL model to automatically document code snippets. First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document. Second, we used such a dataset to train a multi-task DL model taking as input a comment and being able to (i) classify whether it represents a "code summary" or not, and (ii) link it to the code statements it documents. Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%. Third, we run this model on 10k projects, identifying and linking code summaries to the documented code. This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to automatically document code snippets. A comparison with state-of-the-art baselines shows the superiority of the proposed approach, which however, is still far from representing an accurate solution for snippet summarization.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {1–12},
numpages = {12},
keywords = {software documentation, pre-trained transformer models},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639478.3639798,
author = {Dipongkor, Atish Kumar},
title = {Towards Interpreting the Behavior of Large Language Models on Software Engineering Tasks},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639798},
doi = {10.1145/3639478.3639798},
abstract = {Large Language Models (LLMs) have ushered in a significant breakthrough within the field of Natural Language Processing. Building upon this achievement, analogous language models have been developed specifically for code-related tasks, commonly referred to as Large Language Models for Code (LLMsC). Notable examples of LLMsC include CodeBERT, UnixCoder, CoPilot, among others. These models have demonstrated exceptional performance across various Software Engineering (SE) tasks, encompassing code summarization, test case generation, natural language to code conversion, bug triaging, malware detection, program repair, and more.Despite the promising results achieved by LLMsC in SE tasks, there remains fundamental questions regarding their decision-making processes. Understanding these model decision mechanisms is crucial for further enhancing the performance of LLMsC. In pursuit of this objective, my PhD dissertation aims to pioneer novel methodologies for interpreting and comprehending the behavior of LLMsC.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {255–257},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3647634,
author = {Wang, Xinchen and Hu, Ruida and Gao, Cuiyun and Wen, Xin-Cheng and Chen, Yujia and Liao, Qing},
title = {ReposVul: A Repository-Level High-Quality Vulnerability Dataset},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647634},
doi = {10.1145/3639478.3647634},
abstract = {Open-Source Software (OSS) vulnerabilities bring great challenges to the software security and pose potential risks to our society. Enormous efforts have been devoted into automated vulnerability detection, among which deep learning (DL)-based approaches have proven to be the most effective. However, the performance of the DL-based approaches generally relies on the quantity and quality of labeled data, and the current labeled data present the following limitations: (1) Tangled Patches: Developers may submit code changes unrelated to vulnerability fixes within patches, leading to tangled patches. (2) Lacking Inter-procedural Vulnerabilities: The existing vulnerability datasets typically contain function-level and file-level vulnerabilities, ignoring the relations between functions, thus rendering the approaches unable to detect the inter-procedural vulnerabilities. (3) Outdated Patches: The existing datasets usually contain outdated patches, which may bias the model during training.To address the above limitations, in this paper, we propose an automated data collection framework and construct the first repository-level high-quality vulnerability dataset named ReposVul. The proposed framework mainly contains three modules: (1) A vulnerability untangling module, aiming at distinguishing vulnerability-fixing related code changes from tangled patches, in which the Large Language Models (LLMs) and static analysis tools are jointly employed. (2) A multi-granularity dependency extraction module, aiming at capturing the inter-procedural call relationships of vulnerabilities, in which we construct multiple-granularity information for each vulnerability patch, including repository-level, file-level, function-level, and line-level. (3) A trace-based filtering module, aiming at filtering the outdated patches, which leverages the file path trace-based filter and commit time trace-based filter to construct an up-to-date dataset.The constructed repository-level ReposVul encompasses 6,134 CVE entries representing 236 CWE types across 1,491 projects and four programming languages. Thorough data analysis and manual checking demonstrate that ReposVul is high in quality and alleviates the problems of tangled and outdated patches in previous vulnerability datasets.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {472–483},
numpages = {12},
keywords = {open-source software, software vulnerability datasets, data quality},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3644906,
author = {Nikeghbal, Nafiseh and Kargaran, Amir Hossein and Heydarnoori, Abbas},
title = {GIRT-Model: Automated Generation of Issue Report Templates},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644906},
doi = {10.1145/3643991.3644906},
abstract = {Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs) to enable more effective issue management and better alignment with developer expectations. However, these templates are not widely adopted in most repositories, and there is currently no tool available to aid developers in generating them. In this work, we introduce GIRT-Model, an assistant language model that automatically generates IRTs based on the developer's instructions regarding the structure and necessary fields. We create GIRT-Instruct, a dataset comprising pairs of instructions and IRTs, with the IRTs sourced from GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model to create the GIRT-Model.In our experiments, GIRT-Model outperforms general language models (T5 and Flan-T5 with different parameter sizes) in IRT generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a user study in which participants wrote short IRTs with GIRT-Model. Our results show that the participants find GIRT-Model useful in the automated generation of templates. We hope that through the use of GIRT-Model, we can encourage more developers to adopt IRTs in their repositories. We publicly release our code, dataset, and model at https://github.com/ISE-Research/girt-model.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {407–418},
numpages = {12},
keywords = {issue template generation, issue report template, issue template, bug template, GitHub, issue tracker, bug report},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643795.3648388,
author = {Gandhi, Shubham and Patwardhan, Manasi and Khatri, Jyotsana and Vig, Lovekesh and Medicherla, Raveendra Kumar},
title = {Translation of Low-Resource COBOL to Logically Correct and Readable Java leveraging High-Resource Java Refinement},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648388},
doi = {10.1145/3643795.3648388},
abstract = {Automated translation of legacy code to modern programming languages is the need of the hour for modernizing enterprise systems. This work specifically addresses automated COBOL to Java translation. Traditional rule-based tools for this perform statement-wise translation, overlooking possible modularization and refactoring of the source COBOL code to translate to human-readable target Java code. Our investigation reveals that state-of-the-art Large Language Models (LLMs) in the domain of code encounter difficulties with regard to logical correctness and readability when directly translating low-resource COBOL code to Java. To address these challenges, we propose an LLM-based workflow, leveraging temperature sampling and refinement-based strategies, to not only ensure logical correctness of the translation but also maximize the readability of the target Java code. We exploit the fact that, due to their extensive exposure to human-written Java codes during pre-training, the LLMs are more equipped with profound comprehension and capability for refining translated Java codes than COBOL to Java translation. With a dataset sourced from CodeNet, we perform sequential refinement of the translated high-resource Java code with execution-guided logic feedback followed by LLM-based readability feedback. We demonstrate that this yields better performance in terms of logical correctness (81.99% execution accuracy) and readability (0.610 score), than LLM based translation with test cases and readability guidance (60.25% and 0.539) or refinement of the translation task itself (77.95% and 0.572).},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {46–53},
numpages = {8},
keywords = {code translation, low resource programming languages, large language models, code readability, self-refinement},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1109/ICSE48619.2023.00015,
author = {Gao, Shuzheng and Zhang, Hongyu and Gao, Cuiyun and Wang, Chaozheng},
title = {Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00015},
doi = {10.1109/ICSE48619.2023.00015},
abstract = {Previous research on code intelligence usually trains a deep learning model on a fixed dataset in an offline manner. However, in real-world scenarios, new code repositories emerge incessantly, and the carried new knowledge is beneficial for providing up-to-date code intelligence services to developers. In this paper, we aim at the following problem: How to enable code intelligence models to continually learn from ever-increasing data? One major challenge here is catastrophic forgetting, meaning that the model can easily forget knowledge learned from previous datasets when learning from the new dataset. To tackle this challenge, we propose REPEAT, a novel method for continual learning of code intelligence models. Specifically, REPEAT addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization. The representative exemplars replay component selects informative and diverse exemplars in each dataset and uses them to retrain model periodically. The adaptive parameter regularization component recognizes important parameters in the model and adaptively penalizes their changes to preserve the knowledge learned before. We evaluate the proposed approach on three code intelligence tasks including code summarization, software vulnerability detection, and code clone detection. Extensive experiments demonstrate that REPEAT consistently outperforms baseline methods on all tasks. For example, REPEAT improves the conventional fine-tuning method by 1.22, 5.61, and 1.72 on code summarization, vulnerability detection and clone detection, respectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {30–42},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643788.3648013,
author = {Yadav, Anurag Swarnim and Wilson, Joseph N.},
title = {BOSS: A dataset to train ML-based systems to repair programs with out-of-bounds write flaws},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648013},
doi = {10.1145/3643788.3648013},
abstract = {C and C++ are widely-used, mature programming languages. They have been extensively used in development of projects such as Linux, Windows, YouTube, Adobe, Firefox, and Google Chrome. Due to poor memory safety, C and C++ programs are vulnerable to security attacks as are programs in languages that depend on C/C++ library code. As per the Common Weakness Enumeration (CWE), out-of-bounds (OOB) write in C/C++ programs topped the list of the 25 most dangerous software weakness in 2021 and 2022. Fixing OOB write at the source code level still requires human experts. This is a tedious task that may result in erroneous programs. In this paper we propose a technique to create a data set of corresponding flawed and correct programs that can be used to perform supervised training of deep-learning models to automate the process of detecting and patching OOB writes. The proposed technique has two elements: collecting a set of C/C++ programs from online sources (correct programs) and injecting OOB write errors into them, thus yielding a set of corresponding flawed programs. In this paper we focus on four main flaws associated with OOB writes: faulty access, faulty declaration, faulty guard in loops, and faulty usage of memory-write APIs. We have found that popular fault localization tools can not localize complicated bugs in our buffer overflow sample set (BOSS). In addition, the current state-of-the-art machine learning security flaw repair tool could not repair any of the bugs in a randomly selected set of BOSS samples and, in some cases, generated out-of-bound writes as suggested patches. These results lead us to conclude that the bugs injected by our tool are significant and our dataset is useful for training neural program repair models. We also propose two data-augmentation techniques to overcome problems associated with limited-size corpora.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {26–33},
numpages = {8},
keywords = {C/C++, buffer-overwrite, software vulnerabilities, machine-learning based program repair dataset},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3639478.3643122,
author = {Corso, Vincenzo and Mariani, Leonardo and Micucci, Daniela and Riganelli, Oliviero},
title = {Assessing AI-Based Code Assistants in Method Generation Tasks},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643122},
doi = {10.1145/3639478.3643122},
abstract = {AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality. This study compares four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method generation tasks, assessing their ability to produce accurate, correct, and efficient code. Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {380–381},
numpages = {2},
keywords = {AI-based code assistants, code completion, empirical study},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639476.3639770,
author = {Maninger, Daniel and Narasimhan, Krishna and Mezini, Mira},
title = {Towards Trustworthy AI Software Development Assistance},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639770},
doi = {10.1145/3639476.3639770},
abstract = {It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {112–116},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3639477.3639753,
author = {Srinivas, Pooja and Husain, Fiza and Parayil, Anjaly and Choure, Ayush and Bansal, Chetan and Rajmohan, Saravan},
title = {Intelligent Monitoring Framework for Cloud Services: A Data-Driven Approach},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639753},
doi = {10.1145/3639477.3639753},
abstract = {Cloud service owners need to continuously monitor their services to ensure high availability and reliability. Gaps in monitoring can lead to delay in incident detection and significant negative customer impact. Current process of monitor creation is ad-hoc and reactive in nature. Developers create monitors using their tribal knowledge and, primarily, a trial and error based process. As a result, monitors often have incomplete coverage which leads to production issues, or, redundancy which results in noise and wasted effort.In this work, we address this issue by proposing an intelligent monitoring framework that recommends monitors for cloud services based on their service properties. We start by mining the attributes of 30,000+ monitors from 791 production services at Microsoft and derive a structured ontology for monitors. We focus on two crucial dimensions: what to monitor (resources) and which metrics to monitor. We conduct an extensive empirical study and derive key insights on the major classes of monitors employed by cloud services at Microsoft, their associated dimensions, and the interrelationship between service properties and this ontology. Using these insights, we propose a deep learning based framework that recommends monitors based on the service properties. Finally, we conduct a user study with engineers from Microsoft which demonstrates the usefulness of the proposed framework. The proposed framework along with the ontology driven projections, succeeded in creating production quality recommendations for majority of resource classes. This was also validated by the users from the study who rated the framework's usefulness as 4.27 out of 5.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {381–391},
numpages = {11},
keywords = {cloud services, reliability, intelligent monitoring},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639474.3640084,
author = {Sa\u{g}lam, Timur and Hahner, Sebastian and Schmid, Larissa and Burger, Erik},
title = {Automated Detection of AI-Obfuscated Plagiarism in Modeling Assignments},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640084},
doi = {10.1145/3639474.3640084},
abstract = {Plagiarism is a widespread problem in computer science education, exacerbated by the impracticability of manual inspection in large courses. Even worse, tools based on large language models like ChatGPT have made it easier than ever to obfuscate plagiarized solutions. Additionally, most plagiarism detectors only apply to code, and only a few approaches exist for modeling assignments, which lack broad resilience to obfuscation attacks. This paper presents a novel approach for automated plagiarism detection in modeling assignments that combines automated analysis with human inspection. We evaluate our approach with real-world assignments and plagiarism obfuscated by ChatGPT. Our results show that we achieve a significantly higher detection rate for AI-generated attacks and a broader resilience than the state-of-the-art.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {297–308},
numpages = {12},
keywords = {plagiarism detection, obfuscation, ChatGPT, artificial intelligence},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@proceedings{10.1145/3639474,
title = {ICSE-SEET '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639474.3640061,
author = {Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth},
title = {AI-Tutoring in Software Engineering Education},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640061},
doi = {10.1145/3639474.3640061},
abstract = {With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {309–319},
numpages = {11},
keywords = {programming education, automated programming assessment systems, artificial intelligence, ChatGPT, OpenAI, ChatBots},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1109/ICSE48619.2023.00126,
author = {Zhu, Qihao and Sun, Zeyu and Zhang, Wenjie and Xiong, Yingfei and Zhang, Lu},
title = {Tare: Type-Aware Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00126},
doi = {10.1109/ICSE48619.2023.00126},
abstract = {Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules.In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar.The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1443–1445},
numpages = {3},
keywords = {program repair, neural networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00093,
author = {Imtiaz, Sayem Mohammad and Batole, Fraol and Singh, Astha and Pan, Rangeet and Cruz, Breno Dantas and Rajan, Hridesh},
title = {Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00093},
doi = {10.1109/ICSE48619.2023.00093},
abstract = {Can we take a recurrent neural network (RNN) trained to translate between languages and augment it to support a new natural language without retraining the model from scratch? Can we fix the faulty behavior of the RNN by replacing portions associated with the faulty behavior? Recent works on decomposing a fully connected neural network (FCNN) and convolutional neural network (CNN) into modules have shown the value of engineering deep models in this manner, which is standard in traditional SE but foreign for deep learning models. However, prior works focus on the image-based multi-class classification problems and cannot be applied to RNN due to (a) different layer structures, (b) loop structures, (c) different types of input-output architectures, and (d) usage of both nonlinear and logistic activation functions. In this work, we propose the first approach to decompose an RNN into modules. We study different types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN modules can be reused and replaced in various scenarios. We evaluate our approach against 5 canonical datasets (i.e., Math QA, Brown Corpus, Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset. We found that decomposing a trained model has a small cost (Accuracy: -0.6%, BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced without needing to retrain.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1020–1032},
numpages = {13},
keywords = {recurrent neural networks, decomposing, modules, modularity},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00078,
author = {Li, Zhenhao and Luo, Chuan and Chen, Tse-Hsun (Peter) and Shang, Weiyi and He, Shilin and Lin, Qingwei and Zhang, Dongmei},
title = {Did We Miss Something Important? Studying and Exploring Variable-Aware Log Abstraction},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00078},
doi = {10.1109/ICSE48619.2023.00078},
abstract = {Due to the sheer size of software logs, developers rely on automated techniques for log analysis. One of the first and most important steps of automated log analysis is log abstraction, which parses the raw logs into a structured format. Prior log abstraction techniques aim to identify and abstract all the dynamic variables in logs and output a static log template for automated log analysis. However, these abstracted dynamic variables may also contain important information that is useful to different tasks in log analysis. In this paper, we investigate the characteristics of dynamic variables and their importance in practice, and explore the potential of a variable-aware log abstraction technique. Through manual investigations and surveys with practitioners, we find that different categories of dynamic variables record various information that can be important depending on the given tasks, the distinction of dynamic variables in log abstraction can further assist in log analysis. We then propose a deep learning based log abstraction approach, named VALB, which can identify different categories of dynamic variables and preserve the value of specified categories of dynamic variables along with the log templates (i.e., variable-aware log abstraction). Through the evaluation on a widely used log abstraction benchmark, we find that VALB outperforms other state-of-the-art log abstraction techniques on general log abstraction (i.e., when abstracting all the dynamic variables) and also achieves a high variable-aware log abstraction accuracy that further identifies the category of the dynamic variables. Our study highlights the potential of leveraging the important information recorded in the dynamic variables to further improve the process of log analysis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {830–842},
numpages = {13},
keywords = {software logs, log abstraction, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00139,
author = {Koscinski, Viktoria and Hashemi, Sara and Mirakhorli, Mehdi},
title = {On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00139},
doi = {10.1109/ICSE48619.2023.00139},
abstract = {Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1609–1621},
numpages = {13},
keywords = {software security requirements, requirements engineering, generative adversarial networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643788.3648021,
author = {Lajko, Mark and Csuvik, Viktor and Gyimothy, Tibor and Vidacs, Laszlo},
title = {Automated Program Repair with the GPT Family, including GPT-2, GPT-3 and CodeX},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648021},
doi = {10.1145/3643788.3648021},
abstract = {Automated Program Repair (APR) is a promising approach for addressing software defects and improving software reliability. There are various approaches to APR, including using Machine Learning (ML) techniques such as neural networks and evolutionary algorithms, as well as more traditional methods such as static analysis and symbolic execution. In recent years, there has been growing interest in using ML techniques for APR, including the use of large language models such as GPT-2 and GPT-3. These models have the ability to generate human-like text and code, making them well-suited for tasks such as generating repair patches for defective programs. In this paper, we explore the use of the GPT family (including GPT-2, GPT-J-6B, GPT-3 and Codex) for APR of JavaScript programs and evaluate their performance in terms of the number and quality of repair patches generated. Our results show that these state-of-the-art language models are able to generate repair patches that successfully fix the defects in the JavaScript programs, with Codex performing slightly better overall. To be precise, in our self-assembled dataset, Codex was able to generate 108 repair patches that are exactly the same as the developer fix for the first try. If we consider multiple patch generations, up to 201 buggy programs are being repaired automatically from the 1559 evaluation dataset (12.89%).},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {34–41},
numpages = {8},
keywords = {automated program repair, transformers, GPT-3, codex, JavaScript},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00077,
author = {Ding, Zishuo},
title = {Towards Utilizing Natural Language Processing Techniques to Assist in Software Engineering Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00077},
doi = {10.1109/ICSE-Companion58688.2023.00077},
abstract = {Machine learning-based approaches have been widely used to address natural language processing (NLP) problems. Considering the similarity between natural language text and source code, researchers have been working on applying techniques from NLP to deal with code. On the other hand, source code and natural language are by nature different. For example, code is highly structured and executable. Thus, directly applying the NLP techniques may not be optimal, and how to effectively optimize these NLP techniques to adapt to software engineering (SE) tasks remains a challenge. Therefore, to tackle the challenge, in this dissertation, we focus on two research directions: 1) distributed code representations, and 2) logging statements, which are two important intersections between the natural language and source code. For distributed code representations, we first discuss the limitations of existing code embedding techniques, and then, we propose a novel approach to learn more generalizable code embeddings in a task-agnostic manner. For logging statements, we first propose an automated deep learning-based approach to automatically generate accurate logging texts by translating the related source code into short textual descriptions. Then, we make the first attempt to comprehensively study the temporal relations between logging and its corresponding source code, which is later used to detect issues in logging statements. We anticipate that our study can provide useful suggestions and support to developers in utilizing NLP techniques to assist in SE tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {286–290},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00173,
author = {Yan, Cong and Nath, Suman and Lu, Shan},
title = {Generating Test Databases for Database-Backed Applications},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00173},
doi = {10.1109/ICSE48619.2023.00173},
abstract = {Database-backed applications are widely used. To effectively test these applications, one needs to design not only user inputs but also database states, which imposes unique challenges. First, valid database states have to satisfy complicated constraints determined by application semantics, and hence are difficult to synthesize. Second, the state space of a database is huge, as an application can contain tens to hundreds of tables with up to tens of fields per table. Making things worse, each test involving database operations takes significant time to run. Consequently, unhelpful database states and running tests on them can severely waste testing resources.We propose DBGriller, a tool that generates database states to facilitate thorough testing of database-backed applications. To effectively generate valid database states, DBGriller strategically injects minor mutation into existing database states and transforms part of the application-under-test into a stand-alone validity checker. To tackle the huge database state space and save testing time, DBGriller uses program analysis to identify a novel branch-projected DB view that can be used to filter out database states that are unlikely to increase the testing branch coverage. Our evaluation on 9 popular open-source database applications shows that DBGriller can effectively increase branch coverage of existing tests and expose previously unknown bugs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2048–2059},
numpages = {12},
keywords = {automated testing, test data generation, database-backed application, database-state generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644032.3644450,
author = {G\"{o}tharsson, Malte and Stahre, Karl and Gay, Gregory and de Oliveira Neto, Francisco Gomes},
title = {Exploring the Role of Automation in Duplicate Bug Report Detection: An Industrial Case Study},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644450},
doi = {10.1145/3644032.3644450},
abstract = {Duplicate bug reports can increase technical debt and tester work-load in long-running software projects. Many automated techniques have been proposed to detect potential duplicate reports. However, such techniques have not seen widespread industrial adoption. Our objective in this study is to better understand how automated techniques could effectively be employed within a tester's duplicate detection workflow. We are particularly interested in exploring the potential of a human-in-the-loop scenario where tools and humans work together to make duplicate determinations.We have conducted an industrial case study where we characterize the current tester workflow. Based on this characterization, we have developed Bugle---an automated technique based on a complex language model that suggests potential duplicates to testers based on an input bug description that can be freely reformulated if the initial suggestions are irrelevant. We compare the assessments of Bugle and testers of varying experience, capturing how often---and why---opinions might differ between the two, and comparing the strengths and limitations of automated techniques to the current tester workflow. We additionally examine the influence of knowledge and biases on accuracy, the suitability of language models, and the limitations affecting duplicate detection techniques.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {193–203},
numpages = {11},
keywords = {bug reports, duplicate bug reports, automated duplicate bug report detection, natural language processing, software testing},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3639475.3640112,
author = {Sovrano, Francesco and Lognoul, Micha\"{e}l and Bacchelli, Alberto},
title = {An Empirical Study on Compliance with Ranking Transparency in the Software Documentation of EU Online Platforms},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640112},
doi = {10.1145/3639475.3640112},
abstract = {Compliance with the European Union's Platform-to-Business (P2B) Regulation helps fostering a fair, ethical and secure online environment. However, it is challenging for online platforms, and assessing their compliance can be difficult for public authorities. This is partly due to the lack of automated tools for assessing the information (e.g., software documentation) platforms provide concerning ranking transparency. Our study tackles this issue in two ways. First, we empirically evaluate the compliance of six major platforms (Amazon, Bing, Booking, Google, Tripadvisor, and Yahoo), revealing substantial differences in their documentation. Second, we introduce and test automated compliance assessment tools based on ChatGPT and information retrieval technology. These tools are evaluated against human judgments, showing promising results as reliable proxies for compliance assessments. Our findings could help enhance regulatory compliance and align with the United Nations Sustainable Development Goal 10.3, which seeks to reduce inequality, including business disparities, on these platforms.Data and materials: https://doi.org/10.5281/zenodo.10478546.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {46–56},
numpages = {11},
keywords = {software documentation, EU regulations, compliance assessment, ranking transparency, explainability, online platforms},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@inproceedings{10.1145/3639474.3640080,
author = {Galster, Matthias and Mitrovic, Antonija and Malinen, Sanna and Iyer, Sreedevi Sankara and Musa, Ja'afaru and Holland, Jay},
title = {Video-based Training for Meeting Communication Skills},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640080},
doi = {10.1145/3639474.3640080},
abstract = {Background: Discussing and sharing information in development teams is part of any software project. Therefore, software engineers spend significant time in meetings with their team. Communicating effectively and efficiently in those meetings is essential. However, software engineers often do not possess the right skills. On the other hand, training face-to-face meeting communication skills in university settings is resource- and time-consuming. Aims: Our goal is to develop and evaluate a method to support the training of face-to-face meeting communication skills. Method: We develop a method based on active video-watching. Active video-watching supports deep learning by systematically engaging students with video-based learning material. We also implement this method in an online platform for classroom use. Furthermore, we empirically develop a new measurement instrument to assess face-to-face meeting communication skills. To evaluate the training method, we used it in three instances of a second-year software engineering project course. To assess learning gain, we assessed (a) the conceptual knowledge about face-to-face meeting communication, and (b) skills based on our newly developed measurement instrument, both before and after the training. Results: Both conceptual knowledge as well as skill measurement scores based on our instrument increased. Increases are statistically significant. Conclusions: We show the effectiveness of active video-watching for training face-to-face meeting communication skills, one specific soft skill relevant for software engineers. The measurement instrument that we developed can also be used as a stand-alone tool to assess skills of students and potentially practitioners.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {170–179},
numpages = {10},
keywords = {software engineering, soft skills, meeting communication, face-to-face communication, video-based training},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1109/ICSE48619.2023.00217,
author = {Christian, Garrett and Woodlief, Trey and Elbaum, Sebastian},
title = {Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00217},
doi = {10.1109/ICSE48619.2023.00217},
abstract = {Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2604–2616},
numpages = {13},
keywords = {software testing and validation, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00191,
author = {Wen, Xin-Cheng and Chen, Yupan and Gao, Cuiyun and Zhang, Hongyu and Zhang, Jie M. and Liao, Qing},
title = {Vulnerability Detection with Graph Simplification and Enhanced Graph Representation Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00191},
doi = {10.1109/ICSE48619.2023.00191},
abstract = {Prior studies have demonstrated the effectiveness of Deep Learning (DL) in automated software vulnerability detection. Graph Neural Networks (GNNs) have proven effective in learning the graph representations of source code and are commonly adopted by existing DL-based vulnerability detection methods. However, the existing methods are still limited by the fact that GNNs are essentially difficult to handle the connections between long-distance nodes in a code structure graph. Besides, they do not well exploit the multiple types of edges in a code structure graph (such as edges representing data flow and control flow). Consequently, despite achieving state-of-the-art performance, the existing GNN-based methods tend to fail to capture global information (i.e., long-range dependencies among nodes) of code graphs.To mitigate these issues, in this paper, we propose a novel vulnerability detection framework with grAph siMplification and enhanced graph rePresentation LEarning, named AMPLE. AMPLE mainly contains two parts: 1) graph simplification, which aims at reducing the distances between nodes by shrinking the node sizes of code structure graphs; 2) enhanced graph representation learning, which involves one edge-aware graph convolutional network module for fusing heterogeneous edge information into node representations and one kernel-scaled representation module for well capturing the relations between distant graph nodes. Experiments on three public benchmark datasets show that AMPLE outperforms the state-of-the-art methods by 0.39%-35.32% and 7.64%-199.81% with respect to the accuracy and F1 score metrics, respectively. The results demonstrate the effectiveness of AMPLE in learning global information of code graphs for vulnerability detection.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2275–2286},
numpages = {12},
keywords = {software vulnerability, graph simplification, graph representation learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3643691,
title = {RAIE '24: Proceedings of the 2nd International Workshop on Responsible AI Engineering},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {RAIE'24 provides a platform for researchers, innovators, and industry leaders to exchange insights on the current and future landscape of responsible AI engineering. The workshop aims to foster interdisciplinary collaboration, bringing together professionals from fields like software engineering, AI, and social science. Our goal is to address the comprehensive challenges of developing AI systems responsibly and to inspire an increasing number of researchers to contribute to this vital field.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3608134,
author = {Geng, Mingyang and Wang, Shangwen and Dong, Dezun and Wang, Haotian and Li, Ge and Jin, Zhi and Mao, Xiaoguang and Liao, Xiangke},
title = {Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608134},
doi = {10.1145/3597503.3608134},
abstract = {Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {39},
numpages = {13},
keywords = {code summarization, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00029,
author = {Zeng, Zhengran and Zhang, Yuqun and Xu, Yong and Ma, Minghua and Qiao, Bo and Zou, Wentao and Chen, Qingjun and Zhang, Meng and Zhang, Xu and Zhang, Hongyu and Gao, Xuedong and Fan, Hao and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {TraceArk: Towards Actionable Performance Anomaly Alerting for Online Service Systems},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00029},
doi = {10.1109/ICSE-SEIP58684.2023.00029},
abstract = {Performance anomaly alerting based on trace data plays an important role in assuring the quality of online service systems. However, engineers find that many anomalies reported by existing techniques are not of interest for them to take further actions. For a large scale online service with hundreds of different microservices, current methods either fire lots of false alarms by applying simple thresholds to temporal metrics (i.e., latency), or run complex end-to-end deep learning model with limited interpretability. Engineers often feel difficult to understand why anomalies are reported, which hinders the follow-up actions. In this paper, we propose an actionable anomaly alerting approach TraceArk. More specifically, we design an anomaly evaluation model by extracting service impact related anomalous features. A small amount of engineer experience (i.e., feedback) is also incorporated to learn the actionable anomaly alerting model. Comprehensive experiments on a real dataset of Microsoft Exchange service and an anomaly injection dataset collected from an open-source project demonstrate that TraceArk significantly outperforms the existing state-of-the-art approaches. The improvement in F1 is 50.47% and 20.34% on the two datasets, respectively. Furthermore, TraceArk has been running stably for four months in a real production environment and showing a 2.3x improvement in Precision over the previous approach. TraceArk also provides intrepretable alerting details for engineers to take further actions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {258–269},
numpages = {12},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00053,
author = {Tufano, Rosalia},
title = {Automating Code Review},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00053},
doi = {10.1109/ICSE-Companion58688.2023.00053},
abstract = {Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, code review comes at the cost of spending developers' time on reviewing their teammates' code. The goal of this research is to investigate the possibility of using Deep Learning (DL) to automate specific code review tasks. We started by training vanilla Transformer models to learn code changes performed by developers during real code review activities. This gives the models the possibility to automatically (i) revise the code submitted for review without any input from the reviewer; and (ii) implement changes required to address a specific reviewer's comment. While the preliminary results were encouraging, in this first work we tested DL models in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. Thus, in a subsequent work, we exploited a pre-trained Text-To-Text-Transfer-Transformer (T5) to overcome some of these limitations and experiment DL models for code review automation in more realistic and challenging scenarios. The achieved results show the improvements brought by T5 both in terms of applicability (i.e., scenarios in which it can be applied) and performance. Despite this, we are still far from performance levels making these techniques deployable in practice, thus calling for additional research in this area, as we discuss in our future work agenda.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {192–196},
numpages = {5},
keywords = {code review, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639114,
author = {Feng, Siyue and Suo, Wenqi and Wu, Yueming and Zou, Deqing and Liu, Yang and Jin, Hai},
title = {Machine Learning is All You Need: A Simple Token-based Approach for Effective Code Clone Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639114},
doi = {10.1145/3597503.3639114},
abstract = {As software engineering advances and the code demand rises, the prevalence of code clones has increased. This phenomenon poses risks like vulnerability propagation, underscoring the growing importance of code clone detection techniques. While numerous code clone detection methods have been proposed, they often fall short in real-world code environments. They either struggle to identify code clones effectively or demand substantial time and computational resources to handle complex clones. This paper introduces a code clone detection method namely Toma using tokens and machine learning. Specifically, we extract token type sequences and employ six similarity calculation methods to generate feature vectors. These vectors are then input into a trained machine learning model for classification. To evaluate the effectiveness and scalability of Toma, we conduct experiments on the widely used BigCloneBench dataset. Results show that our tool outperforms token-based code clone detectors and most tree-based clone detectors, demonstrating high effectiveness and significant time savings.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {222},
numpages = {13},
keywords = {code clones, machine learning, token},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643660.3643949,
author = {Garlan, David and Schmerl, Bradley and Wohlrab, Rebekka and C\'{a}mara, Javier},
title = {Challenges in Creating Effective Automated Design Environments: An experience report from the domain of generative manufacturing},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643949},
doi = {10.1145/3643660.3643949},
abstract = {The emergence of powerful automated design tools in many domains is changing the nature of design, as human-intensive activities can be increasingly off-loaded to those tools. Rather than having a human consider only handful of options, as has been done historically, such tools now enable the generation of a large space of potential designs, exhibiting different tradeoffs among competing qualities of merit, and supporting systematic exploration of the design space. At the same time, this paradigm raises new challenges centered on enabling humans to effectively navigate that generated space in order to select a design that best meets their requirements. In this paper we describe our experience in the domain of generative manufacturing, in which we developed a novel design environment for airplane parts manufacturing that incorporates a number of sophisticated design tools and attempts to tackle the emergent problems of design space exploration that are faced by designers of those parts. We use this experience to highlight the challenges that we faced and reflect on their applicability more generally to tool-assisted software design environments.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {15–20},
numpages = {6},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3526072.3527522,
author = {Peltom\"{a}ki, Jarkko and Spencer, Frankie and Porres, Ivan},
title = {Wasserstein generative adversarial networks for online test generation for cyber physical systems},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527522},
doi = {10.1145/3526072.3527522},
abstract = {We propose a novel online test generation algorithm WOGAN based on Wasserstein Generative Adversarial Networks. WOGAN is a general-purpose black-box test generator applicable to any system under test having a fitness function for determining failing tests. As a proof of concept, we evaluate WOGAN by generating roads such that a lane assistance system of a car fails to stay on the designated lane. We find that our algorithm has a competitive performance respect to previously published algorithms.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {1–5},
numpages = {5},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@proceedings{10.1145/3643787,
title = {NLBSE '24: Proceedings of the Third ACM/IEEE International Workshop on NL-based Software Engineering},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Natural Language Processing (NLP) refers to the automated elaboration of human language, including both algorithms that take human-produced text as input and algorithms that produce natural-looking text as outputs. NLP is widely used to optimize many aspects of the software development process. Since natural language artifacts are used and reused during the software development life-cycle, the availability of natural language-based approaches and tools has led to improvements in the software process and product efficiency. Indeed, NLP approaches (including LLMs) have proven useful for retrieving key information from a wide range of structured or unstructured sources. Besides, they show promise for the automated generation of fine-grained source code documentation to ease program comprehension and maintenance activities. Literature has shown that many software engineering (SE)-related tasks can benefit from adopting NLP techniques. The main objective of the Natural Language-Based Software Engineering Workshop (NLBSE) is to bring together researchers and industrial practitioners from the NLP and SE communities to share experiences. Our workshop aims to provide directions for future research and encourage the development of increasingly effective NLP solutions for addressing SE-specific challenges.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639074,
author = {Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, Donggyun and Lo, David},
title = {Unveiling Memorization in Code Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639074},
doi = {10.1145/3597503.3639074},
abstract = {The availability of large-scale datasets, advanced architectures, and powerful computational resources have led to effective code models that automate diverse software engineering activities. The datasets usually consist of billions of lines of code from both open-source and private repositories. A code model memorizes and produces source code verbatim, which potentially contains vulnerabilities, sensitive information, or code with strict licenses, leading to potential security and privacy issues.This paper investigates an important problem: to what extent do code models memorize their training data? We conduct an empirical study to explore memorization in large pre-trained code models. Our study highlights that simply extracting 20,000 outputs (each having 512 tokens) from a code model can produce over 40,125 code snippets that are memorized from the training data. To provide a better understanding, we build a taxonomy of memorized contents with 3 categories and 14 subcategories. The results show that the prompts sent to the code models affect the distribution of memorized contents. We identify several key factors of memorization. Specifically, given the same architecture, larger models suffer more from memorization problem. A code model produces more memorization when it is allowed to generate longer outputs. We also find a strong positive correlation between the number of an output's occurrences in the training data and that in the generated outputs, which indicates that a potential way to reduce memorization is to remove duplicates in the training data. We then identify effective metrics that infer whether an output contains memorization accurately. We also make suggestions to deal with memorization.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {72},
numpages = {13},
keywords = {open-source software, memorization, code generation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639787,
author = {Katzy, Jonathan},
title = {Programming Language Models in Multilingual Settings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639787},
doi = {10.1145/3639478.3639787},
abstract = {Large language models have become increasingly utilized in programming contexts. However, due to the recent emergence of this trend, some aspects have been overlooked. We propose a research approach that investigates the inner mechanics of transformer networks, on a neuron, layer, and output representation level, to understand whether there is a theoretical limitation that prevents large language models from performing optimally in a multilingual setting. We propose to approach the investigation into the theoretical limitations, by addressing open problems in machine learning for the software engineering community. This will contribute to a greater understanding of large language models for programming-related tasks, making the findings more approachable to practitioners, and simply their implementation in future models.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {204–206},
numpages = {3},
keywords = {large language models, explainable AI, software engineering, code completion, multilingual, programming languages},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639474.3640051,
author = {van den Aker, Eddy and Rahimi, Ebrahim},
title = {Design principles for generating and presenting automated formative feedback on code quality using software metrics},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640051},
doi = {10.1145/3639474.3640051},
abstract = {Code quality and maintainability are among under-emphasized and often neglected topics in the curriculum of software engineering (SE) in higher education. This neglect tends to overlook research findings that demonstrate SE students' programming submissions most often exhibit severe code quality issues, which are frequently left unaddressed by the students. Furthermore, it can result in the software engineering curriculum becoming indifferent to the essential requirements of the software development industry, where code quality and maintainability play a crucial role in the software's cost throughout its life cycle.Therefore, SE students in higher education should be trained to master the knowledge and skills of writing high-quality code. One possible approach to improving students' understanding of code quality issues is to provide automatically generated formative feedback about the code quality aspects of their programming submissions throughout the code development process. However, while there are tools available for generating automated feedback on the code quality aspects of programming submissions, they often lack a set of theory-driven design principles to underpin the content and presentation of their provided feedback. This lack of theoretical foundation makes it difficult to follow a systematic approach to designing and developing such tools, reasoning about their quality, and evaluating the effectiveness of their generated feedback.To address this lack, this study provides nine contextualized design principles for generating automated formative feedback on code quality. These design principles are rooted in solid educational constructs about feedback and learning dashboards, and empirically validated and contextualized by two focus group sessions consisting of 8 senior SE students and 2 teachers.This approach has resulted in a set of contextualized design principles. These design principles can be used to guide the implementation of tools that provide automated feedback on code quality using software metrics.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {139–150},
numpages = {12},
keywords = {programming education, undergraduate education, design principles, code quality, formative feedback, automated feedback},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3643795.3648390,
author = {Jiang, Shengbei and Zhang, Jiabao and Chen, Wei and Wang, Bo and Zhou, Jianyi and Zhang, Jie},
title = {Evaluating Fault Localization and Program Repair Capabilities of Existing Closed-Source General-Purpose LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648390},
doi = {10.1145/3643795.3648390},
abstract = {Automated debugging is an emerging research field that aims to automatically find and repair bugs. In this field, Fault Localization (FL) and Automated Program Repair (APR) gain the most research efforts. Most recently, researchers have adopted pre-trained Large Language Models (LLMs) to facilitate FL and APR and their results are promising. However, the LLMs they used either vanished (such as Codex) or outdated (such as early versions of GPT). In this paper, we evaluate the performance of recent commercial closed-source general-purpose LLMs on FL and APR, i.e., ChatGPT 3.5, ERNIE Bot 3.5, and IFlytek Spark 2.0. We select three popular LLMs and evaluate them on 120 real-world Java bugs from the benchmark Defects4J. For FL and APR, we designed three kinds of prompts for each, considering different kinds of information. The results show that these LLMs could successfully locate 53.3% and correctly fix 12.5% of these bugs.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {75–78},
numpages = {4},
keywords = {large language model, fault localization, program repair, software debugging},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1109/ICSE48619.2023.00205,
author = {Nashid, Noor and Sintaha, Mifta and Mesbah, Ali},
title = {Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00205},
doi = {10.1109/ICSE48619.2023.00205},
abstract = {Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, CEDAR, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare CEDAR with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, CEDAR outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, CEDAR yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as CEDAR could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2450–2462},
numpages = {13},
keywords = {large language models, transformers, few-shot learning, program repair, test assertion generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639135,
author = {Yang, Yanming and Hu, Xing and Xia, Xin and Lo, David and Yang, Xiaohu},
title = {Streamlining Java Programming: Uncovering Well-Formed Idioms with IdioMine},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639135},
doi = {10.1145/3597503.3639135},
abstract = {Code idioms are commonly used patterns, techniques, or practices that aid in solving particular problems or specific tasks across multiple software projects. They can improve code quality, performance, and maintainability, and also promote program standardization and reuse across projects. However, identifying code idioms is significantly challenging, as existing studies have still suffered from three main limitations. First, it is difficult to recognize idioms that span non-contiguous code lines. Second, identifying idioms with intricate data flow and code structures can be challenging. Moreover, they only extract dataset-specific idioms, so common idioms or well-established code/design patterns that are rarely found in datasets cannot be identified.To overcome these limitations, we propose a novel approach, named IdioMine, to automatically extract generic and specific idioms from both Java projects and libraries. We perform program analysis on Java functions to transform them into concise PDGs, for integrating the data flow and control flow of code fragments. We then develop a novel chain structure, Data-driven Control Chain (DCC), to extract sub-idioms that possess contiguous semantic meanings from PDGs. After that, we utilize GraphCodeBERT to generate code embeddings of these sub-idioms and perform density-based clustering to obtain frequent sub-idioms. We use heuristic rules to identify interrelated sub-idioms among the frequent ones. Finally, we employ ChatGPT to synthesize interrelated sub-idioms into potential code idioms and infer real idioms from them.We conduct well-designed experiments and a user study to evaluate IdioMine's correctness and the practical value of the extracted idioms. Our experimental results show that IdioMine effectively extracts more idioms with better performance in most metrics. We compare our approach with Haggis and ChatGPT, IdioMine outperforms them by 22.8% and 35.5% in Idiom Set Precision (ISP) and by 9.7% and 22.9% in Idiom Coverage (IC) when extracting idioms from libraries. IdioMine also extracts almost twice the size of idioms than the baselines, exhibiting its ability to identify complete idioms. Our user study indicates that idioms extracted by IdioMine are well-formed and semantically clear. Moreover, we conduct a qualitative and quantitative analysis to investigate the primary functionalities of IdioMine's extracted idioms from various projects and libraries.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {230},
numpages = {12},
keywords = {code idiom mining, code pattern, large language model (LLM), clustering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643915.3644098,
author = {Song, Qunying and Anderberg, Rune and Olsson, Henrik and Runeson, Per},
title = {Generating Executable Test Scenarios from Autonomous Vehicle Disengagements using Natural Language Processing},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644098},
doi = {10.1145/3643915.3644098},
abstract = {With the emergence of autonomous vehicles comes requirements on adequate and rigorous testing techniques, particularly as systems continuously adapt to changing environments. Scenario-based, simulated testing is one approach that has received attention, where deriving relevant scenarios from various sources is still a challenge. We therefore explore creating executable test scenarios from textual disengagement reports, collected from autonomous vehicle test drives, by DMV California. We mined information from 183 182 disengagements, using NLP techniques and developed a tool to output scenarios in OpenScenario format. The data quality of the reports was substandard, resulting in only 36 disengagements be useful and half of the generated scenarios were correctly reconstructed. However, the NLP approach was effective and may be used for other data sets. Further work includes working with more and better data sources and advancing the scenario generation.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {98–104},
numpages = {7},
keywords = {testing, driving scenarios, scenario generation, autonomous vehicles, disengagement, natural language processing},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3643787.3648034,
author = {Imran, Mia Mohammad},
title = {Emotion Classification In Software Engineering Texts: A Comparative Analysis of Pre-trained Transformers Language Models},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648034},
doi = {10.1145/3643787.3648034},
abstract = {Emotion recognition in software engineering texts is critical for understanding developer expressions and improving collaboration. This paper presents a comparative analysis of state-of-the-art Pre-trained Language Models (PTMs) for fine-grained emotion classification on two benchmark datasets from GitHub and Stack Overflow. We evaluate six transformer models - BERT, RoBERTa, ALBERT, DeBERTa, CodeBERT and GraphCodeBERT against the current best-performing tool SEntiMoji. Our analysis reveals consistent improvements ranging from 1.17% to 16.79% in terms of macro-averaged and micro-averaged F1 scores, with general domain models outperforming specialized ones. To further enhance PTMs, we incorporate polarity features in attention layer during training, demonstrating additional average gains of 1.0% to 10.23% over baseline PTMs approaches. Our work provides strong evidence for the advancements afforded by PTMs in recognizing nuanced emotions like Anger, Love, Fear, Joy, Sadness, and Surprise in software engineering contexts. Through comprehensive benchmarking and error analysis, we also outline scope for improvements to address contextual gaps.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {73–80},
numpages = {8},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3643991.3644903,
author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole and Quaranta, Luigi},
title = {Leveraging GPT-like LLMs to Automate Issue Labeling},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644903},
doi = {10.1145/3643991.3644903},
abstract = {Issue labeling is a crucial task for the effective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the fine-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classification, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to fine-tune BERT-like LLMs.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {469–480},
numpages = {12},
keywords = {LLM, issue labeling, GPT, software maintenance and evolution, labeling unstructured data},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00152,
author = {Hu, Qiang and Guo, Yuejun and Xie, Xiaofei and Cordy, Maxime and Papadakis, Mike and Ma, Lei and Traon, Yves Le},
title = {Aries: Efficient Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00152},
doi = {10.1109/ICSE48619.2023.00152},
abstract = {Deep learning (DL) plays a more and more important role in our daily life due to its competitive performance in industrial application domains. As the core of DL-enabled systems, deep neural networks (DNNs) need to be carefully evaluated to ensure the produced models match the expected requirements. In practice, the de facto standard to assess the quality of DNNs in the industry is to check their performance (accuracy) on a collected set of labeled test data. However, preparing such labeled data is often not easy partly because of the huge labeling effort, i.e., data labeling is labor-intensive, especially with the massive new incoming unlabeled data every day. Recent studies show that test selection for DNN is a promising direction that tackles this issue by selecting minimal representative data to label and using these data to assess the model. However, it still requires human effort and cannot be automatic. In this paper, we propose a novel technique, named Aries, that can estimate the performance of DNNs on new unlabeled data using only the information obtained from the original test data. The key insight behind our technique is that the model should have similar prediction accuracy on the data which have similar distances to the decision boundary. We performed a large-scale evaluation of our technique on two famous datasets, CIFAR-10 and Tiny-ImageNet, four widely studied DNN models including ResNet101 and DenseNet121, and 13 types of data transformation methods. Results show that the estimated accuracy by Aries is only 0.03% -- 2.60% off the true accuracy. Besides, Aries also outperforms the state-of-the-art labeling-free methods in 50 out of 52 cases and selection-labeling-based methods in 96 out of 128 cases.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1776–1787},
numpages = {12},
keywords = {deep learning testing, performance estimation, distribution shift},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3643090,
author = {Xu, Jingwei and Zeng, Zihan},
title = {Programmable and Semantic Connector for DNN Component Integration: a Software Engineering Perspective},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643090},
doi = {10.1145/3639478.3643090},
abstract = {As deep learning technology continues to evolve, deep neural network (DNN) models have found their way into numerous modern software applications and systems, serving as crucial components. Despite the widespread adoption of DNN models in software, their development process still largely adheres to a craft production model [1]. This craft production approach leads to the creation of unique, highly specialized DNN models that may excel within their target software but prove difficult to standardize or adapt for compatibility with other software systems. In addition, due to the holistic training of these crafted DNN models, they cannot be easily disassembled or reassembled to accommodate new software requirements. Consequently, the reuse of DNN models remains a significant challenge in software engineering, hindering the potential for greater efficiency and adaptability in the development process.At present, the primary approach to reusing DNN models involves retraining them, either by fine-tuning or training from scratch, in the target domain. This retraining process necessitates the use of a new dataset and incurs substantial costs associated with training the model. Moreover, acquiring a new dataset entails additional data collection and labeling efforts, even when the target domain differs only marginally from the original domain. In some cases, it becomes essential to devise distinct DNN structures tailored to the data characteristics or specific software requirements. These factors highlight the craft production nature of DNN model development, lack of scalability, and adaptability.In conventional software engineering, software architecture [2] serves as a blueprint for complex software systems and development projects, as proposed and developed by software engineering researchers. This architectural perspective envisions software as a collection of computational components, connectors, and constraints [3], which dictate the interactions between these components. Incorporating DNN models into software architecture, the primary objective of DNN model integration is to establish components, connectors, and constraints for DNN model design. The architecture of DNN models is naturally constructed through multiple layers, functioning as a DNN component. However, directly stitching together different DNN components presents several challenges: 1) The generated output of each DNN component is difficult to comprehend. 2) directly establishing the connection between DNN components usually requires the expensive cost of DNN model retraining or fine-tuning. 3) the constraints currently present in DNN models are primarily structural but not explicitly semantic. Given that developed DNN components are not easily modified, our primary focus is on establishing connections between DNN components to alter the software's functionality without the need for retraining the DNN components. By doing this, the deep neural networks, functioning as components, could operate cohesively within the software system. As a result, any changes to software requirements would impact only the connections between components, eliminating the need for developers to retrain the models.In this paper, we propose a novel method NeuralNector to solve the problem of DNN component integration. In NeuralNector, we design a programmable semantic connector. The connector could 1) program a clear semantic output for the DNN component's raw output and 2) program a logical rule component meeting the semantic constraints to connect DNN components with the programmed outputs. As shown in Figure 1, by developing an easy-to-establish programmable semantic connector, an effective and adaptable DNN model integration can be achieved, allowing for seamless integration of DNN models into software systems without the need for DNN model retraining. The proposed NeuralNector significantly enhances the efficiency of software development involving deep learning models as integral components. We design comprehensive experiments to evaluate our DNN component integration approach. The evaluation primarily focuses on the classification of transportation and animals from the PASCAL VOC dataset. The concepts from PASCAL are listed in Table 1.Programmable Semantic Connector The training accuracy of each semantic concept extractor is listed in Table 1. Several selected concepts and their observations are depicted in Figure 2. The accuracy of the logical rule component reaches 97.6%, demonstrating that our concept setting is reasonable and that a logical relationship exists between the concept and the original label.DNN components integration We use three classic DNN structures to construct components MA and MB, and build the corresponding programmable semantic connector for each component pair. The results of each DNN structure are shown in Table 2. The results show that our approach can be used for DNN component integration without an excessive focus on the DNN architecture, indicating the compatibility of our approach.Data requirements and transferability of concepts To make the programmable semantic connector practical, we evaluate the performances of concept extractors trained on a smaller dataset (10% of the training data). The results are listed in Table 3. Even with only one-tenth of the original data available, the average accuracy is not significantly affected (93.9% down to 92.0%). We also evaluate the transferability of the concept extractors. The process of this experiment is illustrated in Figure 3. The accuracy of the DNN component MC is 94%. After integrating it with MB, which has a training accuracy of 97.7%, the accuracy for the 12 categories reaches 85.8%. This result indicates that the representations from the DNN component effectively extract common and dataset-independent semantic information from the samples, and the proposed method capitalizes on this advantage to exhibit transferability.In summary, we presented a novel approach for integrating DNN components via a programmable semantic connector. The extensive evaluation demonstrated the effectiveness and compatibility of our approach across various datasets, DNN architectures, and practical scenarios. The semantic concept extractors can be programmed with limited data and possess strong transferability to other DNN components. To this end, our approach shows new possibilities for efficient model integration and adaptation from a software engineering perspective, pushing the development of DNN components toward a mass production paradigm.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {294–295},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644403,
author = {Ma, Zexiong and An, Shengnan and Xie, Bing and Lin, Zeqi},
title = {Compositional API Recommendation for Library-Oriented Code Generation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644403},
doi = {10.1145/3643916.3644403},
abstract = {Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task.To address this, we propose CAPIR (Compositional API Recommendation), which adopts a "divide-and-conquer" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation.To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {87–98},
numpages = {12},
keywords = {API recommendation, code generation, requirements decomposition, large language model},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639478.3639812,
author = {Mahmud, Junayed},
title = {Toward Rapid Bug Resolution for Android Apps},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639812},
doi = {10.1145/3639478.3639812},
abstract = {Bug reports document unexpected behaviors in software, enabling developers to understand, validate, and fix bugs. Unfortunately, a significant portion of bug reports is of low quality, which poses challenges for developers in terms of addressing these issues. Prior research has delved into the information needed for documenting high-quality bug reports and expediting bug report management. Furthermore, researchers have explored the challenges associated with bug report management and proposed various automated techniques. Nevertheless, these techniques exhibit several limitations, including a lexical gap between developers and reporters, difficulties in bug reproduction, and identifying bug locations. Therefore, there is a pressing need for additional efforts to effectively manage bug reports and enhance the quality of both desktop and mobile applications. In this paper, we describe the existing limitations of bug reports and identify potential strategies for addressing them. Our vision encompasses a future where the alleviation of these limitations and successful execution of our proposed new research directions can benefit both reporters and developers, ultimately making the entire software maintenance faster.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {237–241},
numpages = {5},
keywords = {bug reporting, bug localization, GUI, mobile apps},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3623351,
author = {Mastropaolo, Antonio and Zampetti, Fiorella and Bavota, Gabriele and Di Penta, Massimiliano},
title = {Toward Automatically Completing GitHub Workflows},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623351},
doi = {10.1145/3597503.3623351},
abstract = {Continuous integration and delivery (CI/CD) are nowadays at the core of software development. Their benefits come at the cost of setting up and maintaining the CI/CD pipeline, which requires knowledge and skills often orthogonal to those entailed in other software-related tasks. While several recommender systems have been proposed to support developers across a variety of tasks, little automated support is available when it comes to setting up and maintaining CI/CD pipelines. We present GH-WCOM (GitHub Workflow COMpletion), a Transformer-based approach supporting developers in writing a specific type of CI/CD pipelines, namely GitHub workflows. To deal with such a task, we designed an abstraction process to help the learning of the transformer while still making GH-WCOM able to recommend very peculiar workflow elements such as tool options and scripting elements. Our empirical study shows that GH-WCOM provides up to 34.23% correct predictions, and the model's confidence is a reliable proxy for the recommendations' correctness likelihood.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {13},
numpages = {12},
keywords = {continuous integration and delivery, GitHub workflows, pre-trained models, machine learning on code},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00058,
author = {Yerushalmi, Raz},
title = {Enhancing Deep Reinforcement Learning with Executable Specifications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00058},
doi = {10.1109/ICSE-Companion58688.2023.00058},
abstract = {Deep reinforcement learning (DRL) has become a dominant paradigm for using deep learning to carry out tasks where complex policies are learned for reactive systems. However, these policies are "black-boxes", e.g., opaque to humans and known to be susceptible to bugs. For example, it is hard --- if not impossible --- to guarantee that the trained DRL agent adheres to specific safety and fairness properties that may be required. This doctoral dissertation's first and primary contribution is a novel approach to developing DRL agents, which will improve the DRL training process by pushing the learned policy toward high performance on its main task and compliance with such safety and fairness properties, guaranteeing a high probability of compliance while not compromising the performance of the resulting agent. The approach is realized by incorporating domain-specific knowledge captured as key properties defined by domain experts directly into the DRL optimization process while leveraging behavioral languages that are natural to the domain experts. We have validated the proposed approach by extending the AI-Gym Python framework [1] for training DRL agents and integrating it with the BP-Py framework [2] for specifying scenario-based models [3] in a way that allows scenario objects to affect the training process through reward and cost functions, demonstrating dramatic improvement in the safety and performance of the agent. In addition, we have validated the resulting DRL agents using the Marabou verifier [4], confirming that the resulting agents indeed comply (in full) with the required safety and fairness properties. We have applied the approach, training DRL agents for use cases from network communication and robotic navigation domains, exhibiting strong results. A second contribution of this doctoral dissertation is to develop and leverage probabilistic verification methods for deep neural networks to overcome the current scalability limitations of neural network verification technology, limiting the applicability of verification to practical DRL agents. We carried out an initial validation of the concept in the domain of image classification, showing promising results.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {213–217},
numpages = {5},
keywords = {machine learning, deep reinforcement learning, scenario-based modeling, rule-based specifications, domain expertise},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3643665,
title = {FinanSE '24: Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software development has an integral role in every financial organisation; indeed, almost every service provided by a bank utilizes some form of software solution. While SE research has led to solutions and innovations for many popular SE problems, there remain unresolved challenges, particularly, those challenges faced in software development in financial firms. An example of such a challenge is defect prediction, where defects are not equal as some may lead to larger reputational and financial damage than others. Consequently, testing and verification is burdened with a further set of restraints for finance-based SE teams. Financial firms began automating processes as early as the 1960s, and as such, must maintain large legacy systems which may host critical operations. This problem is further exacerbated by the numerous mergers and acquisitions common in the financial sector, which leaves firms with a set of heterogeneous legacy systems that need to communicate with one another effectively and efficiently. Therefore, maintaining these systems while modernizing them leads to intriguing challenges, spanning from model extraction and process optimisation to code translation. Moreover, highly regulated institutions like financial firms require a high degree of transparency and accountability. This requirement facilitates the need for model fairness and explainability for any SE solution, in particular those that rely on AI.The 1st International Workshop on Software Engineering Challenges in Financial Firms (FinanSE 2024) is a forum to bring together academia and industry to share new ideas and results in tackling these challenges.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643991.3644910,
author = {Lin, Hong Yi and Thongtanunam, Patanamon and Treude, Christoph and Charoenwet, Wachiraphan},
title = {Improving Automated Code Reviews: Learning From Experience},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644910},
doi = {10.1145/3643991.3644910},
abstract = {Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a large workload and stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching large language models to provide reviews on submitted code, just as a human would. A recent approach pre-trained and fine-tuned the code intelligent language model on a large-scale code review corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {278–283},
numpages = {6},
keywords = {code review, review comments, neural machine translation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643795.3648379,
author = {Rasnayaka, Sanka and Wang, Guanlin and Shariffdeen, Ridwan and Iyer, Ganesh Neelakanta},
title = {An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648379},
doi = {10.1145/3643795.3648379},
abstract = {Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {111–118},
numpages = {8},
keywords = {LLM for code generation, software engineering},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@proceedings{10.1145/3643788,
title = {APR '24: Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the fifth International Workshop on Automated Program Repair (APR 2024), hosted by International Conference on Software Engineering (ICSE) 2024. Since its inception in 2020, APR has become a central event of the program repair community, reflecting a growing interest in the field among the software engineering, programming language, machine learning and formal methods communities.APR 2024 continues the tradition of fostering interaction among researchers in program repair. As always, we are particularly focused on narrowing the divide between academic research and real-world industry applications.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643661.3643951,
author = {Darnell, Benjamin and Chopra, Hetarth and Councilman, Aaron and Grove, David and Wang, Yu-Xiong and Adve, Vikram},
title = {An Empirical Comparison of Code Generation Approaches for Ansible},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643951},
doi = {10.1145/3643661.3643951},
abstract = {The rapid proliferation of LLM-based programming assistants has enabled fast and accurate automatic code generation for general purpose programming languages. Domain-specific languages like Ansible, a DSL for IT Automation, have seen a lack of support despite being critical to many fields, due to limited public-domain code for training models and a lack of interest from tool developers. To address this issue, we collect a novel dataset of permissively licensed Ansible code, and use it to create Warp, an LLM for code fine-tuned to produce Ansible tasks from a natural language prompt. We evaluate state-of-the-art tools for LLM-based code generation models, comparing multiple common strategies, including fine-tuning base models on Ansible code and retrieval-augmented-generation using documentation, in order to understand challenges with existing methodology and identify future research directions to enable better code generation for DSLs.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {large language models, code generation, domain specific languages, ansible},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00008,
author = {Chaaben, Meriem Ben and Burgue\~{n}o, Lola and Sahraoui, Houari},
title = {Towards Using Few-Shot Prompt Learning for Automating Model Completion},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00008},
doi = {10.1109/ICSE-NIER58687.2023.00008},
abstract = {We propose a simple yet a novel approach to improve completion in domain modeling activities. Our approach exploits the power of large language models by using few-shot prompt learning without the need to train or fine-tune those models with large datasets that are scarce in this field. We implemented our approach and tested it on the completion of static and dynamic domain diagrams. Our initial evaluation shows that such an approach is effective and can be integrated in different ways during the modeling activities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {7–12},
numpages = {6},
keywords = {language models, few-shot learning, prompt learning, domain modeling, model completion},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@proceedings{10.1145/3639478,
title = {ICSE-Companion '24: Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ICSE is the leading and, by far, the largest conference in Software Engineering, attracting researchers, practitioners, and students worldwide. ICSE2024 is co-located with 11 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE48619.2023.00128,
author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
title = {Automated Repair of Programs from Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00128},
doi = {10.1109/ICSE48619.2023.00128},
abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1469–1481},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3650105.3652295,
author = {Niu, Changan and Zhang, Ting and Li, Chuanyi and Luo, Bin and Ng, Vincent},
title = {On Evaluating the Efficiency of Source Code Generated by LLMs},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652295},
doi = {10.1145/3650105.3652295},
abstract = {Recent years have seen the remarkable capabilities of large language models (LLMs) for code generation. Different from existing work that evaluate the correctness of the code generated by LLMs, we propose to further evaluate its efficiency. More efficient code can lead to higher performance and execution efficiency of programs and software completed by LLM-assisted programming. First, we evaluate the efficiency of the code generated by LLMs on two benchmarks, HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several prompts that would enable LLMs to generate more efficient code.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {103–107},
numpages = {5},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3639478.3643062,
author = {Lin, Dayi and Cogo, Filipe Roseiro and Rajbahadur, Gopi Krishnan and Hassan, Ahmed E.},
title = {Technical Brief on Software Engineering for FMware},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643062},
doi = {10.1145/3639478.3643062},
abstract = {Foundation Models (FM) like GPT-4 have given rise to FMware, FM-powered applications, which represent a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. Such challenges jeopardize the quality and trustworthiness of FMware. In our technical brief, we will present the latest research and industrial practices in engineering FMware, and discuss the SE challenges and opportunities facing both researchers and practitioners in the FMware era.The brief is unique in that it is presented from an SE point of view, not an AI point-of-view ensuring that attendees are not bogged into complex mathematical and AI details unless they are essential for contextualizing the SE challenges and opportunities.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {431–433},
numpages = {3},
keywords = {foundation model, FMware, software engineering for FMware},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639121,
author = {Xia, Chunqiu Steven and Paltenghi, Matteo and Le Tian, Jia and Pradel, Michael and Zhang, Lingming},
title = {Fuzz4All: Universal Fuzzing with Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639121},
doi = {10.1145/3597503.3639121},
abstract = {Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language. To realize this potential, we present a novel autoprompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs. We evaluate Fuzz4All on nine systems under test that take in six different languages (C, C++, Go, SMT2, Java, and Python) as inputs. The evaluation shows, across all six languages, that universal fuzzing achieves higher coverage than existing, language-specific fuzzers. Furthermore, Fuzz4All has identified 98 bugs in widely used systems, such as GCC, Clang, Z3, CVC5, OpenJDK, and the Qiskit quantum computing platform, with 64 bugs already confirmed by developers as previously unknown.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {126},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643665.3648046,
author = {Abualhaija, Sallam and Ceci, Marcello and Sannier, Nicolas and Bianculli, Domenico and Zetzsche, Dirk and Bodellini, Marco},
title = {Toward Automated Change Impact Analysis of Financial Regulations},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643665.3648046},
doi = {10.1145/3643665.3648046},
abstract = {Regulation changes over time due to amending or repealing existing legal provisions as well as introducing new ones. The finance field provides a concrete example of heavily regulated area which has seen continuous regulatory changes in the aftermath of the 2008 crisis. Software financial services like online banking or trading must constantly comply with the regulations. Monitoring and analyzing the regulatory change is essential to ensure that such services remain compliant. Regulatory changes can significantly affect existing software systems that were compliant at a certain point in time. However, tracing the regulatory changes entirely manually is time consuming and error-prone. In this position paper, we introduce our vision for automated financial regulations change impact analysis. We aim at characterizing the regulatory changes pertinent to financial regulations, and further providing automated support for both identifying and classifying the regulatory changes as well as analyzing the impact of such changes on existing (potentially compliant) software systems.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
pages = {31–32},
numpages = {2},
keywords = {FinTech, regulatory compliance, regulatory change, change impact analysis},
location = {Lisbon, Portugal},
series = {FinanSE '24}
}

@inproceedings{10.1145/3597503.3639223,
author = {Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin},
title = {Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639223},
doi = {10.1145/3597503.3639223},
abstract = {Understanding and identifying the causes behind developers' emotions (e.g., Frustration caused by 'delays in merging pull requests') can be crucial towards finding solutions to problems and fostering collaboration in open-source communities. Effectively identifying such information in the high volume of communications across the different project channels, such as chats, emails, and issue comments, requires automated recognition of emotions and their causes. To enable this automation, large-scale software engineering-specific datasets that can be used to train accurate machine learning models are required. However, such datasets are expensive to create with the variety and informal nature of software projects' communication channels.In this paper, we explore zero-shot LLMs that are pre-trained on massive datasets but without being fine-tuned specifically for the task of detecting emotion causes in software engineering: ChatGPT, GPT-4, and flan-alpaca. Our evaluation indicates that these recently available models can identify emotion categories when given detailed emotions, although they perform worse than the top-rated models. For emotion cause identification, our results indicate that zero-shot LLMs are effective at recognizing the correct emotion cause with a BLEU-2 score of 0.598. To highlight the potential use of these techniques, we conduct a case study of the causes of Frustration in the last year of development of a popular open-source project, revealing several interesting insights.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {182},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00085,
author = {Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K. and Sen, Siddhartha},
title = {CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-Trained Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00085},
doi = {10.1109/ICSE48619.2023.00085},
abstract = {Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {919–931},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3647633,
author = {Chen, Yuxiao and Wu, Jingzheng and Ling, Xiang and Li, Changjiang and Rui, Zhiqing and Luo, Tianyue and Wu, Yanjun},
title = {When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3647633},
doi = {10.1145/3639478.3647633},
abstract = {In recent years, large language models (LLMs) have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular LLMs in handling repository-level repair tasks. We introduce RepoBugs, a new benchmark comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using GPT3.5 based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of GPT3.5 on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the prompt length limit of LLMs. To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream LLMs show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of LLMs to address repository-level bugs, offering valuable insights for future research.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {459–471},
numpages = {13},
keywords = {large language models, automatic program repair, repository-level bugs, context, static analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@proceedings{10.1145/3643666,
title = {MO2RE 2024: Proceedings of the 1st IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Requirements engineering (RE) is a critical sub-field of software engineering (SE) that deals with identifying, specifying, modeling, analyzing, and validating the needs of stakeholders and constraints of a system [1]. RE covers human-related aspects, as stakeholders need to be involved in eliciting and validating the requirements, as well as more technical aspects, as requirements can be systematically collected (e.g., from app reviews) using data mining techniques and analyzed with natural language processing (NLP) approaches, e.g., to identify quality issues or trace links [2]. Despite the broad spectrum of activities that RE covers, researchers from outside RE often have a misconception that RE is limited to writing and analyzing requirements specifications. Consequently, many researchers in the SE community working on RE-relevant problems (e.g., human-centric SE) are often unaware that such problems belong to the RE research strands. Broadly speaking, RE is under-represented and under-appreciated in the SE community.Despite this limited presence, RE is more and more fundamental to cope with the current state of SE, especially considering the recent disruptive changes in artificial intelligence (AI) and NLP caused by large language models (LLMs) and their applications, ChatGPT being a notable example. Given the increasing pervasiveness of AI-based systems in our daily life, there is a growing need for RE techniques to support sound and structured development of AI systems [3], with a particular interest in explainability, interpretability, reliability, fairness, and other ethical concerns [4]. At the same time, current developments in AI can solve long-standing RE problems, such as automatic requirements tracing, completeness checking, and modeling. AI can further create better connections between RE and other automated SE fields.The 1st International Workshop on (Multi-disciplinary, Open, and RElevant RE) (MO2RE) has the goal to address these issues by raising awareness of RE's diverse aspects and fostering collaboration within the SE community.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3641822.3641877,
author = {Melegati, Jorge and Nascimento, Nicolas and Chanin, Rafael and Sales, Afonso and Wiese, Igor},
title = {Exploring potential implications of intelligent tools for human aspects of software engineering},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641877},
doi = {10.1145/3641822.3641877},
abstract = {Background. The emergence of tools based on artificial intelligence (AI) to support software development suggests an overhaul on how developers program and interact among themselves. This disruption might bring challenges regarding human and social aspects of the software development process. Objective. This paper is a first exploration of the consequences of AI-based tools for software development teams and their members. Method. We conducted a social science fiction exercise, a sort of thought experiment, narrating two fictional stories about a futuristic software company employing AI-based tools. Then, we evaluated the plausibility of one of the scenarios through a qualitative experiment with 38 students to observe their perception regarding the use of AI-based tools. Results. The stories suggest potential challenges related to the adoption of these tools: a change on how developers perceive themselves, a clash between quantitative and qualitative worker contribution assessment, and the training of future developers to handle the imminent changes on their profession. In the qualitative experiment, we collected evidence supporting negative feelings, such as lack of trust and control and fear of being replaced. We also identified other attitudes and perceptions of developers, such as positive feelings towards AI-based tools. Conclusion. We identified several aspects that might influence the adoption of AI-based tools and their implications for individuals involved. They should be further investigated and represent a challenge for the research on human aspects of software engineering. We also demonstrated the use of social science fiction to explore novel research problems.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {121–132},
numpages = {12},
keywords = {AI for SE, social science fiction, human aspects of software development, qualitative experiment},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1145/3643795.3648389,
author = {Dingle, Adam and Krulis, Martin},
title = {Tackling Students' Coding Assignments with LLMs},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648389},
doi = {10.1145/3643795.3648389},
abstract = {State-of-the-art large language models (LLMs) have demonstrated an extraordinary ability to write computer code. This ability can be quite beneficial when integrated into an IDE to assist a programmer with basic coding. On the other hand, it may be misused by computer science students for cheating on coding tests or homework assignments. At present, knowledge about the exact capabilities and limitations of state-of-the-art LLMs is still inadequate. Furthermore, their capabilities have been changing quickly with each new release. In this paper, we present a dataset of 559 programming exercises in 10 programming languages collected from a system for evaluating coding assignments at our university. We have experimented with four well-known LLMs (GPT-3.5, GPT-4, Codey, Code Llama) and asked them to solve these assignments. The evaluation results are intriguing and provide insights into the strengths and weaknesses of the models. In particular, GPT-4 (which performed the best) is currently capable of solving 55% of all our exercises and achieved an average score of 86% on exercises from the introductory programming course (using the best of five generated solutions).},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {94–101},
numpages = {8},
keywords = {LLM, large language model, coding, programming, student assignment, teaching},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1145/3597503.3608130,
author = {Arteaga Garcia, Emily Judith and Nicolaci Pimentel, Jo\~{a}o Felipe and Feng, Zixuan and Gerosa, Marco and Steinmacher, Igor and Sarma, Anita},
title = {How to Support ML End-User Programmers through a Conversational Agent},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608130},
doi = {10.1145/3597503.3608130},
abstract = {Machine Learning (ML) is increasingly gaining significance for enduser programmer (EUP) applications. However, machine learning end-user programmers (ML-EUPs) without the right background face a daunting learning curve and a heightened risk of mistakes and flaws in their models. In this work, we designed a conversational agent named "Newton" as an expert to support ML-EUPs. Newton's design was shaped by a comprehensive review of existing literature, from which we identified six primary challenges faced by ML-EUPs and five strategies to assist them. To evaluate the efficacy of Newton's design, we conducted a Wizard of Oz within-subjects study with 12 ML-EUPs. Our findings indicate that Newton effectively assisted ML-EUPs, addressing the challenges highlighted in the literature. We also proposed six design guidelines for future conversational agents, which can help other EUP applications and software engineering activities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {53},
numpages = {12},
keywords = {end-user programming, conversational agent, wizard of Oz},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/GREENS66463.2025.00018,
author = {Pathania, Priyavanshi and Bamby, Nikhil and Mehra, Rohit and Sikand, Samarth and Sharma, Vibhu Saujanya and Kaulgud, Vikrant and Podder, Sanjay and Burden, Adam P.},
title = {Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00018},
doi = {10.1109/GREENS66463.2025.00018},
abstract = {The proliferation of software and AI comes with a hidden risk: its growing energy and carbon footprint. As concerns regarding environmental sustainability come to the forefront, understanding and optimizing how software impacts the environment becomes paramount. In this paper, we present a state-of-the-art review of methods and tools that enable the measurement of software and AI-related energy and/or carbon emissions. We introduce a taxonomy to categorize the existing work as Monitoring, Estimation, or Black-Box approaches. We delve deeper into the tools and compare them across different dimensions and granularity—for example, whether their measurement encompasses energy and carbon emissions and the components considered (like CPU, GPU, RAM, etc.). We present our observations on the practical use (component wise consolidation of approaches) as well as the challenges that we have identified across the current state-of-the-art. As we start an initiative to address these challenges, we emphasize active collaboration across the community in this important field.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {92–99},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@proceedings{10.1145/3639477,
title = {ICSE-SEIP '24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639219,
author = {Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
title = {Evaluating Large Language Models in Class-Level Code Generation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639219},
doi = {10.1145/3597503.3639219},
abstract = {Recently, many large language models (LLMs) have been proposed, showing advanced proficiency in code generation. Meanwhile, many efforts have been dedicated to evaluating LLMs on code generation benchmarks such as HumanEval. Although being very helpful for comparing different LLMs, existing evaluation focuses on a simple code generation scenario (i.e., function-level or statement-level code generation), which mainly asks LLMs to generate one single code unit (e.g., a function or a statement) for the given natural language description. Such evaluation focuses on generating independent and often small-scale code units, thus leaving it unclear how LLMs perform in real-world software development scenarios.To fill this knowledge gap, we make the first attempt to evaluate LLMs in a more challenging code generation scenario, i.e., class-level code generation. Compared with existing code generation benchmarks, it better reflects real-world software development scenarios due to it comprising broader contextual dependencies and multiple, interdependent units of code. We first manually construct the first class-level code generation benchmark ClassEval of 100 class-level Python code generation tasks with approximately 500 person-hours. Based on the new benchmark ClassEval, we then perform the first study of 11 state-of-the-art LLMs on class-level code generation. Based on our results, we find that all LLMs perform much worse on class-level code generation compared to the method-level. While GPT models still dominate other LLMs on class-level code generation, the performance rankings of other models on method-level code generation no longer holds for class-level code generation. Besides, most models (except GPT models) perform better when generating the class method by method; and they have the limited ability of generating dependent code. Based on our findings, we call for software engineering (SE) researchers' expertise to build more LLM benchmarks based on practical and complicated software development scenarios.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {81},
numpages = {13},
keywords = {class-level code generation, large language model, benchmark},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/WETSEB66605.2025.00013,
author = {Giuffrida, Simone and Salim, Shahid and Ullah, Azmat and Vaccargiu, Matteo},
title = {A Move Sui library for secure, certified and trusted supply chain ownership management},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WETSEB66605.2025.00013},
doi = {10.1109/WETSEB66605.2025.00013},
abstract = {This paper presents MoveChain, a blockchain-based solution designed using the Move programming language, to automate and secure product ownership management in supply chains. Our approach ensures the immutability of ownership records, integrates the Internet of Things (IoT) to monitor real-time product conditions and promotes participant accountability. We implement and evaluate MoveChain on the Sui blockchain, demonstrating its flow, interactions, and cost efficiency. Results indicate that our solution improves security and traceability while addressing the scalability and complexity challenges of blockchain-based supply chain management systems, providing a robust and scalable foundation for property management in various industries.},
booktitle = {Proceedings of the 2025 IEEE/ACM 7th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {50–56},
numpages = {7},
location = {Ottawa, ON, Canada},
series = {WETSEB '25}
}

@inproceedings{10.1145/3639477.3639732,
author = {Song, Yewei and Ezzini, Saad and Tang, Xunzhu and Lothritz, Cedric and Klein, Jacques and Bissyande, Tegawende and Boytsov, Andrey and Ble, Ulrick and Goujon, Anne},
title = {Enhancing Text-to-SQL Translation for Financial System Design},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639732},
doi = {10.1145/3639477.3639732},
abstract = {Text-to-SQL, the task of translating natural language questions into SQL queries, is part of various business processes. Its automation, which is an emerging challenge, will empower software practitioners to seamlessly interact with relational databases using natural language, thereby bridging the gap between business needs and software capabilities.In this paper, we consider Large Language Models (LLMs), which have achieved state of the art for various NLP tasks. Specifically, we benchmark Text-to-SQL performance, the evaluation methodologies, as well as input optimization (e.g., prompting). In light of the empirical observations that we have made, we propose two novel metrics that were designed to adequately measure the similarity between SQL queries.Overall, we share with the community various findings, notably on how to select the right LLM on Text-to-SQL tasks. We further demonstrate that a tree-based edit distance constitutes a reliable metric for assessing the similarity between generated SQL queries and the oracle for benchmarking Text2SQL approaches. This metric is important as it relieves researchers from the need to perform computationally expensive experiments such as executing generated queries as done in prior works. Our work implements financial domain use cases and, therefore contributes to the advancement of Text2SQL systems and their practical adoption in this domain.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {252–262},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/ICSE48619.2023.00180,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Chen, Dongxiao and Ge, Jidong and Luo, Bin},
title = {An Empirical Comparison of Pre-Trained Models of Source Code},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00180},
doi = {10.1109/ICSE48619.2023.00180},
abstract = {While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2136–2148},
numpages = {13},
keywords = {pre-training of source code, AI for SE},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00081,
author = {Bouzenia, Islem and Pradel, Michael},
title = {When to Say What: Learning to Find Condition-Message Inconsistencies},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00081},
doi = {10.1109/ICSE48619.2023.00081},
abstract = {Programs often emit natural language messages, e.g., in logging statements or exceptions raised on unexpected paths. To be meaningful to users and developers, the message, i.e., what to say, must be consistent with the condition under which it gets triggered, i.e., when to say it. However, checking for inconsistencies between conditions and messages is challenging because the conditions are expressed in the logic of the programming language, while messages are informally expressed in natural language. This paper presents CMI-Finder, an approach for detecting condition-message inconsistencies. CMI-Finder is based on a neural model that takes a condition and a message as its input and then predicts whether the two are consistent. To address the problem of obtaining realistic, diverse, and large-scale training data, we present six techniques to generate large numbers of inconsistent examples to learn from automatically. Moreover, we describe and compare three neural models, which are based on binary classification, triplet loss, and fine-tuning, respectively. Our evaluation applies the approach to 300K condition-message statements extracted from 42 million lines of Python code. The best model achieves a precision of 78% at a recall of 72% on a dataset of past bug fixes. Applying the approach to the newest versions of popular open-source projects reveals 50 previously unknown bugs, 19 of which have been confirmed by the developers so far.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {868–880},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643796.3648448,
author = {Szatm\'{a}ri, Attila and Sarhan, Qusay Idrees and Soha, Peter Attila and Balogh, Gergo and Beszedes, Arpad},
title = {On the Integration of Spectrum-Based Fault Localization Tools into IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648448},
doi = {10.1145/3643796.3648448},
abstract = {Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected. SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness. Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available. Reasons for this can be multiple, including the algortihms' sub-optimal effectiveness and other technical weaknesses. But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs. In this paper, we attempt to provide such a list in form of recommendations, based on surveying the most popular SBFL tools and on our own researchers' and tool builders' experience.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {24–29},
numpages = {6},
keywords = {spectrum-based fault localization, SBFL, IDE, debugging},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3597503.3639091,
author = {Li, Zongjie and Wang, Chaozheng and Ma, Pingchuan and Liu, Chaowei and Wang, Shuai and Wu, Daoyuan and Gao, Cuiyun and Liu, Yang},
title = {On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639091},
doi = {10.1145/3597503.3639091},
abstract = {Recent advances in large language models (LLMs) significantly boost their usage in software engineering. However, training a well-performing LLM demands a substantial workforce for data collection and annotation. Moreover, training datasets may be proprietary or partially open, and the process often requires a costly GPU cluster. The intellectual property value of commercial LLMs makes them attractive targets for imitation attacks, but creating an imitation model with comparable parameters still incurs high costs. This motivates us to explore a practical and novel direction: slicing commercial black-box LLMs using medium-sized backbone models.In this paper, we explore the feasibility of launching imitation attacks on LLMs to extract their specialized code abilities, such as "code synthesis" and "code translation." We systematically investigate the effectiveness of launching code ability extraction attacks under different code-related tasks with multiple query schemes, including zero-shot, in-context, and Chain-of-Thought. We also design response checks to refine the outputs, leading to an effective imitation training process. Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs. We summarize our findings and insights to help researchers better understand the threats posed by imitation attacks, including revealing a practical attack surface for generating adversarial code examples against LLMs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {74},
numpages = {13},
keywords = {large language models, imitation attacks},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639155,
author = {Xu, Junjielong and Yang, Ruichun and Huo, Yintong and Zhang, Chengyu and He, Pinjia},
title = {DivLog: Log Parsing with Prompt Enhanced In-Context Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639155},
doi = {10.1145/3597503.3639155},
abstract = {Log parsing, which involves log template extraction from semi-structured logs to produce structured logs, is the first and the most critical step in automated log analysis. However, current log parsers suffer from limited effectiveness for two reasons. First, traditional data-driven log parsers solely rely on heuristics or handcrafted features designed by domain experts, which may not consistently perform well on logs from diverse systems. Second, existing supervised log parsers require model tuning, which is often limited to fixed training samples and causes sub-optimal performance across the entire log source. To address this limitation, we propose DivLog, an effective log parsing framework based on the in-context learning (ICL) ability of large language models (LLMs). Specifically, before log parsing, DivLog samples a small amount of offline logs as candidates by maximizing their diversity. Then, during log parsing, DivLog selects five appropriate labeled candidates as examples for each target log and constructs them into a prompt. By mining the semantics of examples in the prompt, DivLog generates a target log template in a training-free manner. In addition, we design a straightforward yet effective prompt format to extract the output and enhance the quality of the generated log templates. We conducted experiments on 16 widely-used public datasets. The results show that DivLog achieves (1) 98.1% Parsing Accuracy, (2) 92.1% Precision Template Accuracy, and (3) 92.9% Recall Template Accuracy on average, exhibiting state-of-the-art performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {199},
numpages = {12},
keywords = {log parsing, large language model, in-context learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644898,
author = {Casta\~{n}o, Joel and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier and Bogner, Justus},
title = {Analyzing the Evolution and Maintenance of ML Models on Hugging Face},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644898},
doi = {10.1145/3643991.3644898},
abstract = {Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repository mining study, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF - aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Through text analysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {607–618},
numpages = {12},
keywords = {repository mining, software evolution, maintenance},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643916.3644408,
author = {Liu, Yilun and Tao, Shimin and Meng, Weibin and Wang, Jingyu and Ma, Wenbing and Chen, Yuhang and Zhao, Yanqing and Yang, Hao and Jiang, Yanfei},
title = {Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644408},
doi = {10.1145/3643916.3644408},
abstract = {Automated log analysis is crucial in modern software-intensive systems for facilitating program comprehension throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' comprehension of program status and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel interpretable log analysis approach for online scenarios. LogPrompt employs large language models (LLMs) to perform online log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' performance by up to 380.7% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite requiring no in-domain training, outperforms existing approaches trained on thousands of logs by up to 55.9%. We also conduct a human evaluation of LogPrompt's interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment. Code of LogPrompt is available at https://github.com/lunyiliu/LogPrompt.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {35–46},
numpages = {12},
keywords = {large language model, prompt engineering, log analysis, interpretability, online scenario},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639477.3639721,
author = {Brandt, Carolin and Castelluccio, Marco and Holler, Christian and Kratzer, Jason and Zaidman, Andy and Bacchelli, Alberto},
title = {Mind the Gap: What Working With Developers on Fuzz Tests Taught Us About Coverage Gaps},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639721},
doi = {10.1145/3639477.3639721},
abstract = {Can fuzzers generate partial tests that developers find useful enough to complete into functional tests (e.g., by adding assertions)? To address this question, we develop a prototype within the Mozilla ecosystem and open 13 bug reports proposing partial generated tests for currently uncovered code. We found that the majority of the reactions focus on whether the targeted coverage gap is actually worth testing. To investigate further which coverage gaps developers find relevant to close, we design an automated filter to exclude irrelevant coverage gaps before generating tests. From conversations with 13 developers about whether the remaining coverage gaps are worth closing when a partially generated test is available, we learn that the filtering indeed removes clearly non-test-worthy gaps. The developers propose a variety of additional strategies to address the coverage gaps and how to make fuzz tests and reports more useful for developers.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {157–167},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643661.3643952,
author = {Astekin, Merve and Hort, Max and Moonen, Leon},
title = {An Exploratory Study on How Non-Determinism in Large Language Models Affects Log Parsing},
year = {2024},
isbn = {9798400705649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643661.3643952},
doi = {10.1145/3643661.3643952},
abstract = {Most software systems used in production generate system logs that provide a rich source of information about the status and execution behavior of the system. These logs are commonly used to ensure the reliability and maintainability of software systems. The first step toward automated log analysis is generally log parsing, which aims to transform unstructured log messages into structured log templates and extract the corresponding parameters.Recently, Large Language Models (LLMs) such as ChatGPT have shown promising results on a wide range of software engineering tasks, including log parsing. However, the extent to which non-determinism influences log parsing using LLMs remains unclear. In particular, it is important to investigate whether LLMs behave consistently when faced with the same log message multiple times.In this study, we investigate the impact of non-determinism in state-of-the-art LLMs while performing log parsing. Specifically, we select six LLMs, including both paid proprietary and free-to-use models, and evaluate their non-determinism on 16 system logs obtained from a selection of mature open-source projects. The results of our study reveal varying degrees of non-determinism among models. Moreover, they show that there is no guarantee for deterministic results even with a temperature of zero.},
booktitle = {Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering},
pages = {13–18},
numpages = {6},
keywords = {log parsing, large language model, robustness, non-determinism, consistency},
location = {Lisbon, Portugal},
series = {InteNSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00043,
author = {Johnson, Brittany and Bird, Christian and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas},
title = {Make Your Tools Sparkle with Trust: The PICSE Framework for Trust in Software Tools},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00043},
doi = {10.1109/ICSE-SEIP58684.2023.00043},
abstract = {The day to day of a software engineer involves a variety of tasks. While many of these tasks are collaborative and completed as such, it is not always possible or feasible to engage with other engineers for task completion. Software tools, such as code generators and static analysis tools, aim to fill this gap by providing additional support for developers to effectively complete their tasks. With a steady stream of new tools that emerging to support software engineers, including a new breed of tools that rely on artificial intelligence, there are important questions we should aim to answer regarding the trust engineers can, and should, put into their software tools and what it means to build a trustworthy tool. In this paper, we present findings from an industry interview study conducted with 18 engineers across and external to the Microsoft organization. Based on these interviews, we introduce the PICSE (pronounced "pixie") framework for trust in software tools to provide preliminary insights into factors that influence engineer trust in their software tools. We also discuss how the PICSE framework can be considered and applied in practice for designing and developing trustworthy software tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {409–419},
numpages = {11},
keywords = {trust, software tools, artificial intelligence, framework},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3597503.3623328,
author = {Wong, Wai Kin and Wang, Huaijin and Li, Zongjie and Wang, Shuai},
title = {BinAug: Enhancing Binary Similarity Analysis with Low-Cost Input Repairing},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623328},
doi = {10.1145/3597503.3623328},
abstract = {Binary code similarity analysis (BCSA) is a fundamental building block for various software security, reverse engineering, and re-engineering applications. Existing research has applied deep neural networks (DNNs) to measure the similarity between binary code, following the major breakthrough of DNNs in processing media data like images. Despite the encouraging results of DNN-based BCSA, it is however not widely deployed in the industry due to the instability and the black-box nature of DNNs.In this work, we first launch an extensive study over the state-of-the-art (SoTA) BCSA tools, and investigate their erroneous predictions from both quantitative and qualitative perspectives. Then, we accordingly design a low-cost and generic framework, namely Binaug, to improve the accuracy of BCSA tools by repairing their input binary codes. Aligned with the typical workflow of DNN-based BCSA, Binaug obtains the sorted top-K results of code similarity, and then re-ranks the results using a set of carefully-designed transformations. Binaug supports both black- and white-box settings, depending on the accessibility of the DNN model internals. Our experimental results show that Binaug can constantly improve performance of the SoTA BCSA tools by an average of 2.38pt and 6.46pt in the black- and the white-box settings. Moreover, with Binaug, we enhance the F1 score of binary software component analysis, an important downstream application of BCSA, by an average of 5.43pt and 7.45pt in the black- and the white-box settings.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {7},
numpages = {13},
keywords = {binary analysis, DNNs, input repairing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00129,
author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
title = {Automated Program Repair in the Era of Large Pre-Trained Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00129},
doi = {10.1109/ICSE48619.2023.00129},
abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed.In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1482–1494},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3666015.3666022,
author = {Vasylieva, Kseniia and K\"{u}pper, Steffen and Kuhrmann, Marco},
title = {Breaking old Habits: On Success Factors in Software Process Improvement},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666022},
doi = {10.1145/3666015.3666022},
abstract = {Over the years, a substantial body of knowledge of software process improvement (SPI) was accumulated that, among other things, includes numerous success factors that companies should consider when conducting improvement activities. The number of success factors is large and quite often, multiple success factors with similar names and descriptions are available to address a specific phenomenon. This raises the question whether all the success factors are unique and, if not, which ones are actually the same. In this paper, we aim to structure the body of knowledge on success factors in SPI. We conducted a systematic literature review on 103 publications that mention 1.320 success factors. A multi-staged manual and AI-supported analysis reduced the number of success factors to 124, which we categorize into 42 general success factor classes. For 20 of these general success factor classes, we observed a stable number of publications over a period of almost 30 years that, however, show only few success factors constantly studied and re-discovered. A high number of synonyms shows that this area of SPI needs consolidation for which we lay the foundation by providing a big picture and identifying the most relevant success factors as a starting point.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {13–23},
numpages = {11},
keywords = {Critical success factors, SPI, Software process, Software process improvement, Systematic literature study},
location = {M\, Germany},
series = {ICSSP '24}
}

@inproceedings{10.1145/3597503.3639100,
author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
title = {BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639100},
doi = {10.1145/3597503.3639100},
abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54% recall@1 and 0.34 MRR compared with 10.75% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36% to 85.84% and recall from 59.81% to 64.98% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {224},
numpages = {13},
keywords = {software composition analysis, static binary analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639142,
author = {Liu, Zhongxin and Tang, Zhijie and Zhang, Junwei and Xia, Xin and Yang, Xiaohu},
title = {Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639142},
doi = {10.1145/3597503.3639142},
abstract = {Vulnerability analysis is crucial for software security. Inspired by the success of pre-trained models on software engineering tasks, this work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {151},
numpages = {13},
keywords = {source code pre-training, program dependence analysis, vulnerability detection, vulnerability classification, vulnerability assessment},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643916.3644417,
author = {Han, Rui and Han, Wanjiang and Han, Zhuoyan and Tian, Yifan and Chen, Longzheng and Han, Ren},
title = {CRSP: Emulating Human Cooperative Reasoning for Intelligible Story Point Estimation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644417},
doi = {10.1145/3643916.3644417},
abstract = {Software effort estimation plays a critical role in software project development. Inaccurate cost estimation can impact progress and result in budget overruns. The story point estimation technique is a commonly used practice in agile software development for the estimation of software development effort. It allows for the evaluation of relative task workloads by analyzing task titles and descriptions. In previous studies, researchers have mainly focused on providing story point estimation results by task titles. However, in practical scenarios, users are often unable to provide task titles as precise as those found in the training dataset, leading to inaccurate estimation results. To address this problem, we propose a Cooperative Reasoning Story Point estimation method (CRSP). We approach the estimation problem as a question-and-answer challenge, addressing it through a framework of model construction, Monte Carlo Tree search, and model inference. In the model construction phase, we train a generator responsible for generating problem-solving reasoning paths and employ verifier to score the quality of these reasoning paths. During the Monte Carlo Tree search stage, we execute MCTS using generator and verifier to generate candidate solutions. In the final model inference phase, we employ a solver to derive the ultimate answer. To evaluate the effectiveness of CRSP, we modify and adapt the well-known JIRA dataset to make it more compatible with the input format of the question-answering model. The new JIRA dataset contains 21,082 issues from 16 open-source software projects. Across 16 open-source projects, the mean absolute error of CRSP is lower than other baseline methods. In contrast to the traditional regression and classification methods, we pioneer the use of question-and-answer method to address the issue, opening up new directions for future research.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {166–177},
numpages = {12},
keywords = {effort estimation, story point estimation, cooperative reasoning},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643660.3643945,
author = {Lethbridge, Timothy},
title = {TAMVE: Properties of Design Technologies to Address Challenges to Software Design in the Era of Agility and Frameworks},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643945},
doi = {10.1145/3643660.3643945},
abstract = {It has been over 20 years since software design started the transition from the era of big design documents to the era where agility and the use of sophisticated frameworks has become standard. Design in both eras faced challenges: Big documents are rarely maintained properly; agility results in design either being skipped, lost, or dispersed into multiple small files; frameworks such as Ruby on Rails tend to impose a design on the system, encouraging developers to jump into coding. In this position paper, we suggest how design technologies and notations should have five properties that would help overcome many of the challenges to design in the current era. They should be Textual, Analysable, Multi-technology, Visualizable and Example-rich; we use the acronym TAMVE as a mnemonic for this. We explain how the Umple technology goes a considerable distance towards this achieving the TAMVE vision.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {56–59},
numpages = {4},
keywords = {software design, frameworks, agility, umple, modeling},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3644032.3644456,
author = {Canizares, Pablo C. and \'{A}vila, Daniel and Perez-Soler, Sara and Guerra, Esther and De Lara, Juan},
title = {Coverage-based Strategies for the Automated Synthesis of Test Scenarios for Conversational Agents},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644456},
doi = {10.1145/3644032.3644456},
abstract = {Conversational agents - or chatbots - are increasingly used as the user interface to many software services. While open-domain chatbots like ChatGPT excel in their ability to chat about any topic, task-oriented conversational agents are designed to perform goal-oriented tasks (e.g., booking or shopping) guided by a dialogue-based user interaction, which is explicitly designed. Like any kind of software system, task-oriented conversational agents need to be properly tested to ensure their quality. For this purpose, some tools permit defining and executing conversation test cases. However, there are currently no established means to assess the coverage of the design of a task-oriented agent by a test suite, or mechanisms to automate quality test case generation ensuring the agent coverage.To attack this problem, we propose test coverage criteria for task-oriented conversational agents, and define coverage-based strategies to synthesise test scenarios, some oriented to test case reduction. We provide an implementation of the criteria and the strategies that is independent of the agent development platform. Finally, we report on their evaluation on open-source Dialogflow and Rasa agents, and a comparison against a state-of-the-art testing tool. The experiment shows benefits in terms of test generation correctness, increased coverage and reduced testing time.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {23–33},
numpages = {11},
keywords = {testing, test suite generation, task-oriented conversational agents},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3639478.3643110,
author = {Venkatkrishna, Vatsal and Nagabushanam, Durga Shree and Simon, Emmanuel Iko-Ojo and Vidoni, Melina},
title = {Multi-step Automated Generation of Parameter Docstrings in Python: An Exploratory Study},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643110},
doi = {10.1145/3639478.3643110},
abstract = {Documentation debt hinders the effective utilisation of open-source software. Although code summarisation tools have been helpful for developers, most would prefer a detailed account of each parameter in a function rather than a high-level summary. However, generating such a summary is too intricate for a single generative model to produce reliably due to the lack of high-quality training data. Thus, we propose a multi-step approach that combines multiple task-specific models, each adept at producing a specific section of a docstring. The combination of these models ensures the inclusion of each section in the final docstring. We compared the results from our approach with existing generative models using both automatic metrics and a human-centred evaluation with 17 participating developers, which proves the superiority of our approach over existing methods.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {356–357},
numpages = {2},
keywords = {docstrings, pre-trained models, code summarisation, scientific software, documentation debt},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3644033.3644374,
author = {Silva, \'{A}lvaro F. and Mendes, Alexandra and Ferreira, Jo\~{a}o F.},
title = {Leveraging Large Language Models to Boost Dafny’s Developers Productivity},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644374},
doi = {10.1145/3644033.3644374},
abstract = {This research idea paper proposes leveraging Large Language Models (LLMs) to enhance the productivity of Dafny developers. Although the use of verification-aware languages, such as Dafny, has increased considerably in the last decade, these are still not widely adopted. Often the cost of using such languages is too high, due to the level of expertise required from the developers and challenges that they often face when trying to prove a program correct. Even though Dafny automates a lot of the verification process, sometimes there are steps that are too complex for Dafny to perform on its own. One such case is that of missing lemmas, i.e. Dafny is unable to prove a result without being given further help in the form of a theorem that can assist it in the proof of the step.In this paper, we describe preliminary work on using LLMs to assist developers by generating suggestions for relevant lemmas that Dafny is unable to discover and use. Moreover, for the lemmas that cannot be proved automatically, we attempt to provide accompanying calculational proofs. We also discuss ideas for future work by describing a research agenda on using LLMs to increase the adoption of verification-aware languages in general, by increasing developers productivity and by reducing the level of expertise required for crafting formal specifications and proving program properties.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {138–142},
numpages = {5},
keywords = {verification-aware languages, dafny, large language models, generative AI, software productivity, software verification, lemma inference, proof inference, automated program repair},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3639474.3640062,
author = {Ouhbi, Sofia},
title = {Bridging the Theory-Practice Gap in a Maintenance Programming Course: An Experience Report},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640062},
doi = {10.1145/3639474.3640062},
abstract = {This paper presents our experience in teaching a maintenance programming course with the aim of bridging the gap between theory and practice, a recurring issue in previous course offerings. To achieve this goal, we implemented active learning strategies within an active learning classroom setting and redesigned the project work. Our approach involves peer learning and teamwork activities to cover various aspects of legacy code maintenance. For the project work, we adopted an open-ended approach that allowed students to choose their legacy code projects, which could be open-source software or a previous software project they had worked on. Analysis of students' feedback and project reports highlighted the effectiveness of our approach in bridging the gap between theory and practice. We believe that our approach had the potential to enhance students' engagement and critical thinking abilities, as well as improve practical maintenance skills relevant to their future careers.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {359–367},
numpages = {9},
keywords = {software maintenance, software engineering education, open-ended project, group work, active learning, students engagement, generative AI},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3639478.3639811,
author = {Bashir, Sarmad},
title = {Towards AI-centric Requirements Engineering for Industrial Systems},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639811},
doi = {10.1145/3639478.3639811},
abstract = {Engineering large-scale industrial systems mandate an effective Requirements Engineering (RE) process. Such systems necessitate RE process optimization to align with standards, infrastructure specifications, and customer expectations. Recently, artificial intelligence (AI) based solutions have been proposed, aiming to enhance the efficiency of requirements management within the RE process. Despite their advanced capabilities, generic AI solutions exhibit limited adaptability within real-world contexts, mainly because of the complexity and specificity inherent to industrial domains. This limitation notably leads to the continued prevalence of manual practices that not only cause the RE process to be heavily dependent on practitioners' experience, making it prone to errors, but also often contributes to project delays and inefficient resource utilization. To address these challenges, this Ph.D. dissertation focuses on two primary directions: i) conduct a comprehensive focus group study with a large-scale industry to determine the requirements evolution process and their inherent challenges and ii) propose AI solutions tailored for industrial case studies to automate and streamline their RE process and optimize the development of large-scale systems. We anticipate that our research will significantly contribute to the RE domain by providing empirically validated insights in the industrial context.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {242–246},
numpages = {5},
keywords = {requirements engineering, industrial automation, language models},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639580,
author = {OBrien, David and Dyer, Robert and Nguyen, Tien and Rajan, Hridesh},
title = {Data-Driven Evidence-Based Syntactic Sugar Design},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639580},
doi = {10.1145/3597503.3639580},
abstract = {Programming languages are essential tools for developers, and their evolution plays a crucial role in supporting the activities of developers. One instance of programming language evolution is the introduction of syntactic sugars, which are additional syntax elements that provide alternative, more readable code constructs. However, the process of designing and evolving a programming language has traditionally been guided by anecdotal experiences and intuition. Recent advances in tools and methodologies for mining open-source repositories have enabled developers to make data-driven software engineering decisions. In light of this, this paper proposes an approach for motivating data-driven programming evolution by applying frequent subgraph mining techniques to a large dataset of 166,827,154 open-source Java methods. The dataset is mined by generalizing Java control-flow graphs to capture broad programming language usages and instances of duplication. Frequent subgraphs are then extracted to identify potentially impactful opportunities for new syntactic sugars. Our diverse results demonstrate the benefits of the proposed technique by identifying new syntactic sugars involving a variety of programming constructs that could be implemented in Java, thus simplifying frequent code idioms. This approach can potentially provide valuable insights for Java language designers, and serve as a proof-of-concept for data-driven programming language design and evolution.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {203},
numpages = {12},
keywords = {syntactic sugars, data-driven language design, subgraph mining},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3528588,
title = {NLBSE '22: Proceedings of the 1st International Workshop on Natural Language-based Software Engineering},
year = {2022},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3528588.3528659,
author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole},
title = {Issue report classification using pre-trained language models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528659},
doi = {10.1145/3528588.3528659},
abstract = {This paper describes our participation in the tool competition organized in the scope of the 1st International Workshop on Natural Language-based Software Engineering. We propose a supervised approach relying on fine-tuned BERT-based language models for the automatic classification of GitHub issues. We experimented with different pre-trained models, achieving the best performance with fine-tuned RoBERTa (F1 = .8591).},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {29–32},
numpages = {4},
keywords = {BERT, deep learning, issue classification, labeling unstructured data, software maintenance and evolution},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3643787.3648043,
author = {Aracena, Gabriel and Luster, Kyle and Santos, Fabio and Steinmacher, Igor and Gerosa, Marco Aurelio},
title = {Applying Large Language Models to Issue Classification},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648043},
doi = {10.1145/3643787.3648043},
abstract = {Effective prioritization of issue reports in software engineering helps to optimize resource allocation and information recovery. However, manual issue classification is laborious and lacks scalability. As an alternative, many open source software (OSS) projects employ automated processes for this task, yet this relies on substantial datasets for adequate training. This research investigates an automated approach to issue classification based on Generative Pre-trained Transformers (GPT). By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a GPT-based approach to label issues accurately with a reduced training dataset. By reducing reliance on massive data requirements and focusing on few-shot fine-tuning, we found a more accessible and efficient solution for issue classification. Our model predicted issue labels in individual projects up to 93.2% in precision, 95% in recall, and 89.3% in F1-score.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {57–60},
numpages = {4},
keywords = {issue report classification, large language model, natural language processing, software engineering, labeling, multi-class classification, empirical study},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3597503.3608137,
author = {Feng, Sidong and Chen, Chunyang},
title = {Prompting Is All You Need: Automated Android Bug Replay with Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608137},
doi = {10.1145/3597503.3608137},
abstract = {Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {67},
numpages = {13},
keywords = {automated bug replay, large language model, prompt engineering},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639585,
author = {Imran, Mia Mohammad and Chatterjee, Preetha and Damevski, Kostadin},
title = {Shedding Light on Software Engineering-specific Metaphors and Idioms},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639585},
doi = {10.1145/3597503.3639585},
abstract = {Use of figurative language, such as metaphors and idioms, is common in our daily-life communications, and it can also be found in Software Engineering (SE) channels, such as comments on GitHub. Automatically interpreting figurative language is a challenging task, even with modern Large Language Models (LLMs), as it often involves subtle nuances. This is particularly true in the SE domain, where figurative language is frequently used to convey technical concepts, often bearing developer affect (e.g., 'spaghetti code). Surprisingly, there is a lack of studies on how figurative language in SE communications impacts the performance of automatic tools that focus on understanding developer communications, e.g., bug prioritization, incivility detection. Furthermore, it is an open question to what extent state-of-the-art LLMs interpret figurative expressions in domain-specific communication such as software engineering. To address this gap, we study the prevalence and impact of figurative language in SE communication channels. This study contributes to understanding the role of figurative language in SE, the potential of LLMs in interpreting them, and its impact on automated SE communication analysis. Our results demonstrate the effectiveness of fine-tuning LLMs with figurative language in SE and its potential impact on automated tasks that involve affect. We found that, among three state-of-the-art LLMs, the best improved fine-tuned versions have an average improvement of 6.66% on a GitHub emotion classification dataset, 7.07% on a GitHub incivility classification dataset, and 3.71% on a Bugzilla bug report prioritization dataset.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {207},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3641228,
author = {Kumar Dipongkor, Atish},
title = {An Ensemble Method for Bug Triaging using Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641228},
doi = {10.1145/3639478.3641228},
abstract = {This study delves into the automation of bug triaging --- the process of assigning bug reports to appropriate developers and components in software development. At the core of our investigation are six transformer-based Large Language Models (LLMs), which we fine-tuned using a sequence classification method tailored for bug triaging tasks. Our results demonstrate a noteworthy performance of the DeBERTa model, which significantly outperforms its counterparts CodeBERT, DistilBERT, RoBERTa, ALBERT, and BERT in terms of effectiveness. However, it is crucial to note that despite the varying performance of each model, each model exhibits a unique degree of orthogonality, indicating distinct strengths in their bug triaging capabilities. Leveraging these orthogonal characteristics, we propose an ensemble method combining these LLMs through voting and stacking techniques. Remarkably, our findings reveal that the voting-based ensemble method surpasses all individual baselines in performance.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {438–440},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3640024,
author = {Sapozhnikov, Arkadii and Olsthoorn, Mitchell and Panichella, Annibale and Kovalenko, Vladimir and Derakhshanfar, Pouria},
title = {TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640024},
doi = {10.1145/3639478.3640024},
abstract = {Writing software tests is laborious and time-consuming. To address this, prior studies introduced various automated test-generation techniques. A well-explored research direction in this field is unit test generation, wherein artificial intelligence (AI) techniques create tests for a method/class under test. While many of these techniques have primarily found applications in a research context, existing tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly and are tailored to a single technique. This paper introduces Test-Spark, a plugin for IntelliJ IDEA that enables users to generate unit tests with only a few clicks directly within their Integrated Development Environment (IDE). Furthermore, TestSpark also allows users to easily modify and run each generated test and integrate them into the project workflow. TestSpark leverages the advances of search-based test generation tools, and it introduces a technique to generate unit tests using Large Language Models (LLMs) by creating a feedback cycle between the IDE and the LLM. Since TestSpark is an open-source (https://github.com/JetBrains-Research/TestSpark), extendable, and well-documented tool, it is possible to add new test generation methods into the plugin with the minimum effort. This paper also explains our future studies related to TestSpark and our preliminary results. Demo video: https://youtu.be/0F4PrxWfiXo},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {30–34},
numpages = {5},
keywords = {unit test generation, intellij idea plugin, large language models},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644435,
author = {Sergeyuk, Agnia and Lvova, Olga and Titov, Sergey and Serova, Anastasiia and Bagirov, Farid and Kirillova, Evgeniia and Bryksin, Timofey},
title = {Reassessing Java Code Readability Models with a Human-Centered Approach},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644435},
doi = {10.1145/3643916.3644435},
abstract = {To ensure that Large Language Models (LLMs) effectively support user productivity, they need to be adjusted. Existing Code Readability (CR) models can guide this alignment. However, there are concerns about their relevance in modern software engineering since they often miss the developers' notion of readability and rely on outdated code. This research assesses existing Java CR models for LLM adjustments, measuring the correlation between their and developers' evaluations of AI-generated Java code. Using the Repertory Grid Technique with 15 developers, we identified 12 key code aspects influencing CR that were consequently assessed by 390 programmers when labeling 120 AI-generated snippets. Our findings indicate that when AI generates concise and executable code, it's often considered readable by CR models and developers. However, a limited correlation between these evaluations underscores the importance of future research on learning objectives for adjusting LLMs and on the aspects influencing CR evaluations included in predictive models.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {225–235},
numpages = {11},
keywords = {code readability, code readability models, repertory grid technique, AI-generated code, human-computer interaction},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639476.3639774,
author = {Saad, Mootez and Sharma, Tushar},
title = {Naturalness of Attention: Revisiting Attention in Code Language Models},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639774},
doi = {10.1145/3639476.3639774},
abstract = {Language models for code such as CodeBERT offer the capability to learn advanced source code representation, but their opacity poses barriers to understanding of captured properties. Recent attention analysis studies provide initial interpretability insights by focusing solely on attention weights rather than considering the wider context modeling of Transformers. This study aims to shed some light on the previously ignored factors of the attention mechanism beyond the attention weights. We conduct an initial empirical study analyzing both attention distributions and transformed representations in CodeBERT. Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone. Our analysis reveals characterization of how CodeBERT embeds syntactic code properties. The findings demonstrate the importance of incorporating factors beyond just attention weights for rigorously understanding neural code models. This lays the groundwork for developing more interpretable models and effective uses of attention mechanisms in program analysis.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {107–111},
numpages = {5},
keywords = {attention analysis, language models of code, norm analysis, interpretability, explainable AI},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639191,
author = {Gao, Xinyu and Wang, Zhijie and Feng, Yang and Ma, Lei and Chen, Zhenyu and Xu, Baowen},
title = {MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639191},
doi = {10.1145/3597503.3639191},
abstract = {Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF's potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-based and point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems.To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases' realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system's robustness. Our replication package and synthesized testing dataset are publicly available at https://sites.google.com/view/msftest.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {139},
numpages = {13},
keywords = {testing, multi-sensor fusion, perception systems},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643787.3648036,
author = {Ingemann Tuffveson Jensen, Rasmus and Tawosi, Vali and Alamir, Salwa},
title = {Software Vulnerability and Functionality Assessment using Large Language Models},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648036},
doi = {10.1145/3643787.3648036},
abstract = {While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final "approve or reject" recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {25–28},
numpages = {4},
keywords = {software security, functional validation, large language models},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00100,
author = {Gupta, Saksham and Verbruggen, Gust and Singh, Mukul and Gulwani, Sumit and Le, Vu},
title = {Personalized Action Suggestions in Low-Code Automation Platforms},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00100},
doi = {10.1109/ICSE-Companion58688.2023.00100},
abstract = {Automation platforms aim to automate repetitive tasks using workflows, which start with a trigger and then perform a series of actions. However, with many possible actions, the user has to search for the desired action at each step, which hinders the speed of flow development. We propose a personalized transformer model that recommends the next item at each step. This personalization is learned end-to-end from user statistics that are available at inference time. We evaluated our model on workflows from Power Automate users and show that personalization improves top-1 accuracy by 22%. For new users, our model performs similar to a model trained without personalization.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {346–350},
numpages = {5},
keywords = {transformers, personalization, prediction, decoder, recommendation system},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639226,
author = {Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
title = {Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639226},
doi = {10.1145/3597503.3639226},
abstract = {Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {82},
numpages = {13},
keywords = {code translation, bug taxonomy, llm},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639803,
author = {Al-Kaswan, Ali},
title = {Towards Safe, Secure, and Usable LLMs4Code},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639803},
doi = {10.1145/3639478.3639803},
abstract = {Large Language Models (LLMs) are gaining popularity in the field of Natural Language Processing (NLP) due to their remarkable accuracy in various NLP tasks. LLMs designed for coding are trained on massive datasets, which enables them to learn the structure and syntax of programming languages. These datasets are scraped from the web and LLMs memorise information in these datasets. LLMs for code are also growing, making them more challenging to execute and making users increasingly reliant on external infrastructure. We aim to explore the challenges faced by LLMs for code and propose techniques to measure and prevent memorisation. Additionally, we suggest methods to compress models and run them locally on consumer hardware.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {258–260},
numpages = {3},
keywords = {large language models, privacy, memorisation, data leakage, compression},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3650105.3652296,
author = {Wu, Jiahui and Lu, Chengjie and Arrieta, Aitor and Yue, Tao and Ali, Shaukat},
title = {Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652296},
doi = {10.1145/3650105.3652296},
abstract = {Large Language Models (LLMs) are demonstrating outstanding potential for tasks such as text generation, summarization, and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that LLMs can assess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empirical evaluation to assess whether LLMs are effective and robust in performing the task. This reality check is an important step towards devising LLM-based autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from DeepScenario-an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic scenarios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three LLMs (GPT-3.5, Llama2-13B, and Mistral-7B) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, GPT-3.5 achieved the highest robustness compared to Llama2-13B and Mistral-7B, consistently throughout almost all scenarios, roads, and weather conditions; (2) Mistral-7B performed the worst consistently; (3) Llama2-13B achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the LLMs.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {40–51},
numpages = {12},
keywords = {large language models, realistic driving scenarios, robustness},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@proceedings{10.1145/3641822,
title = {CHASE '24: Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {CHASE 2024 continues the tradition of a high-quality venue for research related to the cooperative and human aspects of software engineering. Researchers and practitioners have long recognized the need to investigate the cooperative and human aspects. However, their articles have been scattered across many conferences and communities. The CHASE conference provides academics and practitioners with a unified forum for discussing high-quality research studies, models, methods, and tools for human and cooperative aspects of software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE48619.2023.00112,
author = {Parasaram, Nikhil and Barr, Earl T. and Mechtaev, Sergey},
title = {Rete: Learning Namespace Representation for Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00112},
doi = {10.1109/ICSE48619.2023.00112},
abstract = {A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, defuse chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that Rete generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31% and 59% over the existing state of the art.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1264–1276},
numpages = {13},
keywords = {program repair, deep learning, patch prioritisation, variable representation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00136,
author = {Monjezi, Verya and Trivedi, Ashutosh and Tan, Gang and Tizpaz-Niari, Saeid},
title = {Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00136},
doi = {10.1109/ICSE48619.2023.00136},
abstract = {The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding minimal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions---amplifying existing biases or introducing new ones---that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids---such as severity and causal explanations---crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present DICE: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs.The key goal of DICE is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quantitative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that DICE efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1571–1582},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00209,
author = {Yadavally, Aashish and Nguyen, Tien N. and Wang, Wenbo and Wang, Shaohua},
title = {(Partial) Program Dependence Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00209},
doi = {10.1109/ICSE48619.2023.00209},
abstract = {Code fragments from developer forums often migrate to applications due to the code reuse practice. Owing to the incomplete nature of such programs, analyzing them to early determine the presence of potential vulnerabilities is challenging. In this work, we introduce NEURALPDA, a neural network-based program dependence analysis tool for both complete and partial programs. Our tool efficiently incorporates intrastatement and inter-statement contextual features into statement representations, thereby modeling program dependence analysis as a statement-pair dependence decoding task. In the empirical evaluation, we report that NEURALPDA predicts the CFG and PDG edges in complete Java and C/C++ code with combined F-scores of 94.29% and 92.46%, respectively. The F-score values for partial Java and C/C++ code range from 94.29%--97.17% and 92.46%--96.01%, respectively. We also test the usefulness of the PDGs predicted by NEURALPDA (i.e., PDG*) on the downstream task of method-level vulnerability detection. We discover that the performance of the vulnerability detection tool utilizing PDG* is only 1.1% less than that utilizing the PDGs generated by a program analysis tool. We also report the detection of 14 real-world vulnerable code snippets from StackOverflow by a machine learning-based vulnerability detection tool that employs the PDGs predicted by NEURALPDA for these code snippets.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2501–2513},
numpages = {13},
keywords = {neural partial program analysis, neural program dependence analysis, neural networks, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644398,
author = {Zhao, Zixiao and Das, Millon Madhur and Fard, Fatemeh},
title = {Studying Vulnerable Code Entities in R},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644398},
doi = {10.1145/3643916.3644398},
abstract = {Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years. These models are mainly targeted at popular programming languages such as Java and Python, leaving out many others like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R. In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples. We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java). We limit our study to code summarization. Our results show that the most vulnerable code entity is the identifier, followed by some syntax tokens specific to R. The results can shed light on the importance of token types and help in developing models for code summarization and method name prediction for the R language.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {328–332},
numpages = {5},
keywords = {R, pre-trained code language models},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639478.3640025,
author = {Todd, Liam and Grundy, John and Treude, Christoph},
title = {GitHubInclusifier: Finding and fixing non-inclusive language in GitHub Repositories},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640025},
doi = {10.1145/3639478.3640025},
abstract = {Non-inclusive language in software artefacts has been recognised as a serious problem. We describe a tool to find and fix non-inclusive language in a variety of GitHub repository artefacts. These include various README files, PDFs, code comments, and code. A wide variety of non-inclusive language including racist, ageist, ableist, violent and others are located and issues created, tagging the artefacts for checking. Suggested fixes can be generated using third-party LLM APIs, and approved changes made to documents, including code refactorings, and committed to the repository.The tool and evaluation data are available from: https://github.com/LiamTodd/github-inclusifierThe demo video is available at: https://www.youtube.com/watch?v=1z1QKdQg-nM},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {89–93},
numpages = {5},
keywords = {inclusive language, refactoring, biased language, inappropriate language, software documentation, software maintenance tools},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3623322,
author = {Zhang, Yakun and Zhang, Wenjie and Ran, Dezhi and Zhu, Qihao and Dou, Chengfeng and Hao, Dan and Xie, Tao and Zhang, Lu},
title = {Learning-based Widget Matching for Migrating GUI Test Cases},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623322},
doi = {10.1145/3597503.3623322},
abstract = {GUI test case migration is to migrate GUI test cases from a source app to a target app. The key of test case migration is widget matching. Recently, researchers have proposed various approaches by formulating widget matching as a matching task. However, since these matching approaches depend on static word embeddings without using contextual information to represent widgets and manually formulated matching functions, there are main limitations of these matching approaches when handling complex matching relations in apps. To address the limitations, we propose the first learning-based widget matching approach named TEMdroid (TEst Migration) for test case migration. Unlike the existing approaches, TEMdroid uses BERT to capture contextual information and learns a matching model to match widgets. Additionally, to balance the significant imbalance between positive and negative samples in apps, we design a two-stage training strategy where we first train a hard-negative sample miner to mine hard-negative samples, and further train a matching model using positive samples and mined hard-negative samples. Our evaluation on 34 apps shows that TEM-droid is effective in event matching (i.e., widget matching and target event synthesis) and test case migration. For event matching, TEM-droid's Top1 accuracy is 76%, improving over 17% compared to baselines. For test case migration, TEMdroid's F1 score is 89%, also 7% improvement compared to the baseline approach.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {69},
numpages = {13},
keywords = {test migration, GUI testing, deep learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00012,
author = {Zhu, Liming},
title = {Software Engineering as the Linchpin of Responsible AI},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00012},
doi = {10.1109/ICSE48619.2023.00012},
abstract = {From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {3–4},
numpages = {2},
keywords = {responsible AI, ethical AI, trustworthy AI, AI engineering, SE4AI},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00087,
author = {Kang, Sungmin and Yoo, Shin},
title = {GLAD: Neural Predicate Synthesis to Repair Omission Faults},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00087},
doi = {10.1109/ICSE-Companion58688.2023.00087},
abstract = {Existing template and learning-based Automated Program Repair (APR) tools have successfully found patches for many benchmark faults. However, our analysis of existing results shows that omission faults pose a significant challenge. For template based approaches, omission faults provide no location to apply templates to; for learning based approaches that formulate repair as Neural Machine Translation (NMT), omission faults similarly do not provide faulty code to translate. To address these issues, we propose GLAD, a novel learning-based repair technique that targets if-clause synthesis. GLAD does not require a concrete faulty line as it is based on generative Language Models (LMs) instead of machine translation; consequently, it can repair omission faults. To provide the LM with project-specific information critical to synthesis, we incorporate two components: a type-based grammar that constrains the model, and a dynamic ranking system that evaluates candidate patches using a debugger. Our evaluation shows GLAD is highly orthogonal to existing techniques, correctly fixing 26 Defects4J v1.2 faults that previous NMT-based techniques could not, while maintaining a small runtime cost, underscoring its potential as a lightweight tool to complement existing tools in practice. An inspection of the bugs that GLAD fixes reveals that GLAD can quickly generate expressions that would be challenging for other techniques.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {320–321},
numpages = {2},
keywords = {program repair, machine learning, debugging},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.5555/3623293,
title = {ICSE-SEIP '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@proceedings{10.1145/3643660,
title = {Designing '24: Proceedings of the 1st International Workshop on Designing Software},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The goals of this workshop are to: (1) bring together a group of researchers, practitioners, and educators interested in software design, (2) identify open challenges and new directions for the design of modern software systems, including grand challenges for the community, and (3) discuss novel approaches to designing as well as teaching design. Although the workshop welcomes discussions related to any aspect of software design, the primary focus will be on improving our understanding of design as an activity rather than as an artifact or end product (hence the word designing in the title).},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639081,
author = {Jiang, Yuxuan and Zhang, Chaoyun and He, Shilin and Yang, Zhihao and Ma, Minghua and Qin, Si and Kang, Yu and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Xpert: Empowering Incident Management with Query Recommendations via Large Language Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639081},
doi = {10.1145/3597503.3639081},
abstract = {Large-scale cloud systems play a pivotal role in modern IT infrastructure. However, incidents occurring within these systems can lead to service disruptions and adversely affect user experience. To swiftly resolve such incidents, on-call engineers depend on crafting domain-specific language (DSL) queries to analyze telemetry data. However, writing these queries can be challenging and time-consuming. This paper presents a thorough empirical study on the utilization of queries of KQL, a DSL employed for incident management in a large-scale cloud management system at Microsoft. The findings obtained underscore the importance and viability of KQL queries recommendation to enhance incident management.Building upon these valuable insights, we introduce Xpert, an end-to-end machine learning framework that automates KQL recommendation process. By leveraging historical incident data and large language models, Xpert generates customized KQL queries tailored to new incidents. Furthermore, Xpert incorporates a novel performance metric called Xcore, enabling a thorough evaluation of query quality from three comprehensive perspectives. We conduct extensive evaluations of Xpert, demonstrating its effectiveness in offline settings. Notably, we deploy Xpert in the real production environment of a large-scale incident management system in Microsoft, validating its efficiency in supporting incident management. To the best of our knowledge, this paper represents the first empirical study of its kind, and Xpert stands as a pioneering DSL query recommendation framework designed for incident management.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {92},
numpages = {13},
keywords = {incident management, query generation, large language model},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643517,
author = {P\^{a}rundefinedachi, Profir-Petru and Sugiyama, Mahito},
title = {Bringing Structure to Naturalness: On the Naturalness of ASTs},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643517},
doi = {10.1145/3639478.3643517},
abstract = {Source code comes in different shapes and forms. Previous research has already shown code to be more predictable than natural language at the token level: source code can be natural. More recently, the structure of code --- either as graphs or trees --- has been successfully used to improve the state-of-the-art on numerous tasks: code suggestion, code summarisation, method naming etc. This body of work implicitly assumes that structured representations of code are similarly statistically predictable, i.e. natural. We consider that this view should be made explicit and propose directly studying the Structured Naturalness Hypothesis. Beyond just naming existing research that assumes this hypothesis and formulating it, we also provide evidence for tree representations: TreeLSTM models over ASTs for some languages, such as Ruby, are competitive with n-gram models while handling the syntax token issue highlighted by previous research 'for free'. For other languages, such as Java or Python, we find tree models to perform worse, suggesting that downstream task improvement is uncorrelated to the language modelling task. Further, we show how one may use naturalness signals for near state-of-the-art results on just-in-time defect prediction without manual feature engineering work.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {378–379},
numpages = {2},
keywords = {naturalness, structure, AST, self-cross-entropy},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639475.3640111,
author = {J\"{a}rvenp\"{a}\"{a}, Heli and Lago, Patricia and Bogner, Justus and Lewis, Grace and Muccini, Henry and Ozkaya, Ipek},
title = {A Synthesis of Green Architectural Tactics for ML-Enabled Systems},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640111},
doi = {10.1145/3639475.3640111},
abstract = {The rapid adoption of artificial intelligence (AI) and machine learning (ML) has generated growing interest in understanding their environmental impact and the challenges associated with designing environmentally friendly ML-enabled systems. While Green AI research, i.e., research that tries to minimize the energy footprint of AI, is receiving increasing attention, very few concrete guidelines are available on how ML-enabled systems can be designed to be more environmentally sustainable. In this paper, we provide a catalog of 30 green architectural tactics for ML-enabled systems to fill this gap. An architectural tactic is a high-level design technique to improve software quality, in our case environmental sustainability. We derived the tactics from the analysis of 51 peer-reviewed publications that primarily explore Green AI, and validated them using a focus group approach with three experts. The 30 tactics we identified are aimed to serve as an initial reference guide for further exploration into Green AI from a software engineering perspective, and assist in designing sustainable ML-enabled systems. To enhance transparency and facilitate their widespread use and extension, we make the tactics available online in easily consumable formats. Wide-spread adoption of these tactics has the potential to substantially reduce the societal impact of ML-enabled systems regarding their energy and carbon footprint.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {130–141},
numpages = {12},
keywords = {software architecture, architectural tactics, ML-enabled systems, environmental sustainability, green AI},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@proceedings{10.1145/3639475,
title = {ICSE-SEIS'24: Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00075,
author = {Ronanki, Krishna},
title = {Towards an AI-Centric Requirements Engineering Framework for Trustworthy AI},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00075},
doi = {10.1109/ICSE-Companion58688.2023.00075},
abstract = {Ethical guidelines are an asset for artificial intelligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering(RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {278–280},
numpages = {3},
keywords = {trustworthy AI, EU AI act, requirements engineering, frameworks, AI co-worker, ethical AI, guidelines},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643666.3648579,
author = {Borg, Markus},
title = {Quality Requirements for Code: On the Untapped Potential in Maintainability Specifications},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643666.3648579},
doi = {10.1145/3643666.3648579},
abstract = {Quality requirements are critical for successful software engineering, with maintainability being a key internal quality. Despite significant attention in software metrics research, maintainability has attracted surprisingly little focus in the Requirements Engineering (RE) community. This position paper proposes a synergistic approach, combining code-oriented research with RE expertise, to create meaningful industrial impact. We introduce six illustrative use cases and propose three future research directions. Preliminary findings indicate that the established QUPER model, designed for setting quality targets, does not adequately address the unique aspects of maintainability.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Multi-Disciplinary, Open, and RElevant Requirements Engineering},
pages = {35–38},
numpages = {4},
keywords = {maintainability, quality requirements, source code quality},
location = {Lisbon, Portugal},
series = {MO2RE 2024}
}

@inproceedings{10.1109/ICSE48619.2023.00055,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Luo, Bin},
title = {CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00055},
doi = {10.1109/ICSE48619.2023.00055},
abstract = {Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value.In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of "training/evaluation" task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {537–549},
numpages = {13},
keywords = {pre-training of source code, cross-task transfer learning, few-shot learning, AI for SE},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3647632,
title = {MOBILESoft '24: Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE48619.2023.00106,
author = {Li, Zenan and Zhang, Maorun and Xu, Jingwei and Yao, Yuan and Cao, Chun and Chen, Taolue and Ma, Xiaoxing and L\"{u}, Jian},
title = {Lightweight Approaches to DNN Regression Error Reduction: An Uncertainty Alignment Perspective},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00106},
doi = {10.1109/ICSE48619.2023.00106},
abstract = {Regression errors of Deep Neural Network (DNN) models refer to the case that predictions were correct by the old-version model but wrong by the new-version model. They frequently occur when upgrading DNN models in production systems, causing disproportionate user experience degradation. In this paper, we propose a lightweight regression error reduction approach with two goals: 1) requiring no model retraining and even data, and 2) not sacrificing the accuracy. The proposed approach is built upon the key insight rooted in the unmanaged model uncertainty, which is intrinsic to DNN models, but has not been thoroughly explored especially in the context of quality assurance of DNN models. Specifically, we propose a simple yet effective ensemble strategy that estimates and aligns the two models' uncertainty. We show that a Pareto improvement that reduces the regression errors without compromising the overall accuracy can be guaranteed in theory and largely achieved in practice. Comprehensive experiments with various representative models and datasets confirm that our approaches significantly outperform the state-of-the-art alternatives.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1187–1199},
numpages = {13},
keywords = {software regression, deep neural networks, uncertainty alignment, model ensemble},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639103,
author = {Yang, Wenzhang and Song, Linhai and Xue, Yinxing},
title = {Rust-lancet: Automated Ownership-Rule-Violation Fixing with Behavior Preservation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639103},
doi = {10.1145/3597503.3639103},
abstract = {As a relatively new programming language, Rust is designed to provide both memory safety and runtime performance. To achieve this goal, Rust conducts rigorous static checks against its safety rules during compilation, effectively eliminating memory safety issues that plague C/C++ programs. Although useful, the safety rules pose programming challenges to Rust programmers, since programmers can easily violate safety rules when coding in Rust, leading their code to be rejected by the Rust compiler, a fact underscored by a recent user study. There exists a desire to automate the process of fixing safety-rule violations to enhance Rust's programmability.In this paper, we concentrate on Rust's ownership rules and develop rust-lancet to automatically fix their violations. We devise three strategies for altering code, each intended to modify a Rust program and make it pass Rust's compiler checks. Additionally, we introduce mental semantics to model the behaviors of Rust programs that cannot be compiled due to ownership-rule violations. We design an approach to verify whether modified programs preserve their original behaviors before patches are applied. We apply rust-lancet to 160 safety-rule violations from two sources, successfully fixing 102 violations under the optimal configuration --- more than rustc and six LLM-based techniques. Notably, rust-lancet avoids generating any incorrect patches, a distinction from all other baseline techniques. We also verify the effectiveness of each fixing strategy and behavior preservation validation and affirm the rationale behind these components.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {85},
numpages = {13},
keywords = {rust, program repair, compiler error},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643915.3644088,
author = {Li, Jialong and Zhang, Mingyue and Li, Nianyu and Weyns, Danny and Jin, Zhi and Tei, Kenji},
title = {Exploring the Potential of Large Language Models in Self-adaptive Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644088},
doi = {10.1145/3643915.3644088},
abstract = {Large Language Models (LLMs), with their abilities in knowledge acquisition and reasoning, can potentially enhance the various aspects of Self-adaptive Systems (SAS). Yet, the potential of LLMs in SAS remains largely unexplored and ambiguous, due to the lack of literature from flagship conferences or journals in the field, such as SEAMS and TAAS. The interdisciplinary nature of SAS suggests that drawing and integrating ideas from related fields, such as software engineering and autonomous agents, could unveil innovative research directions for LLMs within SAS. To this end, this paper reports the results of a literature review of studies in relevant fields, summarizes and classifies the studies relevant to SAS, and outlines their potential to specific aspects of SAS. Literature classification: www.github.com/545659928/LLM4SAS},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {77–83},
numpages = {7},
keywords = {large language model, self-adaptive systems, survey},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3639478.3641229,
author = {Yang, Zhou},
title = {Classifying Source Code: How Far Can Compressor-based Classifiers Go?},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641229},
doi = {10.1145/3639478.3641229},
abstract = {Pre-trained language models of code, which are built upon large-scale datasets, millions of trainable parameters, and high computational resources cost, have achieved phenomenal success. Recently, researchers have proposed a compressor-based classifier (Cbc); it trains no parameters but is found to outperform BERT. We conduct the first empirical study to explore whether this lightweight alternative can accurately classify source code. Our study is more than applying Cbc to code-related tasks. We first identify an issue that the original implementation overestimates Cbc. After correction, Cbc's performance on defect prediction drops from 80.7% to 63.0%, which is still comparable to CodeBERT (63.7%). We find that hyperparameter settings affect the performance. Besides, results show that Cbc can outperform CodeBERT when the training data is small, making it a good alternative in low-resource settings.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {450–452},
numpages = {3},
keywords = {defect software prediction, robustness, efficient learning},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643532,
author = {Kumar, Abhishek and Das, Partha Pratim and Chakrabarti, Partha Pratim},
title = {ICLNet: Stepping Beyond Dates for Robust Issue-Commit Link Recovery},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643532},
doi = {10.1145/3639478.3643532},
abstract = {In the field of software engineering, effectively managing software systems is essential. A key aspect of this management is the issue-commit link, which connects reported problems or enhancement requests (issues) with the actual code changes implemented in the software (commits). However, the robustness of various automated link recovery techniques, including the leading ML based model Hybrid Linker remains a subject of discussion. In this study, we investigate the Hybrid Linker model using interpretability tools like LIME and SHAP to understand its decision-making, especially its reliance on specific features. We assess its robustness against adversarial attacks, revealing its sensitivity to non-textual features like issue and commit dates. To address this, we introduce ICLNet (Issue Commit Link Network), which leverages BERT embeddings in a custom neural network. Our extensive adversarial tests show that ICLNet outperforms Hybrid Linker in adversarial settings, demonstrating greater resilience. ICLNet achieves a remarkable average F-score of 88.39% in adversarial scenarios, significantly surpassing Hybrid Linker's 62.11%. This confirms ICLNet's superiority in diverse conditions, highlighting its accuracy and robustness.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {408–409},
numpages = {2},
keywords = {LIME, SHAP, BERT, issue report, adversarial training, commit report, neural network},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/GREENS66463.2025.00016,
author = {Kaplan, Angelika and Keim, Jan and Greiner, Lukas and Sieger, Ralf and Mirandola, Raffaela and Reussner, Ralf},
title = {Responsible and Sustainable AI: Considering Energy Consumption in Automated Text Classification Evaluation Tasks},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00016},
doi = {10.1109/GREENS66463.2025.00016},
abstract = {Text classification is one of the typical and fundamental natural language processing tasks. With the advent of large language models (LLMs), text classification has evolved much further. Based on the growing sizes of LLMs and the increased demands for hardware, and especially energy, questions about sustainability and environmental impacts and responsibility also arise. To assess text classification approaches, researchers usually only use common performance metrics like precision, recall, and f&lt;inf&gt;1&lt;/inf&gt;-score. Green AI, i.e., improving environmental aspects while maintaining performance, is regularly disregarded and not a standard in the evaluation of automated text classification approaches. Yet, minor performance improvements might not justify, e.g., much higher energy consumption. In this paper, we aim to raise awareness for this issue and the corresponding trade-off discussions and decisions. Therefore, we present novel sustainability metrics and provide guidelines for text classification approaches that are suitable for Green AI. In a text classification use case, we showcase the applicability of our proposed metrics and discuss corresponding trade-off decisions.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {76–83},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1109/ICSE48619.2023.00207,
author = {Liu, Shangqing and Wu, Bozhi and Xie, Xiaofei and Meng, Guozhu and Liu, Yang},
title = {ContraBERT: Enhancing Code Pre-Trained Models via Contrastive Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00207},
doi = {10.1109/ICSE48619.2023.00207},
abstract = {Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple perturbations such as renaming variable names. This weakness may be inherited by their downstream models and thereby amplified at an unprecedented scale. To this end, we propose an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning. Specifically, we design nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants. Furthermore, we continue to train the existing pre-trained models by masked language modeling (MLM) and contrastive pre-training task on the original samples with their augmented variants to enhance the robustness of the model. The extensive experiments demonstrate that ContraBERT can effectively improve the robustness of the existing pre-trained models. Further study also confirms that these robustness-enhanced models provide improvements as compared to original models over four popular downstream tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2476–2487},
numpages = {12},
keywords = {code pre-trained models, contrastive learning, model robustness},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643660.3643942,
author = {Eisenreich, Tobias and Speth, Sandro and Wagner, Stefan},
title = {From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643942},
doi = {10.1145/3643660.3643942},
abstract = {Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {52–55},
numpages = {4},
keywords = {requirements, software architecture, architecture evaluation, LLM},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3597503.3639126,
author = {Zhou, Zhenhao and Sha, Chaofeng and Peng, Xin},
title = {On Calibration of Pre-trained Code Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639126},
doi = {10.1145/3597503.3639126},
abstract = {Pre-trained code models have achieved notable success in the field of Software Engineering (SE). However, existing studies have predominantly focused on improving model performance, with limited attention given to other critical aspects such as model calibration. Model calibration, which refers to the accurate estimation of predictive uncertainty, is a vital consideration in practical applications. Therefore, in order to advance the understanding of model calibration in SE, we conduct a comprehensive investigation into the calibration of pre-trained code models in this paper. Our investigation focuses on five pre-trained code models and four code understanding tasks, including analyses of calibration in both in-distribution and out-of-distribution settings. Several key insights are uncovered: (1) pre-trained code models may suffer from the issue of over-confidence; (2) temperature scaling and label smoothing are effective in calibrating code models in in-distribution data; (3) the issue of over-confidence in pre-trained code models worsens in different out-of-distribution settings, and the effectiveness of temperature scaling and label smoothing diminishes. All materials used in our experiments are available at https://github.com/queserasera22/Calibration-of-Pretrained-Code-Models.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {77},
numpages = {13},
keywords = {pre-trained code models, model calibration, model reliability},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639474.3640052,
author = {Cipriano, Bruno Pereira and Alves, Pedro},
title = {LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640052},
doi = {10.1145/3639474.3640052},
abstract = {Large Language Models (LLMs) have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of LLMs in OOP contexts. In this study, we experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. GPT-4 stood out as the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing LLMs with AATs in pedagogical settings. In conclusion, while GPT-4 showcases promise, the deployment of these models in OOP education still mandates supervision.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {162–169},
numpages = {8},
keywords = {programming assignments, teaching, object-oriented programming, object-oriented design, OOP best practices, large language models, GPT-3, GPT-4, bard},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3639478.3643529,
author = {Nazari, Amirmohammad and Chattopadhyay, Souti and Swayamdipta, Swabha and Raghothaman, Mukund},
title = {NomNom: Explanatory Function Names for Program Synthesizers},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643529},
doi = {10.1145/3639478.3643529},
abstract = {Despite great advances in program synthesis techniques, they remain algorithmic black boxes. Although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. One possibility to answer these questions is to use large language models to construct human-readable explanations. Unfortunately, experiments reveal that LLMs frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers. In this paper, we develop an approach to reliably augment the implementation with explanatory names. Experiments and user studies indicate that these names help users in understanding synthesized implementations.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {418–419},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643690.3648238,
author = {Jantunen, Marianna and Meyes, Richard and Kurchyna, Veronika and Meisen, Tobias and Abrahamsson, Pekka and Mohanani, Rahul},
title = {Researchers’ Concerns on Artificial Intelligence Ethics: Results from a Scenario-Based Survey},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648238},
doi = {10.1145/3643690.3648238},
abstract = {The ethical impacts of Artificial Intelligence (AI) are causing concern in many areas of AI research and development. The implementation of AI ethics is still, in many ways, a work in progress, but various initiatives are tackling the issues by creating guidelines and implementation methods. This study investigates concerns about the negative impacts of AI systems posed by researchers working with AI. The study was conducted as a scenario-based survey, in which participants answered the question, "What could go wrong?" regarding five scenarios depicting fictional AI systems. The study concludes with the results from 33 survey participants who gave 161 responses to the scenarios. The results suggest that researchers can identify threats posed by AI systems, particularly regarding their social and ethical consequences. This is even though half of the participants reported limited involvement with AI ethics in their work. The widespread understanding of ethics among researchers could positively impact AI software development due to increased capabilities to bring theoretical AI ethics to practice.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {24–31},
numpages = {8},
keywords = {artificial intelligence, AI ethics, AI impacts, qualitative study, survey},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/3639476.3639769,
author = {Kim, Myeongsoo and Stennett, Tyler and Shah, Dhruv and Sinha, Saurabh and Orso, Alessandro},
title = {Leveraging Large Language Models to Improve REST API Testing},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639769},
doi = {10.1145/3639476.3639769},
abstract = {The widespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machine-interpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {37–41},
numpages = {5},
keywords = {large language models for testing, OpenAPI specification analysis},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639129,
author = {Li, Haoran and Wang, Siqian and Quan, Weihong and Gong, Xiaoli and Su, Huayou and Zhang, Jin},
title = {Prism: Decomposing Program Semantics for Code Clone Detection through Compilation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639129},
doi = {10.1145/3597503.3639129},
abstract = {Code clone detection (CCD) is of critical importance in software engineering, while semantic similarity is a key evaluation factor for CCD. The embedding technique, which represents an object using a numerical vector, is utilized to generate code representations, where code snippets with similar semantics (clone pairs) should have similar vectors. However, due to the diversity and flexibility of high-level program languages, the code representation of clone pairs may be inconsistent. Assembly code provides the program execution trace and can normalize the diversity of high-level languages in terms of the program behavior semantics. After revisiting the assembly language, we find that different assembly codes can align with the computational logic and memory access patterns of cloned pairs. Therefore, the use of multiple assembly languages can capture the behavior semantics to enhance the understanding of programs. Thus, we propose Prism, a new method for code clone detection fusing behavior semantics from multiple architecture assembly code, which directly captures multilingual domains' syntax and semantic information. Additionally, we introduce a multi-feature fusion strategy that leverages global information interaction to expand the representation space. This fusion process allows us to capture the complementary information from each feature and leverage the relationships between them to create a more expressive representation of the code. After testing the OJClone dataset, the Prism model exhibited exceptional performance with precision and recall scores of 0.999 and 0.999, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {217},
numpages = {13},
keywords = {code clone detection, behavior semantics, CISC and RISC, feature fusion},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643521,
author = {Tang, Xunzhu and Tian, Haoye and Chen, Zhenghan and Pian, Weiguo and Ezzini, Saad and Kabore, Abdoul Kader and Habib, Andrew and Klein, Jacques and Bissyande, Tegawende F.},
title = {Learning to Represent Patches},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643521},
doi = {10.1145/3639478.3643521},
abstract = {We propose Patcherizer, a novel patch representation methodology that combines context and structure intention features to capture the semantic changes in Abstract Syntax Trees (ASTs) and surrounding context of code changes. Utilizing graph convolutional neural networks and transformers, Patcherizer effectively captures the underlying intentions of patches, outperforming state-of-the-art representations with significant improvements in BLEU, ROUGE-L, and METEOR metrics for generating patch descriptions.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {396–397},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3528588.3528655,
author = {Opitz, Dominik and Hochgeschwender, Nico},
title = {From zero to hero: generating training data for question-to-cypher models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528655},
doi = {10.1145/3528588.3528655},
abstract = {Graph databases employ graph structures such as nodes, attributes and edges to model and store relationships among data. To access this data, graph query languages (GQL) such as Cypher are typically used, which might be difficult to master for end-users. In the context of relational databases, sequence to SQL models, which translate natural language questions to SQL queries, have been proposed. While these Neural Machine Translation (NMT) models increase the accessibility of relational databases, NMT models for graph databases are not yet available mainly due to the lack of suitable parallel training data. In this short paper we sketch an architecture which enables the generation of synthetic training data for the graph query language Cypher.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {17–20},
numpages = {4},
keywords = {SQL, cypher, data generation, machine learning, neural machine translation},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3597503.3639116,
author = {Nong, Yu and Fang, Richard and Yi, Guangbei and Zhao, Kunsong and Luo, Xiapu and Chen, Feng and Cai, Haipeng},
title = {VGX: Large-Scale Sample Generation for Boosting Learning-Based Software Vulnerability Analyses},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639116},
doi = {10.1145/3597503.3639116},
abstract = {Accompanying the successes of learning-based defensive software vulnerability analyses is the lack of large and quality sets of labeled vulnerable program samples, which impedes further advancement of those defenses. Existing automated sample generation approaches have shown potentials yet still fall short of practical expectations due to the high noise in the generated samples. This paper proposes VGX, a new technique aimed for large-scale generation of high-quality vulnerability datasets. Given a normal program, VGX identifies the code contexts in which vulnerabilities can be injected, using a customized Transformer featured with a new value-flow-based position encoding and pre-trained against new objectives particularly for learning code structure and context. Then, VGX materializes vulnerability-injection code editing in the identified contexts using patterns of such edits obtained from both historical fixes and human knowledge about real-world vulnerabilities.Compared to four state-of-the-art (SOTA) (i.e., pattern-, Transformer-, GNN-, and pattern+Transformer-based) baselines, VGX achieved 99.09--890.06% higher F1 and 22.45%-328.47% higher label accuracy. For in-the-wild sample production, VGX generated 150,392 vulnerable samples, from which we randomly chose 10% to assess how much these samples help vulnerability detection, localization, and repair. Our results show SOTA techniques for these three application tasks achieved 19.15--330.80% higher F1, 12.86--19.31% higher top-10 accuracy, and 85.02--99.30% higher top-50 accuracy, respectively, by adding those samples to their original training data. These samples also helped a SOTA vulnerability detector discover 13 more real-world vulnerabilities (CVEs) in critical systems (e.g., Linux kernel) that would be missed by the original model.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {149},
numpages = {13},
keywords = {vulnerability dataset, vulnerability injection, data quality, vulnerability analysis, deep learning, program generation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643662.3643955,
author = {Heluany, Jessica and Amro, Ahmed and Gkioulos, Vasileios and Katsikas, Sokratis},
title = {Interplay of Digital Twins and Cyber Deception: Unraveling Paths for Technological Advancements},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643955},
doi = {10.1145/3643662.3643955},
abstract = {This research delves into the consolidation of Digital Twin and cyber deception technologies and explores their potential synergy for advancing cybersecurity processes. The study begins with a literature survey and market analysis, revealing a scarcity of mature scientific and commercial contributions in this domain. Most discussions remain theoretical, emphasizing the need for further research to address challenges and practically apply these technologies. Promising applications encompass cyber deception, anomaly detection, and threat intelligence, predominantly utilizing digital twin-based honeypots.The paper contributes by proposing a high-level deception framework tailored for Operational Technology (OT) systems, with seven pivotal functions for a deception network, emphasizing the replication of realistic systems, attracting attackers, controlling connections, monitoring activities, and analyzing detected events. Moreover, an evaluation via a SWOT analysis highlights various strengths, weaknesses, threats, and opportunities inherent in this framework identifying potentially innovative directions such as applications of digital twins, and artificial intelligence. Strengths include improved defender control and enhanced security analysis, while challenges revolve around achieving high realism in digital twins and managing restoration complexities. This study sets a roadmap for further exploration into the effective integration of Digital Twin and honeypot technologies in cybersecurity contexts.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {20–28},
numpages = {9},
keywords = {digital twin, deception, honeypot, cybersecurity},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1145/3639477.3639712,
author = {Hrusto, Adha and Runeson, Per and Ohlsson, Magnus C},
title = {Autonomous Monitors for Detecting Failures Early and Reporting Interpretable Alerts in Cloud Operations},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639712},
doi = {10.1145/3639477.3639712},
abstract = {Detecting failures early in cloud-based software systems is highly significant as it can reduce operational costs, enhance service reliability, and improve user experience. Many existing approaches include anomaly detection in metrics or a blend of metric and log features. However, such approaches tend to be very complex and hardly explainable, and consequently non-trivial for implementation and evaluation in industrial contexts. In collaboration with a case company and their cloud-based system in the domain of PIM (Product Information Management), we propose and implement autonomous monitors for proactive monitoring across multiple services of distributed software architecture, fused with anomaly detection in performance metrics and log analysis using GPT-3. We demonstrated that operations engineers tend to be more efficient by having access to interpretable alert notifications based on detected anomalies that contain information about implications and potential root causes. Additionally, proposed autonomous monitors turned out to be beneficial for the timely identification and revision of potential issues before they propagate and cause severe consequences.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {47–57},
numpages = {11},
keywords = {cloud, monitoring, anomaly detection, failures},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639476.3639778,
author = {Darvishi, Kasra and Noferesti, Morteza and Ezzati-Jivan, Naser},
title = {Toward Adaptive Tracing: Efficient System Behavior Analysis using Language Models},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639778},
doi = {10.1145/3639476.3639778},
abstract = {Tracing, a technique essential for unraveling the complexities of computer systems' behavior, involves the organized collection of low-level events, enabling anomaly identification, performance debugging, and root cause analysis. However, the significant overhead it imposes on large-scale systems, particularly in terms of performance and storage, has made it a less favorable tool for system maintenance. Previous efforts to mitigate tracing's burden have mostly centered around automating trace analysis but have primarily neglected the duration of events, a significant aspect of the information provided by tracers. To address these challenges, we propose an Adaptive Tracing method that leverages Language Models and kernel trace for precise system modeling. This novel approach minimizes overhead by recording detailed traces only during significant behavioral shifts and focusing on subsystems related to the root cause. Using a multi-task model, incorporating system call sequences and durations, we propose a root cause analysis method, enhancing model transparency and enabling targeted system tracing. Evaluation using a dataset of normal and noisy traces from an Apache server reveals that our Adaptive Tracer captures events related to abrupt changes with only 5.8% loss, reducing the collected trace by 77.1%, and accurately determining the respective noise set with 91.3% accuracy, outperforming previous state-of-the-art trace models by 20.9%.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {62–66},
numpages = {5},
keywords = {adaptive tracing, language model, root cause analysis, change detection, trace duration modeling, sequence modeling},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3639476.3639757,
author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Roney, Lindsay and Santos, Joanna C. S.},
title = {Re(gEx|DoS)Eval: Evaluating Generated Regular Expressions and their Proneness to DoS Attacks},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639757},
doi = {10.1145/3639476.3639757},
abstract = {With the recent advances of code generation techniques based on Large Language Models (LLMs), developers are using them for a vast range of tasks, including regex generation. Despite the efforts to generate regexes from natural language, there is no benchmark for LLMs with real-world data and robust test sets. Moreover, a regex can be prone to Denial of Service (DoS) attacks due to catastrophic backtracking. Hence, we need a systematic evaluation process to evaluate the correctness and security of the regexes generated by the language models. In this paper, we describe Re(gEx|DoS)Eval, a framework that includes a dataset of 762 regex descriptions (prompts) from real users, refined prompts with examples, and a robust set of tests. We introduce the pass@k and vulnerable@k metrics to evaluate the generated regexes based on the functional correctness and proneness of ReDoS attacks. Moreover, we demonstrate the Re(gEx|DoS)Eval with three LLMs (T5, Phi, and GPT-3), and describe the future plans to extend this framework.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {52–56},
numpages = {5},
keywords = {regex generation, ReDoS, DoS attack, evaluation, dataset},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639195,
author = {Dong, Chunhao and Jiang, Yanjie and Niu, Nan and Zhang, Yuxia and Liu, Hui},
title = {Context-Aware Name Recommendation for Field Renaming},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639195},
doi = {10.1145/3597503.3639195},
abstract = {Renaming is one of the most popular software refactorings. Although developers may know what the new name should be when they conduct a renaming, it remains valuable for refactoring tools to recommend new names automatically so that developers can simply hit Enter and efficiently accept the recommendation to accomplish the refactoring. Consequently, most IDEs automatically recommend new names for renaming refactorings by default. However, the recommendation made by mainstream IDEs is often incorrect. For example, the precision of IntelliJ IDEA in recommending names for field renamings is as low as 6.3%. To improve the accuracy, in this paper, we propose a context-aware lightweight approach (called CARER) to recommend new names for Java field renamings. Different from mainstream IDEs that rely heavily on initializers and data types of the to-be-renamed fields, CARER exploits both dynamic and static contexts of the renamings as well as naming conventions. We evaluate CARER on 1.1K real-world field renamings discovered from open-source applications. Our evaluation results suggest that CARER can significantly improve the state of the practice in recommending new names for field renamings, improving the precision from 6.30% to 61.15%, and recall from 6.30% to 41.50%. Our evaluation results also suggest that CARER is as efficient as IntelliJ IDEA is, making it suitable to be integrated into IDEs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {235},
numpages = {13},
keywords = {refactoring, rename, recommendation, context-aware},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3644032.3644459,
author = {Guo, Cheng-Yao and Yu, Fang},
title = {Sugar-coated poison defense on deepfake face-swapping attacks},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644459},
doi = {10.1145/3644032.3644459},
abstract = {The deployment of deepfake face-swapping technology has matured, becoming widespread on the Internet. The misuse of this technology raises significant concerns for application security and privacy. To counter deepfake threats, we propose a sugar-coated poison defense targeting the latent vectors of generative models. This strategy aims to impact visual effects without substantially increasing reconstruction loss. We establish metrics for visual effects and reconstruction loss to assess perturbation effects on latent vectors, emphasizing those with the most significant impact on visual effects while minimizing reconstruction loss. Our approach begins by utilizing a facial feature extraction model to convert faces into latent representations. We then introduce two latent selection methods: 1) shap-based latent selection using a linear regression model for approximation, and 2) grid search latent selection employing heuristics of adversarial attacks. These methods pinpoint vectors that, when perturbed, can increase face landmark distances while maintaining low mean square errors, commonly used as the optimization metric in deepfake reconstruction models. We apply inconsistent perturbations to selected latent vectors in video frames, acting as sugar-coated poison for deepfake face-swapping applications. Preliminary results demonstrate that these perturbations can be applied to individual videos, resulting in low reconstruction loss. Importantly, they induce measurable consistency reduction in deepfake videos, making them more discernible and accessible to identify.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {78–87},
numpages = {10},
keywords = {poison defense, deepfake, face-swapping},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1109/WETSEB66605.2025.00010,
author = {Farin, Mashiat Amin and Sikder, Sampad and Raad, Ashabul Yamin and Tahmeed Ahmed, Meah and Tahlil, Tahlil},
title = {HealthChain: A Blockchain-Based Framework for Electronic Health Record Management System},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WETSEB66605.2025.00010},
doi = {10.1109/WETSEB66605.2025.00010},
abstract = {Electronic health management systems (EHMS) play a crucial role in modern healthcare. They store important medical information such as patients’ medical histories, treatment plans, and diagnostic reports. However, the current EHMS landscape faces numerous challenges. These challenges hinder efficient data management, and collaboration among stakeholders, and compromise patient privacy and data security. Blockchain technology offers a promising solution to address these issues. By leveraging its decentralized and immutable nature, blockchain can create a secure, transparent, and tamper-resistant platform for storing, managing, and sharing electronic health records. This paper proposes an end-to-end solution called Healthchain based on a private permissioned blockchain network. Healthchain uses the Istanbul Byzantine fault tolerance (IBFT) consensus algorithm to reduce computation costs and increase speed. Secure multiparty computation allows medical data monetization without compromising privacy or data authenticity (ensured by blockchain) along with Zero knowledge proof and differential privacy for researchers. HealthChain also uses an efficient method for storing medical reports using IPFS, reducing the burden on single storage. It creates awareness about privacy issues and creates awareness about social engineering attacks based on the project. This paper explains the proposed framework, comparing performance with existing traditional and digital systems, and considers Bangladesh as an example to test its viability.},
booktitle = {Proceedings of the 2025 IEEE/ACM 7th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {26–33},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {WETSEB '25}
}

@inproceedings{10.1145/3643795.3648381,
author = {Singha, Ananya and Chopra, Bhavya and Khatry, Anirudh and Gulwani, Sumit and Henley, Austin and Le, Vu and Parnin, Chris and Singh, Mukul and Verbruggen, Gust},
title = {Semantically Aligned Question and Code Generation for Automated Insight Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648381},
doi = {10.1145/3643795.3648381},
abstract = {Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {127–134},
numpages = {8},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1109/ICSE48619.2023.00204,
author = {Le, Van-Hoang and Zhang, Hongyu},
title = {Log Parsing with Prompt-Based Few-Shot Learning},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00204},
doi = {10.1109/ICSE48619.2023.00204},
abstract = {Logs generated by large-scale software systems provide crucial information for engineers to understand the system status and diagnose problems of the systems. Log parsing, which converts raw log messages into structured data, is the first step to enabling automated log analytics. Existing log parsers extract the common part as log templates using statistical features. However, these log parsers often fail to identify the correct templates and parameters because: 1) they often overlook the semantic meaning of log messages, and 2) they require domain-specific knowledge for different log datasets. To address the limitations of existing methods, in this paper, we propose LogPPT to capture the patterns of templates using prompt-based few-shot learning. LogPPT utilises a novel prompt tuning method to recognise keywords and parameters based on a few labelled log data. In addition, an adaptive random sampling algorithm is designed to select a small yet diverse training set. We have conducted extensive experiments on 16 public log datasets. The experimental results show that LogPPT is effective and efficient for log parsing.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2438–2449},
numpages = {12},
keywords = {log parsing, few-shot learning, prompt-tuning, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3650105.3652288,
author = {Venkatesh, Ashwin Prasad Shivarpatna and Sabu, Samkutty and Mir, Amir M. and Reis, Sofia and Bodden, Eric},
title = {The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652288},
doi = {10.1145/3650105.3652288},
abstract = {The application of Large Language Models (LLMs) in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field. In this paper, we investigate the role that current LLMs can play in improving callgraph analysis and type inference for Python programs. Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 LLMs, including OpenAI's GPT series and open-source models such as LLaMA. Our study reveals that LLMs show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis. This contrast emphasizes the need for specialized fine-tuning of LLMs to better suit specific static analysis tasks. Our findings provide a foundation for further research towards integrating LLMs for static analysis tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {35–39},
numpages = {5},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3643690.3648245,
author = {Van Schothorst, Casper and Schuurmans, Robbert and Jansen, Slinger},
title = {Software Ecosystem Orchestration with Topic Modeling},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648245},
doi = {10.1145/3643690.3648245},
abstract = {Software ecosystems are typically networks of organizations collaboratively serving a market for software around a technical platform. Software ecosystem orchestrators are responsible for the health of these networks, for instance by attracting new applications to application stores and by opening new parts of the platform for extension. Considering the amounts of application data that orchestrators have to deal with, we propose an approach for strategic analysis of the software ecosystem that uses topic modeling. With our approach, orchestrators are supported in their task of growing and pruning their platform software ecosystem. We illustrate the use of the approach in two case studies of rapidly expanding and competing software platform ecosystems.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {72–78},
numpages = {7},
keywords = {software ecosystem health, domain analysis, repository mining, topic modelling applications, app store mining},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/3643787.3648042,
author = {Rejithkumar, Gokul and Anish, Preethu Rose and Ghaisas, Smita},
title = {Text-To-Text Generation for Issue Report Classification},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648042},
doi = {10.1145/3643787.3648042},
abstract = {We present our participation in the issue report classification tool competition at the 3rd International Workshop on Natural Language-based Software Engineering. Given the substantial influx of issue reports in large scale software development each day, there is a pronounced need for a system capable of automatically classifying issue reports into categories such as bugs, enhancements, and features. We propose a text-to-text generation-based supervised approach for issue report classification. We fine-tuned and evaluated our approach on the provided dataset of 3000 labeled issue reports (as bugs, enhancements, and questions). Our approach yielded an average cross-repo F1-score of 0.8297 across all classes, which is comparable to the SetFit baseline of 0.8270.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {53–56},
numpages = {4},
keywords = {issue report management, classification, repositories, transformers, text-to-text framework, T5, machine learning, natural language processing},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00098,
author = {Chimalakonda, Sridhar and Das, Debeshee and Mathai, Alex and Tamilselvam, Srikanth and Kumar, Atul},
title = {The Landscape of Source Code Representation Learning in AI-Driven Software Engineering Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00098},
doi = {10.1109/ICSE-Companion58688.2023.00098},
abstract = {Appropriate representation of source code and its relevant properties form the backbone of Artificial Intelligence (AI)/ Machine Learning (ML) pipelines for various software engineering (SE) tasks such as code classification, bug prediction, code clone detection, and code summarization. In the literature, researchers have extensively experimented with different kinds of source code representations (syntactic, semantic, integrated, customized) and ML techniques such as pre-trained BERT models. In addition, it is common for researchers to create hand-crafted and customized source code representations for an appropriate SE task. In a 2018 survey [1], Allamanis et al. listed nearly 35 different ways of of representing source code for different SE tasks like Abstract Syntax Trees (ASTs), customized ASTs, Control Flow Graphs (CFGs), Data Flow Graphs (DFGs) and so on. The main goal of this tutorial is two-fold (i) Present an overview of the state-of-the-art of source code representations and corresponding ML pipelines with an explicit focus on the merits and demerits of each of the representations (ii) Practical challenges in infusing different code-views in the state-of-the-art ML models and future research directions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {342–343},
numpages = {2},
keywords = {code representation, machine learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643796.3648465,
author = {Titov, Sergey and Grotov, Konstantin and Prasad S. Venkatesh, Ashwin},
title = {Hidden Gems in the Rough: Computational Notebooks as an Uncharted Oasis for IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648465},
doi = {10.1145/3643796.3648465},
abstract = {In this paper, we outline potential ways for the further development of computational notebooks in Integrated Development Environments (IDEs). We discuss notebooks integration with IDEs, focusing on three main areas: facilitating experimentation, adding collaborative features, and improving code comprehension. We propose that better support of notebooks will not only benefit the notebooks, but also enhance IDEs by supporting new development processes native to notebooks. In conclusion, we suggest that adapting IDEs for more experimentation-oriented notebook processes will prepare them for the future of AI-powered programming.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {107–109},
numpages = {3},
keywords = {Jupyter notebooks, integrated development environment, user studies, user experience},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3639478.3643114,
author = {Martinez, Matias and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {Energy Consumption of Automated Program Repair},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643114},
doi = {10.1145/3639478.3643114},
abstract = {In the last decade, following current societal needs, software sustainability has emerged as research field [2]. In this paper, we particularly focus on environmental sustainability, defined as "how software product development, maintenance, and use affect energy consumption and the consumption of other natural resources. [...] This dimension is also known as Green Software" [2].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {358–359},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@proceedings{10.5555/3623288,
title = {ICSE-NIER '23: Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3650105.3652301,
author = {Macedo, Marcos and Tian, Yuan and Cogo, Filipe and Adams, Bram},
title = {Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652301},
doi = {10.1145/3650105.3652301},
abstract = {Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {57–68},
numpages = {12},
keywords = {code translation, output format, large language model, LLM, software engineering, benchmarking, evaluation, empirical study, case study},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00089,
author = {Sun, Jiamou and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming and Hoang, Thong and Zhao, Dehai},
title = {Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00089},
doi = {10.1109/ICSE48619.2023.00089},
abstract = {Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {970–982},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528588.3528653,
author = {Liguori, Pietro and Improta, Cristina and De Vivo, Simona and Natella, Roberto and Cukic, Bojan and Cotroneo, Domenico},
title = {Can NMT understand me? towards perturbation-based evaluation of NMT models for code generation},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528653},
doi = {10.1145/3528588.3528653},
abstract = {Neural Machine Translation (NMT) has reached a level of maturity to be recognized as the premier method for the translation between different languages and aroused interest in different research areas, including software engineering. A key step to validate the robustness of the NMT models consists in evaluating the performance of the models on adversarial inputs, i.e., inputs obtained from the original ones by adding small amounts of perturbation. However, when dealing with the specific task of the code generation (i.e., the generation of code starting from a description in natural language), it has not yet been defined an approach to validate the robustness of the NMT models. In this work, we address the problem by identifying a set of perturbations and metrics tailored for the robustness assessment of such models. We present a preliminary experimental evaluation, showing what type of perturbations affect the model the most and deriving useful insights for future directions.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {59–66},
numpages = {8},
keywords = {adversarial inputs, code generation, neural machine translation, robustness testing},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3643690.3648235,
author = {Knutsen, Leif Z and Hannay, Jo E and Riegler, Michael A},
title = {Artificial Intelligence in the Public Sector -- An Agenda for Responsible Innovation through Learning},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648235},
doi = {10.1145/3643690.3648235},
abstract = {The optimism about the benefits of using artificial intelligence to innovate public services is tempered by concerns about its risks, limitations, and disbenefits. Given the rapid changes in the technology itself, the opportunities and needs for cross-sectional solutions, and the nascency of the field of AI-based innovation, we contend that policy, strategy, and implementation must include feedback loops that enable institutional learning for the entire public sector. The scope of challenges creates and imperative to facilitate learning must transcend functional, organizational, geographic, and national boundaries. We propose a learning agenda that includes 1) alignment of strategy and policy; 2) initial understanding of goals, benefits, disbenefits, limitations, and risks; 3) data sharing across jurisdictions; 4) technical robustness and societal alignment in governmental oversight; 5) convergence of architecture for AI support; and 6) a portfolio approach to selecting and learning from enabling service innovation with AI.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {1–6},
numpages = {6},
keywords = {artificial intelligence, public sector, feedback cycles, governance, technology strategy, privacy},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/3597503.3639216,
author = {Gao, Shuzheng and Mao, Wenxin and Gao, Cuiyun and Li, Li and Hu, Xing and Xia, Xin and Lyu, Michael R.},
title = {Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639216},
doi = {10.1145/3597503.3639216},
abstract = {Pre-trained code models have recently achieved substantial improvements in many code intelligence tasks. These models are first pre-trained on large-scale unlabeled datasets in a task-agnostic manner using self-supervised learning, and then fine-tuned on labeled datasets in downstream tasks. However, the labeled datasets are usually limited in size (i.e., human intensive efforts), which may hinder the performance of pre-trained code models in specific tasks. To mitigate this, one possible solution is to leverage the large-scale unlabeled data in the tuning stage by pseudo-labeling, i.e., generating pseudo labels for unlabeled data and further training the pre-trained code models with the pseudo-labeled data. However, directly employing the pseudo-labeled data can bring a large amount of noise, i.e., incorrect labels, leading to suboptimal performance. How to effectively leverage the noisy pseudo-labeled data is a challenging yet under-explored problem.In this paper, we propose a novel approach named HINT to improve pre-trained code models with large-scale unlabeled datasets by better utilizing the pseudo-labeled data. HINT includes two main modules: Hybrid pseudo-labeled data selection and Noise-tolerant Training. In the hybrid pseudo-data selection module, considering the robustness issue, apart from directly measuring the quality of pseudo labels through training loss, we propose to further employ a retrieval-based method to filter low-quality pseudo-labeled data. The noise-tolerant training module aims to further mitigate the influence of errors in pseudo labels by training the model with a noise-tolerant loss function and by regularizing the consistency of model predictions. We evaluate the effectiveness of HINT on three popular code intelligence tasks, including code summarization, defect detection, and assertion generation. We build our method on top of three popular open-source pre-trained code models. The experimental results show that HINT can better leverage those unlabeled data in a task-specific way and provide complementary benefits for pre-trained models, e.g., improving the best baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect detection, and assertion generation, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {80},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643533,
author = {Cai, Zeju and Chen, Jianguo and Chen, Wenqing and Wang, Weicheng and Zhu, Xiangyuan and Ouyang, Aijia},
title = {F-CodeLLM: A Federated Learning Framework for Adapting Large Language Models to Practical Software Development},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643533},
doi = {10.1145/3639478.3643533},
abstract = {Large Language Models (LLMs) have revolutionized code intelligence tasks, but their performance in specific software development tasks often requires fine-tuning with task-specific data. However, acquiring such data is challenging due to privacy concerns. We introduce F-CodeLLM, a novel federated learning framework for adapting LLMs to software development tasks while preserving code data privacy. Leveraging federated learning and LoRA-based efficient fine-tuning, F-CodeLLM allows organizations to collaboratively improve LLMs without sharing sensitive data. Our experiments demonstrate that F-CodeLLM achieves comparable results to centralized fine-tuning methods and excels in multi-language environments, marking a significant advancement in the application of LLMs for software engineering.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {416–417},
numpages = {2},
keywords = {code intelligence, federated fine-tuning, large language model, software development},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639108,
author = {Cao, Shaoheng and Pan, Minxue and Pei, Yu and Yang, Wenhua and Zhang, Tian and Wang, Linzhang and Li, Xuandong},
title = {Comprehensive Semantic Repair of Obsolete GUI Test Scripts for Mobile Applications},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639108},
doi = {10.1145/3597503.3639108},
abstract = {Graphical User Interface (GUI) testing is one of the primary approaches for testing mobile apps. Test scripts serve as the main carrier of GUI testing, yet they are prone to obsolescence when the GUIs change with the apps' evolution. Existing repair approaches based on GUI layouts or images prove effective when the GUI changes between the base and updated versions are minor, however, they may struggle with substantial changes. In this paper, a novel approach named COSER is introduced as a solution to repairing broken scripts, which is capable of addressing larger GUI changes compared to existing methods. COSER incorporates both external semantic information from the GUI elements and internal semantic information from the source code to provide a unique and comprehensive solution. The efficacy of COSER was demonstrated through experiments conducted on 20 Android apps, resulting in superior performance when compared to the state-of-the-art tools METER and GUIDER. In addition, a tool that implements the COSER approach is available for practical use and future research.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {90},
numpages = {13},
keywords = {GUI test script repair, Android testing, regression testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644897,
author = {Mir, Amir M. and Keshani, Mehdi and Proksch, Sebastian},
title = {On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644897},
doi = {10.1145/3643991.3644897},
abstract = {Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without pruning. We find that CG pruning is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%). However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding. Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {457–468},
numpages = {12},
keywords = {call graphs, machine learning, pruning, software analysis, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639477.3639746,
author = {Froemmgen, Alexander and Austin, Jacob and Choy, Peter and Ghelani, Nimesh and Kharatyan, Lera and Surita, Gabriela and Khrapko, Elena and Lamblin, Pascal and Manzagol, Pierre-Antoine and Revaj, Marcus and Tabachnyk, Maxim and Tarlow, Daniel and Villela, Kevin and Zheng, Daniel and Chandra, Satish and Maniatis, Petros},
title = {Resolving Code Review Comments with Machine Learning},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639746},
doi = {10.1145/3639477.3639746},
abstract = {Code reviews are a critical part of the software development process, taking a significant amount of the code authors' and the code reviewers' time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. At Google, we see millions of reviewer comments per year, and authors require an average of ~60 minutes active shepherding time between sending changes for review and finally submitting the change. In our measurements, the required active work time that the code author must devote to address reviewer comments grows almost linearly with the number of comments. However, with machine learning (ML), we have an opportunity to automate and streamline the code-review process, e.g., by proposing code changes based on a comment's text.We describe our application of recent advances in large sequence models in a real-world setting to automatically resolve code-review comments in the day-to-day development workflow at Google. We present the evolution of this feature from an asynchronous generation of suggested edits after the reviewer sends feedback, to an interactive experience that suggests code edits to the reviewer at review time. In deployment, code-change authors at Google address 7.5% of all reviewer comments by applying an ML-suggested edit. The impact of this will be to reduce the time spent on code reviews by hundreds of thousands of engineer hours annually at Google scale. Unsolicited, very positive feedback highlights that the impact of ML-suggested code edits increases Googlers' productivity and allows them to focus on more creative and complex tasks.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {204–215},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/ICSE48619.2023.00063,
author = {Mahbub, Parvez and Shuvo, Ohiduzzaman and Rahman, Mohammad Masudur},
title = {Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00063},
doi = {10.1109/ICSE48619.2023.00063},
abstract = {Software bugs claim ≈ 50% of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {640–652},
numpages = {13},
keywords = {software bug, bug explanation, software engineering, software maintenance, natural language processing, deep learning, transformers},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643660.3643947,
author = {Diaz-Pace, Jorge Andres and Garlan, David},
title = {The Architect in the Maze: On the Effective Usage of Automated Design Exploration},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643947},
doi = {10.1145/3643660.3643947},
abstract = {Designing a software architecture that satisfies a set of quality-attribute requirements has traditionally been a challenging activity for human architects, as it involves the exploration and assessment of alternative design decisions. The development of automated optimization tools for the architecture domain has opened new opportunities, because these tools are able to explore a large space of alternatives, and thus extend the architect's capabilities. In this context, however, architects need to efficiently navigate through a large space and understand the main relations between design decisions and feasible quality-attribute tradeoffs in a maze of possible alternatives. Although Machine Learning (ML) techniques can help to reduce the complexity of the task by sifting through the data generated by the tools, the standard techniques often fall short because they cannot offer architectural insights or relevant answers to the architect's questions. In this paper, and based on previous experiences, we argue that ML techniques should be adapted to the architecture domain, and propose a conceptual framework towards that goal. Furthermore, we show how the framework can be instantiated by adapting clustering techniques to answer architectural questions regarding a client-server design space.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {9–14},
numpages = {6},
keywords = {design exploration, automated tools, applied machine learning, quality attributes, explainability},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1145/3643664.3648207,
author = {Alami, Adam and Zahedi, Mansooreh and Ernst, Neil},
title = {Are You a Real Software Engineer? Best Practices in Online Recruitment for Software Engineering Studies},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648207},
doi = {10.1145/3643664.3648207},
abstract = {Online research platforms, such as Prolific, offer rapid access to diverse participant pools but also pose unique challenges in participant qualification and skill verification. Previous studies reported mixed outcomes and challenges in leveraging online platforms for the recruitment of qualified software engineers. Drawing from our experience in conducting three different studies using Prolific, we propose best practices for recruiting and screening participants to enhance the quality and relevance of both qualitative and quantitative software engineering (SE) research samples. We propose refined best practices for recruitment in SE research on Prolific. (1) Iterative and controlled prescreening, enabling focused and manageable assessment of submissions (2) task-oriented and targeted questions that assess technical skills, knowledge of basic SE concepts, and professional engagement. (3) AI detection to verify the authenticity of free-text responses. (4) Qualitative and manual assessment of responses, ensuring authenticity and relevance in participant answers (5) Additional layers of prescreening are necessary when necessary to collect data relevant to the topic of the study. (6) Fair or generous compensation post-qualification to incentivize genuine participation. By sharing our experiences and lessons learned, we contribute to the development of effective and rigorous methods for SE empirical research. particularly the ongoing effort to establish guidelines to ensure reliable data collection. These practices have the potential to transferability to other participant recruitment platforms.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {52–57},
numpages = {6},
keywords = {empirical software engineering, prolific, participant recruitment, online research platforms},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@proceedings{10.1145/3666015,
title = {ICSSP '24: Proceedings of the 2024 International Conference on Software and Systems Processes},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {M\, Germany}
}

@inproceedings{10.1145/3643664.3648204,
author = {Casta\~{n}o, Joel and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Franch, Xavier},
title = {Lessons Learned from Mining the Hugging Face Repository},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648204},
doi = {10.1145/3643664.3648204},
abstract = {The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing. This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models. Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies. We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis. Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach. The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to establish causality in repository mining studies, particularly within the ML model of HF context. This transition is inspired by existing frameworks and is adapted to suit the unique characteristics of the HF model ecosystem. Our report serves as a guiding framework for researchers, contributing to the responsible and sustainable advancement of ML, and fostering a deeper understanding of the broader implications of ML models.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {repository mining, cohort studies, research methodology},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@inproceedings{10.1145/3643788.3648015,
author = {Shariffdeen, Ridwan and Noller, Yannic and Mirchev, Martin and Ruan, Haifeng and Xiang, Gao and Costa, Andreea and Duck, Gregory J and Roychoudhury, Abhik},
title = {APR Competition 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648015},
doi = {10.1145/3643788.3648015},
abstract = {This report outlines the objectives, methodology, challenges, and results of the first Automated Program Repair Competition held at the APR Workshop 2024. The competition utilized Cerberus, a program repair framework, to evaluate the program repair tools using different repair configurations for each track in the competition. The competition was organized in three phases: first the participants integrated their tools with Cerberus, second the integrated tools were tested using public benchmarks and participants were able to fix any identified issues. In the last phase, the submitted tools and baseline comparison tools were evaluated against private benchmark programs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {46–49},
numpages = {4},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00075,
author = {Dong, Jinhao and Lou, Yiling and Hao, Dan and Tan, Lin},
title = {Revisiting Learning-Based Commit Message Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00075},
doi = {10.1109/ICSE48619.2023.00075},
abstract = {Commit messages summarize code changes and help developers understand the intention. To alleviate human efforts in writing commit messages, researchers have proposed various automated commit message generation techniques, among which learning-based techniques have achieved great success in recent years. However, existing evaluation on learning-based commit message generation relies on the automatic metrics (e.g., BLEU) widely used in natural language processing (NLP) tasks, which are aggregated scores calculated based on the similarity between generated commit messages and the ground truth. Therefore, it remains unclear what generated commit messages look like and what kind of commit messages could be precisely generated by existing learning-based techniques.To fill this knowledge gap, this work performs the first study to systematically investigate the detailed commit messages generated by learning-based techniques. In particular, we first investigate the frequent patterns of the commit messages generated by state-of-the-art learning-based techniques. Surprisingly, we find the majority (~90%) of their generated commit messages belong to simple patterns (i.e., addition/removal/fix/avoidance patterns). To further explore the reasons, we then study the impact of datasets, input representations, and model components. We surprisingly find that existing learning-based techniques have competitive performance even when the inputs are only represented by change marks (i.e., "+"/"-"/""). It indicates that existing learning-based techniques poorly utilize syntax and semantics in the code while mostly focusing on change marks, which could be the major reason for generating so many pattern-matching commit messages. We also find that the pattern ratio in the training set might also positively affect the pattern ratio of generated commit messages; and model components might have different impact on the pattern ratio.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {794–805},
numpages = {12},
keywords = {commit message generation, deep learning, pattern-based},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00185,
author = {Shi, Ensheng and Wang, Yanlin and Gu, Wenchao and Du, Lun and Zhang, Hongyu and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
title = {CoCoSoDa: Effective Contrastive Learning for Code Search},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00185},
doi = {10.1109/ICSE48619.2023.00185},
abstract = {Code search aims to retrieve semantically relevant code snippets for a given natural language query. Recently, many approaches employing contrastive learning have shown promising results on code representation learning and greatly improved the performance of code search. However, there is still a lot of room for improvement in using contrastive learning for code search. In this paper, we propose CoCoSoDa to effectively utilize contrastive learning for code search via two key factors in contrastive learning: data augmentation and negative samples. Specifically, soft data augmentation is to dynamically masking or replacing some tokens with their types for input sequences to generate positive samples. Momentum mechanism is used to generate large and consistent representations of negative samples in a mini-batch through maintaining a queue and a momentum encoder. In addition, multimodal contrastive learning is used to pull together representations of code-query pairs and push apart the unpaired code snippets and queries. We conduct extensive experiments to evaluate the effectiveness of our approach on a large-scale dataset with six programming languages. Experimental results show that: (1) CoCoSoDa outperforms 18 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation studies show the effectiveness of each component of our approach. (3) We adapt our techniques to several different pre-trained models such as RoBERTa, CodeBERT, and GraphCodeBERT and observe a significant boost in their performance in code search. (4) Our model performs robustly under different hyper-parameters. Furthermore, we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2198–2210},
numpages = {13},
keywords = {code search, contrastive learning, soft data augmentation, momentum mechanism},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3649105,
author = {Zhou, Minghui and Zhang, Yuxia and Tan, Xin},
title = {Open Source Software Digital Sociology: Quantifying and Managing Complex Open Source Software Ecosystem},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3649105},
doi = {10.1145/3643991.3649105},
abstract = {Open Source Software (OSS) ecosystems have revolutionized computing and society. However, the complex nature of their formation and sustainability presents significant challenges for practitioners and researchers. To understand and manage these complex ecosystems, we propose the concept of OSS digital sociology, aiming to uncover the mechanisms behind OSS ecosystems. This tutorial will illustrate why OSS digital sociology, and the challenges and research achievements in this field.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {743–744},
numpages = {2},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639474.3640083,
author = {Parthasarathy, P. D. and Joshi, Swaroop},
title = {Teaching Digital Accessibility to Industry Professionals using the Community of Practice framework: An Experience Report},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640083},
doi = {10.1145/3639474.3640083},
abstract = {Despite recent initiatives aimed at improving accessibility, the field of digital accessibility remains markedly behind contemporary advancements in the software industry, as many real-world software and web applications continue to fall short of accessibility requirements. A persisting skills deficit within the existing technology workforce has been an enduring impediment, hindering organizations from delivering truly accessible software products. This, in turn, elevates the risk of isolating and excluding a substantial portion of potential users. In this paper, we report lessons learned from a training program for teaching digital accessibility using the Communities of Practice (CoP) framework to industry professionals. We recruited 66 participants from a large multinational software company and assigned them to two groups: one participating in a CoP and the other using self-paced learning. We report experiences from designing the training program, conducting the actual training, and assessing the efficiency of the two approaches. Based on these findings, we provide recommendations for practitioners in Learning and Development teams and educators in designing accessibility courses for industry professionals.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {191–200},
numpages = {10},
keywords = {accessibility, massive open online courses, community of practice, computing education, global computing education},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3643691.3648589,
author = {Scantamburlo, Teresa and Falcarin, Paolo and Veneri, Alberto and Fabris, Alessandro and Gallese, Chiara and Billa, Valentina and Rotolo, Francesca and Marcuzzi, Federico},
title = {Software Systems Compliance with the AI Act: Lessons Learned from an International Challenge},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648589},
doi = {10.1145/3643691.3648589},
abstract = {In this experience paper, we present the lessons learned from the First University of St. Gallen Grand Challenge 2023, a competition involving interdisciplinary teams tasked with assessing the legal compliance of real-world AI-based systems with the European Union's Artificial Intelligence Act (AI Act). The AI Act is the very first attempt in the world to regulate AI systems and its potential impact is huge. The competition provided firsthand experience and practical knowledge regarding the AI Act's requirements. It also highlighted challenges and opportunities for the software engineering and AI communities.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {44–51},
numpages = {8},
keywords = {AI act, requirements engineering, legal compliance, requirements validation, conformity assessment},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00161,
author = {Nam, Daye and Myers, Brad and Vasilescu, Bogdan and Hellendoorn, Vincent},
title = {Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00161},
doi = {10.1109/ICSE48619.2023.00161},
abstract = {Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-learning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1890–1906},
numpages = {17},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639130,
author = {Keim, Jan and Corallo, Sophie and Fuch\ss{}, Dominik and Hey, Tobias and Telge, Tobias and Koziolek, Anne},
title = {Recovering Trace Links Between Software Documentation And Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639130},
doi = {10.1145/3597503.3639130},
abstract = {Introduction Software development involves creating various artifacts at different levels of abstraction and establishing relationships between them is essential. Traceability link recovery (TLR) automates this process, enhancing software quality by aiding tasks like maintenance and evolution. However, automating TLR is challenging due to semantic gaps resulting from different levels of abstraction. While automated TLR approaches exist for requirements and code, architecture documentation lacks tailored solutions, hindering the preservation of architecture knowledge and design decisions. Methods This paper presents our approach TransArC for TLR between architecture documentation and code, using component-based architecture models as intermediate artifacts to bridge the semantic gap. We create transitive trace links by combining the existing approach ArDoCo for linking architecture documentation to models with our novel approach ArCoTL for linking architecture models to code.Results We evaluate our approaches with five open-source projects, comparing our results to baseline approaches. The model-to-code TLR approach achieves an average F1-score of 0.98, while the documentation-to-code TLR approach achieves a promising average F1-score of 0.82, significantly outperforming baselines. Conclusion Combining two specialized approaches with an intermediate artifact shows promise for bridging the semantic gap. In future research, we will explore further possibilities for such transitive approaches.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {215},
numpages = {13},
keywords = {software traceability, software architecture, documentation, transitive links, intermediate artifacts, information retrieval},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00214,
author = {Zhou, Jiayuan and Pacheco, Michael and Chen, Jinfu and Hu, Xing and Xia, Xin and Lo, David and Hassan, Ahmed E.},
title = {CoLeFunDa: Explainable Silent Vulnerability Fix Identification},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00214},
doi = {10.1109/ICSE48619.2023.00214},
abstract = {It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (e.g., the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important.However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (i.e., code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, i.e., silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5% (25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2565–2577},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639214,
author = {Arefin, Mohammad Rifat and Shetiya, Suraj and Wang, Zili and Csallner, Christoph},
title = {Fast Deterministic Black-box Context-free Grammar Inference},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639214},
doi = {10.1145/3597503.3639214},
abstract = {Black-box context-free grammar inference is a hard problem as in many practical settings it only has access to a limited number of example programs. The state-of-the-art approach Arvada heuristically generalizes grammar rules starting from flat parse trees and is non-deterministic to explore different generalization sequences. We observe that many of Arvada's generalization steps violate common language concept nesting rules. We thus propose to pre-structure input programs along these nesting rules, apply learnt rules recursively, and make black-box context-free grammar inference deterministic. The resulting TreeVada yielded faster runtime and higher-quality grammars in an empirical comparison. The TreeVada source code, scripts, evaluation parameters, and training data are open-source and publicly available (https://doi.org/10.6084/m9.figshare.23907738).},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {117},
numpages = {12},
keywords = {grammar inference, oracle, nested language concepts, bracket-implied nesting structure, deterministic synthesis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00127,
author = {Meng, Xiangxin and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong and Hu, Chunming},
title = {Template-Based Neural Program Repair},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00127},
doi = {10.1109/ICSE48619.2023.00127},
abstract = {In recent years, template-based and NMT-based automated program repair methods have been widely studied and achieved promising results. However, there are still disadvantages in both methods. The template-based methods cannot fix the bugs whose types are beyond the capabilities of the templates and only use the syntax information to guide the patch synthesis, while the NMT-based methods intend to generate the small range of fixed code for better performance and may suffer from the OOV (Out-of-vocabulary) problem. To solve these problems, we propose a novel template-based neural program repair approach called TENURE to combine the template-based and NMT-based methods. First, we build two large-scale datasets for 35 fix templates from template-based method and one special fix template (single-line code generation) from NMT-based method, respectively. Second, the encoder-decoder models are adopted to learn deep semantic features for generating patch intermediate representations (IRs) for different templates. The optimized copy mechanism is also used to alleviate the OOV problem. Third, based on the combined patch IRs for different templates, three tools are developed to recover real patches from the patch IRs, replace the unknown tokens, and filter the patch candidates with compilation errors by leveraging the project-specific information. On Defects4J-v1.2, TENURE can fix 79 bugs and 52 bugs with perfect and Ochiai fault localization, respectively. It is able to repair 50 and 32 bugs as well on Defects4J-v2.0. Compared with the existing template-based and NMT-based studies, TENURE achieves the best performance in all experiments.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1456–1468},
numpages = {13},
keywords = {automated program repair, fix templates, neural machine translation, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623337,
author = {Ye, He and Monperrus, Martin},
title = {ITER: Iterative Neural Repair for Multi-Location Patches},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623337},
doi = {10.1145/3597503.3623337},
abstract = {Automated program repair (APR) has achieved promising results, especially using neural networks. Yet, the overwhelming majority of patches produced by APR tools are confined to one single location. When looking at the patches produced with neural repair, most of them fail to compile, while a few uncompilable ones go in the right direction. In both cases, the fundamental problem is to ignore the potential of partial patches. In this paper, we propose an iterative program repair paradigm called ITER founded on the concept of improving partial patches until they become plausible and correct. First, ITER iteratively improves partial single-location patches by fixing compilation errors and further refining the previously generated code. Second, ITER iteratively improves partial patches to construct multi-location patches, with fault localization re-execution. ITER is implemented for Java based on battle-proven deep neural networks and code representation. ITER is evaluated on 476 bugs from 10 open-source projects in Defects4J 2.0. ITER succeeds in repairing 15.5% of them, including 9 uniquely repaired multi-location bugs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {10},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3526072.3527531,
author = {Humeniuk, Dmytro and Antoniol, Giuliano and Khomh, Foutse},
title = {AmbieGen tool at the SBST 2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527531},
doi = {10.1145/3526072.3527531},
abstract = {AmbieGen is a tool for generating test cases for cyber-physical systems (CPS). In the context of SBST 2022 CPS tool competition, it has been adapted to generating virtual roads to test a car lane keeping assist system. AmbieGen leverages a two objective NSGA-II algorithm to produce the test cases. It has achieved the highest final score, accounting for the test case efficiency, effectiveness and diversity in both testing configurations.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {43–46},
numpages = {4},
keywords = {competition, genetic algorithm, test cases, virtual roads},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3643786.3648023,
author = {Sorokin, Lev and Kerscher, Niklas},
title = {Guiding the Search Towards Failure-Inducing Test Inputs Using Support Vector Machines},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648023},
doi = {10.1145/3643786.3648023},
abstract = {In this paper, we present NSGA-II-SVM (Non-dominated Sorting Genetic Algorithm with Support Vector Machine Guidance), a novel learnable evolutionary and search-based testing algorithm that leverages Support Vector Machine (SVM) classification models to direct the search towards failure-revealing test inputs. Supported by genetic search, NSGA-II-SVM creates iteratively SVM-based models of the test input space, learning which regions in the search space are promising to be explored. A subsequent sampling and repetition of evolutionary search iterations allow to refine and make the model more accurate in the prediction. Our preliminary evaluation of NSGA-II-SVM by testing an Automated Valet Parking system shows that NSGA-II-SVM is more effective in identifying more critical test cases than a state of the art learnable evolutionary testing technique as well as na\"{\i}ve random search.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {9–12},
numpages = {4},
keywords = {search-based testing, automated driving, machine learning, SVM},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@inproceedings{10.1145/3639477.3639754,
author = {Huang, Junjie and Liu, Jinyang and Chen, Zhuangbin and Jiang, Zhihan and Li, Yichen and Gu, Jiazhen and Feng, Cong and Yang, Zengyin and Yang, Yongqiang and Lyu, Michael R.},
title = {FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639754},
doi = {10.1145/3639477.3639754},
abstract = {Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA1, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies.To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets. It leverages hierarchy-guided contrastive learning to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations. We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that Fault-ProfIT outperforms state-of-the-art methods. Our ablation study and analysis also verify the effectiveness of hierarchy-guided contrastive learning. Additionally, we have deployed FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {392–404},
numpages = {13},
keywords = {incident management, incident tickets, fault patterns},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643991.3644923,
author = {Chen, Binger and Golebiowski, Jacek and Abedjan, Ziawasch},
title = {Data Augmentation for Supervised Code Translation Learning},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644923},
doi = {10.1145/3643991.3644923},
abstract = {Data-driven program translation has been recently the focus of several lines of research. A common and robust strategy is supervised learning. However, there is typically a lack of parallel training data, i.e., pairs of code snippets in the source and target language. While many data augmentation techniques exist in the domain of natural language processing, they cannot be easily adapted to tackle code translation due to the unique restrictions of programming languages. In this paper, we develop a novel rule-based augmentation approach tailored for code translation data, and a novel retrieval-based approach that combines code samples from unorganized big code repositories to obtain new training data. Both approaches are language-independent. We perform an extensive empirical evaluation on existing Java-C#-benchmarks showing that our method improves the accuracy of state-of-the-art supervised translation techniques by up to 35%.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {444–456},
numpages = {13},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644886,
author = {Ni, Chao and Shen, Liyu and Yang, Xiaohu and Zhu, Yan and Wang, Shaohua},
title = {MegaVul: A C/C++ Vulnerability Dataset with Comprehensive Code Representations},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644886},
doi = {10.1145/3643991.3644886},
abstract = {We constructed a newly large-scale and comprehensive C/C++ vulnerability dataset named MegaVul by crawling the Common Vulnerabilities and Exposures (CVE) database and CVE-related open-source projects. Specifically, we collected all crawlable descriptive information of the vulnerabilities from the CVE database and extracted all vulnerability-related code changes from 28 Git-based websites. We adopt advanced tools to ensure the extracted code integrality and enrich the code with four different transformed representations. Totally, MegaVul contains 17,380 vulnerabilities collected from 992 open-source repositories spanning 169 different vulnerability types disclosed from January 2006 to October 2023. Thus, MegaVul can be used for a variety of software security-related tasks including detecting vulnerabilities and assessing vulnerability severity. All information is stored in the JSON format for easy usage. MegaVul is publicly available on GitHub and will be continuously updated. It can be easily extended to other programming languages.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {738–742},
numpages = {5},
keywords = {common vulnerabilities and exposures, C/C++ code, code representation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643665.3648047,
author = {Lherondelle, Agathe and Babbar, Varun and Satsangi, Yash and Silavong, Fran and Eloul, Shaltiel and Moran, Sean},
title = {Topical: Automatic Repository Tagging using Attention on Hybrid Code Embeddings},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643665.3648047},
doi = {10.1145/3643665.3648047},
abstract = {This paper presents Topical, a novel deep neural network for repository level embeddings. Existing methods, reliant on natural language documentation or na\"{\i}ve aggregation techniques, are outperformed by Topical's utilization of an attention mechanism. This mechanism generates repository-level representations from source code, full dependency graphs, and script level textual data. Trained on publicly accessible GitHub repositories, Topical surpasses multiple baselines in tasks such as repository auto-tagging, highlighting the attention mechanism's efficacy over traditional aggregation methods. Topical also demonstrates scalability and efficiency, making it a valuable contribution to repository-level representation computation. For further research, the accompanying tools, code, and training dataset are provided at: https://github.com/jpmorganchase/topical.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
pages = {23–30},
numpages = {8},
keywords = {attention mechanism, repository tagging, code embedding},
location = {Lisbon, Portugal},
series = {FinanSE '24}
}

@inproceedings{10.1145/3643795.3648387,
author = {Li, Zhiming and Cao, Yushi and Xu, Xiufeng and Jiang, Junzhe and Liu, Xu and Teo, Yon Shin and Lin, Shang-Wei and Liu, Yang},
title = {LLMs for Relational Reasoning: How Far are We?},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648387},
doi = {10.1145/3643795.3648387},
abstract = {Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting1.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {119–126},
numpages = {8},
keywords = {large language models, relational reasoning, program induction},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{10.1109/ICSE48619.2023.00165,
author = {Zhao, Dehai and Xing, Zhenchang and Xia, Xin and Ye, Deheng and Xu, Xiwei and Zhu, Liming},
title = {SeeHow: Workflow Extraction from Programming Screencasts through Action-Aware Video Analytics},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00165},
doi = {10.1109/ICSE48619.2023.00165},
abstract = {Programming screencasts (e.g., video tutorials on Youtube or live coding stream on Twitch) are important knowledge source for developers to learn programming knowledge, especially the workflow of completing a programming task. Nonetheless, the image nature of programming screencasts limits the accessibility of screencast content and the workflow embedded in it, resulting in a gap to access and interact with the content and workflow in programming screencasts. Existing non-intrusive methods are limited to extract either primitive human-computer interaction (HCI) actions or coarse-grained video fragments. In this work, we leverage Computer Vision (CV) techniques to build a programming screencast analysis tool which can automatically extract code-line editing steps (enter text, delete text, edit text and select text) from screencasts. Given a programming screencast, our approach outputs a sequence of coding steps and code snippets involved in each step, which we refer to as programming workflow. The proposed method is evaluated on 41 hours of tutorial videos and live coding screencasts with diverse programming environments. The results demonstrate our tool can extract code-line editing steps accurately and the extracted workflow steps can be intuitively understood by developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1946–1957},
numpages = {12},
keywords = {screencast, computer vision, workflow extraction, action recognition},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00061,
author = {Yusuf, Imam Nur Bani},
title = {Towards Automated Embedded Systems Programming},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00061},
doi = {10.1109/ICSE-Companion58688.2023.00061},
abstract = {Writing code for embedded systems poses unique challenges due to hardware involvement. Developers often need to learn domain-specific knowledge to write embedded codes. Learning such knowledge is time-consuming and hinders developers' productivity. This paper presents a proposal for an automated code generation approach, specifically designed for embedded systems. The work is composed of three milestones, i.e., understanding the needs of embedded developers by analyzing posts from discussion forums, developing a tool to recommend driver libraries of I/O hardware and generate its interface configurations and usage patterns, and improving the generation accuracy of the prior tool using program analysis techniques. The tool will be evaluated using various metrics from machine translation, classification, and information retrieval fields.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {224–226},
numpages = {3},
keywords = {code generation, library recommendation, embedded system, hardware configuration},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643786.3648024,
author = {Mancu, Andrei and Laurent, Thomas and Rieger, Franz and Arcaini, Paolo and Ishikawa, Fuyuki and Rueckert, Daniel},
title = {More is Not Always Better: Exploring Early Repair of DNNs},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648024},
doi = {10.1145/3643786.3648024},
abstract = {DNN repair is an effective technique applied after training to enhance the class-specific accuracy of classifier models, where a low failure rate is required on specific classes. The repair methods introduced in recent studies assume that they are applied to fully trained models. In this paper, we argue that this could not always be the best choice. We analyse the performance of DNN models under various training times and repair combinations. Through meticulously designed experiments on two real-world datasets and a carefully curated assessment score, we show that applying DNN repair earlier in the training process, and not only at its end, can be beneficial. Thus, we encourage the research community to consider the idea of when to apply DNN repair in the model development.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {13–16},
numpages = {4},
keywords = {deep neural networks, DNN repair, DNN training, safety-critical},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@inproceedings{10.1145/3639478.3643124,
author = {Liu, Changshu and Cetin, Pelin and Patodia, Yogesh and Ray, Baishakhi and Chakraborty, Saikat and Ding, Yangruibo},
title = {Automated Code Editing with Search-Generate-Modify},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643124},
doi = {10.1145/3639478.3643124},
abstract = {Code editing is essential in evolving software development. In literature, several automated code editing tools are proposed, which leverage Information Retrieval-based techniques and Machine Learning-based code generation and code editing models.A patch that is obtained by search &amp; retrieval, even if incorrect, can provide helpful guidance to a code generation model. However, a retrieval-guided patch produced by a code generation model can still be a few tokens off from the intended patch. Such generated patches can be slightly modified to create the intended patches. We propose SarGaM which mimics a developer's behavior - search for related patches, generate or write code and then modify to adapt it to the right context. Our evaluation of SarGaM on edit generation shows superior performance w.r.t. the current state-of-the-art techniques. SarGaM also shows its effectiveness on automated program repair tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {398–399},
numpages = {2},
keywords = {bug fixing, automated program repair, edit-based neural network},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00021,
author = {Wu, Xiongfei and Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Hu, Yujing and Huang, Ruochen and Ma, Lei and Zhao, Jianjun},
title = {Widget Detection-Based Testing for Industrial Mobile Games},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00021},
doi = {10.1109/ICSE-SEIP58684.2023.00021},
abstract = {The fast advances in mobile hardware and widespread smartphone usage have fueled the growth of global mobile gaming in the past decade. As a result, the need for quality assurance of mobile gaming has become increasingly pressing. While general-purpose testing methods have been developed for mobile applications, they become struggling when being applied to mobile games due to the unique characteristics of mobile games, such as dynamic loading and stunning visual effects. There comes a growing industrial demand for automated testing techniques with high compatibility (compatible with various resolutions, and platforms) and non-intrusive characteristics (without packaging external modules into the source code, e.g., POCO). To fulfill these demands, in this paper, we introduce our experience in adopting the widget detection-based testing technique WDTest, for mobile games at NetEase Games. To this end, we have constructed by far the largest graphical user interface (GUI) dataset for mobile games and conducted comprehensive evaluations on the performance of state-of-the-art widget detection techniques in the context of mobile gaming.We leverage widget detection techniques to develop WDTest, which performs automated testing using only screenshots as input. Our evaluation shows that WDTest outperforms the widely used tool Monkey in achieving three times more coverage of unique UI in gaming scenarios. Our further experiments demonstrate that WDTest can be applied to general mobile applications without additional fine-tuning. Furthermore, we conducted a thorough survey at NetEase Games to gain a comprehensive understanding of widget detection-based testing techniques and identify challenges in industrial mobile game testing. The results show that testers are overall satisfied with the compatibility testing aspect of widget detection-based testing, but not much with functionality testing. This survey also highlights several unique characteristics of mobile games, providing valuable insights for future research directions.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {173–184},
numpages = {12},
keywords = {mobile game testing, GUI testing, GUI detection, software quality assurance},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3528588.3528664,
author = {Kallis, Rafael and Chaparro, Oscar and Di Sorbo, Andrea and Panichella, Sebastiano},
title = {NLBSE'22 tool competition},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528664},
doi = {10.1145/3528588.3528664},
abstract = {We report on the organization and results of the first edition of the Tool Competition from the International Workshop on Natural Language-based Software Engineering (NLBSE'22). This year, five teams submitted multiple classification models to automatically classify issue reports as bugs, enhancements, or questions. Most of them are based on BERT (Bidirectional Encoder Representations from Transformers) and were fine-tuned and evaluated on a benchmark dataset of 800k issue reports. The goal of the competition was to improve the classification performance of a baseline model based on fastText. This report provides details of the competition, including its rules, the teams and contestant models, and the ranking of models based on their average classification performance across the issue types.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {25–28},
numpages = {4},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3597503.3639181,
author = {Wang, Zhaohui and Zhang, Min and Yang, Jingran and Shao, Bojie and Zhang, Min},
title = {MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639181},
doi = {10.1145/3597503.3639181},
abstract = {Deep neural networks (DNNs) have shown powerful performance in various applications and are increasingly being used in decisionmaking systems. However, concerns about fairness in DNNs always persist. Some efficient white-box fairness testing methods about individual fairness have been proposed. Nevertheless, the development of black-box methods has stagnated, and the performance of existing methods is far behind that of white-box methods. In this paper, we propose a novel black-box individual fairness testing method called Model-Agnostic Fairness Testing (MAFT). By leveraging MAFT, practitioners can effectively identify and address discrimination in DL models, regardless of the specific algorithm or architecture employed. Our approach adopts lightweight procedures such as gradient estimation and attribute perturbation rather than non-trivial procedures like symbol execution, rendering it significantly more scalable and applicable than existing methods. We demonstrate that MAFT achieves the same effectiveness as state-of-the-art white-box methods whilst improving the applicability to large-scale networks. Compared to existing black-box approaches, our approach demonstrates distinguished performance in discovering fairness violations w.r.t effectiveness (~ 14.69\texttimes{}) and efficiency (~ 32.58\texttimes{}).},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {121},
numpages = {12},
keywords = {software bias, fairness testing, test case generation, deep neural network},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00200,
author = {Wang, Wenxuan and Huang, Jen-tse and Wu, Weibin and Zhang, Jianping and Huang, Yizhan and Li, Shuqing and He, Pinjia and Lyu, Michael R.},
title = {MTTM: Metamorphic Testing for Textual Content Moderation Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00200},
doi = {10.1109/ICSE48619.2023.00200},
abstract = {The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness (0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2387–2399},
numpages = {13},
keywords = {software testing, metamorphic relations, NLP software, textual content moderation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3608139,
author = {Mahmud, Junayed and De Silva, Nadeeshan and Khan, Safwat Ali and Mostafavi, Seyed Hooman and Mansur, S M Hasan and Chaparro, Oscar and Marcus, Andrian (Andi) and Moran, Kevin},
title = {On Using GUI Interaction Data to Improve Text Retrieval-based Bug Localization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608139},
doi = {10.1145/3597503.3608139},
abstract = {One of the most important tasks related to managing bug reports is localizing the fault so that a fix can be applied. As such, prior work has aimed to automate this task of bug localization by formulating it as an information retrieval problem, where potentially buggy files are retrieved and ranked according to their textual similarity with a given bug report. However, there is often a notable semantic gap between the information contained in bug reports and identifiers or natural language contained within source code files. For user-facing software, there is currently a key source of information that could aid in bug localization, but has not been thoroughly investigated - information from the graphical user interface (GUI).In this paper, we investigate the hypothesis that, for end user-facing applications, connecting information in a bug report with information from the GUI, and using this to aid in retrieving potentially buggy files, can improve upon existing techniques for text retrieval-based bug localization. To examine this phenomenon, we conduct a comprehensive empirical study that augments four baseline text-retrieval techniques for bug localization with GUI interaction information from a reproduction scenario to (i) filter out potentially irrelevant files, (ii) boost potentially relevant files, and (iii) reformulate text-retrieval queries. To carry out our study, we source the current largest dataset of fully-localized and reproducible real bugs for Android apps, with corresponding bug reports, consisting of 80 bug reports from 39 popular open-source apps. Our results illustrate that augmenting traditional techniques with GUI information leads to a marked increase in effectiveness across multiple metrics, including a relative increase in Hits@10 of 13--18%. Additionally, through further analysis, we find that our studied augmentations largely complement existing techniques, pushing additional buggy files into the top-10 results while generally preserving top ranked files from the baseline techniques.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {40},
numpages = {13},
keywords = {bug localization, GUI, natural language processing, mobile apps},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639475.3640099,
author = {Tiwari, Deepika and Toady, Tim and Monperrus, Martin and Baudry, Benoit},
title = {With Great Humor Comes Great Developer Engagement},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640099},
doi = {10.1145/3639475.3640099},
abstract = {The worldwide collaborative effort for the creation of software is technically and socially demanding. The more engaged developers are, the more value they impart to the software they create. Engaged developers, such as Margaret Hamilton programming Apollo 11, can succeed in tackling the most difficult engineering tasks. In this paper, we dive deep into an original vector of engagement - humor - and study how it fuels developer engagement. First, we collect qualitative and quantitative data about the humorous elements present within three significant, real-world software projects: faker, which helps developers introduce humor within their tests; lolcommits, which captures a photograph after each contribution made by a developer; and volkswagen, an exercise in satire, which accidentally led to the invention of an impactful software tool. Second, through a developer survey, we receive unique insights from 125 developers, who share their real-life experiences with humor in software.Our analysis of the three case studies highlights the prevalence of humor in software, and unveils the worldwide community of developers who are enthusiastic about both software and humor. We also learn about the caveats of humor in software through the valuable insights shared by our survey respondents. We report clear evidence that, when practiced responsibly, humor increases developer engagement and supports them in addressing hard engineering and cognitive tasks. The most actionable highlight of our work is that software tests and documentation are the best locations in code to practice humor.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {1–11},
numpages = {11},
keywords = {humor, developer engagement, responsibility, culture, faking},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@proceedings{10.1145/3643656,
title = {FTW '24: Proceedings of the 1st International Workshop on Flaky Tests},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Flaky tests are tests that exhibit inconsistent behavior, producing different outcomes when executed multiple times without any changes to the code under test or the test itself. Test flakiness poses a significant challenge for software development teams, as it undermines the reliability and validity of the testing process, leading to wasted time, effort, and resources, as well as reduced confidence in the quality of the software product. Flaky tests as a research topic has grown in interest significantly within the software engineering community in recent years. This has produced a wide array of empirical studies on the causes of flaky tests and experimental tools for their detection and repair. Despite these considerable advances, flakiness remains to be one of the main challenges software developers are facing today.The 1st International Flaky Tests Workshop 2024 (FTW 2024), co-located with ICSE 2024, aims to bring together researchers and practitioners from academia and industry to share their insights, experiences, and solutions on the topic of flaky tests. The workshop will feature a diverse program, including a keynote speech, paper presentations, and a panel discussion. Furthermore, the workshop will also provide a platform for networking and collaboration among the participants, fostering a vibrant research community on this emerging and important topic.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639109,
author = {Wang, Jun and Li, Yanhui and Chen, Zhifei and Chen, Lin and Zhang, Xiaofang and Zhou, Yuming},
title = {Knowledge Graph Driven Inference Testing for Question Answering Software},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639109},
doi = {10.1145/3597503.3639109},
abstract = {In the wake of developments in the field of Natural Language Processing, Question Answering (QA) software has penetrated our daily lives. Due to the data-driven programming paradigm, QA software inevitably contains bugs, i.e., misbehaving in real-world applications. Current testing techniques for testing QA software include two folds, reference-based testing and metamorphic testing.This paper adopts a different angle to achieve testing for QA software: we notice that answers to questions would have inference relations, i.e., the answers to some questions could be logically inferred from the answers to other questions. If these answers on QA software do not satisfy the inference relations, an inference bug is detected. To generate the questions with the inference relations automatically, we propose a novel testing method Knowledge Graph driven Inference Testing (KGIT), which employs facts in the Knowledge Graph (KG) as the seeds to logically construct test cases containing questions and contexts with inference relations. To evaluate the effectiveness of KGIT, we conduct an extensive empirical study with more than 2.8 million test cases generated from the large-scale KG YAGO4 and three QA models based on the state-of-the-art QA model structure. The experimental results show that our method (a) could detect a considerable number of inference bugs in all three studied QA models and (b) is helpful in retraining QA models to improve their inference ability.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {119},
numpages = {13},
keywords = {question answering, software testing, knowledge graph, inference rules},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639213,
author = {Luo, Feng and Luo, Ruijie and Chen, Ting and Qiao, Ao and He, Zheyuan and Song, Shuwei and Jiang, Yu and Li, Sixing},
title = {SCVHunter: Smart Contract Vulnerability Detection Based on Heterogeneous Graph Attention Network},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639213},
doi = {10.1145/3597503.3639213},
abstract = {Smart contracts are integral to blockchain's growth, but their vulnerabilities pose a significant threat. Traditional vulnerability detection methods rely heavily on expert-defined complex rules that are labor-intensive and dificult to adapt to the explosive expansion of smart contracts. Some recent studies of neural network-based vulnerability detection also have room for improvement. Therefore, we propose SCVHunter, an extensible framework for smart contract vulnerability detection. Specifically, SCVHunter designs a heterogeneous semantic graph construction phase based on intermediate representations and a vulnerability detection phase based on a heterogeneous graph attention network for smart contracts. In particular, SCVHunter allows users to freely point out more important nodes in the graph, leveraging expert knowledge in a simpler way to aid the automatic capture of more information related to vulnerabilities. We tested SCVHunter on reentrancy, block info dependency, nested call, and transaction state dependency vulnerabilities. Results show remarkable performance, with accuracies of 93.72%, 91.07%, 85.41%, and 87.37% for these vulnerabilities, surpassing previous methods.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {170},
numpages = {13},
keywords = {blockchain, smart contract, vulnerability detection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644928,
author = {Shahini, Xhulja and Metzger, Andreas and Pohl, Klaus},
title = {An Empirical Study on Just-in-time Conformal Defect Prediction},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644928},
doi = {10.1145/3643991.3644928},
abstract = {Code changes can introduce defects that affect software quality and reliability. Just-in-time (JIT) defect prediction techniques provide feedback at check-in time on whether a code change is likely to contain defects. This immediate feedback allows practitioners to make timely decisions regarding potential defects. However, a prediction model may deliver false predictions, that may negatively affect practitioners' decisions. False positive predictions lead to unnecessarily spending resources on investigating clean code changes, while false negative predictions may result in overlooking defective changes. Knowing how uncertain a defect prediction is, would help practitioners to avoid wrong decisions. Previous research in defect prediction explored different approaches to quantify prediction uncertainty for supporting decision-making activities. However, these approaches only offer a heuristic quantification of uncertainty and do not provide guarantees.In this study, we use conformal prediction (CP) as a rigorous uncertainty quantification approach on top of JIT defect predictors. We assess how often CP can provide guarantees for JIT defect predictions. We also assess how many false JIT defect predictions CP can filter out. We experiment with two state-of-the-art JIT defect prediction techniques (DeepJIT and CC2Vec) and two widely used datasets (Qt and OpenStack).Our experiments show that CP can ensure correctness with a 95% probability, for only 27% (for DeepJIT) and 9% (for CC2Vec) of the JIT defect predictions. Additionally, our experiments indicate that CP might be a valuable technique for filtering out the false predictions of JIT defect predictors. CP can filter out up to 100% of false negative predictions and 90% of false positives generated by CC2Vec, and up to 86% of false negative predictions and 83% of false positives generated by DeepJIT.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {88–99},
numpages = {12},
keywords = {defect prediction, quality assurance, conformal prediction, machine learning, deep learning, correctness guarantees, uncertainty},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00079,
author = {Mansur, S M Hasan},
title = {Toward Automated Tools to Support Ethical GUI Design},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00079},
doi = {10.1109/ICSE-Companion58688.2023.00079},
abstract = {Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack the guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. This paper depicts promising results from preliminary work that inspire towards addressing this problem and proposes a comprehensive research agenda that is aimed at implementing a developer-facing automated tool to support ethical GUI design.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {294–298},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643786.3648025,
author = {Li Calsi, Davide and Laurent, Thomas and Arcaini, Paolo and Ishikawa, Fuyuki},
title = {Federated Repair of Deep Neural Networks},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648025},
doi = {10.1145/3643786.3648025},
abstract = {As DNNs are embedded in more and more critical systems, it is essential to ensure that they perform well on specific inputs. DNN repair has shown good results in fixing specific misclassifications in already trained models using additional data, even surpassing additional training. In safety-critical applications, such as autonomous driving, collaboration between industrial actors would lead to more representative datasets for repair, that would enable to obtain more robust models and thus safer systems. However, these companies are reluctant to share their data, to both protect their intellectual property and the privacy of their users. Federated Learning is an approach that allows for collaborative, privacy-preserving training of DNNs. Inspired by this technique, this work proposes Federated Repair in order to collaboratively repair a DNN model without the need for sharing any raw data. We implemented Federated Repair based on a state-of-the-art DNN repair technique, and applied it to three DNN models, with federation size from 2 to 10. Results show that Federated Repair can achieve the same repair efficiency as non-federated DNN repair using the pooled data, despite the presence of rounding errors when aggregating clients' results.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {17–24},
numpages = {8},
keywords = {deep neural networks, DNN repair, federation},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@inproceedings{10.1109/ICSE48619.2023.00074,
author = {Xu, Shengbin and Yao, Yuan and Xu, Feng and Gu, Tianxiao and Xu, Jingwei and Ma, Xiaoxing},
title = {Data Quality Matters: A Case Study of Obsolete Comment Detection},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00074},
doi = {10.1109/ICSE48619.2023.00074},
abstract = {Machine learning methods have achieved great success in many software engineering tasks. However, as a data-driven paradigm, how would the data quality impact the effectiveness of these methods remains largely unexplored. In this paper, we explore this problem under the context of just-in-time obsolete comment detection. Specifically, we first conduct data cleaning on the existing benchmark dataset, and empirically observe that with only 0.22% label corrections and even 15.0% fewer data, the existing obsolete comment detection approaches can achieve up to 10.7% relative accuracy improvement. To further mitigate the data quality issues, we propose an adversarial learning framework to simultaneously estimate the data quality and make the final predictions. Experimental evaluations show that this adversarial learning framework can further improve the relative accuracy by up to 18.1% compared to the state-of-the-art method. Although our current results are from the obsolete comment detection problem, we believe that the proposed two-phase solution, which handles the data quality issues through both the data aspect and the algorithm aspect, is also generalizable and applicable to other machine learning based software engineering tasks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {781–793},
numpages = {13},
keywords = {obsolete comment detection, machine learning for software engineering, data quality},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00156,
author = {Li, Linyi and Zhang, Yuhao and Ren, Luyao and Xiong, Yingfei and Xie, Tao},
title = {Reliability Assurance for Deep Neural Network Architectures against Numerical Defects},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00156},
doi = {10.1109/ICSE48619.2023.00156},
abstract = {With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1827–1839},
numpages = {13},
keywords = {neural network, numerical defect, testing, fix},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639161,
author = {Yu, Shengcheng and Fang, Chunrong and Du, Mingzhe and Ling, Yuchen and Chen, Zhenyu and Su, Zhendong},
title = {Practical Non-Intrusive GUI Exploration Testing with Visual-based Robotic Arms},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639161},
doi = {10.1145/3597503.3639161},
abstract = {Graphical User Interface (GUI) testing has been a significant topic in the software engineering community. Most existing GUI testing frameworks are intrusive and can only support some specific platforms, which are quite limited. With the development of distinct scenarios, diverse embedded systems or customized operating systems on different devices do not support existing intrusive GUI testing frameworks. Some approaches adopt robotic arms to replace the interface invoking of mobile apps under test and use computer vision technologies to identify GUI elements. However, some challenges remain unsolved with such approaches. First, existing approaches assume that GUI screens are fixed so that they cannot be adapted to diverse systems with different screen conditions. Second, existing approaches use XY-plane robotic arm system, which cannot flexibly simulate human testing operations. Third, existing approaches ignore the compatibility bugs of apps and only focus on the crash bugs. To sum up, a more practical approach is required for the non-intrusive scenario.In order to solve the remaining challenges, we propose a practical non-intrusive GUI testing framework with visual-based robotic arms, namely RoboTest. RoboTest integrates a set of novel GUI screen and widget detection algorithm that is adaptive to detecting screens of different sizes and then to extracting GUI widgets from the detected screens. Then, a complete set of widely-used testing operations are applied with a 4-DOF robotic arm, which can more effectively and flexibly simulate human testing operations. During the app exploration, RoboTest integrates the specially designed Principle of Proximity-guided (PoP-guided) exploration strategy, which chooses close widgets of the previous operation targets to reduce the robotic arm movement overhead and improve exploration efficiency. Moreover, RoboTest can effectively detect some compatibility bugs beyond crash bugs with a GUI comparison on different devices of the same test operations. We evaluate RoboTest with 20 real-world mobile apps, together with a case study on a representative industrial embedded system. The results show that RoboTest can effectively, efficiently, and generally explore the AUT to find bugs and reduce app exploration time overhead from the robotic arm movement.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {130},
numpages = {13},
keywords = {GUI testing, non-intrusive testing, GUI understanding, robotic arm},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639475.3640103,
author = {Gohar, Usman and Hunter, Michael C. and Marczak-Czajka, Agnieszka and Lutz, Robyn R. and Cohen, Myra B. and Cleland-Huang, Jane},
title = {Towards Engineering Fair and Equitable Software Systems for Managing Low-Altitude Airspace Authorizations},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640103},
doi = {10.1145/3639475.3640103},
abstract = {Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across a diverse range of applications. This has introduced operational complexities within shared airspaces and an increase in reported incidents, raising safety concerns. In response, the U.S. Federal Aviation Administration (FAA) is developing a UAS Traffic Management (UTM) system to control access to airspace based on an sUAS's predicted ability to safely complete its mission. However, a fully automated system capable of swiftly approving or denying flight requests can be prone to bias and must consider safety, transparency, and fairness to diverse stakeholders. In this paper, we present an initial study that explores stakeholders' perspectives on factors that should be considered in an automated system. Results indicate flight characteristics and environmental conditions were perceived as most important but pilot and drone capabilities should also be considered. Further, several respondents indicated an aversion to any AI-supported automation, highlighting the need for full transparency in automated decision-making. Results provide a societal perspective on the challenges of automating UTM flight authorization decisions and help frame the ongoing design of a solution acceptable to the broader sUAS community.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {177–188},
numpages = {12},
keywords = {drones, sUAS, fairness, machine learning, software engineering},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@inproceedings{10.1145/3597503.3639104,
author = {Luo, Changhua and Meng, Wei and Wang, Shuai},
title = {Strengthening Supply Chain Security with Fine-grained Safe Patch Identification},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639104},
doi = {10.1145/3597503.3639104},
abstract = {Enhancing supply chain security is crucial, often involving the detection of patches in upstream software. However, current security patch analysis works yield relatively low recall rates (i.e., many security patches are missed). In this work, we offer a new solution to detect safe patches and assist downstream developers in patch propagation. Specifically, we develop SPatch to detect fine-grained safe patches. SPatch leverages fine-grained patch analysis and a new differential symbolic execution technique to analyze the functional impacts of code changes.We evaluated SPatch on various software, including the Linux kernel and OpenSSL, and demonstrated that it outperformed existing methods in detecting safe patches, resulting in observable security benefits. In our case studies, we updated hundreds of functions in modern software using safe patches detected by SPatch without causing any regression issues. Our detected safe security patches have been merged into the latest version of downstream software like ProtonVPN.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {89},
numpages = {12},
keywords = {supply chain security, fine-grained patch analysis, differential symbolic execution},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639718,
author = {Zheng, Dan and Sen, Koushik},
title = {Dynamic Inference of Likely Symbolic Tensor Shapes in Python Machine Learning Programs},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639718},
doi = {10.1145/3639477.3639718},
abstract = {In machine learning programs, it is often tedious to annotate the dimensions of shapes of various tensors that get created during execution. We present a dynamic likely tensor shape inference analysis, called ShapeIt, that annotates the dimensions of shapes of tensor expressions with symbolic dimension values and establishes the symbolic relationships among those dimensions. Such annotations can be used to understand the machine learning code written in popular frameworks, such as PyTorch and JAX, and to find bugs related to tensor shape mismatch. We have implemented ShapeIt on top of a novel dynamic analysis framework for Python, called Pynsy, which works by instrumenting Python bytecode on the fly. Our evaluation of ShapeIt on several tensor programs illustrates that ShapeIt could effectively infer symbolic shapes and their relationships for various neural network programs with low runtime overhead.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {147–156},
numpages = {10},
keywords = {program analysis, dynamic analysis, program instrumentation, tensor shape inference, dynamic invariant analysis},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643786.3648022,
author = {Shome, Arumoy and Cruz, Luis and Van Deursen, Arie},
title = {Data vs. Model Machine Learning Fairness Testing: An Empirical Study},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648022},
doi = {10.1145/3643786.3648022},
abstract = {Although several fairness definitions and bias mitigation techniques exist in the literature, all existing solutions evaluate fairness of Machine Learning (ML) systems after the training stage. In this paper, we take the first steps towards evaluating a more holistic approach by testing for fairness both before and after model training. We evaluate the effectiveness of the proposed approach and position it within the ML development lifecycle, using an empirical analysis of the relationship between model dependent and independent fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5 real-world datasets and 1600 fairness evaluation cycles. We find a linear relationship between data and model fairness metrics when the distribution and the size of the training data changes. Our results indicate that testing for fairness prior to training can be a "cheap" and effective means of catching a biased data collection process early; detecting data drifts in production systems and minimising execution of full training cycles thus reducing development time and costs.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {1–8},
numpages = {8},
keywords = {SE4ML, ML fairness testing, empirical software engineering, data-centric AI},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@inproceedings{10.1109/ICSE48619.2023.00024,
author = {Guan, Hao and Xiao, Ying and Li, Jiaying and Liu, Yepang and Bai, Guangdong},
title = {A Comprehensive Study of Real-World Bugs in Machine Learning Model Optimization},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00024},
doi = {10.1109/ICSE48619.2023.00024},
abstract = {Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs).In this work, we conduct the first empirical study to identify and characterize MOBs. We collect a comprehensive dataset containing 371 MOBs from TensorFlow and PyTorch, the most extensively used open-source ML frameworks, covering the entire development time span of their optimizers (May 2019 to August 2022). We then investigate the collected bugs from various perspectives, including their symptoms, root causes, life cycles, detection and fixes. Our work unveils the status quo of MOBs in the wild, and reveals their features on which future detection techniques can be based. Our findings also serve as a warning to the developers and the users of ML frameworks, and an appeal to our research community to enact dedicated countermeasures.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {147–158},
numpages = {12},
keywords = {machine learning, model optimization, bugs},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3639786,
author = {Yan, Yanfu},
title = {On Improving Management of Duplicate Video-Based Bug Reports},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639786},
doi = {10.1145/3639478.3639786},
abstract = {Video-based bug reports have become a promising alternative to text-based reports for programs centered around a graphical user interface (GUI), as they allow for seamless documentation of software faults by visually capturing buggy behavior on app screens. However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. Therefore, my research endeavors to overcome these challenges by advancing the bug report management task of duplicate detection for video-based reports. The objectives of my research are fourfold: (i) investigate the benefits of tailoring recent advancements in the computer vision domain for learning both visual and textual patterns from video frames depicting GUI screens to detect duplicate reports; (ii) adapt the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens; (iii) construct a more comprehensive and realistic benchmark which contains video-based bug reports derived from real bugs; (iv) conduct an empirical evaluation to potentially demonstrate state-of-the-art improvements achieved by the proposed approach.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {201–203},
numpages = {3},
keywords = {bug reporting, GUI learning, duplicate video retrieval},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3644882,
author = {Mostafavi Ghahfarokhi, Mojtaba and Asgari, Arash and Abolnejadian, Mohammad and Heydarnoori, Abbas},
title = {DistilKaggle: A Distilled Dataset of Kaggle Jupyter Notebooks},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644882},
doi = {10.1145/3643991.3644882},
abstract = {Jupyter notebooks have become indispensable tools for data analysis and processing in various domains. However, despite their widespread use, there is a notable research gap in understanding and analyzing the contents and code metrics of these notebooks. This gap is primarily attributed to the absence of datasets that encompass both Jupyter notebooks and extracted their code metrics. To address this limitation, we introduce DistilKaggle, a unique dataset specifically curated to facilitate research on code metrics in Jupyter notebooks, utilizing the Kaggle repository as a prime source. Through an extensive study, we identify thirty-four code metrics that significantly impact Jupyter notebook code quality. These features such as lines of code cell, mean number of words in markdown cells, performance tier of developer, etc., are crucial for understanding and improving the overall effectiveness of computational notebooks. The DistilKaggle dataset which is derived from a vast collection of notebooks constitutes two distinct datasets: (i) Code Cells and Markdown Cells Dataset which is presented in two CSV files, allowing for easy integration into researchers' workflows as dataframes. It provides a granular view of the content structure within 542,051 Jupyter notebooks, enabling detailed analysis of code and markdown cells; and (ii) The Notebook Code Metrics Dataset focused on the identified code metrics of notebooks. Researchers can leverage this dataset to access Jupyter notebooks with specific code quality characteristics, surpassing the limitations of filters available on the Kaggle website. Furthermore, the reproducibility of the notebooks in our dataset is ensured through the code cells and markdown cells datasets, offering a reliable foundation for researchers to build upon. Given the substantial size of our datasets, it becomes an invaluable resource for the research community, surpassing the capabilities of individual Kaggle users to collect such extensive data. For accessibility and transparency, both the dataset and the code utilized in crafting this dataset are publicly available at https://github.com/ISE-Research/DistilKaggle.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {647–651},
numpages = {5},
keywords = {open dataset, Kaggle, Jupyter notebooks, code metrics, code quality},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3639164,
author = {Gao, Hui and Kuang, Hongyu and Assun\c{c}\~{a}o, Wesley K. G. and Mayr-Dorn, Christoph and Rong, Guoping and Zhang, He and Ma, Xiaoxing and Egyed, Alexander},
title = {TRIAD: Automated Traceability Recovery based on Biterm-enhanced Deduction of Transitive Links among Artifacts},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639164},
doi = {10.1145/3597503.3639164},
abstract = {Traceability allows stakeholders to extract and comprehend the trace links among software artifacts introduced across the software life cycle, to provide significant support for software engineering tasks. Despite its proven benefits, software traceability is challenging to recover and maintain manually. Hence, plenty of approaches for automated traceability have been proposed. Most rely on textual similarities among software artifacts, such as those based on Information Retrieval (IR). However, artifacts in different abstraction levels usually have different textual descriptions, which can greatly hinder the performance of IR-based approaches (e.g., a requirement in natural language may have a small textual similarity to a Java class). In this work, we leverage the consensual biterms and transitive relationships (i.e., inner- and outer-transitive links) based on intermediate artifacts to improve IR-based traceability recovery. We first extract and filter biterms from all source, intermediate, and target artifacts. We then use the consensual biterms from the intermediate artifacts to enrich the texts of both source and target artifacts, and finally deduce outer and inner-transitive links to adjust text similarities between source and target artifacts. We conducted a comprehensive empirical evaluation based on five systems widely used in other literature to show that our approach can outperform four state-of-the-art approaches in Average Precision over 15% and Mean Average Precision over 10% on average.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {216},
numpages = {13},
keywords = {software traceability, information retrieval, transitive links},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00046,
author = {Sawant, Neela and Sengamedu, Srinivasan H.},
title = {Code Compliance Assessment as a Learning Problem},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00046},
doi = {10.1109/ICSE-SEIP58684.2023.00046},
abstract = {Manual code reviews and static code analyzers are the traditional mechanisms to verify if source code complies with coding policies. However, they are hard to scale. We formulate code compliance assessment as a machine learning (ML) problem, to take as input a natural language policy and code, and generate a prediction on the code's compliance, non-compliance, or irrelevance. Our intention for ML-based automation is to scale the development of Amazon CodeGuru, a commercial code analyzer. We explore key research questions on model formulation, training data, and evaluation setup. We obtain a joint code-text representation space (embeddings) which preserves compliance relationships via the vector distance of code and policy embeddings. As there is no task-specific data, we re-interpret and filter commonly available software datasets with additional pretraining and pre-finetuning tasks that reduce the semantic gap. We benchmarked our approach on two listings of coding policies (CWE and CBP). This is a zero-shot evaluation as none of the policies occur in the training set. On CWE and CBP respectively, our tool Policy2Code achieves classification accuracies of (59%, 71%) and search MRR of (0.05, 0.21) compared to CodeBERT with classification accuracies of (37%, 54%) and MRR of (0.02, 0.02). In a user study, 24% Policy2Code detections were accepted compared to 7% for CodeBERT. Policy2Code is considered a useful ML-based aid to supplement manual efforts.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {445–454},
numpages = {10},
keywords = {code embeddings, natural language analysis},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00135,
author = {Gesi, Jiri and Shen, Xinyun and Geng, Yunfan and Chen, Qihong and Ahmed, Iftekhar},
title = {Leveraging Feature Bias for Scalable Misprediction Explanation of Machine Learning Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00135},
doi = {10.1109/ICSE48619.2023.00135},
abstract = {Interpreting and debugging machine learning models is necessary to ensure the robustness of the machine learning models. Explaining mispredictions can help significantly in doing so. While recent works on misprediction explanation have proven promising in generating interpretable explanations for mispredictions, the state-of-the-art techniques "blindly" deduce misprediction explanation rules from all data features, which may not be scalable depending on the number of features. To alleviate this problem, we propose an efficient misprediction explanation technique named Bias Guided Misprediction Diagnoser (BGMD), which leverages two prior knowledge about data: a) data often exhibit highly-skewed feature distributions and b) trained models in many cases perform poorly on subdataset with under-represented features. Next, we propose a technique named MAPS (Mispredicted Area UPweight Sampling). MAPS increases the weights of subdataset during model retraining that belong to the group that is prone to be mispredicted because of containing under-represented features. Thus, MAPS make retrained model pay more attention to the under-represented features. Our empirical study shows that our proposed BGMD outperformed the state-of-the-art misprediction diagnoser and reduces diagnosis time by 92%. Furthermore, MAPS outperformed two state-of-the-art techniques on fixing the machine learning model's performance on mispredicted data without compromising performance on all data. All the research artifacts (i.e., tools, scripts, and data) of this study are available in the accompanying website [1].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1559–1570},
numpages = {12},
keywords = {machine learning, data imbalance, rule induction, misprediction explanation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528588.3528651,
author = {Moharil, Ambarish and Sharma, Arpit},
title = {Identification of intra-domain ambiguity using transformer-based machine learning},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528651},
doi = {10.1145/3528588.3528651},
abstract = {Recently, the application of neural word embeddings for detecting cross-domain ambiguities in software requirements has gained a significant attention from the requirements engineering (RE) community. Several approaches have been proposed in the literature for estimating the variation of meaning of commonly used terms in different domains. A major limitation of these techniques is that they are unable to identify and detect the terms that have been used in different contexts within the same application domain, i.e. intra-domain ambiguities or in a requirements document of an interdisciplinary project. We propose an approach based on the idea of bidirectional encoder representations from Transformers (BERT) and clustering for identifying such ambiguities. For every context in which a term has been used in the document, our approach returns a list of its most similar words and also provides some example sentences from the corpus highlighting its context-specific interpretation. We apply our approach to a computer science (CS) specific corpora and a multi-domain corpora which consists of textual data from eight different application domains. Our experimental results show that this approach is very effective in identifying and detecting intra-domain ambiguities.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {51–58},
numpages = {8},
keywords = {ambiguity, machine learning, natural language, requirements, word embedding},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3643916.3644405,
author = {Yang, Haiyang and Chen, Hao and Kuai, Zhirui and Tu, Shuyuan and Kuang, Li},
title = {ASKDetector: An AST-Semantic and Key Features Fusion based Code Comment Mismatch Detector},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644405},
doi = {10.1145/3643916.3644405},
abstract = {Code comments are essential for programming comprehension. Nevertheless, developers often neglect to update comments after modifying the source code. Wrong code comments may lead to bugs in the maintenance process, thus affecting the reliability of the software. So, timely comment mismatch detection is crucial for software development and maintenance. However, existing works have the following two limitations: 1) the lack of use of code structural and sequential information, and 2) the ignorance of existing associations between code and comments. In this paper, we propose a new model called ASKDetector (AST-Semantic and Key features fusion based mismatch Detector). For the first limitation, we encode code with an attention-based preorder traversal abstract syntax tree sequence to obtain both order and structural information. And CodeBERT is utilized to capture contextual semantic features further. For the second one, we encode extracted association information between the code snippets and comments to reduce the semantic gap. The correlations between the encoders are learned through a fusion layer and a multi-layer perceptron. The experimental results prove that our detector outperforms the state-of-the-art model in evaluation metrics, where our F1 and accuracy exceed an average of 3.4%.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {392–402},
numpages = {11},
keywords = {reliability of open source software, software maintenance, code comment mismatch, program comprehension, deep learning},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643796.3648467,
author = {Nghiem, Khanh and Nguyen, Anh Minh and Bui, Nghi},
title = {Envisioning the Next-Generation AI Coding Assistants: Insights &amp; Proposals},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648467},
doi = {10.1145/3643796.3648467},
abstract = {As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants. AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses. We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {115–117},
numpages = {3},
keywords = {IDE, artificial intelligence, human-computer interaction, large language models, Docify AI, CodeVista, GitHub copilot, AI4SE},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643786.3648026,
author = {Essbai, Wasim and Bombarda, Andrea and Bonfanti, Silvia and Gargantini, Angelo},
title = {A Framework for Including Uncertainty in Robustness Evaluation of Bayesian Neural Network Classifiers},
year = {2024},
isbn = {9798400705748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643786.3648026},
doi = {10.1145/3643786.3648026},
abstract = {Neural networks (NNs) play a crucial role in safety-critical fields, requiring robustness assurance. Bayesian Neural Networks (BNNs) address data uncertainty, providing probabilistic outputs. However, the literature on BNN robustness assessment is still limited, mainly focusing on adversarial examples, which are often impractical in real-world applications. This paper introduces a fresh perspective on BNN classifier robustness, considering natural input variations while accounting for prediction uncertainties. Our approach excludes predictions labeled as "unknown", enabling practitioners to define alteration probabilities, penalize errors beyond a specified threshold, and tolerate varying error levels below it. We present a systematic approach for evaluating the robustness of BNNs, introducing new evaluation metrics that account for prediction uncertainty. We conduct a comparative study using two NNs - standard MLP and Bayesian MLP - on the MNIST dataset. Our results show that by leveraging estimated uncertainty, it is possible to enhance the system's robustness.},
booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Deep Learning for Testing and Testing for Deep Learning},
pages = {25–32},
numpages = {8},
keywords = {robustness, bayesian neural networks, alterations, uncertainty},
location = {Lisbon, Portugal},
series = {DeepTest '24}
}

@inproceedings{10.1145/3597503.3639186,
author = {Wang, Dinghua and Li, Shuqing and Xiao, Guanping and Liu, Yepang and Sui, Yulei and He, Pinjia and Lyu, Michael R.},
title = {An Exploratory Investigation of Log Anomalies in Unmanned Aerial Vehicles},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639186},
doi = {10.1145/3597503.3639186},
abstract = {Unmanned aerial vehicles (UAVs) are becoming increasingly ubiquitous in our daily lives. However, like many other complex systems, UAVs are susceptible to software bugs that can lead to abnormal system behaviors and undesirable consequences. It is crucial to study such software bug-induced UAV anomalies, which are often manifested in flight logs, to help assure the quality and safety of UAV systems. However, there has been limited research on investigating the code-level patterns of software bug-induced UAV anomalies. This impedes the development of effective tools for diagnosing and localizing bugs within UAV system code.To bridge the research gap and deepen our understanding of UAV anomalies, we carried out an empirical study on this subject. We first collected 178 real-world abnormal logs induced by software bugs in two popular open-source UAV platforms, i.e., PX4 and Ardupilot. We then examined each of these abnormal logs and compiled their common patterns. In particular, we investigated the most severe anomalies that led to UAV crashes, and identified their features. Based on our empirical findings, we further summarized the challenges of localizing bugs in system code by analyzing anomalous UAV flight data, which can offer insights for future research in this field.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {210},
numpages = {13},
keywords = {UAV anomaly, software bug, crash, code pattern, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00164,
author = {Gao, Shuzheng and Gao, Cuiyun and Wang, Chaozheng and Sun, Jun and Lo, David and Yu, Yue},
title = {Two Sides of the Same Coin: Exploiting the Impact of Identifiers in Neural Code Comprehension},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00164},
doi = {10.1109/ICSE48619.2023.00164},
abstract = {Previous studies have demonstrated that neural code comprehension models are vulnerable to identifier naming. By renaming as few as one identifier in the source code, the models would output completely irrelevant results, indicating that identifiers can be misleading for model prediction. However, identifiers are not completely detrimental to code comprehension, since the semantics of identifier names can be related to the program semantics. Well exploiting the two opposite impacts of identifiers is essential for enhancing the robustness and accuracy of neural code comprehension, and still remains under-explored. In this work, we propose to model the impact of identifiers from a novel causal perspective, and propose a counterfactual reasoning-based framework named CREAM. CREAM explicitly captures the misleading information of identifiers through multitask learning in the training stage, and reduces the misleading impact by counterfactual inference in the inference stage. We evaluate CREAM on three popular neural code comprehension tasks, including function naming, defect detection and code classification. Experiment results show that CREAM not only significantly outperforms baselines in terms of robustness (e.g., +37.9% on the function naming task at F1 score), but also achieve improved results on the original datasets (e.g., +0.5% on the function naming task at F1 score).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1933–1945},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/GREENS66463.2025.00007,
author = {Marini, Niccol\`{o} and Pampaloni, Leonardo and Di Martino, Filippo and Verdecchia, Roberto and Vicario, Enrico},
title = {Green AI: Which Programming Language Consumes the Most?},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00007},
doi = {10.1109/GREENS66463.2025.00007},
abstract = {AI is demanding an evergrowing portion of environmental resources. Despite their potential impact on AI environmental sustainability, the role that programming languages play in AI (in)efficiency is to date still unknown. With this study, we aim to understand the impact that programming languages can have on AI environmental sustainability. To achieve our goal, we conduct a controlled empirical experiment by considering five programming languages (C++, Java, Python, MATLAB, and R), seven AI algorithms (KNN, SVC, AdaBoost, decision tree, logistic regression, naive bayses, and random forest), three popular datasets, and the training and inference phases. The collected results show that programming languages have a considerable impact on AI environmental sustainability. Compiled and semi-compiled languages (C++, Java) consistently consume less than interpreted languages (Python, MATLAB, R), which require up to 54x more energy. Some languages are cumulatively more efficient in training, while others in inference. Which programming language consumes the most highly depends on the algorithm considered. Ultimately, algorithm implementation might be the most determining factor in Green AI, regardless of the language used. As conclusion, while making AI more environmentally sustainable is paramount, a trade-off between energy efficiency and implementation ease should always be considered. Green AI can be achieved without the need of completely disrupting the development practices and technologies currently in place.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {12–19},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3643787.3648030,
author = {Rejithkumar, Gokul and Anish, Preethu Rose and Sonar, Pratik and Ghaisas, Smita},
title = {Automated Extraction of Compliance Elements in Software Engineering Contracts Using Natural Language Generation},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648030},
doi = {10.1145/3643787.3648030},
abstract = {Software Engineering (SE) contracts are legally binding agreements governing software development, usage, related responsibilities, and rights between the parties involved. SE contracts contain obligatory clauses which encompass compliance elements such as Regulations, Standards, and Policies. Identifying these elements is essential for ensuring compliance by aiding in contract negotiation, assignment of the obligatory clauses to relevant departments within the organization and for identification of high-level software requirements. Non-compliance may lead to penalties, damaging an organization's reputation and trust. In this paper, we automate the extraction of compliance elements from SE contracts. We employed a text-to-text generation approach and conducted experiments using the T5 model for the automated extraction of compliance elements from obligatory contractual clauses. The text-to-text generation approach yielded a mean F1-score of 0.92 for ROUGE-L.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {69–72},
numpages = {4},
keywords = {text-to-text generation, T5, legal language, compliance, regulation, standard, policy},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3643916.3644412,
author = {Pepe, Federica and Nardone, Vittoria and Mastropaolo, Antonio and Bavota, Gabriele and Canfora, Gerardo and Di Penta, Massimiliano},
title = {How do Hugging Face Models Document Datasets, Bias, and Licenses? An Empirical Study},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644412},
doi = {10.1145/3643916.3644412},
abstract = {Pre-trained Machine Learning (ML) models help to create ML-intensive systems without having to spend conspicuous resources on training a new model from the ground up. However, the lack of transparency for such models could lead to undesired consequences in terms of bias, fairness, trustworthiness of the underlying data, and, potentially even legal implications. Taking as a case study the transformer models hosted by Hugging Face, a popular hub for pre-trained ML models, this paper empirically investigates the transparency of pre-trained transformer models. We look at the extent to which model descriptions (i) specify the datasets being used for their pre-training, (ii) discuss their possible training bias, (iii) declare their license, and whether projects using such models take these licenses into account. Results indicate that pre-trained models still have a limited exposure of their training datasets, possible biases, and adopted licenses. Also, we found several cases of possible licensing violations by client projects. Our findings motivate further research to improve the transparency of ML models, which may result in the definition, generation, and adoption of Artificial Intelligence Bills of Materials.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {370–381},
numpages = {12},
keywords = {ML-intensive systems, pre-trained models, transparency, bias, and fairness, deep learning, empirical study},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3528588.3528665,
author = {Izadi, Maliheh and Ahmadabadi, Matin Nili},
title = {On the evaluation of NLP-based models for software engineering},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528665},
doi = {10.1145/3528588.3528665},
abstract = {NLP-based models have been increasingly incorporated to address SE problems. These models are either employed in the SE domain with little to no change, or they are greatly tailored to source code and its unique characteristics. Many of these approaches are considered to be outperforming or complementing existing solutions. However, an important question arises here: Are these models evaluated fairly and consistently in the SE community?. To answer this question, we reviewed how NLP-based models for SE problems are being evaluated by researchers. The findings indicate that currently there is no consistent and widely-accepted protocol for the evaluation of these models. While different aspects of the same task are being assessed in different studies, metrics are defined based on custom choices, rather than a system, and finally, answers are collected and interpreted case by case. Consequently, there is a dire need to provide a methodological way of evaluating NLP-based models to have a consistent assessment and preserve the possibility of fair and efficient comparison.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {48–50},
numpages = {3},
keywords = {software engineering, natural language processing, evaluation},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3639478.3643081,
author = {Lian, Xiaoli and Wang, Shuaisong and Ma, Jieping and Tan, Xin and Liu, Fang and Shi, Lin and Gao, Cuiyun and Zhang, Li},
title = {Imperfect Code Generation: Uncovering Weaknesses in Automatic Code Generation by Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643081},
doi = {10.1145/3639478.3643081},
abstract = {The task of code generation has received significant attention in recent years, especially when the pre-trained large language models (LLMs) for code have consistently achieved state-of-the-art performance. However, there is currently a lack of a comprehensive weakness taxonomy in the field, uncovering weaknesses in automatic code generation by LLMs. This may lead the community to invest excessive efforts into well-known hotspots while neglecting many crucial yet unrecognized issues that deserve more attention. To bridge this gap, we conduct a systematic study on analyzing the weaknesses based on three state-of-the-art LLMs across three widely-used code generation datasets. Our study identifies eight types of weaknesses and assesses their prevalence across each LLM and dataset, aiming to inform and shape the trajectory of future research in the domain.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {422–423},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639583,
author = {Chen, Zirui and Hu, Xing and Xia, Xin and Gao, Yi and Xu, Tongtong and Lo, David and Yang, Xiaohu},
title = {Exploiting Library Vulnerability via Migration Based Automating Test Generation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639583},
doi = {10.1145/3597503.3639583},
abstract = {In software development, developers extensively utilize third-party libraries to avoid implementing existing functionalities. When a new third-party library vulnerability is disclosed, project maintainers need to determine whether their projects are affected by the vulnerability, which requires developers to invest substantial effort in assessment. However, existing tools face a series of issues: static analysis tools produce false alarms, dynamic analysis tools require existing tests and test generation tools have low success rates when facing complex vulnerabilities.Vulnerability exploits, as code snippets provided for reproducing vulnerabilities after disclosure, contain a wealth of vulnerability-related information. This study proposes a new method based on vulnerability exploits, called Vesta (Vulnerability Exploit-based Software Testing Auto-Generator), which provides vulnerability exploit tests as the basis for developers to decide whether to update dependencies. Vesta extends the search-based test generation methods by adding a migration step, ensuring the similarity between the generated test and the vulnerability exploit, which increases the likelihood of detecting potential library vulnerabilities in a project.We perform experiments on 30 vulnerabilities disclosed in the past five years, involving 60 vulnerability-project pairs, and compare the experimental results with the baseline method, Transfer. The success rate of Vesta is 71.7% which is a 53.4% improvement over Transfer in the effectiveness of verifying exploitable vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {228},
numpages = {12},
keywords = {library vulnerabilities, search-based test generation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3640042,
author = {Minn, Wei and Tun, Yan Naing and Shar, Lwin Khin and Jiang, Lingxiao},
title = {DronLomaly: Runtime Log-based Anomaly Detector for DJI Drones},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640042},
doi = {10.1145/3639478.3640042},
abstract = {We present an automated tool for realtime detection of anomalous behaviors while a DJI drone is executing a flight mission. The tool takes sensor data logged by drone at fixed time intervals and performs anomaly detection using a Bi-LSTM model. The model is trained on baseline flight logs from a successful mission physically or via a simulator. The tool has two modules --- the first module is responsible for sending the log data to the remote controller station, and the second module is run as a service in the remote controller station powered by a Bi-LSTM model, which receives the log data and produces visual graphs showing the realtime flight anomaly statuses with respect to various sensor readings on a dashboard. We have successfully evaluated the tool on three datasets including industrial test scenarios. DronLomaly is released as an open-source tool on GitHub [10], and the demo video can be found at [17].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {6–10},
numpages = {5},
keywords = {drone security, anomaly detection, log analysis, deep learning},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00149,
author = {Ahmed, Toufique and Ghosh, Supriyo and Bansal, Chetan and Zimmermann, Thomas and Zhang, Xuchao and Rajmohan, Saravan},
title = {Recommending Root-Cause and Mitigation Steps for Cloud Incidents Using Large Language Models},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00149},
doi = {10.1109/ICSE48619.2023.00149},
abstract = {Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1737–1749},
numpages = {13},
keywords = {incident management, service quality, GPT-3.x, large language models},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639210,
author = {Fu, Jingzhou and Liang, Jie and Wu, Zhiyong and Jiang, Yu},
title = {Sedar: Obtaining High-Quality Seeds for DBMS Fuzzing via Cross-DBMS SQL Transfer},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639210},
doi = {10.1145/3597503.3639210},
abstract = {Effective DBMS fuzzing relies on high-quality initial seeds, which serve as the starting point for mutation. These initial seeds should incorporate various DBMS features to explore the state space thoroughly. While built-in test cases are typically used as initial seeds, many DBMSs lack comprehensive test cases, making it difficult to apply state-of-the-art fuzzing techniques directly.To address this, we propose Sedar which produces initial seeds for a target DBMS by transferring test cases from other DBMSs. The underlying insight is that many DBMSs share similar functionalities, allowing seeds that cover deep execution paths in one DBMS to be adapted for other DBMSs. The challenge lies in converting these seeds to a format supported by the grammar of the target database. Sedar follows a three-step process to generate seeds. First, it executes existing SQL test cases within the DBMS they were designed for and captures the schema information during execution. Second, it utilizes large language models (LLMs) along with the captured schema information to guide the generation of new test cases based on the responses from the LLM. Lastly, to ensure that the test cases can be properly parsed and mutated by fuzzers, Sedar temporarily comments out unparsable sections for the fuzzers and uncomments them after mutation. We integrate Sedar into the DBMS fuzzers Sqirrel and Griffin, targeting DBMSs such as Virtuoso, MonetDB, DuckDB, and ClickHouse. Evaluation results demonstrate significant improvements in both fuzzers. Specifically, compared to Sqirrel and Griffin with non-transferred seeds, Sedar enhances code coverage by 72.46%-214.84% and 21.40%-194.46%; compared to Sqirrel and Griffin with native test cases of these DBMSs as initial seeds, incorporating the transferred seeds of Sedar results in an improvement in code coverage by 4.90%-16.20% and 9.73%-28.41%. Moreover, Sedar discovered 70 new vulnerabilities, with 60 out of them being uniquely found by Sedar with transferred seeds, and 19 of them have been assigned with CVEs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {146},
numpages = {12},
keywords = {DBMS fuzzing, initial seeds, vulnerability detection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3640325,
author = {Jin, Wuxia and Xu, Shuo and Chen, Dawei and He, Jiajun and Zhong, Dinghong and Fan, Ming and Chen, Hongxu and Zhang, Huijia and Liu, Ting},
title = {PyAnalyzer: An Effective and Practical Approach for Dependency Extraction from Python Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3640325},
doi = {10.1145/3597503.3640325},
abstract = {Dependency extraction based on static analysis lays the groundwork for a wide range of applications. However, dynamic language features in Python make code behaviors obscure and nondeterministic; consequently, it poses huge challenges for static analyses to resolve symbol-level dependencies. Although prosperous techniques and tools are adequately available, they still lack sufficient capabilities to handle object changes, first-class citizens, varying call sites, and library dependencies. To address the fundamental difficulty for dynamic languages, this work proposes an effective and practical method namely PyAnalyzer for dependency extraction. PyAnalyzer uniformly models functions, classes, and modules into first-class heap objects, propagating the dynamic changes of these objects and class inheritance. This manner better simulates dynamic features like duck typing, object changes, and first-class citizens, resulting in high recall results without compromising precision. Moreover, PyAnalyzer leverages optional type annotations as a shortcut to express varying call sites and resolve library dependencies on demand. We collected two micro-benchmarks (278 small programs), two macro-benchmarks (59 real-world applications), and 191 real-world projects (10MSLOC) for comprehensive comparisons with 7 advanced techniques (i.e., Understand, Sourcetrail, Depends, ENRE19, PySonar2, PyCG, and Type4Py). The results demonstrated that PyAnalyzer achieves a high recall and hence improves the F1 by 24.7% on average, at least 1.4x faster without an obvious compromise of memory efficiency. Our work will benefit diverse client applications.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {112},
numpages = {12},
keywords = {dependency extraction, Python, dynamic features},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00166,
author = {Mansur, S M Hasan and Salma, Sabiha and Awofisayo, Damilola and Moran, Kevin},
title = {AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00166},
doi = {10.1109/ICSE48619.2023.00166},
abstract = {Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability.In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1958–1970},
numpages = {13},
keywords = {dark pattern, UI analysis, UI design},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3644864,
author = {Weyssow, Martin and Di Sipio, Claudio and Di Ruscio, Davide and Sahraoui, Houari},
title = {CodeLL: A Lifelong Learning Dataset to Support the Co-Evolution of Data and Language Models of Code},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644864},
doi = {10.1145/3643991.3644864},
abstract = {Motivated by recent work on lifelong learning applications for language models (LMs) of code, we introduce CodeLL, a lifelong learning dataset focused on code changes. Our contribution addresses a notable research gap marked by the absence of a long-term temporal dimension in existing code change datasets, limiting their suitability in lifelong learning scenarios. In contrast, our dataset aims to comprehensively capture code changes across the entire release history of open-source software repositories. In this work, we introduce an initial version of CodeLL, comprising 71 machine-learning-based projects mined from Software Heritage. This dataset enables the extraction and in-depth analysis of code changes spanning 2,483 releases at both the method and API levels. CodeLL enables researchers studying the behaviour of LMs in lifelong fine-tuning settings for learning code changes. Additionally, the dataset can help studying data distribution shifts within software repositories and the evolution of API usages over time.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {637–641},
numpages = {5},
keywords = {deep learning for code, lifelong learning, dataset},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/SVM66695.2025.00009,
author = {Rahman, Akond and Skjellum, Anthony and Zhang, Yue},
title = {An Exploratory Study of Security Vulnerabilities in Machine Learning Deployment Projects},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00009},
doi = {10.1109/SVM66695.2025.00009},
abstract = {Machine learning (ML) deployment projects are used by practitioners to automatically deploy ML models. While ML deployment projects aid practitioners, security vulnerabilities in these projects can make ML deployment infrastructure susceptible to security attacks. A systematic characterization of vulnerabilities can aid in identifying activities to secure ML deployment projects used by practitioners. We conduct an empirical study with 149 vulnerabilities mined from 12 open source ML deployment projects to characterize vulnerabilities in ML deployment projects. From our empirical study, we (i) find 68 of the 149 vulnerabilities are critically or highly severe; (ii) derive 10 consequences of vulnerabilities, e.g., unauthorized access to trigger ML deployments; and (iii) observe established quality assurance activities, such as code review to be used in the ML deployment projects. We conclude our paper by providing a set of recommendations for practitioners and researchers. Dataset used for our paper is available online.},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {33–36},
numpages = {4},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1145/3597503.3639162,
author = {Liu, Tianyang and Ji, Weixing and Dong, Xiaohui and Yao, Wuhuang and Wang, Yizhuo and Liu, Hui and Peng, Haiyang and Wang, Yuxuan},
title = {JLeaks: A Featured Resource Leak Repository Collected From Hundreds of Open-Source Java Projects},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639162},
doi = {10.1145/3597503.3639162},
abstract = {High-quality defect repositories are vital in defect detection, localization, and repair. However, existing repositories collected from open-source projects are either small-scale or inadequately labeled and packed. This paper systematically summarizes the programming APIs of system resources (i.e., file, socket, and thread) in Java. Additionally, this paper demonstrates the exceptions that may cause resource leaks in the chained and nested streaming operations. A semi-automatic toolchain is built to improve the efficiency of defect extraction, including automatic building for large legacy Java projects. Accordingly, 1,094 resource leaks were collected from 321 open-source projects on GitHub. This repository, named JLeaks, was built by round-by-round filtering and cross-validation, involving the review of approximately 3,185 commits from hundreds of projects. JLeaks is currently the largest resource leak repository, and each defect in JLeaks is well-labeled and packed, including causes, locations, patches, source files, and compiled bytecode files for 254 defects. We have conducted a detailed analysis of JLeaks for defect distribution, root causes, and fix approaches. We compare JLeaks with two well-known resource leak repositories, and the results show that JLeaks is more informative and complete, with high availability, uniqueness, and consistency. Additionally, we show the usability of JLeaks in two application scenarios. Future studies can leverage our repository to encourage better design and implementation of defect-related algorithms and tools.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {140},
numpages = {13},
keywords = {resource leak, defect repository, open-source projects, java language},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623334,
author = {Li, Tianlin and Cao, Yue and Zhang, Jian and Zhao, Shiqian and Huang, Yihao and Liu, Aishan and Guo, Qing and Liu, Yang},
title = {RUNNER: Responsible UNfair NEuron Repair for Enhancing Deep Neural Network Fairness},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623334},
doi = {10.1145/3597503.3623334},
abstract = {Deep Neural Networks (DNNs), an emerging software technology, have achieved impressive results in a variety of fields. However, the discriminatory behaviors towards certain groups (a.k.a. unfairness) of DNN models increasingly become a social concern, especially in high-stake applications such as loan approval and criminal risk assessment. Although there has been a number of works to improve model fairness, most of them adopt an adversary to either expand the model architecture or augment training data, which introduces excessive computational overhead. Recent work diagnoses responsible unfair neurons first and fixes them with selective retraining. Unfortunately, existing diagnosis process is time-consuming due to multi-step training sample analysis, and selective retraining may cause a performance bottleneck due to indirectly adjusting unfair neurons on biased samples. In this paper, we propose Responsible UNfair NEuron Repair (RUNNER) that improves existing works in three key aspects: (1) efficiency: we design the Importance-based Neuron Diagnosis that identifies responsible unfair neurons in one step with a novel importance criterion of neurons; (2) effectiveness: we design the Neuron Stabilizing Retraining by adding a loss term that measures the activation distance of responsible unfair neurons from different subgroups in all sources; (3) generalization: we investigate the effectiveness on both structured tabular data and large-scale unstructured image data, which is often ignored in prior studies. Our extensive experiments across 5 datasets show that RUUNER can effectively and efficiently diagnose and repair the DNNs regarding unfairness. On average, our approach significantly reduces computing overhead from 341.7s to 29.65s, and achieves improved fairness up to 79.3%. Besides, RUNNER also keeps state-of-the-art results on the unstructured dataset.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {9},
numpages = {13},
keywords = {deep learning repair, fairness, model interpretation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643691.3648583,
author = {Schenkenfelder, Bernhard and Brandst\"{a}tter, Ulrich and Fischer, Lukas and Ramler, Rudolf and Laister, Dominik and Hartl, Martin and Wurm, Markus},
title = {Responsible AI Engineering: The Case of an Inclusive Image Annotation Team in a Global Technology Company},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648583},
doi = {10.1145/3643691.3648583},
abstract = {Artificial Intelligence (AI) models require a high quantity of high-quality data to learn from. For tolling applications, traffic cameras can collect massive amounts of images. However, annotating them is expensive, time-consuming, and requires distinct skills. People on the autism spectrum, on the other hand, typically have a hard time finding work, especially fulfilling work that benefits from their skills. Based on six interviews, we present the case of an inclusive traffic image annotation team in a global technology company that bridges the gap between the growing demand for high-quality image annotation and the difficult employment situation of people with autism. In particular, we explore relevant use cases in the pilot project, what requirements can be identified to improve software tool support, and what the enabling factors are in general. The results indicate that this initiative has a positive impact along many dimensions. The overall goal of this paper is to provide researchers and practitioners with a blueprint for creating inclusive AI annotation workplaces-with modifications where necessary.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {8–15},
numpages = {8},
keywords = {autism, image annotation, artificial intelligence, case study},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1145/3643991.3644878,
author = {Pramod, Dushan and De Silva, Thushan and Thabrew, Udith and Shariffdeen, Ridwan and Wickramanayake, Sandareka},
title = {BugsPHP: A dataset for Automated Program Repair in PHP},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644878},
doi = {10.1145/3643991.3644878},
abstract = {Automated Program Repair (APR) improves developer productivity by saving debugging and bug-fixing time. While APR has been extensively explored for C/C++ and Java programs, there is little research on bugs in PHP programs due to the lack of a benchmark PHP bug dataset. This is surprising given that PHP has been one of the most widely used server-side languages for over two decades, being used in a variety of contexts such as e-commerce, social networking, and content management. This paper presents a benchmark dataset of PHP bugs on real-world applications called BugsPHP, which can enable research on analysis, testing, and repair for PHP programs. The dataset consists of training and test datasets, separately curated from GitHub and processed locally. The training dataset includes more than 600,000 bug-fixing commits. The test dataset contains 513 manually validated bug-fixing commits equipped with developer-provided test cases to assess patch correctness.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {128–132},
numpages = {5},
keywords = {automated program repair, PHP application errors},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3526072.3527533,
author = {Starace, Luigi Libero Lucio and Romdhana, Andrea and Di Martino, Sergio},
title = {GenRL at the SBST 2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527533},
doi = {10.1145/3526072.3527533},
abstract = {GenRL is a Deep Reinforcement Learning-based tool designed to generate test cases for Lane-Keeping Assist Systems. In this paper, we briefly presents GenRL, and summarize the results of its participation in the Cyber-Physical Systems (CPS) tool competition at SBST 2022.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {49–50},
numpages = {2},
keywords = {advanced driver assistance systems, automotive simulators, cyber-physical systems, reinforcement learning, search-based software testing},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3643916.3644406,
author = {Wang, Che and Li, Yue and Gao, Jianbo and Wang, Ke and Zhang, Jiashuo and Guan, Zhi and Chen, Zhong},
title = {SolaSim: Clone Detection for Solana Smart Contracts via Program Representation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644406},
doi = {10.1145/3643916.3644406},
abstract = {The open-source nature of smart contracts provides the facility for developers to clone contracts and introduces the risk of vulnerability proliferation as well. Despite intensive research on smart contract clone detection in recent years, existing techniques are still unsatisfactory in detecting Solana smart contracts. To fill this gap, in this paper, we designed a clone detection tool SolaSim for Solana smart contracts and conducted an empirical study to understand the code reuse in the Solana ecosystem. Specifically, SolaSim is based on the semantic metadata extractor and the similarity checker. For each contract, the semantic metadata extractor generates an instruction-level weighted Attributed Control Flow Graph (ACFG) and its semantic metadata (i.e., a combination of high-level semantic and structure information) based on Rust Mid-level Intermediate Representation. The similarity checker adopts a combinatorial optimization algorithm to compute the statistical similarity of a pair of contracts. The evaluation results demonstrated the effectiveness of SolaSim in identifying clones with 94.3% accuracy and it can identify up to Type-3 clone level. Notably, we found there are over 50% clone ratios in the Solana smart contracts ecosystem, in which most of them are cloned from famous open-sourced projects.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {258–269},
numpages = {12},
keywords = {smart contract, solana, code reuse, clone detection},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3597503.3639174,
author = {Mastropaolo, Antonio and Ciniselli, Matteo and Di Penta, Massimiliano and Bavota, Gabriele},
title = {Evaluating Code Summarization Techniques: A New Metric and an Empirical Characterization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639174},
doi = {10.1145/3597503.3639174},
abstract = {Several code summarization techniques have been proposed in the literature to automatically document a code snippet or a function. Ideally, software developers should be involved in assessing the quality of the generated summaries. However, in most cases, researchers rely on automatic evaluation metrics such as BLEU, ROUGE, and METEOR. These metrics are all based on the same assumption: The higher the textual similarity between the generated summary and a reference summary written by developers, the higher its quality. However, there are two reasons for which this assumption falls short: (i) reference summaries, e.g., code comments collected by mining software repositories, may be of low quality or even outdated; (ii) generated summaries, while using a different wording than a reference one, could be semantically equivalent to it, thus still being suitable to document the code snippet. In this paper, we perform a thorough empirical investigation on the complementarity of different types of metrics in capturing the quality of a generated summary. Also, we propose to address the limitations of existing metrics by considering a new dimension, capturing the extent to which the generated summary aligns with the semantics of the documented code snippet, independently from the reference summary. To this end, we present a new metric based on contrastive learning to capture said aspect. We empirically show that the inclusion of this novel dimension enables a more effective representation of developers' evaluations regarding the quality of automatically generated summaries.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {218},
numpages = {13},
keywords = {code summarization, contrastive learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00037,
author = {Yan, Ming and Chen, Junjie and Mao, Hangyu and Jiang, Jiajun and Hao, Jianye and Li, Xingjian and Tian, Zhao and Chen, Zhichao and Li, Dong and Xian, Zhangkong and Guo, Yanwei and Liu, Wulong and Wang, Bin and Sun, Yuefeng and Cui, Yongshun},
title = {Achieving Last-Mile Functional Coverage in Testing Chip Design Software Implementations},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00037},
doi = {10.1109/ICSE-SEIP58684.2023.00037},
abstract = {Defective chips may cause huge losses (even disasters), and thus ensuring the reliability of chips is fundamentally important. To ensure the functional correctness of chips, adequate testing is essential on the chip design implementation (CDI), which is the software implementation of the chip under design in hardware description languages, before putting on fabrication. Over the years, while some techniques targeting CDI functional testing have been proposed, there are still a number of hard-to-cover functionality points due to huge input space and complex constraints among variables in a test input. We call the coverage of these points last-mile functional coverage.Here, we propose the first technique targeting the significant challenge of improving last-mile functional coverage in CDI functional testing, called LMT, which does not rely on domain knowledge and CDI internal information. LMT first identifies the relevant variables in test inputs to the coverage of last-mile functionality points inspired by the idea of feature selection in machine learning, so as to largely reduce the search space. It then incorporates Generative Adversarial Network (GAN) to learn to generate valid test inputs (that satisfy complex constraints among variables) with a larger possibility. We conducted a practical study on two industrial CDIs in Huawei to evaluate LMT. The results show that LMT achieves at least 49.27% and 75.09% higher last-mile functional coverage than the state-of-the-art CDI test input generation techniques under the same number of test inputs, and saves at least 94.24% and 84.45% testing time to achieve the same functional coverage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {343–354},
numpages = {12},
keywords = {chip design testing, test generation, functional coverage, machine learning},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3597503.3639182,
author = {Meem, Fairuz Nawer and Smith, Justin and Johnson, Brittany},
title = {Exploring Experiences with Automated Program Repair in Practice},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639182},
doi = {10.1145/3597503.3639182},
abstract = {Automated program repair, also known as APR, is an approach for automatically repairing software faults. There is a large amount of research on automated program repair, but very little offers in-depth insights into how practitioners think about and employ APR in practice. To learn more about practitioners' perspectives and experiences with current APR tools and techniques, we administered a survey, which received valid responses from 331 software practitioners. We analyzed survey responses to gain insights regarding factors that correlate with APR awareness, experience, and use. We established a strong correlation between APR awareness and tool use and attributes including job position, company size, total coding experience, and preferred language of software practitioners. We also found that practitioners are using other forms of support, such as co-workers and ChatGPT, more frequently than APR tools when fixing software defects. We learned about the drawbacks that practitioners encounter while utilizing existing APR tools and the impact that each drawback has on their practice. Our findings provide implications for research and practice centered on development, adoption, and use of APR.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {86},
numpages = {11},
keywords = {automated program repair, software bugs, software tools},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643664.3648202,
author = {Minetto Napole\~{a}o, Bianca and Sarkar, Ritika and Hall\'{e}, Sylvain and Petrillo, Fabio and Kalinowski, Marcos},
title = {Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648202},
doi = {10.1145/3643664.3648202},
abstract = {Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and to support the selection of relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine with approximately 74% recall and 15% precision. The use of such algorithms with conservative thresholds to minimize the risk of missing papers can already significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {34–41},
numpages = {8},
keywords = {systematic review update, SLR update, searching for evidence, selecting evidence},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00150,
author = {Lee, Cheryl and Yang, Tianyi and Chen, Zhuangbin and Su, Yuxin and Lyu, Michael R.},
title = {Eadro: An End-to-End Troubleshooting Framework for Microservices on Multi-Source Data},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00150},
doi = {10.1109/ICSE48619.2023.00150},
abstract = {The complexity and dynamism of microservices pose significant challenges to system reliability, and thereby, automated troubleshooting is crucial. Effective root cause localization after anomaly detection is crucial for ensuring the reliability of microservice systems. However, two significant issues rest in existing approaches: (1) Microservices generate traces, system logs, and key performance indicators (KPIs), but existing approaches usually consider traces only, failing to understand the system fully as traces cannot depict all anomalies; (2) Troubleshooting microservices generally contains two main phases, i.e., anomaly detection and root cause localization. Existing studies regard these two phases as independent, ignoring their close correlation. Even worse, inaccurate detection results can deeply affect localization effectiveness. To overcome these limitations, we propose Eadro, the first end-to-end framework to integrate anomaly detection and root cause localization based on multi-source data for troubleshooting large-scale microservices. The key insights of Eadro are the anomaly manifestations on different data sources and the close connection between detection and localization. Thus, Eadro models intra-service behaviors and inter-service dependencies from traces, logs, and KPIs, all the while leveraging the shared knowledge of the two phases via multi-task learning. Experiments on two widely-used benchmark microservices demonstrate that Eadro outperforms state-of-the-art approaches by a large margin. The results also show the usefulness of integrating multi-source data. We also release our code and data to facilitate future research.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1750–1762},
numpages = {13},
keywords = {microservices, root cause localization, anomaly detection, traces},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3645065,
author = {Patil, Sangameshwar and Ravindran, B.},
title = {Zero Shot Learning based Alternatives for Class Imbalanced Learning Problem in Enterprise Software Defect Analysis},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3645065},
doi = {10.1145/3643991.3645065},
abstract = {Software defect reports are an important type of text data for enterprises as they provide actionable information for improving software quality. Identifying the software defect type automatically can greatly enhance and expedite defect management. Class imbalance is a real-life problem in enterprise software defect classification task and adversely affects the automation effort. We show that zero shot learning based technique can be a good alternative to the well-known supervised learning and SMOTE techniques.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {140–141},
numpages = {2},
keywords = {class imbalance, software defect analysis, zero shot learning},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644871,
author = {Scalco, Simone and Paramitha, Ranindya},
title = {Hash4Patch: A Lightweight Low False Positive Tool for Finding Vulnerability Patch Commits},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644871},
doi = {10.1145/3643991.3644871},
abstract = {[Context:] Patch commits are useful to complete vulnerability datasets for training ML models and for developers to find a safe version for their dependencies. [Objective:] However, there is a gap in the state-of-the-art (SOTA) for a lightweight low False Positive patch commit finder. [Method:] We implemented Hash4Patch, a new tool to be used along with a current SOTA patch finder. We then validated it with a dataset of 160 CVEs. [Results:] Our approach significantly reduced the False Positives produced by a state-of-the-art tool with only 1 minute of additional running time on average. [Conclusions:] Our tool is able to effectively and efficiently reduce the number of alerts found by other patch commit finders, thus minimizing the manual effort needed by developers.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {733–737},
numpages = {5},
keywords = {vulnerability, patch commit, lightweight, hash search},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643916.3644416,
author = {Cotroneo, Domenico and Improta, Cristina and Liguori, Pietro and Natella, Roberto},
title = {Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644416},
doi = {10.1145/3643916.3644416},
abstract = {AI-based code generators have become pivotal in assisting developers in writing software starting from natural language (NL). However, they are trained on large amounts of data, often collected from unsanitized online sources (e.g., GitHub, HuggingFace). As a consequence, AI models become an easy target for data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code.To address this threat, this work investigates the security of AI code generators by devising a targeted data poisoning strategy. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our study shows that AI code generators are vulnerable to even a small amount of poison. Notably, the attack success strongly depends on the model architecture and poisoning rate, whereas it is not influenced by the type of vulnerabilities. Moreover, since the attack does not impact the correctness of code generated by pre-trained models, it is hard to detect. Lastly, our work offers practical insights into understanding and potentially mitigating this threat.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {280–292},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3639478.3640033,
author = {Shivarpatna Venkatesh, Ashwin Prasad and Sabu, Samkutty and Wang, Jiawei and M. Mir, Amir and Li, Li and Bodden, Eric},
title = {TypeEvalPy: A Micro-benchmarking Framework for Python Type Inference Tools},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640033},
doi = {10.1145/3639478.3640033},
abstract = {In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques. This paper introduces TypeEvalPy, a comprehensive micro-benchmarking framework for evaluating type inference tools. TypeEvalPy contains 154 code snippets with 845 type annotations across 18 categories that target various Python features. The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment. Through our analysis, we compare the performance of six type inference tools, highlighting their strengths and limitations. Our findings provide a foundation for further research and optimization in the domain of Python type inference.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {49–53},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643692.3648264,
author = {Sarmiento, Ricardo and De La Cruz, Marina and Ortega, Alfonso and Baena-Galle, Roberto and Girard, Terrence M. and Casetti-Dinescu, Dana I. and Cervantes, Alejandro},
title = {Grammar evolution and symbolic regression for astrometric centering of Hubble Space Telescope images},
year = {2024},
isbn = {9798400705731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643692.3648264},
doi = {10.1145/3643692.3648264},
abstract = {Symbolic regression, in general, and genetic models, in particular, are promising approaches to mathematical modeling in astrometry where it is not always clear which is the fittest analytic expression depending on the problem under consideration. Several attempts and increasing research efforts are being made in this direction mainly from the Genetic Programming (GP) viewpoint. Our proposal is, as far as we know, the first one to apply Grammatical Evolution (GE) in this domain. GE (and further GE extensions) aim to outperform GP limitations by incorporating formal languages tools to guarantee the correctness (both syntactic and semantic) of the generated expressions. The current contribution is a first proof to check the viability of GE on astrometric real datasets. Its success in finding adequate parameters for predefined families of functions in star centering (Gaussian and Moffat PSFs) with simple and naive GE experiments supports our hypothesis on taking advantage of the expressive power of GE to tackle astrometry scenarios of interest and hence greatly improve current astrometric software thanks to specific genetic approaches.},
booktitle = {Proceedings of the 13th ACM/IEEE International Workshop on Genetic Improvement},
pages = {13–20},
numpages = {8},
keywords = {grammatical evolution, symbolic regression, astrometry, WFPC2, hubble space telescope},
location = {Lisbon, Portugal},
series = {GI '24}
}

@inproceedings{10.1145/3643991.3644919,
author = {Le, Triet Huynh Minh and Du, Xiaoning and Babar, M. Ali},
title = {Are Latent Vulnerabilities Hidden Gems for Software Vulnerability Prediction? An Empirical Study},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644919},
doi = {10.1145/3643991.3644919},
abstract = {Collecting relevant and high-quality data is integral to the development of effective Software Vulnerability (SV) prediction models. Most of the current SV datasets rely on SV-fixing commits to extract vulnerable functions and lines. However, none of these datasets have considered latent SVs existing between the introduction and fix of the collected SVs. There is also little known about the usefulness of these latent SVs for SV prediction. To bridge these gaps, we conduct a large-scale study on the latent vulnerable functions in two commonly used SV datasets and their utilization for function-level and line-level SV predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more than 100k latent vulnerable functions in the studied datasets. We find that these latent functions can increase the number of SVs by 4\texttimes{} on average and correct up to 5k mislabeled functions, yet they have a noise level of around 6%. Despite the noise, we show that the state-of-the-art SV prediction model can significantly benefit from such latent SVs. The improvements are up to 24.5% in the performance (F1-Score) of function-level SV predictions and up to 67% in the effectiveness of localizing vulnerable lines. Overall, our study presents the first promising step toward the use of latent SVs to improve the quality of SV datasets and enhance the performance of SV prediction tasks.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {716–727},
numpages = {12},
keywords = {software vulnerability, software security, deep learning, data quality, SZZ algorithm},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639478.3639790,
author = {Ramkumar, Kushal},
title = {Sustainable Adaptive Security},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639790},
doi = {10.1145/3639478.3639790},
abstract = {With software systems permeating our lives, we are entitled to expect that such systems are secure-by-design, and that such security endures throughout the use of these systems and their subsequent evolution. During my PhD, I aim to engineer sustainable adaptive security solutions that reflect such enduring protection in the dynamically changing security theatre of cyber-physical systems. I have chosen the example of a smart home as a cyber-physical system to motivate &amp; illustrate sustainable adaptive security, discuss challenges for sustainably secure systems, and my research plan for engineering them.This research was funded by Science Foundation Ireland grant 13/RC/2094_P2.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {228–230},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3526072.3527538,
author = {Gambi, Alessio and Jahangirova, Gunel and Riccio, Vincenzo and Zampetti, Fiorella},
title = {SBST tool competition 2022},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527538},
doi = {10.1145/3526072.3527538},
abstract = {We report on the organization, challenges, and results of the tenth edition of the Java Unit Testing Competition as well as the second edition of the Cyber-Physical Systems (CPS) Testing Competition.Java Unit Testing Competition. Seven tools, i.e., BBC, EvoSuite, Kex, Kex-Reflection, Randoop, UTBot, and UTBot-Mocks, were executed on a benchmark with 65 classes sampled from four open-source Java projects, for two time budgets: 30 and 120 seconds.CPS Testing Tool Competition. Six tools, i.e., AdaFrenetic, AmbieGen, FreneticV, GenRL, EvoMBT and WOGAN competed on testing two driving agents by generating simulation-based tests. We considered one configuration for each test subject and evaluated the tools' effectiveness and efficiency as well as the failure diversity.This paper describes our methodology, the statistical analysis of the results together with the competing tools, and the challenges faced while running the competition experiments.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {25–32},
numpages = {8},
keywords = {Java, autonomous vehicles, cyber-physical systems, search based software engineering, software testing, test case generation, tool competition, unit testing},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1109/SVM66695.2025.00005,
author = {AlOtaibi, Nourah Saad and Felemban, Muhamad and Mahmood, Sajjad},
title = {Edge-Based Detection of Label Flipping Attacks in Federated Learning Using Explainable AI},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00005},
doi = {10.1109/SVM66695.2025.00005},
abstract = {Federated Learning (FL) is a decentralized machine learning approach that enables collaborative training among distributed clients while preserving data privacy, making it increasingly popular for privacy-sensitive applications over traditional centralized models. However, it introduces new security vulnerabilities that challenge conventional approaches to software vulnerability management. Among these, label flipping attacks (LFAs)—where malicious clients intentionally mislabel data—pose a unique threat to the integrity of FL models. This study presents an AI-driven, edge-based vulnerability detection technique, leveraging explainable AI (XAI) techniques to enhance edge-based security within FL environments. Our method combines Grad-CAM visualizations with DBSCAN clustering to analyze class-specific behavior across clients. By detecting anomalies in Grad-CAM activation patterns, we identify malicious clients with flipped class labels, exploiting patterns in their Grad-CAM heatmaps. This approach is particularly robust against LFAs, examining each class independently and capturing patterns without relying on global model behavior. Empirical results on benchmark datasets such as MNIST and FashionMNIST demonstrate that our method accurately detects LFAs, even when malicious clients constitute a substantial portion of the network. This class-specific, XAI-driven approach contributes to the security of FL by offering an explainable, and scalable solution for managing vulnerabilities in distributed AI systems.},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {1–8},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1145/3639478.3643098,
author = {Ren, Rui and Yang, Jingbang and Yang, Linxiao and Gu, Xinyue and Sun, Liang},
title = {SLIM: a Scalable Light-weight Root Cause Analysis for Imbalanced Data in Microservice},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643098},
doi = {10.1145/3639478.3643098},
abstract = {The newly deployed service - one kind of change service, could lead to a new type of minority fault. Existing state-of-the-art methods for fault localization rarely consider the imbalanced fault classification in change service. This paper proposes a novel method that utilizes decision rule sets to deal with highly imbalanced data by optimizing the F1 score subject to cardinality constraints. The proposed method greedily generates the rule with maximal marginal gain and uses an efficient minorize-maximization (MM) approach to select rules iteratively, maximizing a non-monotone submodular lower bound. Compared with existing fault localization algorithms, our algorithm can adapt to the imbalanced fault scenario of change service, and provide interpretable fault causes which are easy to understand and verify. Our method can also be deployed in the online training setting, with only about 15% training overhead compared to the current SOTA methods. Empirical studies showcase that our algorithm outperforms existing fault localization algorithms in both accuracy and model interpretability.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {328–330},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00017,
author = {Barry, Mariam and Bifet, Albert and Billy, Jean-Luc},
title = {StreamAI: Dealing with Challenges of Continual Learning Systems for Serving AI in Production},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00017},
doi = {10.1109/ICSE-SEIP58684.2023.00017},
abstract = {How to build, deploy, update &amp; maintain dynamic models which continuously learn from streaming data? This paper covers the industrialization aspects of these questions in production systems. In today's fast-changing environments, organizations are faced with the crucial challenge of predictive analytics in online fashion from big data and deploying Artificial Intelligence models at scale. Applications include cyber-security, cloud infrastructure, social networks and financial markets. Online learning models that learn continuously and adapt to the potentially evolving data distributions have demonstrated efficiency for big data stream learning. However, the challenges of deploying and maintaining such models in production (serving) have stalled their adoption. In this paper, we first categorize key challenges faced by the R&amp;D, MLOps and governance teams for deploying automated and self-training AI models in production. Next, we highlight the challenges related to stream-based online machine-learning systems. Finally, we propose StreamAI, a technology-agnostic architecture to deal with the MLOps journey (learning, serving, maintenance) of online models in production. We conclude with open research questions for AI, MLOps and software engineering to bridge the gaps between industry needs and research-oriented development.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {134–137},
numpages = {4},
keywords = {StreamAI, AI, challenges, production, serving, online learning, MLOps, streaming data, industry, banking},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00198,
author = {Chen, Binger and Abedjan, Ziawasch},
title = {DUETCS: Code Style Transfer through Generation and Retrieval},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00198},
doi = {10.1109/ICSE48619.2023.00198},
abstract = {Coding style has direct impact on code comprehension. Automatically transferring code style to user's preference or consistency can facilitate project cooperation and maintenance, as well as maximize the value of open-source code. Existing work on automating code stylization is either limited to code formatting or requires human supervision in pre-defining style checking and transformation rules. In this paper, we present unsupervised methods to assist automatic code style transfer for arbitrary code styles. The main idea is to leverage Big Code database to learn style and content embedding separately to generate or retrieve a piece of code with the same functionality and the desired target style. We carefully encode style and content features, so that a style embedding can be learned from arbitrary code. We explored the capabilities of novel attention-based style generation models and meta-learning and implemented our ideas in DUETCS. We complement the learning-based approach with a retrieval mode, which uses the same embeddings to directly search for the desired piece of code in Big Code. Our experiments show that DUETCS captures more style aspects than existing baselines.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2362–2373},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00138,
author = {Chang, Zhiyuan and Li, Mingyang and Wang, Qing and Li, Shoubin and Wang, Junjie},
title = {Cross-Domain Requirements Linking via Adversarial-Based Domain Adaptation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00138},
doi = {10.1109/ICSE48619.2023.00138},
abstract = {Requirements linking is the core of software system maintenance and evolution, and it is critical to assuring software quality. In practice, however, the requirements links are frequently absent or incorrectly labeled, and reconstructing such ties is time-consuming and error-prone. Numerous learning-based approaches have been put forth to address the problem. However, these approaches will lose effectiveness for the Cold-Start projects with few labeled samples. To this end, we propose RADIATION, an adversarial-based domain adaptation approach for cross-domain requirements linking. Generally, RADIATION firstly adopts an IDF-based Masking strategy to filter the domain-specific features. Then it pre-trains a linking model in the source domain with sufficient labeled samples and adapts the model to target domains using a distance-enhanced adversarial technique without using any labeled target samples. Evaluation on five public datasets shows that RADIATION could achieve 66.4% precision, 89.2% recall, and significantly outperform state-of-the-art baselines by 13.4%-42.9% F1. In addition, the designed components, i.e., IDF-based Masking and Distance-enhanced Loss, could significantly improve performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1596–1608},
numpages = {13},
keywords = {cross-domain requirements linking, domain adaptation, adversarial learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643915.3644107,
author = {Imrie, Calum and Howard, Rhys and Thuremella, Divya and Proma, Nawshin Mannan and Pandey, Tejas and Lewinska, Paulina and Cannizzaro, Ricardo and Hawkins, Richard and Paterson, Colin and Kunze, Lars and Hodge, Victoria},
title = {Aloft: Self-Adaptive Drone Controller Testbed},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644107},
doi = {10.1145/3643915.3644107},
abstract = {Aerial drones are increasingly being considered as a valuable tool for inspection in safety critical contexts. Nowhere is this more true than in mining operations which present a dynamic and dangerous environment for human operators. Drones can be deployed in a number of contexts including efficient surveying as well as search and rescue missions. Operating in these dynamic contexts is challenging however and requires the drones control software to detect and adapt to conditions at run-time.To help in the development of such systems we present Aloft, a simulation supported testbed for investigating self-adaptive controllers for drones in mines. Aloft utilises the Robot Operating system (ROS) and a model environment using Gazebo to provide a physics-based testing. The simulation environment is constructed from a 3D point cloud collected in a physical mock-up of a mine and contains features expected to be found in real-world contexts.Aloft allows members of the research community to deploy their own self-adaptive controllers into the control loop of the drone to evaluate the effectiveness and robustness of controllers in a challenging environment. To demonstrate our system we provide a self-adaptive drone controller and operating scenario as an exemplar. The self-adaptive drone controller provided utilises a two-layered architecture with a MAPE-K feedback loop. The scenario is an inspection task during which we inject a communications failure. The aim of the controller is to detect this loss of communication and autonomously perform a return home behaviour. Limited battery life presents a constraint on the mission, which therefore means that the drone should complete its mission as fast as possible. Humans, however, might also be present within the environment. This poses a safety risk and the drone must be able to avoid collisions during autonomous flight.In this paper we describe the controller framework and the simulation environment and provide information on how a user might construct and evaluate their own controllers in the presence of disruptions at run-time.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {70–76},
numpages = {7},
keywords = {self-adaptive systems, unmanned aerial vehicles, controller framework testing, mining operations},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3639478.3640045,
author = {Marussy, Krist\'{o}f and Ficsor, Attila and Semer\'{a}th, Oszk\'{a}r and Varr\'{o}, D\'{a}niel},
title = {Refinery: Graph Solver as a Service: Refinement-based Generation and Analysis of Consistent Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640045},
doi = {10.1145/3639478.3640045},
abstract = {Various software and systems engineering scenarios rely on the systematic construction of consistent graph models. However, automatically generating a diverse set of consistent graph models for complex domain specifications is challenging. First, the graph generation problem must be specified with mathematical precision. Moreover, graph generation is a computationally complex task, which necessitates specialized logic solvers. Refinery is a novel open-source software framework to automatically synthesize a diverse set of consistent domain-specific graph models. The framework offers an expressive high-level specification language using partial models to succinctly formulate a wide range of graph generation challenges. Moreover, it provides a modern cloud-based architecture for a scalable graph solver as a service, which uses logic reasoning rules to efficiently synthesize a diverse set of solutions to graph generation problems by partial model refinement. Applications include system-level architecture synthesis, test generation for modeling tools or traffic scenario synthesis for autonomous vehicles.Video demonstration: https://youtu.be/Qy_3udNsWsMContinuously deployed at: https://refinery.sen/ices/},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {64–68},
numpages = {5},
keywords = {model generation, partial modeling, logic solver, cloud service},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3623347,
author = {Stalnaker, Trevor and Wintersgill, Nathan and Chaparro, Oscar and Di Penta, Massimiliano and German, Daniel M and Poshyvanyk, Denys},
title = {BOMs Away! Inside the Minds of Stakeholders: A Comprehensive Study of Bills of Materials for Software Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623347},
doi = {10.1145/3597503.3623347},
abstract = {Software Bills of Materials (SBOMs) have emerged as tools to facilitate the management of software dependencies, vulnerabilities, licenses, and the supply chain. While significant effort has been devoted to increasing SBOM awareness and developing SBOM formats and tools, recent studies have shown that SBOMs are still an early technology not yet adequately adopted in practice. Expanding on previous research, this paper reports a comprehensive study that investigates the current challenges stakeholders encounter when creating and using SBOMs. The study surveyed 138 practitioners belonging to five stakeholder groups (practitioners familiar with SBOMs, members of critical open source projects, AI/ML, cyberphysical systems, and legal practitioners) using differentiated questionnaires, and interviewed 8 survey respondents to gather further insights about their experience. We identified 12 major challenges facing the creation and use of SBOMs, including those related to the SBOM content, deficiencies in SBOM tools, SBOM maintenance and verification, and domain-specific challenges. We propose and discuss 4 actionable solutions to the identified challenges and present the major avenues for future research and development.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {44},
numpages = {13},
keywords = {software bill of materials, survey, interviews, software supply chain, open source software},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643535,
author = {Li, Zhengxin and Zhao, Junfeng and Kang, Jia},
title = {Multi-source Anomaly Detection For Microservice Systems},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643535},
doi = {10.1145/3639478.3643535},
abstract = {Microservices architecture has advantages such as independent development, independent deployment, scalability, and reusability. However, faults are inevitable during the operation of microservices systems. This paper introduces an anomaly detection approach based on multi-source data, which combines multi-level attention mechanisms and multi-scale convolutional neural networks. It designs feature extraction modules for different data sources, effectively capturing features of log data and KPI (Key Performance Indicator) data. The extracted features from different data sources are input in parallel to an attention network, where they are weighted and fused. Finally, the fused features are input into the anomaly detection model for detection. We deployed an open-source benchmark microservices system, TrainTicket [1], and injected various typical faults to validate our approach. Experimental results indicate that compared to existing approach, this approach can more accurately identify anomalies.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {414–415},
numpages = {2},
keywords = {microservices, anomaly detection, multi-source data, attention network, feature extraction drfg},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@proceedings{10.1145/3643915,
title = {SEAMS '24: Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, AA, Portugal}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00063,
author = {Szalay, Rich\'{a}rd},
title = {Towards Strengthening Software Library Interfaces with Granular and Interactive Type Migrations},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00063},
doi = {10.1109/ICSE-Companion58688.2023.00063},
abstract = {The interface boundaries of software projects are a crucial perimeter from both a design and security point of view. Design decisions of libraries will inadvertently affect client code which can neither legally nor technically change the library's contract. Mistakes allowed by the interface design, such as argument selection defects, can only be caught with existing tools once made. This is made worse in C++, as functions may take parameters which types are implicitly convertible to one another. Instead, I proposed a proactive step to detect and improve when a library interface exhibits properties that may lead to inadvertent misuse. Actionable fixes for the reports are possible through an interactive type refactoring method. Existing refactorings for type migration required the new types and the migration process to be well-defined in advance; otherwise, the code might be changed to a non-compilable state. This paper summarises my thesis proposal in which the problem of weak function interfaces is detected and solved by the Fictive Types method, which first "colours" the software project with new type information, deferring the complete rewrite once the required interface of the new type is fully explored.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {230–234},
numpages = {5},
keywords = {C++ programming language, software refactoring, static analysis, type safety, strong typing, library interface},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643915.3644096,
author = {de Lemos, Rogerio},
title = {Bio-inspired computing systems: handle with care, discard if need it},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644096},
doi = {10.1145/3643915.3644096},
abstract = {Nature has an excellent track record in solving problems, and while biological inspired approaches draw inspiration from nature, they should not emulate it blindly. What works for nature may not work for computer systems - bio-inspired computing comes to the rescue. In this position paper, we look into the problem of bio-inspired computing from two perspectives, that of models and algorithms. In the context of self-adaptive software systems, the challenge is to come up with approaches that are able to generate specific solutions on demand and during operational-time.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {107–109},
numpages = {3},
keywords = {feedback control loop, models, reinforcement learning, bio-inspired computing},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00037,
author = {He, Hao and Zhou, Minghui and Wang, Qingye and Li, Jingyue},
title = {Open Source Software Onboarding as a University Course: An Experience Report},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00037},
doi = {10.1109/ICSE-SEET58685.2023.00037},
abstract = {Without newcomers, open source software (OSS) projects are hardly sustainable. Yet, newcomers face a steep learning curve during OSS onboarding in which they must overcome a multitude of technical, social, and knowledge barriers. To ease the onboarding process, OSS communities are utilizing mentoring, task recommendation (e.g., "good first issues"), and engagement programs (e.g., Google Summer of Code). However, newcomers must first cultivate their motivation for OSS contribution and learn the necessary preliminaries before they can take advantage of these mechanisms. We believe this gap can be filled by a dedicated, practice-oriented OSS onboarding course. In this paper, we present our experience of teaching an OSS onboarding course at Peking University. The course contains a series of lectures, labs, and invited talks to prepare students with the required skills and motivate them to contribute to OSS. In addition, students are required to complete a semester-long course project in which they plan and make actual contributions to OSS projects. They can either contribute to some recommended OSS projects with dedicated mentors, or contribute to any OSS project they prefer. Finally, 16 out of the 19 enrolled students have successfully contributed to OSS projects, and five have retained. However, the onboarding trajectories, final contributions, and retention outcomes differ vastly between the two groups of students with different course project choices, yielding lessons for software engineering education.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {324–336},
numpages = {13},
keywords = {open source software, open source onboarding, software engineering education},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00043,
author = {Lai, Zhongzheng and Yuan, Dong and Chen, Huaming and Zhang, Yu and Bao, Wei},
title = {elessDT: A Digital Twin Platform for Real-Time Evaluation of Wireless Software Applications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00043},
doi = {10.1109/ICSE-Companion58688.2023.00043},
abstract = {Wireless technology has become one of the most important parts of our daily routine. Besides being used for communication, the wireless signal has been applied to various Wireless Software Applications (WSAs). The signal fluctuation caused by the measurement system or the environmental dynamic can significantly influence WSAs' performance, making it challenging to evaluate WSAs in real-world scenarios. To overcome these challenges, we propose WirelsssDT, a wireless digital twin platform, using digital twin and real-time ray tracing technologies to emulate the wireless signals propagation and generate emulation data for real-time WSAs evaluation. In this demonstration, we evaluate a wireless indoor localisation mobile application with two typical prediction algorithms: 1) Kalman Filter-based Trilateration and 2) Deep Recurrent Neural Network, as a case study to demonstrate the capabilities of WirelessDT. The source code is available at https://github.com/codelzz/WirelessDT, and the demonstration video is available at https://youtu.be/9Kl-3jgMBUA.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {146–150},
numpages = {5},
keywords = {emulation tool, wireless signal emulation, wireless software evaluation, digital twin},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643656.3643897,
author = {Soha, Peter Attila and Vancsics, Bela and Gergely, Tam\'{a}s and Beszedes, Arpad},
title = {Flaky Tests in the AI Domain},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643897},
doi = {10.1145/3643656.3643897},
abstract = {In this position paper, we investigate how frequently is randomness the cause of flakiness in the traditional and in the AI-enabled software domains. Based on previous works, it seems that while in the general domain flakiness rarely stems from randomness, in the AI domain it is a frequent phenomenon. Thus, we urge a discussion about a classification scheme of flaky tests based on whether they are caused by the inherent randomness of the AI-enabled SUT or some other reason. This way, better identification, classification and proper handling of flakiness in such systems will be possible.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {20–21},
numpages = {2},
keywords = {flaky test, artificial intelligence, machine learning, randomness},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00066,
author = {Mammadov, Tural},
title = {Learning Program Models from Generated Inputs},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00066},
doi = {10.1109/ICSE-Companion58688.2023.00066},
abstract = {Recent advances in Machine Learning (ML) show that Neural Machine Translation (NMT) models can mock the program behavior when trained on input-output pairs. Such models can mock the functionality of existing programs and serve as quick-to-deploy reverse engineering tools. Still, the problem of automatically learning such predictive and reversible models from programs needs to be solved. This work introduces a generic approach for automated and reversible program behavior modeling. It achieves 94% of overall accuracy in the conversion of Markdown-to-HTML and HTML-to-Markdown markups.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {245–247},
numpages = {3},
keywords = {software testing, security testing, reverse engineering, deep learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3643082,
author = {Li, Yi and Nguyen, Tien N. and Cai, Yuchen and Yadavally, Aashish and Mishra, Abhishek and Montejo, Genesis},
title = {Neural Exception Handling Recommender},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643082},
doi = {10.1145/3639478.3643082},
abstract = {Practical code reuse often leads to the incorporation of code fragments from developer forums into applications. However, these fragments, being incomplete, frequently lack details on exception handling. Integrating exception handling into a codebase is not a straightforward task, requiring developers to understand and remember which API methods may trigger exceptions and which exceptions should be handled. To address that, we introduce EHBlock, a learning-based exception handling recommender for Java code snippets. EHBlock analyzes a given code snippet and suggests whether a try-catch block is necessary. It employs a Relational Graph Convolutional Network (R-GCN) to learn exception handling from complete code. R-GCN considers program dependencies in the surrounding context, allowing EHBlock to learn the identities of APIs and their relations with corresponding exception types that need to be handled. Our empirical evaluation shows that EHBlock achieves a 12.3% improvement in F-score compared to the state-of-the-art approach in determining the need of try-catch blocks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {316–317},
numpages = {2},
keywords = {AI4SE, exception handling, graph convolutional network},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643916.3644413,
author = {Dhaouadi, Mouna and Oakes, Bentley James and Famelis, Michalis},
title = {Rationale Dataset and Analysis for the Commit Messages of the Linux Kernel Out-of-Memory Killer},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644413},
doi = {10.1145/3643916.3644413},
abstract = {Code commit messages can contain useful information on why a developer has made a change. However, the presence and structure of rationale in real-world code commit messages is not well studied. Here, we detail the creation of a labelled dataset to analyze the code commit messages of the Linux Kernel Out-Of-Memory Killer component. We study aspects of rationale information, such as presence, temporal evolution, and structure. We find that 98.9% of commits in our dataset contain sentences with rationale information, and that experienced developers report rationale in about 60% of the sentences in their commits. We report on the challenges we faced and provide examples for our labelling.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {415–425},
numpages = {11},
keywords = {developer rationale, dataset, Linux kernel, commit messages},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@proceedings{10.1145/3643664,
title = {WSESE '24: Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WSESE 2024 was a one-day event held on April 16, 2024, in Lisbon, Portugal. The theme of the workshop was "Methodological Issues with Empirical Studies in Software Engineering". The primary goal was to gain a better understanding of the adoption of the empirical paradigm in SE. Specifically, our focus was on identifying, discussing and finding solutions for the issues in the empirical methods currently employed. The workshop provided an opportunity for researchers and practitioners to discuss current methodological challenges and explore ways to address them.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00102,
author = {Laaber, Christoph and Yue, Tao and Ali, Shaukat and Schwitalla, Thomas and Nyg\r{a}rd, Jan F.},
title = {Challenges of Testing an Evolving Cancer Registration Support System in Practice},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00102},
doi = {10.1109/ICSE-Companion58688.2023.00102},
abstract = {The Cancer Registry of Norway (CRN) is a public body responsible for capturing and curating cancer patient data histories to provide a unified access to research data and statistics for doctors, patients, and policymakers. For this purpose, CRN develops and operates a complex, constantly-evolving, and socio-technical software system. Recently, machine learning (ML) algorithms have been introduced into this system to augment the manual decisions made by humans with automated decision support from learned models. To ensure that the system is correct and robust and cancer patients' data are properly handled and do not violate privacy concerns, automated testing solutions are being developed. In this paper, we share the challenges that we identified when developing automated testing solutions at CRN. Such testing potentially impacts the quality of cancer data for years to come, which is also used by the system's stakeholders to make critical decisions. The challenges identified are not specific to CRN but are also valid in the context of other healthcare registries. We also provide some details on initial solutions that we are investigating to solve the identified challenges.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {355–359},
numpages = {5},
keywords = {research challenges, healthcare, cancer registry, software testing, evolution},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623336,
author = {Dong, Chaopeng and Li, Siyuan and Yang, Shouguo and Xiao, Yang and Wang, Yongpan and Li, Hong and Li, Zhi and Sun, Limin},
title = {LibvDiff: Library Version Difference Guided OSS Version Identification in Binaries},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623336},
doi = {10.1145/3597503.3623336},
abstract = {Open-source software (OSS) has been extensively employed to expedite software development, inevitably exposing downstream software to the peril of potential vulnerabilities. Precisely identifying the version of OSS not only facilitates the detection of vulnerabilities associated with it but also enables timely alerts upon the release of 1-day vulnerabilities. However, current methods for identifying OSS versions rely heavily on version strings or constant features, which may not be present in compiled OSS binaries or may not be representative when only function code changes are made. As a result, these methods are often imprecise in identifying the version of OSS binaries being used.To this end, we propose LibvDiff, a novel approach for identifying open-source software versions. It detects subtle differences through precise symbol information and function-level code changes using binary code similarity detection. LibvDiff introduces a candidate version filter based on a novel version coordinate system to improve efficiency by quantifying gaps between versions and rapidly identifying potential versions. To speed up the code similarity detection process, LibvDiff proposes a function call-based anchor path filter to minimize the number of functions compared in the target binary. We evaluate the performance of LibvDiff through comprehensive experiments under various compilation settings and two datasets (one with version strings, and the other without version strings), which demonstrate that our approach achieves 94.5% and 78.7% precision in two datasets, outperforming state-of-the-art works (including both academic methods and industry tools) by an average of 54.2% and 160.3%, respectively. By identifying and analyzing OSS binaries in real-world firmware images, we make several interesting findings, such as developers having significant differences in their updates to different OSS, and different vendors may also utilize identical OSS binaries.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {66},
numpages = {12},
keywords = {open-source software, version identification, vulnerability detection, firmware analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643083,
author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
title = {A Transformer-based Model for Assisting Dockerfile Revising},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643083},
doi = {10.1145/3639478.3643083},
abstract = {Dockerfile plays an important role in the containerized software development process since it specifies the structure and functionality of the built Docker image. Currently, Dockerfile writing and modification still rely on manual operations which can be time-consuming. Thus, there is a need for automation tools to support the Dockerfile revising process. In this study, we focus on utilizing pre-training techniques for the tasks in the Dockerfile revising scenario. We propose a Transformer-based model and pre-train it with an instruction-aware objective. Furthermore, we fine-tune our model in two downstream tasks, including revision opportunity estimation and revision activity prediction. The experimental results show that our model outperforms the baseline models.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {312–313},
numpages = {2},
keywords = {docker, dockerfile, deep learning, transformer, pre-training},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00094,
author = {Lyu, Yunbo and Le-Cong, Thanh and Kang, Hong Jin and Widyasari, Ratnadira and Zhao, Zhipeng and Le, Xuan-Bach D. and Li, Ming and Lo, David},
title = {Chronos: Time-Aware Zero-Shot Identification of Libraries from Vulnerability Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00094},
doi = {10.1109/ICSE48619.2023.00094},
abstract = {Tools that alert developers about library vulnerabilities depend on accurate, up-to-date vulnerability databases which are maintained by security researchers. These databases record the libraries related to each vulnerability. However, the vulnerability reports may not explicitly list every library and human analysis is required to determine all the relevant libraries. Human analysis may be slow and expensive, which motivates the need for automated approaches. Researchers and practitioners have proposed to automatically identify libraries from vulnerability reports using extreme multi-label learning (XML).While state-of-the-art XML techniques showed promising performance, their experimental settings do not practically fit what happens in reality. Previous studies randomly split the vulnerability reports data for training and testing their models without considering the chronological order of the reports. This may unduly train the models on chronologically newer reports while testing the models on chronologically older ones. However, in practice, one often receives chronologically new reports, which may be related to previously unseen libraries. Under this practical setting, we observe that the performance of current XML techniques declines substantially, e.g., F1 decreased from 0.7 to 0.24 under experiments without and with consideration of chronological order of vulnerability reports.We propose a practical library identification approach, namely Chronos, based on zero-shot learning. The novelty of Chronos is three-fold. First, Chronos fits into the practical pipeline by considering the chronological order of vulnerability reports. Second, Chronos enriches the data of the vulnerability descriptions and labels using a carefully designed data enhancement step. Third, Chronos exploits the temporal ordering of the vulnerability reports using a cache to prioritize prediction of versions of libraries that recently had reports of vulnerabilities.In our experiments, Chronos achieves an average F1-score of 0.75, 3x better than the best XML-based approach. Data enhancement and the time-aware adjustment improve Chronos over the vanilla zero-shot learning model by 27% in average F1.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1033–1045},
numpages = {13},
keywords = {zero-shot learning, library identification, unseen labels, extreme multi-label classification, vulnerability reports},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3639791,
author = {Hossain, Soneya Binta},
title = {Ensuring Critical Properties of Test Oracles for Effective Bug Detection},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639791},
doi = {10.1145/3639478.3639791},
abstract = {With software becoming essential in all aspects of our lives, especially in critical areas like medical and avionic systems, the need for robust and reliable software is more critical than ever. Even seemingly insignificant software bugs can compromise system stability and security, as evidenced by a simple copy-paste error in Apple devices accepting invalid SSL certificates and a date formatting issue causing a widespread Twitter outage. These realities underscore the need for effective testing and bug detection mechanisms to ensure software reliability. At the heart of this challenge are test oracles, a fundamental component of testing, which play a crucial role in detecting software bugs.Recognizing the pivotal role of test oracles, my research conducts large-scale studies to understand their impact on bug detection effectiveness, identify limitations in existing test adequacy metrics and automated oracle generation methods. Based on the findings, my research identifies three key properties of test oracles essential for effective bug detection, referred to as CCS (check, correct, strong). These properties ensure that test oracles thoroughly check codes, are correct based on the specification and strong for bug detection. To enforce the CCS properties, my research introduces a set of methods, leading to the development of OracleGuru framework that significantly enhances the quality of test oracles.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {176–180},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3647632.3651394,
author = {Thung, Ferdian and Irsan, Ivana Clairine and Liu, Jiakun and Lo, David},
title = {Towards Benchmarking the Coverage of Automated Testing Tools in Android against Manual Testing},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3651394},
doi = {10.1145/3647632.3651394},
abstract = {Android apps are commonly used nowadays as smartphones have become irreplaceable parts of modern lives. To ensure that these apps work correctly, developers would need to test them. Testing these apps is laborious, tedious, and often time consuming. Thus, many automated testing tools for Android have been proposed. These tools generate test cases that aim to achieve as much code coverage as possible. A lot of testing methodologies are employed such as model-based testing, search-based testing, random testing, fuzzing, concolic execution, and mutation. Despite much efforts, it is not perfectly clear how far these testing tools can cover user behaviours. To fill this gap, we want to measure the gap between the coverage of automated testing tools and manual testing. In this preliminary work, we selected a set of 11 Android apps and ran state-of-the-art automated testing tools on them. We also manually tested these apps by following a guideline on actions that we need to exhaust when exploring the apps. Our work highlights that automated tools need to close some gaps before they can achieve coverage that is comparable to manual testing. We also present some limitations that future automated tools need to overcome to achieve such coverage.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {74–77},
numpages = {4},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1145/3597503.3623312,
author = {Tileria, Marcos and Blasco, Jorge and Dash, Santanu Kumar},
title = {DocFlow: Extracting Taint Specifications from Software Documentation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623312},
doi = {10.1145/3597503.3623312},
abstract = {Security practitioners routinely use static analysis to detect security problems and privacy violations in Android apps. The soundness of these analyses depends on how the platform is modelled and the list of sensitive methods. Collecting these methods often becomes impractical given the number of methods available, the pace at which the Android platform is updated, and the proprietary libraries Google releases on each new version. Despite the constant evolution of the Android platform, app developers cope with all these new features thanks to the documentation that comes with each new Android release. In this work, we take advantage of the rich documentation provided by platforms like Android and propose DocFlow, a framework to generate taint specifications for a platform, directly from its documentation. DocFlow models the semantics of API methods using their documentation to detect sensitive methods (sources and sinks) and assigns them semantic labels. Our approach does not require access to source code, enabling the analysis of proprietary libraries for which the code is unavailable. We evaluate DocFlow using Android platform packages and closed-source Google Play Services libraries. Our results show that our framework detects sensitive methods with high precision, adapts to new API versions, and can be easily extended to detect other method types. Our approach provides evidence that Android documentation encodes rich semantic information to categorise sensitive methods, removing the need to analyse source code or perform feature extraction.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {61},
numpages = {12},
keywords = {taint analysis, documentation, android, natural language processing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639184,
author = {Chow, Yiu Wai and Di Grazia, Luca and Pradel, Michael},
title = {PyTy: Repairing Static Type Errors in Python},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639184},
doi = {10.1145/3597503.3639184},
abstract = {Gradual typing enables developers to annotate types of their own choosing, offering a flexible middle ground between no type annotations and a fully statically typed language. As more and more code bases get type-annotated, static type checkers detect an increasingly large number of type errors. Unfortunately, fixing these errors requires manual effort, hampering the adoption of gradual typing in practice. This paper presents PyTy, an automated program repair approach targeted at statically detectable type errors in Python. The problem of repairing type errors deserves specific attention because it exposes particular repair patterns, offers a warning message with hints about where and how to apply a fix, and because gradual type checking serves as an automatic way to validate fixes. We addresses this problem through three contributions: (i) an empirical study that investigates how developers fix Python type errors, showing a diverse set of fixing strategies with some recurring patterns; (ii) an approach to automatically extract type error fixes, which enables us to create a dataset of 2,766 error-fix pairs from 176 GitHub repositories, named PyTyDefects; (iii) the first learning-based repair technique for fixing type errors in Python. Motivated by the relative data scarcity of the problem, the neural model at the core of PyTy is trained via cross-lingual transfer learning. Our evaluation shows that PyTy offers fixes for ten frequent categories of type errors, successfully addressing 85.4% of 281 real-world errors. This effectiveness outperforms state-of-the-art large language models asked to repair type errors (by 2.1x) and complements a previous technique aimed at type errors that manifest at runtime. Finally, 20 out of 30 pull requests with PyTy-suggested fixes have been merged by developers, showing the usefulness of PyTy in practice.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {87},
numpages = {13},
keywords = {automatic program repair, type annotation, transfer learning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00065,
author = {Mojica-Hanke, Anamaria},
title = {Towards Machine Learning Guided by Best Practices},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00065},
doi = {10.1109/ICSE-Companion58688.2023.00065},
abstract = {Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where multiple studies related to the use of Machine Learning in Software Engineering have been published in conferences and journals. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional software, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and focused on their domain (e.g., biomechanics), but for the best of our knowledge, there is no guideline focused on the SE community. This thesis aims to reduce the gap of not having clear guidelines in the SE community by using possible sources of practices such as question-and-answer communities and also previous research studies. As a result, we will present a set of practices with an SE perspective, for researchers and practitioners, including a tool for searching them.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {240–244},
numpages = {5},
keywords = {machine learning, good practices, software engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643787.3648044,
author = {Hai, Nam Le and Bui, Nghi D. Q.},
title = {Dopamin: Transformer-based Comment Classifiers through Domain Post-Training and Multi-level Layer Aggregation},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648044},
doi = {10.1145/3643787.3648044},
abstract = {Code comments provide important information for understanding the source code. They can help developers understand the overall purpose of a function or class, as well as identify bugs and technical debt. However, an overabundance of comments is meaningless and counterproductive. As a result, it is critical to automatically filter out these comments for specific purposes. In this paper, we present Dopamin, a Transformer-based tool for dealing with this issue. Our model excels not only in presenting knowledge sharing of common categories across multiple languages, but also in achieving robust performance in comment classification by improving comment representation. As a result, it outperforms the STACC baseline by 3% on the NLBSE'24 Tool Competition dataset in terms of average F1-score, while maintaining a comparable inference time for practical use. The source code is publicity available at https://github.com/FSoft-AI4Code/Dopamin.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {61–64},
numpages = {4},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3643691.3648586,
author = {Wen, Jiawen and Yuan, Dong and Ma, Lei and Chen, Huaming},
title = {Code Ownership in Open-Source AI Software Security},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648586},
doi = {10.1145/3643691.3648586},
abstract = {As open-source AI software projects become an integral component in the AI software development, it is critical to develop a novel measurement method to ensure the security of the open-source AI projects for developers. Code ownership, pivotal in the evolution of such projects, offers insights into developer engagement and potential vulnerabilities. In this paper, we leverage the code ownership metrics to empirically investigate the correlation with the latent vulnerabilities across five prominent open-source AI software projects. The findings from the large-scale empirical study suggest a positive relationship between high-level ownership (characterised by a limited number of minor contributors) and a decrease in vulnerabilities. Furthermore, we innovatively introduce the time metrics, anchored on the project's duration, individual source code file timelines, and the count of impacted releases. These metrics adeptly categorise distinct phases of open-source AI software projects and their respective vulnerability intensities. With these novel code ownership metrics, we have implemented a Python-based command-line application to aid project curators and quality assurance professionals in evaluating and benchmarking their on-site projects. We anticipate this work will embark a continuous research development for securing and measuring open-source AI project security.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {28–35},
numpages = {8},
keywords = {open-source software, AI, security management, code ownership, process metrics, empirical software engineering},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00083,
author = {Ran, Dezhi and Wang, Hao and Wang, Wenyu and Xie, Tao},
title = {Badge: Prioritizing UI Events with Hierarchical Multi-Armed Bandits for Automated UI Testing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00083},
doi = {10.1109/ICSE48619.2023.00083},
abstract = {To assure high quality of mobile applications (apps for short), automated UI testing triggers events (associated with UI elements on app UIs) without human intervention, aiming to maximize code coverage and find unique crashes. To achieve high test effectiveness, automated UI testing prioritizes a UI event based on its exploration value (e.g., the increased code coverage of future exploration rooted from the UI event). Various strategies have been proposed to estimate the exploration value of a UI event without considering its exploration diversity (reflecting the variance of covered code entities achieved by explorations rooted from this UI event across its different triggerings), resulting in low test effectiveness, especially on complex mobile apps. To address the preceding problem, in this paper, we propose a new approach named Badge to prioritize UI events considering both their exploration values and exploration diversity for effective automated UI testing. In particular, we design a hierarchical multi-armed bandit model to effectively estimate the exploration value and exploration diversity of a UI event based on its historical explorations along with historical explorations rooted from UI events in the same UI group. We evaluate Badge on 21 highly popular industrial apps widely used by previous related work. Experimental results show that Badge outperforms state-of-the-art/practice tools with 18%-146% relative code coverage improvement and finding 1.19--5.20x unique crashes, demonstrating the effectiveness of Badge. Further experimental studies confirm the benefits brought by Badge's individual algorithms.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {894–905},
numpages = {12},
keywords = {GUI testing, mobile testing, mobile app, android, multi-armed bandits, reinforcement learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639110,
author = {Pan, Shengyi and Bao, Lingfeng and Zhou, Jiayuan and Hu, Xing and Xia, Xin and Li, Shanping},
title = {Towards More Practical Automation of Vulnerability Assessment},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639110},
doi = {10.1145/3597503.3639110},
abstract = {It is increasingly suggested to identify emerging software vulnerabilities (SVs) through relevant development activities (e.g., issue reports) to allow early warnings to open source software (OSS) users. However, the support for the following assessment of the detected SVs has not yet been explored. SV assessment characterizes the detected SVs to prioritize limited remediation resources on the critical ones. To fill this gap, we aim to enable early vulnerability assessment based on SV-related issue reports (SIR). Besides, we observe the following concerns of the existing assessment techniques: 1) the assessment output lacks rationale and practical value; 2) the associations between Common Vulnerability Scoring System (CVSS) metrics have been ignored; 3) insufficient evaluation scenarios and metrics. We address these concerns to enhance the practicality of our proposed early vulnerability assessment approach (namely proEVA). Specifically, based on the observation of strong associations between CVSS metrics, we propose a prompt-based model to exploit such relations for CVSS metrics prediction. Moreover, we design a curriculum-learning (CL) schedule to guide the model better learn such hidden associations during training. Aside from the standard classification metrics adopted in existing works, we propose two severity-aware metrics to provide a more comprehensive evaluation regarding the prioritization of the high-severe SVs. Experimental results show that proEVA significantly outperforms the baselines in both types of metrics. We further discuss the transferability of the prediction model regarding the upgrade of the assessment system, an important yet overlooked evaluation scenario in existing works. The results verify that proEVA is more efficient and flexible in migrating to different assessment systems.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {148},
numpages = {13},
keywords = {software security, vulnerability assessment, CVSS},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643691.3648585,
author = {Pintz, Maximilian and Becker, Daniel and Mock, Michael},
title = {PARMA: a Platform Architecture to enable Automated, Reproducible, and Multi-party Assessments of AI Trustworthiness},
year = {2024},
isbn = {9798400705724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643691.3648585},
doi = {10.1145/3643691.3648585},
abstract = {As AI applications are emerging in diverse fields - e.g., industry, healthcare or finance - weaknesses and failures of such applications might bare unacceptable risks which need to be rigorously assessed, quantified and, if necessary, mitigated. One crucial component of an effective AI trustworthiness assessment and risk management are systematic evaluations of the AI application based on properly chosen and executed tests. In addition to the known requirements of providing facilities for automated and reproducible tests, an assessment platform for Trustworthy AI must support the integration of different AI models and data sets, must be extensible for AI risk specific metrics and test tools, and should facilitate collaboration between model providers, assessment tool developers and auditors. In this paper, we develop an architecture of a platform for automated, reproducible and collaborative assessments of AI applications, based on an in-depth requirements analysis that maps use cases and collaboration scenarios to technical requirements.},
booktitle = {Proceedings of the 2nd International Workshop on Responsible AI Engineering},
pages = {20–27},
numpages = {8},
location = {Lisbon, Portugal},
series = {RAIE '24}
}

@inproceedings{10.1145/3643915.3644100,
author = {Van Landuyt, Dimitri and Halasz, David and Verreydt, Stef and Weyns, Danny},
title = {Towards Understanding Trust in Self-adaptive Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644100},
doi = {10.1145/3643915.3644100},
abstract = {Self-adaptive systems (SASs) can change their structures autonomously and dynamically adapt their behaviors aiming at (i) attaining longer-term system goals and (ii) coping with inevitable dynamics and changes in their operational environments that are difficult to anticipate. As SASs directly or indirectly interact with, and affect humans, such degrees of autonomy create the necessity for these systems to be trusted or considered trustworthy.While the notions of 'trust' and 'trustworthiness' have been investigated for over a decade, particularly by the SEAMS community, trust is a broad concept that covers diverse notions and techniques and there is currently no clear view on the state of the art. To that end, we present the outcomes of an exploratory literature study that clarifies how trust as a foundational concept has been concretized and used in SASs. Based on an analysis of a set of 16 articles from the published SEAMS proceedings, we provide (i) a summary of the diverse quality attributes of SASs influenced by trust, (ii) a clarification on the different participant roles to trust establishment in SASs, and (iii) a summary of trust qualification or quantification approaches used in literature. This review provides a more holistic view on the current state of the art for attaining trust in the engineering of self-adaptive systems, and identifies research gaps worthy of further investigation.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {207–213},
numpages = {7},
keywords = {trust, self-adaptive systems, trustworthiness, trust metrics},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3643665.3648048,
author = {Kapel, Eileen and Cruz, Luis and Spinellis, Diomidis and van Deursen, Arie},
title = {Enhancing Incident Management: Insights from a Case Study at ING},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643665.3648048},
doi = {10.1145/3643665.3648048},
abstract = {An incident management process is necessary in businesses that depend strongly on software and services. A proper process is essential to guarantee that incidents are well-handled, especially in a financial software-defined business needing to adhere to guidelines and regulations. This paper aims to enhance understanding of the current state of practice through a single-case exploratory case study, at the international bank ING, by interviewing 15 subject matter experts on the incident management process. The research identifies eight core observations on tool usage, the challenges experienced and future opportunities. Core challenges include monitoring data quality, the complexity of the environment, and the balance between minimising incident resolution time and following procedural guidelines. Future opportunities can lessen these challenges by making better use of available tooling and employing machine learning approaches. This requires tight supervision on the use of best practices and good monitoring data quality. The findings emphasise the need for a strengthened focus on improving the quality of monitoring data, handling environment complexity, incident clustering, and better support for regulatory compliance.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
pages = {1–8},
numpages = {8},
keywords = {empirical, incident management, case study, interview},
location = {Lisbon, Portugal},
series = {FinanSE '24}
}

@inproceedings{10.1145/3643991.3644922,
author = {Preda, Anamaria-Roberta and Mayr-Dorn, Christoph and Mashkoor, Atif and Egyed, Alexander},
title = {Supporting High-Level to Low-Level Requirements Coverage Reviewing with Large Language Models},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644922},
doi = {10.1145/3643991.3644922},
abstract = {Refining high-level requirements into low-level ones is a common task, especially in safety-critical systems engineering. The objective is to describe every important aspect of the high-level requirement in a low-level requirement, ensuring a complete and correct implementation of the system's features. To this end, standards and regulations for safety-critical systems require reviewing the coverage of high-level requirements by all its low-level requirements to ensure no missing aspects.The challenge of supporting automatic reviews for requirements coverage originates from the distinct levels of abstraction between high-level and low-level requirements, their reliance on natural language, and the often different vocabulary used. The rise of Large Language Models (LLMs), trained on extensive text corpora and capable of contextualizing both high-level and low-level requirements, opens new avenues for addressing this challenge.This paper presents an initial study to explore the performance of LLMs in assessing requirements coverage. We employed GPT-3.5 and GPT-4 to analyze requirements from five publicly accessible data sets, determining their ability to detect if low-level requirements sufficiently address the corresponding high-level requirement. Our findings reveal that GPT-3.5, utilizing a zero-shot prompting strategy augmented with the prompt of explaining, correctly identifies complete coverage in four out of five evaluation data sets. Additionally, it exhibits an impressive 99.7% recall rate in accurately identifying instances where coverage is incomplete due to removing a single low-level requirement across our entire set of evaluation data.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {242–253},
numpages = {12},
keywords = {coverage, traceability, requirements, design definitions, high-level requirements, low-level requirements, requirements satisfaction assessment, large language models, GPT},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3647632.3651390,
author = {Thung, Ferdian and Liu, Jiakun and Rattanukul, Pattarakrit and Maoz, Shahar and Toch, Eran and Gao, Debin and Lo, David},
title = {Towards Speedy Permission-Based Debloating for Android Apps},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3651390},
doi = {10.1145/3647632.3651390},
abstract = {Android apps typically include many functionalities that not all users require. These result in software bloat that increases possible attack surface and app size. Common functionalities that users may not require are related to permissions that they intend to disallow in the first place. As these permissions are disallowed, their related code would never be executed and therefore can be safely removed. Existing work has proposed a solution to debloat Android apps according to the disallowed permissions. However, for large and complex applications, the debloating process could take hours, typically due the long time that may be needed to construct call graph for analysis. In this work, we propose MiniAppPerm, that speeds up the permission-based debloating by constructing a partial call graph instead of a complete call graph. Our preliminary experiments on a set of apps in Google Play show that MiniAppPerm can reduce the call graph construction time by up to 85.3%. We also checked that the debloated apps can run without crashes.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {84–87},
numpages = {4},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1145/3643916.3644428,
author = {Zhang, Huan and Min, Weihuan and Wei, Zhao and Kuang, Li and Gao, Honghao and Miao, Huaikou},
title = {A Just-in-time Software Defect Localization Method based on Code Graph Representation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644428},
doi = {10.1145/3643916.3644428},
abstract = {Traditional software defect localization aims to locate defective files, methods, or code lines based on symptoms such as defect reports. In comparison, Just-In-Time (JIT) software defect localization focuses on identifying defective code lines when a defective code change is initially submitted. It can identify issues at the code line level before the defect becomes apparent, preventing it from adversely affecting the software. Although researchers have proposed various methods for JIT defect localization, existing methods still have the following shortcomings: (1) Most methods rely heavily on tokens from single code lines to calculate naturalness for defect localization, which makes it challenging to effectively distinguish between code lines that have the same content but different labels (defective code lines or non-defective code lines) - termed Duplicate Lines with Different Labels (DLDL). (2) Existing methods represent code in the form of sequences, neglecting the structural information of the code. Therefore, we propose a JIT defect localization method based on code graph representation. First, we construct code linelevel code graphs for code changes to distinguish DLDL explicitly. Next, to extract sequential and structural information from the code, we propose a code graph representation model with contrastive learning to generate graph feature vectors and node scores with rich semantics. Finally, we calculate the naturalness of code lines based on the graph feature vectors and node scores. Using this naturalness, we identify defective code lines. Experimental results show that our JIT defect localization method outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {293–303},
numpages = {11},
keywords = {just-in-time software defect localization, code graph representation, contrastive learning},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643796.3648456,
author = {Abramov, Evgeny and Palchikov, Nikolai},
title = {Embedding-based search in JetBrains IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648456},
doi = {10.1145/3643796.3648456},
abstract = {Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {62–65},
numpages = {4},
keywords = {integrated development environment, programming, embedding-based search, code search, in-project code search},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3639478.3640035,
author = {AlOmar, Eman Abdullah and Knobloch, Benjamin and Kain, Thomas and Kalish, Christopher and Mkaouer, Mohamed Wiem and Ouni, Ali},
title = {AntiCopyPaster 2.0: Whitebox just-in-time code duplicates extraction},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640035},
doi = {10.1145/3639478.3640035},
abstract = {AntiCopyPaster is an IntelliJ IDEA plugin, implemented to detect and refactor duplicate code interactively as soon as a duplicate is introduced. The plugin only recommends the extraction of a duplicate when it is worth it. In contrast to current Extract Method refactoring approaches, our tool seamlessly integrates with the developer's workflow and actively provides recommendations for refactorings. This work extends our tool to allow developers to customize the detection rules, i.e., metrics, based on their needs and preferences. The plugin and its source code are publicly available on GitHub at https://github.com/refactorings/anti-copy-paster. The demonstration video can be found on YouTube: https://youtu.be/Y1sbfpds2Ms.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {84–88},
numpages = {5},
keywords = {refactoring, duplicated code, software quality},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643659.3648603,
author = {Winsten, Jesper and Soloviev, Valentin and Peltom\"{a}ki, Jarkko and Porres, Ivan},
title = {Adaptive Test Generation for Unmanned Aerial Vehicles using WOGAN-UAV},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648603},
doi = {10.1145/3643659.3648603},
abstract = {WOGAN-UAV is a test generation tool based on Wasserstein generative adversarial networks. In this paper, we present how WOGAN-UAV works and how it can be applied in the Unmanned Aerial Vehicle Testing Competition held as a part of SBFT 2024 workshop.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {43–44},
numpages = {2},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00031,
author = {Yu, Shih-Yuan and Achamyeleh, Yonatan Gizachew and Wang, Chonghan and Kocheturov, Anton and Eisen, Patrick and Faruque, Mohammad Abdullah Al},
title = {CFG2VEC: Hierarchical Graph Neural Network for Cross-Architectural Software Reverse Engineering},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00031},
doi = {10.1109/ICSE-SEIP58684.2023.00031},
abstract = {Mission-critical embedded software is critical to our society's infrastructure but can be subject to new security vulnerabilities as technology advances. When security issues arise, Reverse Engineers (REs) use Software Reverse Engineering (SRE) tools to analyze vulnerable binaries. However, existing tools have limited support, and REs undergo a time-consuming, costly, and error-prone process that requires experience and expertise to understand the behaviors of software and vulnerabilities. To improve these tools, we propose cfg2vec, a Hierarchical Graph Neural Network (GNN) based approach. To represent binary, we propose a novel Graph-of-Graph (GoG) representation, combining the information of control-flow and function-call graphs. Our cfg2vec learns how to represent each binary function compiled from various CPU architectures, utilizing hierarchical GNN and the siamese network-based supervised learning architecture. We evaluate cfg2vec's capability of predicting function names from stripped binaries. Our results show that cfg2vec outperforms the state-of-the-art by 24.54% in predicting function names and can even achieve 51.84% better given more training data. Additionally, cfg2vec consistently outperforms the state-of-the-art for all CPU architectures, while the baseline requires multiple training to achieve similar performance. More importantly, our results demonstrate that our cfg2vec could tackle binaries built from unseen CPU architectures, thus indicating that our approach can generalize the learned knowledge. Lastly, we demonstrate its practicability by implementing it as a Ghidra plugin used during resolving DARPA Assured MicroPatching (AMP) challenges.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {281–291},
numpages = {11},
keywords = {software reverse engineering, binary analysis, cross-architecture, machine learning, graph neural network},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00169,
author = {McGuire, Sean and Schultz, Erin and Ayoola, Bimpe and Ralph, Paul},
title = {Sustainability is Stratified: Toward a Better Theory of Sustainable Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00169},
doi = {10.1109/ICSE48619.2023.00169},
abstract = {Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or "pillars"---environmental, social, economic, technical and individual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1996–2008},
numpages = {13},
keywords = {sustainable development, software engineering, sustainable software engineering, scoping review, meta-synthesis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644401,
author = {Pan, Xinglu and Liu, Chenxiao and Zou, Yanzhen and Xie, Tao and Xie, Bing},
title = {MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644401},
doi = {10.1145/3643916.3644401},
abstract = {Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate. (3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set. (4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation. These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {74–86},
numpages = {13},
keywords = {code comment, comment generation, deep learning, evaluation},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643916.3644414,
author = {Chen, Xiangping and Li, Yangzi and Tang, Zhicao and Huang, Yuan and Zhou, Haojie and Tang, Mingdong and Zheng, Zibin},
title = {ESGen: Commit Message Generation Based on Edit Sequence of Code Change},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644414},
doi = {10.1145/3643916.3644414},
abstract = {Commit messages provide important information for comprehending the code changes, and a number of researchers try to generate commit messages by using an automatic way. These research on commit message generation has profited from the code tokens or code structures such as AST. Since the edit sequence of code change is also important for capturing the code change intent, we propose a new commit message generation method called ESGen, which extracts AST edit sequences of code changes as model input. Specifically, we employ an O(ND) difference algorithm to extract the edit sequence from AST by comparing the ASTs before and after applying the code changes. Then, we construct a Bi-Encoder, which encodes the textual information and the AST edit sequence information of code change. The experimental results show that ESGen outperforms other baseline models, improving the BLEU-4 to 15.14. Also, when applying the edit sequence to 7 baseline models, they improve the BLEU-4 scores of these models by an average of 8.5%. Additionally, a human evaluation confirmed the effectiveness of ESGen in generating commit messages.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {112–124},
numpages = {13},
keywords = {commit message generation, code change, edit sequence, biencoder, abstract syntax tree},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00038,
author = {Chang, Shuanglong and Gao, Juntao and Yang, Yilong},
title = {InputGen: A Tool for Automatic Generation of Prototype Inputs to Support Rapid Requirements Validation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00038},
doi = {10.1109/ICSE-Companion58688.2023.00038},
abstract = {Prototyping is an effective and efficient way of requirement validation to avoid introducing errors in the early stage of software development. Our previous work RM2PT can automatically generate prototypes from requirements models to support incremental and rapid requirements validation. Although the stakeholders can validate requirements through executing the system operations of the generated prototype, the input parameters of system operations still need to be manually typed by the stakeholders. Unlike software testing, the input of system operation must be valid and reasonable for the stakeholders under the specific scenario of use case. This is usually hard to be achieved by the stakeholders who have less knowledge and concern about the state and interface of target system. In this paper, we propose a tool named InputGen to automatically refactor and enhance the generated prototype from RM2PT. The enhanced prototype can automatically generate valid input data of the system operations for requirement validation. In addition, the enhanced prototype provide an external interface to load the initial data from an external file, which can save the time of modeling the data functionality for the administrator. We demonstrate that the enhanced prototype can improve requirements validation efficiency by 13.77 times over the originally generated prototype from RM2PT. Overall, the results were satisfactory. The proposed tool can be further extended and applied for the requirements validation in the software industry.The tool can be downloaded at https://rm2pt.com/advs/inputgen, and a demo video casting its features is at https://youtu.be/iR_ojHyzDvQ},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {122–126},
numpages = {5},
keywords = {prototype, requirements model, requirements validation, input data, automatically generate},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528588.3528661,
author = {Trautsch, Alexander and Herbold, Steffen},
title = {Predicting issue types with seBERT},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528661},
doi = {10.1145/3528588.3528661},
abstract = {Pre-trained transformer models are the current state-of-the-art for natural language models processing. seBERT is such a model, that was developed based on the BERT architecture, but trained from scratch with software engineering data. We fine-tuned this model for the NLBSE challenge for the task of issue type prediction. Our model dominates the baseline fastText for all three issue types in both recall and precision to achieve an overall F1-score of 85.7%, which is an increase of 4.1% over the baseline.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {37–39},
numpages = {3},
keywords = {BERT, issue type prediction, natural language processing, seBERT},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3639475.3640110,
author = {Rani, Pooja and Zellweger, Jonas and Kousadianos, Veronika and Cruz, Luis and Kehrer, Timo and Bacchelli, Alberto},
title = {Energy Patterns for Web: An Exploratory Study},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640110},
doi = {10.1145/3639475.3640110},
abstract = {As the energy footprint generated by software is increasing at an alarming rate, understanding how to develop energy-efficient applications has become a necessity. Previous work has introduced catalogs of coding practices, also known as energy patterns. These patterns are yet limited to Mobile or third-party libraries. In this study, we focus on the Web domain---a main source of energy consumption. First we investigated whether and how Mobile energy patterns can be ported to this domain and found that 20 patterns could be ported. Then, we interviewed six expert web developers from different companies to challenge the ported patterns. Most developers expressed concerns for antipatterns, specifically with functional antipatterns, and were able to formulate guidelines to locate these patterns in the source code. Finally, to quantify the effect of Web energy patterns on energy consumption, we set up an automated pipeline to evaluate two ported patterns: 'Dynamic Retry Delay' (DRD) and 'Open Only When Necessary' (OOWN). With this, we found no evidence that the DRD pattern consumes less energy than its antipattern, while the opposite is true for OOWN. Data and Material: https://doi.org/10.5281/zenodo.8404487},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {12–22},
numpages = {11},
keywords = {green software engineering, energy patterns, web applications, software sustainability, coding practices, energy consumption},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@inproceedings{10.1145/3643991.3644881,
author = {Sharma, Tushar},
title = {Multi-faceted Code Smell Detection at Scale using DesigniteJava 2.0},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644881},
doi = {10.1145/3643991.3644881},
abstract = {Code smell detection tools not only help practitioners and researchers detect maintainability issues but also enable repository mining and empirical research involving code smells. However, current tools for detecting code smells exhibit notable shortcomings, such as limited coverage for a diverse kind of smells at varying granularities, lack of maintenance, and inadequate support for large-scale mining studies. To address the limitations, the first major version of DesigniteJava supported code smells detection at architecture, design, and implementation smells along with commonly used code quality metrics. This paper presents DesigniteJava 2.0 that adds testability and test smell detection support. Also, the tool offers new analysis modes, including an optimized multi-commit analysis mode, to support large-scale multi-commit analysis. We show that the optimized multi-commit mode reduces analysis time by up to 46% without compromising the analysis efficacy. The tool is available online. Replication package including all the validation data and scripts can be found online [27]. Demonstration video can be found on YouTube.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {284–288},
numpages = {5},
keywords = {code smell detection tool, repository mining},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3639107,
author = {Xie, Fuman and Yan, Chuan and Meng, Mark Huasong and Teng, Shaoming and Zhang, Yanjun and Bai, Guangdong},
title = {Are Your Requests Your True Needs? Checking Excessive Data Collection in VPA App},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639107},
doi = {10.1145/3597503.3639107},
abstract = {Virtual personal assistants (VPA) services encompass a large number of third-party applications (or apps) to enrich their functionalities. These apps have been well examined to scrutinize their data collection behaviors against their declared privacy policies. Nonetheless, it is often overlooked that most users tend to ignore privacy policies at the installation time. Dishonest developers thus can exploit this situation by embedding excessive declarations to cover their data collection behaviors during compliance auditing.In this work, we present Pico, a privacy inconsistency detector, which checks the VPA app's privacy compliance by analyzing (in)consistency between data requested and data essential for its functionality. Pico understands the app's functionality topics from its publicly available textual data, and leverages advanced GPT-based language models to address domain-specific challenges. Based on the counterparts with similar functionality, suspicious data collection can be detected through the lens of anomaly detection. We apply Pico to understand the status quo of data-functionality compliance among all 65,195 skills in the Alexa app store. Our study reveals that 21.7% of the analyzed skills exhibit suspicious data collection, including Top 10 popular Alexa skills that pose threats to 54,116 users. These findings should raise an alert to both developers and users, in the compliance with the purpose limitation principle in data regulations.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {205},
numpages = {12},
keywords = {virtual personal assistant, privacy compliance, alexa skills},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639725,
author = {Groot, Tom and Ochoa Venegas, Lina and Laz\u{a}r, Bogdan and Kr\"{u}ger, Jacob},
title = {A Catalog of Unintended Software Dependencies in Multi-Lingual Systems at ASML},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639725},
doi = {10.1145/3639477.3639725},
abstract = {Multi-lingual software systems build on interconnected components that are implemented in different programming languages. The multi-lingual nature of such systems causes additional complexity, for instance, when developers aim to identify what components of a system use the same data. Organizations and developers typically aim to adhere to a specified system architecture to avoid certain dependencies between multi-lingual components. However, such dependencies may still be introduced and only resolved later on. Thus, we refer to them as unintended dependencies: dependencies that may exist, but are not wanted by the developers or organization. There has been little research on multi-lingual systems so far, and dependencies within such systems have not been studied explicitly. With this paper, we tackle this issue by contributing a catalog of unintended software dependencies in multi-lingual systems. We elicited it by interviewing 17 practitioners at ASML. We report eight types of unintended dependencies, their causes, the resulting problems, and how they can be resolved. Further, we connect our findings to research on software smells and dependencies in monolingual systems. Our contributions serve as recommendations for practitioners on how to deal with unintended dependencies, as supportive evidence for existing research, and as basis for new techniques for managing dependencies in (multi-lingual) systems.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {240–251},
numpages = {12},
keywords = {dependencies, software architecture, multi-lingual systems, software quality, software maintenance},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/ICSE48619.2023.00013,
author = {Wang, Deze and Chen, Boxing and Li, Shanshan and Luo, Wei and Peng, Shaoliang and Dong, Wei and Liao, Xiangke},
title = {One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00013},
doi = {10.1109/ICSE48619.2023.00013},
abstract = {As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned with the entire dataset on code summarization. Our experiments on three probing tasks show that adapter tuning significantly outperforms full-model fine-tuning and effectively overcomes catastrophic forgetting.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {5–16},
numpages = {12},
keywords = {transfer learning, adapter, multilingual task},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643794.3648288,
author = {Nwiran, Belal and Krzyzak, Adam},
title = {MobileNetV3 Layer Sensitivity and Sparsity},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648288},
doi = {10.1145/3643794.3648288},
abstract = {State-of-the-art Convolution Neural Networks (CNNs) have hundreds of millions of parameters and require billions of operations. Deploying such CNNs in resource-constrained environments, such as IoT devices, can be challenging. To tackle this issue, a range of methods have been explored; including compression approaches like pruning, automation like network architecture search, and efficient architectures like separable depthwise convolution. In this work, we study the sensitivity of the MobileNetV3 layers, defined as a layer's impact on a model accuracy, and calculate the maximum sparsity that its layers can have with minimal accuracy loss compared to the unpruned model.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {38–43},
numpages = {6},
keywords = {sparsity, pruning, convolution neural networks, deep learning},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1145/3526072.3527534,
author = {Ferdous, Raihana and Hung, Chia-kang and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo},
title = {EvoMBT at the SBST 2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527534},
doi = {10.1145/3526072.3527534},
abstract = {EvoMBT is a model-based test generator that uses search algorithms to generate tests from a given extended finite state machine (EFSM). In the context of Cyber-physical systems (CPS) testing, and in particular self-driving cars, we model a set of road configurations as an EFSM and use EvoMBT to generate different roads for testing the car. This report briefly introduces EvoMBT and summarizes its results in the Cyber-physical systems testing competition at SBST 2022. Overall the results achieved by EvoMBT are promising where effectiveness and efficiency scores are quite good while the scores related to diversity need improvement.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {51–52},
numpages = {2},
keywords = {advanced driver assistance systems, cyber-physical systems, model-based testing, search-based testing},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3639478.3640034,
author = {Alexopoulos, Georgios and Mitropoulos, Dimitris},
title = {nvshare: Practical GPU Sharing without Memory Size Constraints},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640034},
doi = {10.1145/3639478.3640034},
abstract = {GPUs are essential for accelerating Machine Learning (ML) workloads. A common practice is deploying ML jobs as containers managed by an orchestrator such as Kubernetes. Kubernetes schedules GPU workloads by exclusively assigning a device to a single job, which leads to massive GPU underutilization, especially for interactive development jobs with significant idle periods. Current GPU sharing approaches assign a fraction of GPU memory to each co-located job to avoid memory contention and out-of-memory errors. However, this is impractical, as it requires a priori knowledge of memory usage and does not fully address GPU underutilization. We propose nvshare, which transparently enables page faults (i.e., exceptions that are raised when an entity attempts to access a resource) to allow virtual GPU memory oversubscription. In this way we permit each application to utilize the entire physical GPU memory (Video RAM). To prevent thrashing (a situation in which page faults dominate execution time) in a reliable manner, nvshare serializes overlapping GPU bursts from different applications. We compared nvshare with KubeShare, a state-of-the-art GPU sharing solution. Our results indicate that both perform equally well in conventional sharing cases where total GPU memory usage fits into VRAM. For memory oversubscription scenarios, which KubeShare does not support, nvshare outperforms the sequential execution baseline by up to 1.35x. A video of nvshare is available at https://www.youtube.com/watch?v=9n-5sc5AICY},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {16–20},
numpages = {5},
keywords = {graphics processing unit, resource sharing, machine learning},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00020,
author = {Martie, Lee and Rosenberg, Jessie and Demers, Veronique and Zhang, Gaoyuan and Bhardwaj, Onkar and Henning, John and Prasad, Aditya and Stallone, Matt and Lee, Ja Young and Yip, Lucy and Adesina, Damilola and Paikari, Elahe and Resendiz, Oscar and Shaw, Sarah and Cox, David},
title = {Rapid Development of Compositional AI},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00020},
doi = {10.1109/ICSE-NIER58687.2023.00020},
abstract = {Compositional AI systems, which combine multiple artificial intelligence components together with other application components to solve a larger problem, have no known pattern of development and are often approached in a bespoke and ad hoc style. This makes development slower and harder to reuse for future applications. To support the full rapid development cycle of compositional AI applications, we have developed a novel framework called (Bee)* (written as a regular expression and pronounced as "beestar"). We illustrate how (Bee)* supports building integrated, scalable, and interactive compositional AI applications with a simplified developer experience.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {78–83},
numpages = {6},
keywords = {rapid development, agile, compositional, artificial intelligence, framework},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00032,
author = {Chen, Zhongqi and Zhang, Neng and Si, Pengyue and Chen, Qinde and Liu, Chao and Zheng, Zibin},
title = {ShellFusion: An Answer Generator for Shell Programming Tasks via Knowledge Fusion},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00032},
doi = {10.1109/ICSE-Companion58688.2023.00032},
abstract = {Shell programming is widely used to accomplish various tasks in Unix and Linux platforms. However, the large number of shell commands available, e.g., 50,000+ commands are documented in the Ubuntu Manual Pages (MPs), makes it a big challenge for programmers to find appropriate commands for a task. Although there are some tutorials (e.g., TLDR) with examples manually created to address the challenge, the tutorials only cover a limited number of frequently used commands for shell beginners and provide limited support for users to search commands by a task. In this paper, we introduce a novel web-based tool, ShellFusion, which can automatically generate comprehensive answers (including relevant commands, scripts, and explanations) for shell programming tasks by fusing multi-source knowledge mined from Q&amp;A posts, Ubuntu MPs, and TLDR tutorials. Our evaluation on 434 shell programming tasks shows that ShellFusion significantly outperforms the state-of-the-art approaches by at least 179.6% in terms of MRR@K and MAP@K. A user study conducted with 20 shell programmers further shows that ShellFusion can help users address programming tasks more efficiently and accurately.ShellFusion Tool: http://shellfusion.cn/Demo Video: https://youtu.be/P0YJzpKBmnA},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {93–97},
numpages = {5},
keywords = {shell programming, answer generation, knowledge fusion},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644439,
author = {Begoug, Mahi and Chouchen, Moataz and Ouni, Ali},
title = {TerraMetrics: An Open Source Tool for Infrastructure-as-Code (IaC) Quality Metrics in Terraform},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644439},
doi = {10.1145/3643916.3644439},
abstract = {Infrastructure-as-Code (IaC) constitutes a pivotal DevOps methodology, leading edge of software deployment onto cloud platforms. IaC relies on source code files rather than manual configuration to manage the infrastructure of a software system. Terraform, an IaC tool and its declarative configuration language named HCL, has recently garnered considerable attention among IaC practitioners. Like other software artefacts, Terraform files could be affected by misconfigurations, faults, and smells. Therefore, DevOps practitioners might benefit from a quality assurance tool to help them perform quality assurance activities on Terrafrom artefacts. This paper introduces TerraMetrics, an open-source tool designed to characterize the quality of Terraform artefacts by providing a catalogue of 40 quality metrics. TerraMetrics leverages the Terraform Abstract Syntax Tree (AST) to extract the metric list, offering a potentially enduring solution compared to conventional regular expressions. This tool comprises three main components: (i) a parser transforming HCL code into an AST, (ii) visitors that traverse the AST nodes to extract the metrics, and (iii) collectors for storing the collected metrics in JSON format. The TerraMetrics tool is publicly available as an Open Source tool, with a demo video, at: https://github.com/stilab-ets/terametrics.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {450–454},
numpages = {5},
keywords = {infrastructure-as-code, terraform, HCL, quality metrics, AST},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3643991.3644934,
author = {Begoug, Mahi and Chouchen, Moataz and Ouni, Ali and Abdullah Alomar, Eman and Mkaouer, Mohamed Wiem},
title = {Fine-Grained Just-In-Time Defect Prediction at the Block Level in Infrastructure-as-Code (IaC)},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644934},
doi = {10.1145/3643991.3644934},
abstract = {Infrastructure-as-Code (IaC) is an emerging software engineering practice that leverages source code to facilitate automated configuration of software systems' infrastructure. IaC files are typically complex, containing hundreds of lines of code and dependencies, making them prone to defects, which can result in breaking online services at scale. To help developers early identify and fix IaC defects, research efforts have introduced IaC defect prediction models at the file level. However, the granularity of the proposed approaches remains coarse-grained, requiring developers to inspect hundreds of lines of code in a file, while only a small fragment of code is defective. To alleviate this issue, we introduce a machine-learning-based approach to predict IaC defects at a fine-grained level, focusing on IaC blocks, i.e., small code units that encapsulate specific behaviours within an IaC file. We trained various machine learning algorithms based on a mixture of code, process, and change-level metrics. We evaluated our approach on 19 open-source projects that use Terraform, a widely used IaC tool. The results indicated that there is no single algorithm that consistently outperforms the others in 19 projects. Overall, among the six algorithms, we observed that the LightGBM model achieved a higher average of 0.21 in terms of MCC and 0.71 in terms of AUC. Models analysis reveals that the developer's experience and the relative number of added lines tend to be the most important features. Additionally, we found that blocks belonging to the most frequent types are more prone to defects. Our defect prediction models have also shown sensitivity to concept drift, indicating that IaC practitioners should regularly retrain their models.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {100–112},
numpages = {13},
keywords = {defect prediction, infrastructure-as-code, IaC, terraform},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3608140,
author = {Ding, Yangruibo and Steenhoek, Benjamin and Pei, Kexin and Kaiser, Gail and Le, Wei and Ray, Baishakhi},
title = {TRACED: Execution-aware Pre-training for Source Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608140},
doi = {10.1145/3597503.3608140},
abstract = {Most existing pre-trained language models for source code focus on learning the static code text, typically augmented with static code structures (abstract syntax tree, dependency graphs, etc.). However, program semantics will not be fully exposed before the real execution. Without an understanding of the program execution, statically pre-trained models fail to comprehensively capture the dynamic code properties, such as the branch coverage and the runtime variable values, and they are consequently less effective at code understanding tasks, such as retrieving semantic clones and detecting software vulnerabilities.To close the gap between the static nature of language models and the dynamic characteristics of programs, we introduce TRACED, an execution-aware pre-training strategy for source code. Specifically, we pre-train code language models with a combination of source code, executable inputs, and corresponding execution traces. Our goal is to teach code models the complicated execution logic during the pre-training, enabling the model to statically estimate the dynamic code properties without repeatedly executing code during task-specific fine-tuning.To illustrate the effectiveness of our proposed approach, we fine-tune and evaluate TRACED on three downstream tasks: static execution estimation, clone retrieval, and vulnerability detection. The empirical results show that TRACED relatively improves the statically pre-trained code models by 12.4% for complete execution path prediction and by 25.2% for runtime variable value predictions. TRACED also significantly outperforms statically pre-trained models in clone retrieval and vulnerability detection across four public benchmarks.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {36},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00035,
author = {Heo, Jinseok and Jeong, Hohyeon and Choi, Dongwook and Lee, Eunseok},
title = {REFERENT: Transformer-Based Feedback Generation Using Assignment Information for Programming Course},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00035},
doi = {10.1109/ICSE-SEET58685.2023.00035},
abstract = {Students require feedback on programming assignments to improve their programming skills. An Automated feedback generation (AFG) technique proposes to provide feedback-corrected submissions for incorrect student programming submissions in programming courses. However, these techniques are limited as they rely on the availability of correct submissions as a reference to generate feedback. In situations where correct submissions are not available, they resort to using mutation operators, which can lead to a search space explosion problem. In this work, we propose REFERENT, Transformer-based feedback generation using assignment information. REFERENT uses transfer learning on a pre-trained model with data from students' submission history from the past assignment. To generate assignment-related feedback, we use a title, tag, assignment description, and test case as assignment information. REFERENT can generate feedback without a reference program in limited resources. We conducted a preliminary study to confirm the effectiveness of REFERENT and the feasibility of using assignment information. REFERENT generated feedback for 32.7% of incorrect submissions without reference programs and that its performance increased up to 50.7% when reference programs were used. We also check whether the submission history, assignment information, and repair knowledge of open-source software help generate feedback.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {308–313},
numpages = {6},
keywords = {programming assignment, automated feedback generation, transformer, transfer learning, assignment information},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@proceedings{10.1145/3643692,
title = {GI '24: Proceedings of the 13th ACM/IEEE International Workshop on Genetic Improvement},
year = {2024},
isbn = {9798400705731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The GI workshops continue to bring together researchers from across the world to exchange ideas about using optimisation techniques, particularly evolutionary computation, such as genetic programming, to improve existing software.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639476.3639776,
author = {Rukmono, Satrio Adi and Ochoa, Lina and Chaudron, Michel},
title = {Deductive Software Architecture Recovery via Chain-of-thought Prompting},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639776},
doi = {10.1145/3639476.3639776},
abstract = {As software evolves, software architecture recovery techniques can help for effective maintenance. We envision a deductive software architecture recovery approach supported by Large Language Models (LLMs). Unlike existing inductive (bottom-up) recovery techniques, which reconstruct architecture by considering the properties observed at implementation level, our top-down approach starts with architectural properties and seeks their manifestations in the implementation. It employs a known Reference Architecture (RA) and involves two phases: RA definition and code units classification. A proof-of-concept with GPT-4 emulates deductive reasoning via chain-of-thought prompting. It demonstrates the deductive SAR approach, applying it to the Android application K-9 Mail and achieving a 70% accuracy in classifying 54 classes and 184 methods. The future plans focus on evaluating and refining the approach through ground-truth assessments, deeper exploration of reference architectures, and advancing toward automated human-like software architecture explanations. We highlight the potential for LLMs in achieving more comprehensive and explainable software architecture recovery.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {92–96},
numpages = {5},
keywords = {software architecture, software architecture recovery, deductive SAR, chain-of-thought prompting},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1109/ICSE48619.2023.00090,
author = {Qi, Binhang and Sun, Hailong and Gao, Xiang and Zhang, Hongyu and Li, Zhaotian and Liu, Xudong},
title = {Reusing Deep Neural Network Models through Model Re-Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00090},
doi = {10.1109/ICSE48619.2023.00090},
abstract = {Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11% weights of the original models, resulting 42.41% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85%. Moreover, reusing the re-engineered models inherits an average of 57% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {983–994},
numpages = {12},
keywords = {model reuse, deep neural network, re-engineering, DNN modularization},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3526073.3527589,
author = {Habibullah, Khan Mohammad and Gay, Gregory and Horkoff, Jennifer},
title = {Non-functional requirements for machine learning: an exploration of system scope and interest},
year = {2023},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526073.3527589},
doi = {10.1145/3526073.3527589},
abstract = {Systems that rely on Machine Learning (ML systems) have differing demands on quality---non-functional requirements (NFRs)---compared to traditional systems. NFRs for ML systems may differ in their definition, scope, and importance. Despite the importance of NFRs for ML systems, our understanding of their definitions and scope---and of the extent of existing research---is lacking compared to our understanding in traditional domains.Building on an investigation into importance and treatment of ML system NFRs in industry, we make three contributions towards narrowing this gap: (1) we present clusters of ML system NFRs based on shared characteristics, (2) we use Scopus search results---as well as inter-coder reliability on a sample of NFRs---to estimate the number of relevant studies on a subset of the NFRs, and (3), we use our initial reading of titles and abstracts in each sample to define the scope of NFRs over parts of the system (e.g., training data, ML model). These initial findings form the groundwork for future research in this emerging domain.},
booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
pages = {29–36},
numpages = {8},
keywords = {machine learning, machine learning systems, non-functional requirements, requirements engineering},
location = {Pittsburgh, Pennsylvania},
series = {SE4RAI '22}
}

@inproceedings{10.1145/3597503.3639117,
author = {Sun, Yuqiang and Wu, Daoyuan and Xue, Yue and Liu, Han and Wang, Haijun and Xu, Zhengzi and Xie, Xiaofei and Liu, Yang},
title = {GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639117},
doi = {10.1145/3597503.3639117},
abstract = {Smart contracts are prone to various vulnerabilities, leading to substantial financial losses over time. Current analysis tools mainly target vulnerabilities with fixed control- or data-flow patterns, such as re-entrancy and integer overflow. However, a recent study on Web3 security bugs revealed that about 80% of these bugs cannot be audited by existing tools due to the lack of domain-specific property description and checking. Given recent advances in Large Language Models (LLMs), it is worth exploring how Generative Pre-training Transformer (GPT) could aid in detecting logic vulnerabilities.In this paper, we propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool. By breaking down each logic vulnerability type into scenarios and properties, GPTScan matches candidate vulnerabilities with GPT. To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation. Evaluation on diverse datasets with around 400 contract projects and 3K Solidity files shows that GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects like Web3Bugs. It effectively detects ground-truth logic vulnerabilities with a recall of over 70%, including 9 new vulnerabilities missed by human auditors. GPTScan is fast and cost-effective, taking an average of 14.39 seconds and 0.01 USD to scan per thousand lines of Solidity code. Moreover, static confirmation helps GPTScan reduce two-thirds of false positives.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {166},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00143,
author = {Poozhithara, Jeffy Jahfar and Asuncion, Hazeline U. and Lagesse, Brent},
title = {Keyword Extraction from Specification Documents for Planning Security Mechanisms},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00143},
doi = {10.1109/ICSE48619.2023.00143},
abstract = {Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1661–1673},
numpages = {13},
keywords = {vulnerability prediction, CVE, CWE, keyword extraction},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00035,
author = {Hoag, Austin and Kostas, James E. and Silva, Bruno Castro da and Thomas, Philip S. and Brun, Yuriy},
title = {Seldonian Toolkit: Building Software with Safe and Fair Machine Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00035},
doi = {10.1109/ICSE-Companion58688.2023.00035},
abstract = {We present the Seldonian Toolkit, which enables software engineers to integrate provably safe and fair machine learning algorithms into their systems. Software systems that use data and machine learning are routinely deployed in a wide range of settings from medical applications, autonomous vehicles, the criminal justice system, and hiring processes. These systems, however, can produce unsafe and unfair behavior, such as suggesting potentially fatal medical treatments, making racist or sexist predictions, or facilitating radicalization and polarization. To reduce these undesirable behaviors, software engineers need the ability to easily integrate their machine-learning-based systems with domain-specific safety and fairness requirements defined by domain experts, such as doctors and hiring managers. The Seldonian Toolkit provides special machine learning algorithms that enable software engineers to incorporate such expert-defined requirements of safety and fairness into their systems, while provably guaranteeing those requirements will be satisfied. A video demonstrating the Seldonian Toolkit is available at https://youtu.be/wHR-hDm9jX4/.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {107–111},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00155,
author = {Haq, Fitash Ul and Shin, Donghwan and Briand, Lionel C.},
title = {Many-Objective Reinforcement Learning for Online Testing of DNN-Enabled Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00155},
doi = {10.1109/ICSE48619.2023.00155},
abstract = {Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously.In this paper, we present MORLOT (Many-Objective Reinforcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1814–1826},
numpages = {13},
keywords = {DNN testing, reinforcement learning, many objective search, self-driving cars, online testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528227.3528569,
author = {Lakshman, Shashank Bangalore and Eisty, Nasir U.},
title = {Software engineering approaches for TinyML based IoT embedded vision: a systematic literature review},
year = {2023},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528227.3528569},
doi = {10.1145/3528227.3528569},
abstract = {Internet of Things (IoT) has catapulted human ability to control our environments through ubiquitous sensing, communication, computation, and actuation. Over the past few years, IoT has joined forces with Machine Learning (ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning) has enabled the deployment of ML models for embedded vision on extremely lean edge hardware, bringing the power of IoT and ML together. However, TinyML powered embedded vision applications are still in a nascent stage, and they are just starting to scale to widespread real-world IoT deployment. To harness the true potential of IoT and ML, it is necessary to provide product developers with robust, easy-to-use software engineering (SE) frameworks and best practices that are customized for the unique challenges faced in TinyML engineering. Through this systematic literature review, we aggregated the key challenges reported by TinyML developers and identified state-of-art SE approaches in large-scale Computer Vision, Machine Learning, and Embedded Systems that can help address key challenges in TinyML based IoT embedded vision. In summary, our study draws synergies between SE expertise that embedded systems developers and ML developers have independently developed to help address the unique challenges in the engineering of TinyML based IoT embedded vision.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
pages = {33–40},
numpages = {8},
keywords = {IoT, TinyML, embedded vision, software engineering, systematic literature review},
location = {Pittsburgh, Pennsylvania},
series = {SERP4IoT '22}
}

@inproceedings{10.1109/ICSE48619.2023.00088,
author = {Pan, Shengyi and Bao, Lingfeng and Xia, Xin and Lo, David and Li, Shanping},
title = {Fine-Grained Commit-Level Vulnerability Type Prediction by CWE Tree Structure},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00088},
doi = {10.1109/ICSE48619.2023.00088},
abstract = {Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that TreeVul significantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {957–969},
numpages = {13},
keywords = {software security, vulnerability type, CWE},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528226.3528372,
author = {Verheijke, Darin and Rocha, Henrique},
title = {An exploratory study on solidity guards and ether exchange constructs},
year = {2023},
isbn = {9781450393317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528226.3528372},
doi = {10.1145/3528226.3528372},
abstract = {Ethereum is a blockchain platform that enables the use of smart contracts. Smart contracts will execute a set of instructions without an intermediary party when called upon. The possibility to make calls to another contract or exchange cryptocurrency allows for potential exploits to occur, most notable reentrancy. The Solidity language for coding smart contracts has syntactic constructs created to be safer alternatives, and guards to aid in securing code against exploits. In this paper, we collect a total of 26,799 verified Solidity smart contracts from Etherscan, to analyze the language constructs used in calling another contract or exchanging ether. We also analyze the usage of guards to make the code more secure. For instance, even though call is the unsafest function, it is still used by 50% of the contracts in our dataset. The safe method transfer is used by approximately one-third of contracts, and send is rarely used. We noticed that contracts using call have a higher average and median size in Lines of Code than normal. We also found an increased percentage of call contracts using more guards. Moreover, 97% of all contracts are using the require guard, with 23 uses of require on average per contract. This may be an indication that Solidity developers are using more guards to prevent exploits in their contracts},
booktitle = {Proceedings of the 5th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {1–8},
numpages = {8},
keywords = {call, ethereum, guards, smart contracts, solidity},
location = {Pittsburgh, Pennsylvania},
series = {WETSEB '22}
}

@inproceedings{10.1145/3597503.3639089,
author = {Bradley, Nick C. and Fritz, Thomas and Holmes, Reid},
title = {Supporting Web-Based API Searches in the IDE Using Signatures},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639089},
doi = {10.1145/3597503.3639089},
abstract = {Developers frequently use the web to locate API examples that help them solve their programming tasks. While sites like Stack Overflow (SO) contain API examples embedded within their textual descriptions, developers cannot access this API knowledge directly. Instead they need to search for and browse results to select relevant SO posts and then read through individual posts to figure out which answers contain information about the APIs that are relevant to their task. This paper introduces an approach, called Scout, that automatically analyzes search results to extract API signature information. These signatures are used to group and rank examples and allow for a unique API-based presentation that reduces the amount of information the developer needs to consider when looking for API information on the web. This succinct representation enables Scout to be integrated fully within an IDE panel so that developers can search and view API examples without losing context on their development task. Scout also uses this integration to automatically augment queries with contextual information that tailors the developer's queries, and ranks the results according to the developer's needs. In an experiment with 40 developers, we found that Scout reduces the number of queries developers need to perform by 19% and allows them to solve almost half their tasks directly from the API-based representation, reducing the number of complete SO posts viewed by approximately 64%.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {186},
numpages = {12},
keywords = {API signatures, code search, controlled experiment},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644900,
author = {Wu, Jiaqi and Bao, Lingfeng and Yang, Xiaohu and Xia, Xin and Hu, Xing},
title = {A Large-Scale Empirical Study of Open Source License Usage: Practices and Challenges},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644900},
doi = {10.1145/3643991.3644900},
abstract = {The popularity of open source software (OSS) has led to a significant increase in the number of available licenses, each with their own set of terms and conditions. This proliferation of licenses has made it increasingly challenging for developers to select an appropriate license for their projects and to ensure that they are complying with the terms of those licenses. As a result, there is a need for empirical studies to identify current practices and challenges in license usage, both to help developers make informed decisions about license selection and to ensure that OSS is being used and distributed in a legal and ethical manner. Moreover, the development of new licenses might be required to better meet the needs of the open source community and address emerging legal issues.In this paper, we conduct a large-scale empirical study of license usage across five package management platforms, i.e., Maven, NPM, PyPI, RubyGems, and Cargo. Our objective is to examine the current trends and potential issues in license usage of the OSS community. In total, we analyze the licenses of 33,710,877 packages across the selected five platforms. We statistically analyze licenses in package management platforms from multiple perspectives, e.g., license usage, license incompatibility, license updates, and license evolution. Moreover, we conduct a comparative study of various aspects of core packages and common packages in these platforms. Our results reveal irregularities in license names and license incompatibilities that require attention. We observe both similarities and differences in license usage across the five platforms, with Cargo being the most standardized among them. Finally, we discuss some implications for actions based on our findings.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {595–606},
numpages = {12},
keywords = {OSS licenses, empirical study, package management platform},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3649400,
author = {Smith, Carol},
title = {Trustworthy by Design},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3649400},
doi = {10.1145/3597503.3649400},
abstract = {The relatively recent public release of generative artificial intelligence (AI) systems has ignited a significant leap in awareness of the capabilities of AI. In parallel, there has been a recognition of AI system limitations and the bias inherent in systems created by humans. Expectations are rising for more trustworthy, human-centered, and responsible software connecting humans to powerful systems that augment their abilities. There are decades of practice designing systems that work with, and for humans, that we can build upon to face the new challenges and opportunities brought by dynamic AI systems.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {3},
numpages = {4},
keywords = {keynote, ethics, trust, emerging technology, AI},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3526072,
title = {SBST '22: Proceedings of the 15th Workshop on Search-Based Software Testing},
year = {2022},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3643991.3644931,
author = {Islam, Md Anaytul and Asaduzzman, Muhammad and Wang, Shaowei},
title = {On the Executability of R Markdown Files},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644931},
doi = {10.1145/3643991.3644931},
abstract = {R Markdown files are examples of literate programming documents that combine R code with results and explanations. Such dynamic documents are designed to execute easily and reproduce study results. However, little is known about the executability of R Markdown files which can cause frustration among its users who intend to reuse the document. This paper presents a large-scale study on the executability of R Markdown files collected from GitHub. Results from our study show that a significant number of R Markdown files (64.95%) are not executable, even after our best efforts. To better understand the challenges, we categorize the exceptions encountered while executing the documents into different categories. Finally, we develop a classifier to determine which Markdown files are likely to be executable. Such a classifier can be utilized by search engines in their ranking which helps developers to find literate programming documents as learning resources.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {254–264},
numpages = {11},
keywords = {R Markdown, GitHub, executability, literate programming},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3623314,
author = {Neelofar, Neelofar and Aleti, Aldeida},
title = {Towards Reliable AI: Adequacy Metrics for Ensuring the Quality of System-level Testing of Autonomous Vehicles},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623314},
doi = {10.1145/3597503.3623314},
abstract = {AI-powered systems have gained widespread popularity in various domains, including Autonomous Vehicles (AVs). However, ensuring their reliability and safety is challenging due to their complex nature. Conventional test adequacy metrics, designed to evaluate the effectiveness of traditional software testing, are often insufficient or impractical for these systems. White-box metrics, which are specifically designed for these systems, leverage neuron coverage information. These coverage metrics necessitate access to the underlying AI model and training data, which may not always be available. Furthermore, the existing adequacy metrics exhibit weak correlations with the ability to detect faults in the generated test suite, creating a gap that we aim to bridge in this study.In this paper, we introduce a set of black-box test adequacy metrics called "Test suite Instance Space Adequacy" (TISA) metrics, which can be used to gauge the effectiveness of a test suite. The TISA metrics offer a way to assess both the diversity and coverage of the test suite and the range of bugs detected during testing. Additionally, we introduce a framework that permits testers to visualise the diversity and coverage of the test suite in a two-dimensional space, facilitating the identification of areas that require improvement.We evaluate the efficacy of the TISA metrics by examining their correlation with the number of bugs detected in system-level simulation testing of AVs. A strong correlation, coupled with the short computation time, indicates their effectiveness and efficiency in estimating the adequacy of testing AVs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {68},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3643119,
author = {Plein, Laura and Ou\'{e}draogo, Wendk\^{u}uni C. and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643119},
doi = {10.1145/3639478.3643119},
abstract = {Tests suites are a key ingredient in various software automation tasks. Recently, various studies [4] have demonstrated that they are paramount in the adoption of latest innovations in software engineering, such as automated program repair (APR) [3]. Test suites are unfortunately often too scarce in software development projects. Generally, they are provided for regression testing, while new bugs are discovered by users who then describe them informally in bug reports. In recent literature, a new trend of research in APR has attempted to leverage bug reports in generate-and-validate pipelines for program repair. Even in such cases, when an APR tool generates a patch candidate, if test cases are unavailable, developers must manually validate the patch, leading to a threat to validity.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {360–361},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3526072.3527535,
author = {Peltom\"{a}ki, Jarkko and Spencer, Frankie and Porres, Ivan},
title = {WOGAN at the SBST 2022 CPS tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527535},
doi = {10.1145/3526072.3527535},
abstract = {WOGAN is an online test generation algorithm based on Wasserstein generative adversarial networks. In this note, we present how WOGAN works and summarize its performance in the SBST 2022 CPS tool competition concerning the AI of a self-driving car.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {53–54},
numpages = {2},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1109/ICSE48619.2023.00201,
author = {Xiao, Dongwei and Liu, Zhibo and Wang, Shuai},
title = {Metamorphic Shader Fusion for Testing Graphics Shader Compilers},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00201},
doi = {10.1109/ICSE48619.2023.00201},
abstract = {Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers --- like traditional software --- may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems.This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2400–2412},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643915.3644089,
author = {Chan, Kenneth H. and Zilberman, Sol and Polanco, Nick and Siegel, Joshua E. and Cheng, Betty H. C.},
title = {SafeDriveRL: Combining Non-cooperative Game Theory with Reinforcement Learning to Explore and Mitigate Human-based Uncertainty for Autonomous Vehicles},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644089},
doi = {10.1145/3643915.3644089},
abstract = {Increasingly, artificial intelligence (AI) is being used to support automotive systems, including autonomous vehicles (AVs) with self-driving capabilities. The premise is that learning-enabled systems (LESs), those systems that have one or more AI components, use statistical models to make better informed adaptation decisions and mitigate potentially dangerous situations. These AI techniques largely focus on uncertainty factors that can be explicitly identified and defined (e.g., environmental conditions). However, the unexpected behavior of human actors is a source of uncertainty that is challenging to explicitly model and define. In order to train a learning-enabled AV, developers may use a combination of real-world monitored data and simulated external actor behaviors (e.g., human-driven vehicles, pedestrians, etc.), where participants follow defined sets of rules such as traffic laws. However, if uncertain human behaviors are not sufficiently captured during training, then the AV may not be able to safely handle unexpected behavior induced by human-operated vehicles (e.g., unexpected sudden lane changes). This work introduces a non-cooperative game theory and reinforcement learning-based (RL) framework to discover and assess an AV's ability to handle high-level uncertain behavior(s) induced by human-based rewards. The discovered synthetic data can then be used to reconfigure the AV to robustify onboard behaviors.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {214–220},
numpages = {7},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3644033.3644372,
author = {Robinette, Preston K. and Manzanas Lopez, Diego and Serbinowska, Serena and Leach, Kevin and Johnson, Taylor T},
title = {Case Study: Neural Network Malware Detection Verification for Feature and Image Datasets},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644372},
doi = {10.1145/3644033.3644372},
abstract = {Malware, or software designed with harmful intent, is an ever-evolving threat that can have drastic effects on both individuals and institutions. Neural network malware classification systems are key tools for combating these threats but are vulnerable to adversarial machine learning attacks. These attacks perturb input data to cause misclassification, bypassing protective systems. Existing defenses often rely on enhancing the training process, thereby increasing the model's robustness to these perturbations, which is quantified using verification. While training improvements are necessary, we propose focusing on the verification process used to evaluate improvements to training. As such, we present a case study that evaluates a novel verification domain that will help to ensure tangible safeguards against adversaries and provide a more reliable means of evaluating the robustness and effectiveness of anti-malware systems. To do so, we describe malware classification and two types of common malware datasets (feature and image datasets), demonstrate the certified robustness accuracy of malware classifiers using the Neural Network Verification (NNV) and Neural Network Enumeration (nnenum) tools1, and outline the challenges and future considerations necessary for the improvement and refinement of the verification of malware classification. By evaluating this novel domain as a case study, we hope to increase its visibility, encourage further research and scrutiny, and ultimately enhance the resilience of digital systems against malicious attacks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {127–137},
numpages = {11},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00049,
author = {Zhang, Ziqi and Li, Yuanchun and Liu, Bingyan and Cai, Yifeng and Li, Ding and Guo, Yao and Chen, Xiangqun},
title = {FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00049},
doi = {10.1109/ICSE48619.2023.00049},
abstract = {Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called "software 2.0". In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {460–472},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623310,
author = {Zhong, Wenkang and Li, Chuanyi and Liu, Kui and Xu, Tongtong and Ge, Jidong and Bissyande, Tegawende F. and Luo, Bin and Ng, Vincent},
title = {Practical Program Repair via Preference-based Ensemble Strategy},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623310},
doi = {10.1145/3597503.3623310},
abstract = {To date, over 40 Automated Program Repair (APR) tools have been designed with varying bug-fixing strategies, which have been demonstrated to have complementary performance in terms of being effective for different bug classes. Intuitively, it should be feasible to improve the overall bug-fixing performance of APR via assembling existing tools. Unfortunately, simply invoking all available APR tools for a given bug can result in unacceptable costs on APR execution as well as on patch validation (via expensive testing). Therefore, while assembling existing tools is appealing, it requires an efficient strategy to reconcile the need to fix more bugs and the requirements for practicality. In light of this problem, we propose a Preference-based Ensemble Program Repair framework (P-EPR), which seeks to effectively rank APR tools for repairing different bugs. P-EPR is the first non-learning-based APR ensemble method that is novel in its exploitation of repair patterns as a major source of knowledge for ranking APR tools and its reliance on a dynamic update strategy that enables it to immediately exploit and benefit from newly derived repair results. Experimental results show that P-EPR outperforms existing strategies significantly both in flexibility and effectiveness.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {5},
numpages = {13},
keywords = {program repair, ensemble strategy},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00113,
author = {Ezzini, Saad and Abualhaija, Sallam and Arora, Chetan and Sabetzadeh, Mehrdad},
title = {AI-Based Question Answering Assistance for Analyzing Natural-Language Requirements},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00113},
doi = {10.1109/ICSE48619.2023.00113},
abstract = {By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1277–1289},
numpages = {13},
keywords = {natural-language requirements, question answering (QA), language models, natural language processing (NLP), natural language generation (NLG), BERT, T5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00056,
author = {Liao, Lizhi},
title = {Addressing Performance Regressions in DevOps: Can We Escape from System Performance Testing?},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00056},
doi = {10.1109/ICSE-Companion58688.2023.00056},
abstract = {Performance regression is an important type of performance issue in software systems. It indicates that the performance of the same features in the new version of the system becomes worse than that of previous versions, such as increased response time or higher resource utilization. In order to prevent performance regressions, current practices often rely on conducting extensive system performance testing before releasing the system into production based on the testing results. However, faced with a great demand for resources and time to perform system performance testing, it is often challenging to adopt such approaches to the practice of fast-paced development and release cycles, e.g., DevOps. This thesis focuses on addressing software performance regressions in DevOps without relying on expensive system performance tests. More specifically, I first propose a series of approaches to helping developers detect performance regressions and locate their root causes by only utilizing the readily-available operational data when the software system is running in the field and used by real end users. I then leverage small-scale performance testing and architectural modeling to estimate the impact of source code changes on the end-to-end performance of the system in order to detect performance regressions early in the software development phase. Through various case studies on open-source projects and successful adoptions by our industrial research collaborator, we expect that our study will provide helpful insights for researchers and practitioners who are interested in addressing performance regressions in DevOps without expensive system performance testing.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {203–207},
numpages = {5},
keywords = {performance regression, performance regression root cause, field testing, performance modeling, performance engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639079,
author = {Ma, Yimeng and Huang, Yu and Leach, Kevin},
title = {Breaking the Flow: A Study of Interruptions During Software Engineering Activities},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639079},
doi = {10.1145/3597503.3639079},
abstract = {In software engineering, interruptions during tasks can have significant implications for productivity and well-being. While previous studies have investigated the effect of interruptions on productivity, to the best of our knowledge, no prior work has yet distinguished the effect of different types of interruptions on software engineering activities.This study explores the impact of interruptions on software engineering tasks, analyzing in-person and on-screen interruptions with different levels of urgency and dominance. Participants completed code writing, code comprehension, and code review tasks while experiencing interruptions. We collect physiological data using the Empatica EmbracePlus wristband and self-perceived evaluations through surveys. Results show that on-screen interruptions with high dominance of requester significantly increase time spent on code comprehension. In-person and on-screen interruptions combined significantly affect the time spent on code review, with varied effects based on specific interruption combinations. Both interruption type and task significantly influence stress measures, with code comprehension and review tasks associated with lower stress measures compared to code writing. Interestingly, in-person interruptions present a positive impact on physiological measures, indicating reduced stress measures. However, participants' self-perceived stress scores do not align with physiological data, with higher stress reported during in-person interruptions despite lower physiological stress measures. These findings shed light on and emphasize the potential importance of considering the complex relationship between interruptions, objective measures, and subjective experiences in software development. We discuss insights that we hope can inform interruption management and implications on stress among software engineers. (ChatGPT was used to revise and shorten paragraphs in this manuscript.)},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {185},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3643662,
title = {EnCyCriS/SVM '24: Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Increasing system interconnectivity, decentralization, and introduction of new, more intelligent technologies, result in critical infrastructures becoming exposed to increased risk of cyber, physical, and combined cyber-physical attacks. Cyberattacks on critical systems can inflict severe consequences to people, society, economy, and national security, and can have adverse effects on safety and reliability of critical infrastructures. The joint EnCyCriS-SVM workshop facilitates discourse and discussions among researchers, practitioners, and students who are working on challenges and solutions related to the industrial revolution. Focus is given on sharing industry experience and project results pertaining to cyber threats on critical systems; secure software engineering; and attack detection and response mechanisms.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE48619.2023.00040,
author = {Liu, Jiahao and Zeng, Jun and Wang, Xiang and Liang, Zhenkai},
title = {Learning Graph-Based Code Representations for Source-Level Functional Similarity Detection},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00040},
doi = {10.1109/ICSE48619.2023.00040},
abstract = {Detecting code functional similarity forms the basis of various software engineering tasks. However, the detection is challenging as functionally similar code fragments can be implemented differently, e.g., with irrelevant syntax. Recent studies incorporate program dependencies as semantics to identify syntactically different yet semantically similar programs, but they often focus only on local neighborhoods (e.g., one-hop dependencies), limiting the expressiveness of program semantics in modeling functionalities. In this paper, we present Tailor that explicitly exploits deep graph-structured code features for functional similarity detection. Given source-level programs, Tailor first represents them into code property graphs (CPGs) --- which combine abstract syntax trees, control flow graphs, and data flow graphs --- to collectively reason about program syntax and semantics. Then, Tailor learns representations of CPGs by applying a CPG-based neural network (CPGNN) to iteratively propagate information on them. It improves over prior work on code representation learning through a new graph neural network (GNN) tailored to CPG structures instead of the off-the-shelf GNNs used previously. We systematically evaluate Tailor on C and Java programs using two public benchmarks. Experimental results show that Tailor outperforms the state-of-the-art approaches, achieving 99.8% and 99.9% F-scores in code clone detection and 98.3% accuracy in source code classification.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {345–357},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3666015.3666017,
author = {Akta\c{s}, Kemal and Kilinc, H. Hakan},
title = {Interaction Prediction and Anomaly Detection in a Microservices-based Telecommunication Platform},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666017},
doi = {10.1145/3666015.3666017},
abstract = {In microservice platforms with high number of users and heavy traffic, it is necessary to monitor the system, take quick action against errors and ensure the maintainability of the system. However, debugging on these platforms can take a long time. This difficulty arises from the need of understanding the behavior of microservices and detecting their interactions. In this study, which aims to increase the efficiency of DevOps engineers on the work/time unit, it was observed that providing microservice flows and interactions saves operations teams a significant amount of time during debugging. Accordingly, the study focused on microservice interactions and anomaly detection. Firstly, using the log patterns extracted from the microservice logs, different machine learning models were created to predict the previous and next microservices with which the current microservices interacted at a certain moment, and their performances were compared. Then, anomalous data were injected into the microservice logs, models were developed to detect these data and their performances were compared. In the experiments, unsupervised and supervised algorithms are used with 6 different datasets, and successful estimation results were obtained that can contribute positively to the debugging process.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {56–65},
numpages = {10},
keywords = {Anomaly Detection, Debugging, Interaction Prediction, Microservice},
location = {M\, Germany},
series = {ICSSP '24}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00015,
author = {Lanubile, Filippo and Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Quaranta, Luigi},
title = {Teaching MLOps in Higher Education through Project-Based Learning},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00015},
doi = {10.1109/ICSE-SEET58685.2023.00015},
abstract = {Building and maintaining production-grade ML-enabled components is a complex endeavor that goes beyond the current approach of academic education, focused on the optimization of ML model performance in the lab. In this paper, we present a project-based learning approach to teaching MLOps, focused on the demonstration and experience with emerging practices and tools to automatize the construction of ML-enabled components. We examine the design of a course based on this approach, including laboratory sessions that cover the end-to-end ML component life cycle, from model building to production deployment. Moreover, we report on preliminary results from the first edition of the course. During the present year, an updated version of the same course is being delivered in two independent universities; the related learning outcomes will be evaluated to analyze the effectiveness of project-based learning for this specific subject.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {95–100},
numpages = {6},
keywords = {machine learning, data science, software engineering for AI, model deployment, reproducibility},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00040,
author = {Yang, Chengran and Xu, Bowen and Liu, Jiakun and Lo, David},
title = {TECHSUMBOT: A Stack Overflow Answer Summarization Tool for Technical Query},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00040},
doi = {10.1109/ICSE-Companion58688.2023.00040},
abstract = {Stack Overflow is a popular platform for developers to seek solutions to programming-related problems. However, prior studies identified that developers may suffer from the redundant, useless, and incomplete information retrieved by the Stack Overflow search engine. To help developers better utilize the Stack Overflow knowledge, researchers proposed tools to summarize answers to a Stack Overflow question. However, existing tools use hand-craft features to assess the usefulness of each answer sentence and fail to remove semantically redundant information in the result. Besides, existing tools only focus on a certain programming language and cannot retrieve up-to-date new posted knowledge from Stack Overflow. In this paper, we propose TechSumBot, an automatic answer summary generation tool for a technical problem. Given a question, TechSumBot first retrieves answers using the Stack Overflow search engine, then TechSumBot 1) ranks each answers sentence based on the sentence's usefulness, 2) estimates the centrality of each sentence to all candidates, and 3) removes the semantic redundant information. Finally, TechSumBot returns the top 5 ranked answer sentences as the answer summary. We implement TechSumBot in the form of a search engine website. To evaluate TechSumBot in both automatic and manual manners, we construct the first Stack Overflow multi-answer summarization benchmark and design a manual evaluation study to assess the effectiveness of TechSumBot and state-of-the-art baselines from the NLP and SE domain. Both results indicate that the summaries generated by TechSumBot are more diverse, useful, and similar to the ground truth summaries.Tool Link: www.techsumbot.comVideo Link: https://youtube.coni/watch7v-ozuJOp_vILMReplication Package: https://github.com/TechSumBot/TechSumBot},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {132–135},
numpages = {4},
keywords = {summarization, question retrieval},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639134,
author = {Zhan, Qi and Hu, Xing and Li, Zhiyang and Xia, Xin and Lo, David and Li, Shanping},
title = {PS3: Precise Patch Presence Test based on Semantic Symbolic Signature},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639134},
doi = {10.1145/3597503.3639134},
abstract = {During software development, vulnerabilities have posed a significant threat to users. Patches are the most effective way to combat vulnerabilities. In a large-scale software system, testing the presence of a security patch in every affected binary is crucial to ensure system security. Identifying whether a binary has been patched for a known vulnerability is challenging, as there may only be small differences between patched and vulnerable versions. Existing approaches mainly focus on detecting patches that are compiled in the same compiler options. However, it is common for developers to compile programs with very different compiler options in different situations, which causes inaccuracy for existing methods. In this paper, we propose a new approach named PS3, referring to precise patch presence test based on semantic-level symbolic signature. PS3 exploits symbolic emulation to extract signatures that are stable under different compiler options. Then PS3 can precisely test the presence of the patch by comparing the signatures between the reference and the target at semantic level.To evaluate the effectiveness of our approach, we constructed a dataset consisting of 3,631 (CVE, binary) pairs of 62 recent CVEs in four C/C++ projects. The experimental results show that PS3 achieves scores of 0.82, 0.97, and 0.89 in terms of precision, recall, and F1 score, respectively. PS3 outperforms the state-of-the-art baselines by improving 33% in terms of F1 score and remains stable in different compiler options.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {167},
numpages = {12},
keywords = {patch presence test, binary analysis, software security},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643655.3643882,
author = {Chambers, Theodore and Cleland-Huang, Jane and Vierhauser, Michael},
title = {Self-Adaptation of Loosely Coupled Systems across a System of Small Uncrewed Aerial Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643882},
doi = {10.1145/3643655.3643882},
abstract = {The use of small autonomous Uncrewed Aerial Systems (sUAS) for Emergency Response requires rapid deployments into shared operational environments. We refer to these as "Pop-up Drone Zones" (PuDZ), representing a System of Systems (SoS), in which individual systems provide services such as air traffic control, environmental modeling, and support for sUAS autonomy. Each system needs the ability to configure itself dynamically at the start of the mission and adapt throughout the mission, in response to occurring changes. However, system-level choreography at the SoS level can be challenging, as it requires a global perspective of all individual systems, and a deep understanding of their inter-dependencies. We, therefore, propose a publish-subscribe architecture that enables the exchange of regional data between MAPE-K-enabled systems in support of coordinated self-adaptation whilst maintaining loose coupling between the interconnected systems. We illustrate and validate our approach through several PuDZ-related change scenarios in simulation and physical field tests.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {37–44},
numpages = {8},
keywords = {systems of systems, MAPE-K, self-adaptation},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1109/ICSE48619.2023.00067,
author = {Mailach, Alina and Siegmund, Norbert},
title = {Socio-Technical Anti-Patterns in Building ML-Enabled Software: Insights from Leaders on the Forefront},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00067},
doi = {10.1109/ICSE48619.2023.00067},
abstract = {Although machine learning (ML)-enabled software systems seem to be a success story considering their rise in economic power, there are consistent reports from companies and practitioners struggling to bring ML models into production. Many papers have focused on specific, and purely technical aspects, such as testing and pipelines, but only few on socio-technical aspects.Driven by numerous anecdotes and reports from practitioners, our goal is to collect and analyze socio-technical challenges of productionizing ML models centered around and within teams. To this end, we conducted the largest qualitative empirical study in this area, involving the manual analysis of 66 hours of talks that have been recorded by the MLOps community.By analyzing talks from practitioners for practitioners of a community with over 11,000 members in their Slack workspace, we found 17 anti-patterns, often rooted in organizational or management problems. We further list recommendations to overcome these problems, ranging from technical solutions over guidelines to organizational restructuring. Finally, we contextualize our findings with previous research, confirming existing results, validating our own, and highlighting new insights.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {690–702},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639477.3639752,
author = {Bhukar, Karan and Kumar, Harshit and Mahindru, Ruchi and Arora, Rohan and Nagar, Seema and Aggarwal, Pooja and Paradkar, Amit},
title = {Dynamic Alert Suppression Policy for Noise Reduction in AIOps},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639752},
doi = {10.1145/3639477.3639752},
abstract = {As IT environments evolve in both size and complexity, observability tools are needed to monitor their health. As the anomalous events are detected, alerts are generated, leading to alert notifications to the Site Reliability Engineers(SREs). However, most of these notifications turn out to be false alarms, leading to alert fatigue, and inefficiencies. Existing approaches for reducing alert noise rely on static policies that can quickly become outdated in dynamic IT environments and are therefore difficult to maintain. In this work, we propose a novel unsupervised approach, Dynamic-X-Y, guided by a well known moving average envelope statistical method, to learn custom tailored alert suppression policy from historical alerts and events data. At run-time, these learned policies are applied to incoming events/alerts to reduce false alert notifications. We validate our approach on two different datasets, log anomaly and metric anomaly events/alerts, to show percentage increase in accuracy over state-of-the-art methods by 7.39% and 35.7%, respectively.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {178–188},
numpages = {11},
keywords = {event suppression, alert management, AIOps, persistent detection, peak detection},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3528588.3528654,
author = {Phan, Hung and Jannesari, Ali},
title = {Story point level classification by text level graph neural network},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528654},
doi = {10.1145/3528588.3528654},
abstract = {Estimating the software projects' efforts developed by agile methods is important for project managers or technical leads. It provides a summary as a first view of how many hours and developers are required to complete the tasks. There are research works on automatic predicting the software efforts, including Term Frequency - Inverse Document Frequency (TFIDF) as the traditional approach for this problem. Graph Neural Network is a new approach that has been applied in Natural Language Processing for text classification. The advantages of Graph Neural Network are based on the ability to learn information via graph data structure, which has more representations such as the relationships between words compared to approaches of vectorizing sequence of words. In this paper, we show the potential and possible challenges of Graph Neural Network text classification in story point level estimation. By the experiments, we show that the GNN Text Level Classification can achieve as high accuracy as about 80% for story points level classification, which is comparable to the traditional approach. We also analyze the GNN approach and point out several current disadvantages that the GNN approach can improve for this problem or other problems in software engineering.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {75–78},
numpages = {4},
keywords = {graph neural network, story point estimation, term frequency},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1109/ICSE48619.2023.00092,
author = {Ren, Xiaoning and Lin, Yun and Xue, Yinxing and Liu, Ruofan and Sun, Jun and Feng, Zhiyong and Dong, Jin Song},
title = {DeepArc: Modularizing Neural Networks for the Model Maintenance},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00092},
doi = {10.1109/ICSE48619.2023.00092},
abstract = {Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1008–1019},
numpages = {12},
keywords = {architecture, modularization, neural networks},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00050,
author = {Cui, Xing and Wu, Jingzheng and Wu, Yanjun and Wang, Xu and Luo, Tianyue and Qu, Sheng and Ling, Xiang and Yang, Mutian},
title = {An Empirical Study of License Conflict in Free and Open Source Software},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00050},
doi = {10.1109/ICSE-SEIP58684.2023.00050},
abstract = {Free and Open Source Software (FOSS) has become the fundamental infrastructure of mainstream software projects. FOSS is subject to various legal terms and restrictions, depending on the type of open source license in force. Hence it is important to remain compliant with the FOSS license terms. Identifying the licenses that provide FOSS and understanding the terms of those licenses is not easy, especially when dealing with a large amount of reuse that is common in modern software development. Since reused software is often large, automated license analysis is needed to address these issues and support users in license compliant reuse of FOSS. However, existing license assessment tools can only identify the name and quantity of licenses embedded in software and thus cannot identify whether the licenses are being used safely and correctly. Moreover, they cannot provide a comprehensive analysis of the compatibility and potential risk that come with the term conflicts.In this paper, we propose DIKE, an automated tool that can perform license detection and conflict analysis for FOSS. First, DIKE extracts 12 terms under 3,256 unique open source licenses by manual analysis and Natural Language Processing (NLP) and constructs a license knowledge base containing the responsibilities of the terms. Second, DIKE scans all licenses from the code snippet for the input software and outputs the scan results in a tree structure. Third, the scan results match the license knowledge base to detect license conflicts from terms and conditions. DIKE designs two solutions for software with license conflicts: license replacement and code replacement. To demonstrate the effectiveness of DIKE, we first evaluate with the term extraction and responsibility classification, and the results show that their F1-scores reach 0.816 and 0.948, respectively. In addition, we conduct a measurement study of 16,341 popular projects from GitHub based on our proposed DIKE to explore the conflict of license usage in FOSS. The results show that 1,787 open source licenses are used in the project, and 27.2% of licenses conflict. Our new findings suggest that conflicts are prevalent in FOSS, warning the open source community about intellectual property risks.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {495–505},
numpages = {11},
keywords = {free and open source software, license analysis, license conflict, natural language processing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3643655.3643884,
author = {Borges, Marcos and Manzano, Wallace and Rocha, Lincoln and Maia, Paulo and Nakagawa, Elisa},
title = {Towards Automatic Generation of Systems-of-Systems Architectural Configurations},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643884},
doi = {10.1145/3643655.3643884},
abstract = {The increasing reliance on Systems-of-Systems (SoS) across critical domains, such as emergency response, healthcare, and smart cities, underscores the need for reliable architectures capable of seamlessly accommodating new SoS missions that emerge at runtime. Designing reliable SoS architectures faces challenges in predicting emergent behaviors due to incomplete understanding of Constituent Systems (CS) behaviors at design time, dynamic changes in requirements, and environmental influences. At the same time, it lacks solutions for automatically generating SoS architectural reconfigurations at runtime. The main contribution of this paper is to present an approach named AUTONOMOUS that can automatically generate SoS architectural configurations. AUTONOMOUS refers to a process for functional suitability that extracts information from SoS mission models, engineers a model generator, and finally generates the architectural configurations. We evaluated the effectiveness of our approach through a case study in the space domain. Results demonstrate success in generating SoS architectural configurations, indicating that AUTONOMOUS could be further explored as a mechanism to reconfigure SoS architectures automatically at runtime.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {29–36},
numpages = {8},
keywords = {system-of-systems, architectural configurations, automatic generation, runtime},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3639478.3643060,
author = {H. Fard, Fatemeh},
title = {Technical Briefing on Parameter Efficient Fine-Tuning of (Large) Language Models for Code-Intelligence},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643060},
doi = {10.1145/3639478.3643060},
abstract = {Large Language Models (LLMs) have gained much attention in the Software Engineering (SE) community, specifically for code-related tasks. Though a common approach is to fine-tune these models fully, it is a computationally heavy and time-consuming process that is not accessible to all. More importantly, with billions of parameters in the models, fully fine-tuning them for new tasks or domains is infeasible or inefficient. This technical briefing covers the alternative approach -Parameter Efficient Fine Tuning (PEFT), discussing the state-of-the-art techniques and reflecting on the few studies of using PEFT in Software Engineering and how changing the current PEFT architectures in natural language processing could enhance the performance for code-related tasks.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {434–435},
numpages = {2},
keywords = {parameter efficient fine tuning, code language models, large language models},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639168,
author = {Cao, Sicong and Sun, Xiaobing and Wu, Xiaoxue and Lo, David and Bo, Lili and Li, Bin and Liu, Wei},
title = {Coca: Improving and Explaining Graph Neural Network-Based Vulnerability Detection Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639168},
doi = {10.1145/3597503.3639168},
abstract = {Recently, Graph Neural Network (GNN)-based vulnerability detection systems have achieved remarkable success. However, the lack of explainability poses a critical challenge to deploy black-box models in security-related domains. For this reason, several approaches have been proposed to explain the decision logic of the detection model by providing a set of crucial statements positively contributing to its predictions. Unfortunately, due to the weakly-robust detection models and suboptimal explanation strategy, they have the danger of revealing spurious correlations and redundancy issue.In this paper, we propose Coca, a general framework aiming to 1) enhance the robustness of existing GNN-based vulnerability detection models to avoid spurious explanations; and 2) provide both concise and effective explanations to reason about the detected vulnerabilities. Coca consists of two core parts referred to as Trainer and Explainer. The former aims to train a detection model which is robust to random perturbation based on combinatorial contrastive learning, while the latter builds an explainer to derive crucial code statements that are most decisive to the detected vulnerability via dual-view causal inference as explanations. We apply Coca over three typical GNN-based vulnerability detectors. Experimental results show that Coca can effectively mitigate the spurious correlation issue, and provide more useful high-quality explanations.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {155},
numpages = {13},
keywords = {contrastive learning, causal inference, explainability},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00107,
author = {Yuan, Yuanyuan and Pang, Qi and Wang, Shuai},
title = {Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00107},
doi = {10.1109/ICSE48619.2023.00107},
abstract = {Various deep neural network (DNN) coverage criteria have been proposed to assess DNN test inputs and steer input mutations. The coverage is characterized via neurons having certain outputs, or the discrepancy between neuron outputs. Nevertheless, recent research indicates that neuron coverage criteria show little correlation with test suite quality.In general, DNNs approximate distributions, by incorporating hierarchical layers, to make predictions for inputs. Thus, we champion to deduce DNN behaviors based on its approximated distributions from a layer perspective. A test suite should be assessed using its induced layer output distributions. Accordingly, to fully examine DNN behaviors, input mutation should be directed toward diversifying the approximated distributions.This paper summarizes eight design requirements for DNN coverage criteria, taking into account distribution properties and practical concerns. We then propose a new criterion, Neural Coverage (NLC), that satisfies all design requirements. NLC treats a single DNN layer as the basic computational unit (rather than a single neuron) and captures four critical properties of neuron output distributions. Thus, NLC accurately describes how DNNs comprehend inputs via approximated distributions. We demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks (classification and generation) and data formats (image and text). Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1200–1212},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644033.3644371,
author = {Zeyen, Olivier and Cordy, Maxime and Perrouin, Gilles and Acher, Mathieu},
title = {Preprocessing is What You Need: Understanding and Predicting the Complexity of SAT-based Uniform Random Sampling},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644371},
doi = {10.1145/3644033.3644371},
abstract = {Despite its NP-completeness, the Boolean satisfiability problem gave birth to highly efficient tools that are able to find solutions to a Boolean formula and compute their number. Boolean formulae compactly encode huge, constrained search spaces for variability-intensive systems, e.g., the possible configurations of the Linux kernel. These search spaces are generally too big to explore exhaustively, leading most testing approaches to sample a few solutions before analysing them. A desirable property of such samples is uniformity: each solution should get the same selection probability. This property motivated the design of uniform random samplers, relying on SAT solvers and counters and achieving different tradeoffs between uniformity and scalability. Though we can observe their performance in practice, understanding the complexity these tools face and accurately predicting it is an under-explored problem. Indeed, structural metrics such as the number of variables and clauses involved in a formula poorly predict the sampling complexity. More elaborated ones, such as minimal independent support (MIS), are intractable to compute on large formulae. We provide an efficient parallel algorithm to compute a related metric, the number of equivalence classes, and demonstrate that this metric is highly correlated to time and memory usage of uniform random sampling and model counting tools. We explore the role of formula preprocessing on various metrics and show its positive influence on correlations. Relying on these correlations, we train an efficient classifier (F1-score 0.97) to predict whether uniformly sampling a given formula will exceed a specified budget. Our results allow us to characterise the similarities and differences between (uniform) sampling, solving and counting.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {23–32},
numpages = {10},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00168,
author = {Liu, Zhe and Chen, Chunyang and Wang, Junjie and Su, Yuhui and Huang, Yuekai and Hu, Jun and Wang, Qing},
title = {Ex Pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00168},
doi = {10.1109/ICSE48619.2023.00168},
abstract = {Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1983–1995},
numpages = {13},
keywords = {GUI testing, deep learning, program analysis, empirical study},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643915.3644093,
author = {Dautov, Rustem and Husom, Erik Johannes},
title = {Raft Protocol for Fault Tolerance and Self-Recovery in Federated Learning},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644093},
doi = {10.1145/3643915.3644093},
abstract = {Federated Learning (FL) has emerged as a decentralised machine learning paradigm for distributed systems, particularly in edge and IoT environments. However, ensuring fault tolerance and self-recovery in such scenarios remains challenging, because of the centralised model aggregation which acts as a single point of failure. A possible solution to this challenge would rely on the continuous replication of the global FL state across participating nodes and the functional suitability of any node to replace the aggregator in case of failures. These functional requirements can be implemented using one of the existing distributed consensus algorithm, such as Raft. Our approach utilises Raft's leader election and log replication mechanisms to enable automatic stateful recovery after failures and thus to improve fault tolerance. The log replication process efficiently maintains consistency and coherence across distributed FL nodes, ensuring uninterrupted training process and model convergence. This enhances the robustness of the overall FL system, especially in dynamic and unreliable cyber-physical conditions. To demonstrate the viability of our approach, we present a proof-of-concept implementation based on the existing FL framework Flower. We conduct a series of experiments to measure the aggregator re-election time and traffic overheads associated with the state replication. Despite the expected traffic overheads growing with the number of FL nodes, the results demonstrate a resilient self-recovering system capable of withstanding node failures while maintaining model consistency.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {110–121},
numpages = {12},
keywords = {federated learning, fault tolerance, self-recovery, flower, raft, consensus algorithm},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3597503.3639141,
author = {He, Shuai and Fu, Cai and Hu, Hong and Chen, Jiahe and Lv, Jianqiang and Jiang, Shuai},
title = {MalwareTotal: Multi-Faceted and Sequence-Aware Bypass Tactics against Static Malware Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639141},
doi = {10.1145/3597503.3639141},
abstract = {Recent methods have demonstrated that machine learning (ML) based static malware detection models are vulnerable to adversarial attacks. However, the generated malware often fails to generalize to production-level anti-malware software (AMS), as they usually involve multiple detection methods. This calls for universal solutions to the problem of malware variants generation. In this work, we demonstrate how the proposed method, MalwareTotal, has allowed malware variants to continue to abound in ML-based, signature-based, and hybrid anti-malware software. Given a malicious binary, we develop sequential bypass tactics that enable malicious behavior to be concealed within multi-faceted manipulations. Through 12 experiments on real-world malware, we demonstrate that an attacker can consistently bypass detection (98.67%, and 100% attack success rate against ML-based methods EMBER and MalConv, respectively; 95.33%, 92.63%, and 98.52% attack success rate against production-level anti-malware software ClamAV, AMS A, and AMS B, respectively) without modifying the malware functionality. We further demonstrate that our approach outperforms state-of-the-art adversarial malware generation techniques both in attack success rate and query consumption (the number of queries to the target model). Moreover, the samples generated by our method have demonstrated transferability in the real-world integrated malware detector, VirusTotal. In addition, we show that common mitigation such as adversarial training on known attacks cannot effectively defend against the proposed attack. Finally, we investigate the value of the generated adversarial examples as a means of hardening victim models through an adversarial training procedure, and demonstrate that the accuracy of the retrained model against generated adversarial examples increases by 88.51 percentage points.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {172},
numpages = {12},
keywords = {anti-malware software robustness, black-box attacks, binary manipulation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623320,
author = {Gao, Cuiying and Huang, Gaozhun and Li, Heng and Wu, Bang and Wu, Yueming and Yuan, Wei},
title = {A Comprehensive Study of Learning-based Android Malware Detectors under Challenging Environments},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623320},
doi = {10.1145/3597503.3623320},
abstract = {Recent years have witnessed the proliferation of learning-based Android malware detectors. These detectors can be categorized into three types, String-based, Image-based and Graph-based. Most of them have achieved good detection performance under the ideal setting. In reality, however, detectors often face out-of-distribution samples due to the factors such as code obfuscation, concept drift (e.g., software development technique evolution and new malware category emergence), and adversarial examples (AEs). This problem has attracted increasing attention, but there is a lack of comparative studies that evaluate the existing various types of detectors under these challenging environments. In order to fill this gap, we select 12 representative detectors from three types of detectors, and evaluate them in the challenging scenarios involving code obfuscation, concept drift and AEs, respectively. Experimental results reveal that none of the evaluated detectors can maintain their ideal-setting detection performance, and the performance of different types of detectors varies significantly under various challenging environments. We identify several factors contributing to the performance deterioration of detectors, including the limitations of feature extraction methods and learning models. We also analyze the reasons why the detectors of different types show significant performance differences when facing code obfuscation, concept drift and AEs. Finally, we provide practical suggestions from the perspectives of users and researchers, respectively. We hope our work can help understand the detectors of different types, and provide guidance for enhancing their performance and robustness.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {12},
numpages = {13},
keywords = {android malware detection, machine learning, code obfuscation, concept drift, adversarial examples},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00073,
author = {Mu, Fangwen and Chen, Xiao and Shi, Lin and Wang, Song and Wang, Qing},
title = {Developer-Intent Driven Code Comment Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00073},
doi = {10.1109/ICSE48619.2023.00073},
abstract = {Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {768–780},
numpages = {13},
keywords = {code comment generation, intent-controllable comment generation, automated comment-intent labeling},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623313,
author = {Chen, Liuqing and Chen, Yunnong and Xiao, Shuhong and Song, Yaxuan and Sun, Lingyun and Zhen, Yankun and Zhou, Tingting and Chang, Yanfang},
title = {EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623313},
doi = {10.1145/3597503.3623313},
abstract = {When translating UI design prototypes to code in industry, automatically generating code from design prototypes can expedite the development of applications and GUI iterations. However, in design prototypes without strict design specifications, UI components may be composed of fragmented elements. Grouping these fragmented elements can greatly improve the readability and maintainability of the generated code. Current methods employ a two-stage strategy that introduces hand-crafted rules to group fragmented elements. Unfortunately, the performance of these methods is not satisfying due to visually overlapped and tiny UI elements. In this study, we propose EGFE, a novel method for automatically End-to-end Grouping Fragmented Elements via UI sequence prediction. To facilitate the UI understanding, we innovatively construct a Transformer encoder to model the relationship between the UI elements with multi-modal representation learning. The evaluation on a dataset of 4606 UI prototypes collected from professional UI designers shows that our method outperforms the state-of-the-art baselines in the precision (by 29.75%), recall (by 31.07%), and F1-score (by 30.39%) at edit distance threshold of 4. In addition, we conduct an empirical study to assess the improvement of the generated front-end code. The results demonstrate the effectiveness of our method on a real software engineering application. Our end-to-end fragmented elements grouping method creates opportunities for improving UI-related software engineering tasks.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {11},
numpages = {12},
keywords = {UI elements grouping, fragmented elements grouping, end-to-end pipeline, multi-modal transformer},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00010,
author = {Trinkenreich, Bianca and Stol, Klaas-Jan and Steinmacher, Igor and Gerosa, Marco A. and Sarma, Anita and Lara, Marcelo and Feathers, Michael and Ross, Nicholas and Bishop, Kevin},
title = {A Model for Understanding and Reducing Developer Burnout},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00010},
doi = {10.1109/ICSE-SEIP58684.2023.00010},
abstract = {Job burnout is a type of work-related stress associated with a state of physical or emotional exhaustion that also involves a sense of reduced accomplishment and loss of personal identity. Burnt out can affect one's physical and mental health and has become a leading industry concern and can result in high workforce turnover. Through an empirical study at Globant, a large multi-national company, we created a theoretical model to evaluate the complex interplay among organizational culture, work satisfaction, and team climate, and how they impact developer burnout. We conducted a survey of developers in software delivery teams (n=3,281) to test our model and analyzed the data using structural equation modeling, moderation, and multi-group analysis. Our results show that Organizational Culture, Climate for Learning, Sense of Belonging, and Inclusiveness are positively associated with Work Satisfaction, which in turn is associated with Reduced Burnout. Our model generated through a large-scale survey can guide organizations in how to reduce workforce burnout by creating a climate for learning, inclusiveness in teams, and a generative organizational culture where new ideas are welcome, information is actively sought and bad news can be shared without fear.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {48–60},
numpages = {13},
keywords = {job burnout, work satisfaction, culture, belonging, inclusiveness},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3597503.3623338,
author = {Wang, Yutong and Rubio-Gonz\'{a}lez, Cindy},
title = {Predicting Performance and Accuracy of Mixed-Precision Programs for Precision Tuning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623338},
doi = {10.1145/3597503.3623338},
abstract = {A mixed-precision program is a floating-point program that utilizes different precisions for different operations, providing the opportunity of balancing the trade-off between accuracy and performance. Precision tuning aims to find a mixed-precision version of a program that improves its performance while maintaining a given accuracy. Unfortunately, existing precision tuning approaches are either limited to small-scale programs, or suffer from efficiency issues. In this paper, we propose FPLearner, a novel approach that addresses these limitations. Our insight is to leverage a Machine Learning based technique, Graph Neural Networks, to learn the representation of mixed-precision programs to predict their performance and accuracy. Such prediction models can then be used to accelerate the process of dynamic precision tuning by reducing the number of program runs. We create a dataset of mixed-precision programs from five diverse HPC applications for training our models, which achieve 96.34% F1 score in performance prediction and 97.03% F1 score in accuracy prediction. FPLearner improves the time efficiency of two dynamic precision tuners, Precimonious and HiFPTuner, by an average of 25.54% and up to 61.07% while achieving precision tuning results of comparable or better quality.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {15},
numpages = {13},
keywords = {program representation, graph neural networks, floating point, mixed precision, numerical software, program optimization, precision tuning},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639084,
author = {Zhong, Hao and Meng, Na},
title = {Compiler-directed Migrating API Callsite of Client Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639084},
doi = {10.1145/3597503.3639084},
abstract = {API developers evolve software libraries to fix bugs, add new features, or refactor code, but the evolution can introduce API-breaking changes (e.g., API renaming). To benefit from such evolution, the programmers of client projects have to repetitively upgrade the callsites of libraries, since API-breaking changes introduce many compilation errors. It is tedious and error-prone to resolve such errors, especially when programmers are often unfamiliar with the API usages of newer versions. To migrate client code, the prior approaches either mine API mappings or learn edit scripts, but both the research lines have inherent limitations. For example, mappings alone cannot handle complex cases, and there is no sufficient source (e.g., migration commits) for learning edit scripts.In this paper, we propose a new research direction. When a library is replaced with a newer version, each type of API-breaking change introduces a type of compilation error. For example, renaming the name of an API method causes undefined-method errors at its callsites. Based on this observation, we propose to resolve errors that are introduced by migration, according to their locations and types that are reported by compilers. In this way, a migration tool can incrementally migrate complex cases, even without any change examples. Towards this direction, we propose the first approach, called LibCatch. It defines 14 migration operators, and in a compiler-directed way, it exploits the combinations of migration operators to generate migration solutions, until its predefined criteria are satisfied. We conducted two evaluations. In the first evaluation, we use LibCatch to handle 123 migration tasks. LibCatch reduced migration-related compilation errors for 92.7% of tasks, and eliminated such errors for 32.4% of tasks. We inspect the tasks whose errors are eliminated, and find that 33.9% of them produce identical edits to manual migration edits. In the second evaluation, we use two tools and LibCatch to migrate 15 real client projects in the wild. LibCatch resolved all compilation errors of 7 projects, and reduced the compilation errors of 6 other projects to no more than two errors. As a comparison, the compared two tools reduced the compilation errors of only 1 project.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {226},
numpages = {12},
keywords = {code migration, compiler, API library},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639098,
author = {Song, Yi and Zhang, Xihao and Xie, Xiaoyuan and Liu, Quanming and Gao, Ruizhi and Xing, Chenliang},
title = {ReClues: Representing and indexing failures in parallel debugging with program variables},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639098},
doi = {10.1145/3597503.3639098},
abstract = {Failures with different root causes can greatly disrupt multi-fault localization, therefore, categorizing failures into distinct groups according to the culprit fault is highly important. In such a failure indexing task, the crux lies in the failure proximity, which comprises two points, i.e., how to effectively represent failures (e.g., extract the signature of failures) and how to properly measure the distance between those proxies for failures. Existing research has proposed a variety of failure proximities. The majority of them extract signatures of failures from execution coverage or suspiciousness ranking lists, and accordingly employ the Euclid or the Kendall tau distances, etc. However, such strategies may not properly reflect the essential characteristics of failures, thus resulting in unsatisfactory effectiveness. In this paper, we propose a new failure proximity, namely, the program variable-based failure proximity, and further present a novel failure indexing approach, ReClues. Specifically, ReClues utilizes the run-time values of program variables to represent failures, and designs a set of rules to measure the similarity between them. Experimental results demonstrate the competitiveness of ReClues: it can achieve 44.12% and 27.59% improvements in faults number estimation, as well as 47.56% and 26.27% improvements in clustering effectiveness, compared with the state-of-the-art technique in this field, in simulated and real-world environments, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {111},
numpages = {13},
keywords = {failure proximity, clustering, failure indexing, parallel debugging, program variable},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623321,
author = {Murali, Aniruddhan and Mathews, Noble and Alfadel, Mahmoud and Nagappan, Meiyappan and Xu, Meng},
title = {FuzzSlice: Pruning False Positives in Static Analysis Warnings through Function-Level Fuzzing},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623321},
doi = {10.1145/3597503.3623321},
abstract = {Manual confirmation of static analysis reports is a daunting task. This is due to both the large number of warnings and the high density of false positives among them. Fuzzing techniques have been proposed to verify static analysis warnings. However, a major limitation is that fuzzing the whole project to reach all static analysis warnings is not feasible. This can take several days and exponential machine time to increase code coverage linearly.Therefore, we propose FuzzSlice, a novel framework that automatically prunes possible false positives among static analysis warnings. Unlike prior work that mostly focuses on confirming true positives among static analysis warnings, which inevitably requires end-to-end fuzzing, FuzzSlice focuses on ruling out potential false positives, which are the majority in static analysis reports. The key insight that we base our work on is that a warning that does not yield a crash when fuzzed at the function level in a given time budget is a possible false positive. To achieve this, FuzzSlice first aims to generate compilable code slices at the function level. Then, FuzzSlice fuzzes these code slices instead of the entire binary to prune possible false positives. FuzzSlice is also unlikely to misclassify a true bug as a false positive because the crashing input can be reproduced by a fuzzer at the function level as well. We evaluate FuzzSlice on the Juliet synthetic dataset and real-world complex C projects: openssl, tmux and openssh-portable. Our evaluation shows that the ground truth in the Juliet dataset had 864 false positives which were all detected by FuzzSlice. For the open-source repositories, we were able to get the developers from two of these open-source repositories to independently label these warnings. FuzzSlice automatically identifies 33 out of 53 false positives confirmed by developers in these two repositories. This implies that FuzzSlice can reduce the number of false positives by 62.26% in the open-source repositories and by 100% in the Juliet dataset.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {65},
numpages = {13},
keywords = {fuzzing, static analysis warning, vulnerability},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639082,
author = {Guo, Hanyang and Dai, Hong-Ning and Luo, Xiapu and Zheng, Zibin and Xu, Gengyang and He, Fengliang},
title = {An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639082},
doi = {10.1145/3597503.3639082},
abstract = {Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS (e.g., Android). As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics (e.g., hand-tracking and face-tracking data). Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {159},
numpages = {13},
keywords = {virtual reality, metaverse, static analysis, security and privacy},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00116,
author = {Wei, Guannan and Jia, Songlin and Gao, Ruiqi and Deng, Haotian and Tan, Shangyin and Bra\v{c}evac, Oliver and Rompf, Tiark},
title = {Compiling Parallel Symbolic Execution with Continuations},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00116},
doi = {10.1109/ICSE48619.2023.00116},
abstract = {Symbolic execution is a powerful program analysis and testing technique. Symbolic execution engines are usually implemented as interpreters, and the induced interpretation overhead can dramatically inhibit performance. Alternatively, implementation choices based on instrumentation provide a limited ability to transform programs. However, the use of compilation and code generation techniques beyond simple instrumentation remains underexplored for engine construction, leaving potential performance gains untapped.In this paper, we show how to tap some of these gains using sophisticated compilation techniques: We present GenSym, an optimizing symbolic-execution compiler that generates symbolic code which explores paths and generates tests in parallel. The key insight of GenSym is to compile symbolic execution tasks into cooperative concurrency via continuation-passing style, which further enables efficient parallelism. The design and implementation of GenSym is based on partial evaluation and generative programming techniques, which make it high-level and performant at the same time. We compare the performance of GenSym against the prior symbolic-execution compiler LLSC and the state-of-the-art symbolic interpreter KLEE. The results show an average 4.6\texttimes{} speedup for sequential execution and 9.4\texttimes{} speedup for parallel execution on 20 benchmark programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1316–1328},
numpages = {13},
keywords = {continuation, metaprogramming, code generation, compiler, symbolic execution},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3643108,
author = {Liu, Yilun and Tao, Shimin and Meng, Weibin and Yao, Feiyu and Zhao, Xiaofeng and Yang, Hao},
title = {LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643108},
doi = {10.1145/3639478.3643108},
abstract = {Automated log analysis plays a crucial role in software maintenance as it allows for efficient identification and resolution of issues. However, traditional methods employed in log analysis heavily rely on extensive historical data for training purposes and lack rationales for its predictions. The performance of these traditional methods significantly deteriorates when in-domain logs for training are limited and unseen log data are the majority, particularly in rapidly changing online environments. Additionally, the lack of rationales hampers the interpretability of analysis results and impacts analysts' subsequent decision-making processes. To address these challenges, we proposes LogPrompt, an novel approach that leverages large language models (LLMs) and advanced prompting techniques to achieve performance improvements in zero-shot scenarios (i.e., no in-domain training). Moreover, LogPrompt has garnered positive evaluations from experienced practitioners in its log interpretation ability. Code available at https://github.com/lunyiliu/LogPrompt.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {364–365},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00017,
author = {Wu, Mingyuan and Lu, Minghai and Cui, Heming and Chen, Junjie and Zhang, Yuqun and Zhang, Lingming},
title = {JITfuzz: Coverage-Guided Fuzzing for JVM Just-in-Time Compilers},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00017},
doi = {10.1109/ICSE48619.2023.00017},
abstract = {As a widely-used platform to support various Javabytecode-based applications, Java Virtual Machine (JVM) incurs severe performance loss caused by its real-time program interpretation mechanism. To tackle this issue, the Just-in-Time compiler (JIT) has been widely adopted to strengthen the efficacy of JVM. Therefore, how to effectively and efficiently detect JIT bugs becomes critical to ensure the correctness of JVM. In this paper, we propose a coverage-guided fuzzing framework, namely JITfuzz, to automatically detect JIT bugs. In particular, JITfuzz adopts a set of optimization-activating mutators to trigger the usage of typical JIT optimizations, e.g., function inlining and simplification. Meanwhile, given JIT optimizations are closely coupled with program control flows, JITfuzz also adopts mutators to enrich the control flows of target programs. Moreover, JITfuzz also proposes a mutator scheduler which iteratively schedules mutators according to the coverage updates to maximize the code coverage of JIT. To evaluate the effectiveness of JITfuzz, we conduct a set of experiments based on a benchmark suite with 16 popular JVM-based projects from GitHub. The experimental results suggest that JITfuzz outperforms the state-of-the-art mutation-based and generation-based JVM fuzzers by 27.9% and 18.6% respectively in terms of edge coverage on average. Furthermore, JITfuzz also successfully detects 36 previously unknown bugs (including 23 JIT bugs) and 27 bugs (including 18 JIT bugs) have been confirmed by the developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {56–68},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643915.3644092,
author = {Islam, Md Nafee Al and Cleland-Huang, Jane and Vierhauser, Michael},
title = {ADAM: Adaptive Monitoring of Runtime Anomalies in Small Uncrewed Aerial Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644092},
doi = {10.1145/3643915.3644092},
abstract = {Small Uncrewed Aerial Systems (sUAS), commonly referred to as drones, have become ubiquitous in many domains. Examples range from drones taking part in search-and-rescue operations to drones being used for delivering medical supplies or packages. As sUAS commonly exhibit safety-critical behavior, ensuring their safe operation has become a top priority. Thus, continuous and rigorous monitoring of sUAS at runtime is essential. However, sUAS generate vast amounts of data, for example, multi-variate time series which need to be analyzed to detect potential emerging issues. This poses a significant challenge, due to resource constraints imposed on the onboard computation capabilities of sUAS. To alleviate this problem, we introduce ADAM, a novel adaptive monitoring anomaly detection framework for sUAS. ADAM selectively monitors a subset of data streams, which serve as indicators of anomalous behavior. In the event of a raised alert, ADAM adjusts its monitoring strategy, enabling additional detectors and taking further mitigation actions. We evaluated the effectiveness of ADAM through simulations in Gazebo, analysis of real flight logs taken from sUAS forums, and tests with real drones. Results confirm that ADAM can enhance safety and efficiency of sUAS operations, by dynamically managing anomaly detection, reducing CPU and memory usage by up to 65%.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {44–55},
numpages = {12},
keywords = {adaptive monitoring, drone, sUAS, anomaly detection, self-adaptive systems, UAV},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3597503.3639207,
author = {Cui, Ziyu and Dou, Wensheng and Gao, Yu and Wang, Dong and Song, Jiansen and Zheng, Yingying and Wang, Tao and Yang, Rui and Xu, Kang and Hu, Yixin and Wei, Jun and Huang, Tao},
title = {Understanding Transaction Bugs in Database Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639207},
doi = {10.1145/3597503.3639207},
abstract = {Transactions are used to guarantee data consistency and integrity in Database Management Systems (DBMSs), and have become an indispensable component in DBMSs. However, faulty designs and implementations of DBMSs' transaction processing mechanisms can introduce transaction bugs, and lead to severe consequences, e.g., incorrect database states and DBMS crashes. An in-depth understanding of real-world transaction bugs can significantly promote effective techniques in combating transaction bugs in DBMSs.In this paper, we conduct the first comprehensive study on 140 transaction bugs collected from six widely-used DBMSs, i.e., MySQL, PostgreSQL, SQLite, MariaDB, CockroachDB, and TiDB. We investigate these bugs from their bug manifestations, root causes, bug impacts and bug fixing. Our study reveals many interesting findings and provides useful guidance for transaction bug detection, testing, and verification.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {163},
numpages = {13},
keywords = {database system, transaction bug, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643659.3648560,
author = {Humeniuk, Dmytro and Khomh, Foutse},
title = {AmbieGenVAE at the SBFT 2024 Tool Competition - Cyber-Physical Systems Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648560},
doi = {10.1145/3643659.3648560},
abstract = {Testing and verification of autonomous systems is critically important. In the context of SBFT 2024 Cyber-physical systems (CPS) testing tool competition, we present our tool AmbieGenVAE for generating virtual roads to test an autonomous vehicle lane keeping assist system. AmbieGenVAE leverages optimization in a Variational Autoencoder latent space to produce challenging test scenarios. It has achieved the highest score for one of the test subjects and the second-highest final score among 2 other submitted tools.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {45–46},
numpages = {2},
keywords = {lane-keeping assist system, testing, genetic algorithms, latent space},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3644032.3644467,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Tauseef, Qasim and Seferi, Gkerta},
title = {Machine Learning-based Test Case Prioritization using Hyperparameter Optimization},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644467},
doi = {10.1145/3644032.3644467},
abstract = {Continuous integration pipelines execute extensive automated test suites to validate new software builds. In this fast-paced development environment, delivering timely testing results to developers is critical to ensuring software quality. Test case prioritization (TCP) emerges as a pivotal solution, enabling the prioritization of fault-prone test cases for immediate attention. Recent advancements in machine learning have showcased promising results in TCP, offering the potential to revolutionize how we optimize testing workflows. Hyperparameter tuning plays a crucial role in enhancing the performance of ML models. However, there needs to be more work investigating the effects of hyperparameter tuning on TCP. Therefore, we explore how optimized hyperparameters influence the performance of various ML classifiers, focusing on the Average Percentage of Faults Detected (APFD) metric. Through empirical analysis of ten real-world, large-scale, diverse datasets, we conduct a grid search-based tuning with 885 hyperparameter combinations for four machine learning models. Our results provide model-specific insights and demonstrate an average 15% improvement in model performance with hyperparameter tuning compared to default settings. We further explain how hyperparameter tuning improves precision (max = 1), recall (max = 0.9633), F1-score (max = 0.9662), and influences APFD value (max = 0.9835), indicating a direct connection between tuning and prioritization performance. Hence, this study underscores the importance of hyperparameter tuning in optimizing failure prediction models and their direct impact on prioritization performance.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {125–135},
numpages = {11},
keywords = {hyperparameter optimization, test case prioritization, machine learning, continuous integration},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3639478.3640031,
author = {Khatiri, Sajad and Panichella, Sebastiano and Tonella, Paolo},
title = {Simulation-based Testing of Unmanned Aerial Vehicles with Aerialist},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640031},
doi = {10.1145/3639478.3640031},
abstract = {Simulation-based testing is crucial for ensuring the safety and reliability of unmanned aerial vehicles (UAVs), especially as they become more autonomous and get increasingly used in commercial scenarios. The complexity and automated nature of UAVs requires sophisticated simulation environments for effectively testing their safety requirements. The primary challenges in setting up these environments pose significant barriers to the practical, widespread adoption of UAVs. We address this issue by introducing Aerialist (unmanned AERIAL vehIcle teST bench), a novel UAV test bench, built on top of PX4 firmware, that facilitates or automates all the necessary steps of definition, generation, execution, and analysis of system-level UAV test cases in simulation environments. Moreover, it also supports parallel and scalable execution and analysis of test cases on Kubernetes clusters. This makes Aerialist a unique platform for research and development of test generation approaches for UAVs. To evaluate Aerialist's support for UAV developers in defining, generating, and executing UAV test cases, we implemented a search-based approach for generating realistic simulation-based test cases using real-world UAV flight logs. We confirmed its effectiveness in improving the realism and representativeness of simulation-based UAV tests.Code Repository: https://github.com/skhatiri/AerialistDemo Video: https://youtu.be/k_bqYpWItSg},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {134–138},
numpages = {5},
keywords = {unmanned aerial vehicles, test generation, simulation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639205,
author = {Zhang, Chenyangguang and Jia, Tong and Shen, Guopeng and Zhu, Pinyan and Li, Ying},
title = {MetaLog: Generalizable Cross-System Anomaly Detection from Logs with Meta-Learning},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639205},
doi = {10.1145/3597503.3639205},
abstract = {Log-based anomaly detection plays a crucial role in ensuring the stability of software. However, current approaches for log-based anomaly detection heavily depend on a vast amount of labeled historical data, which is often unavailable in many real-world systems. To mitigate this problem, we leverage the features of the abundant historical labeled logs of mature systems to help construct anomaly detection models of new systems with very few labels, that is, to generalize the model ability trained from labeled logs of mature systems to achieve anomaly detection on new systems with insufficient data labels. Specifically, we propose MetaLog, a generalizable cross-system anomaly detection approach. MetaLog first incorporates a globally consistent semantic embedding module to obtain log event semantic embedding vectors in a shared global space. Then it leverages the meta-learning paradigm to improve the model's generalization ability. We evaluate MetaLog's performance on four public log datasets (HDFS, BGL, OpenStack, and Thunderbird) from four different systems. Results show that MetaLog reaches over 80% F1-score when using only 1% labeled logs of the target system, showing similar performance with state-of-the-art supervised anomaly detection models trained with 100% labeled data. Besides, it outperforms state-of-art transfer-learning-based cross-system anomaly detection models by 20% in the same settings of 1% labeled training logs of the target system.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {154},
numpages = {12},
keywords = {meta-learning, anomaly detection, system logs},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643916.3646556,
author = {Gong, Yuanjun and Nie, Jianglei and You, Wei and Shi, Wenchang and Huang, Jianjun and Liang, Bin and Zhang, Jian},
title = {SICode: Embedding-Based Subgraph Isomorphism Identification for Bug Detection},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3646556},
doi = {10.1145/3643916.3646556},
abstract = {Given a known buggy code snippet, searching for similar patterns in a target project to detect unknown bugs is a reasonable approach. In practice, a search unit, such as a function, may appear quite different from the buggy snippet but actually contains a similar buggy substructure. Utilizing subgraph isomorphism identification can effectively hunt potential bugs by checking whether an approximate copy of the buggy subgraph exists within the target code graphs. Regrettably, subgraph isomorphism identification is an NP-complete problem.In this paper, we propose an embedding-based method, SICode, to efficiently perform subgraph isomorphism identification for code graphs. We train a graph embedding model and the subgraph isomorphism relationship between two graphs can be measured by comparing their embedding vectors. In this manner, we can efficiently identify potential buggy code graphs via vector arithmetic without solving an NP-complete problem. A cascading loss scheme is presented to ensure the identification performance.SICode exhibits greater scalability than classic subgraph isomorphism algorithms, such as VF2, and maintains high precision and recall. Experiments also demonstrate that SICode offers advantages in detecting sub-structurally similar bugs. Our approach spotted 20 previously-unknown bugs in real-world projects, among which, 18 bugs were confirmed by their developers and ranked within the top ten results of retrieval. This result is very encouraging for detecting subtle sub-structurally similar bugs.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {304–315},
numpages = {12},
keywords = {subgraph isomorphism, bug detection, cascading loss, graph embedding},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1109/ICSE48619.2023.00022,
author = {Croft, Roland and Babar, M. Ali and Kholoosi, M. Mehdi},
title = {Data Quality for Software Vulnerability Datasets},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00022},
doi = {10.1109/ICSE48619.2023.00022},
abstract = {The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security domain. These data-driven solutions are enabled by large software vulnerability datasets used for training and benchmarking. However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. Our study seeks to address such shortcomings by inspecting five inherent data quality attributes for four state-of-the-art software vulnerability datasets and the subsequent impacts that issues can have on software vulnerability prediction models. Surprisingly, we found that all the analyzed datasets exhibit some data quality problems. In particular, we found 20--71% of vulnerability labels to be inaccurate in real-world datasets, and 17--99% of data points were duplicated. We observed that these issues could cause significant impacts on downstream models, either preventing effective model training or inflating benchmark performance. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {121–133},
numpages = {13},
keywords = {machine learning, data quality, software vulnerability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00014,
author = {Kim, Dong Jae and Locke, Steve and Chen, Tse-Hsun (Peter) and Toma, Andrei and Sporea, Steve and Weinkam, Laura and Sajedi, Sarah},
title = {Challenges in Adopting Artificial Intelligence Based User Input Verification Framework in Reporting Software Systems},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00014},
doi = {10.1109/ICSE-SEIP58684.2023.00014},
abstract = {Artificial intelligence is driving new industrial solutions for challenging problems once considered impossible. Many large-scale companies use AI to identify opportunities to improve business processes and products. Despite the promise and perils of AI, many traditional software systems (e.g., taxation or reporting) are implemented without AI in mind. Adopting AI-based capabilities in such software can be challenging due to a lack of resources and uncertainties in requirements. This paper documents our experience working with our industry partner on adopting AI capabilities in enterprise software. The enterprise software receives and processes thousands of user inputs with different configuration settings daily, which makes manual user input verification infeasible. To assist our industry partner, we design and integrate an AI-based input verification framework into the software. However, during the design and integration of the framework, we encounter many challenges that range from the requirement engineering process to the development, adoption, and verification process. We discuss the challenges we encountered and their corresponding solutions while working with our industrial partner to integrate the AI-based input verification framework into their non-AI software. Our experience report may provide valuable insight to practitioners and researchers on better integrating AI-based capabilities with existing software systems.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {99–109},
numpages = {11},
keywords = {experience report, testing, user input},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3597503.3639178,
author = {Woodlief, Trey and Toledo, Felipe and Elbaum, Sebastian and Dwyer, Matthew B},
title = {S3C: Spatial Semantic Scene Coverage for Autonomous Vehicles},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639178},
doi = {10.1145/3597503.3639178},
abstract = {Autonomous vehicles (AVs) must be able to operate in a wide range of scenarios including those in the long tail distribution that include rare but safety-critical events. The collection of sensor input and expected output datasets from such scenarios is crucial for the development and testing of such systems. Yet, approaches to quantify the extent to which a dataset covers test specifications that capture critical scenarios remain limited in their ability to discriminate between inputs that lead to distinct behaviors, and to render interpretations that are relevant to AV domain experts. To address this challenge, we introduce S3C, a framework that abstracts sensor inputs to coverage domains that account for the spatial semantics of a scene. The approach leverages scene graphs to produce a sensor-independent abstraction of the AV environment that is interpretable and discriminating. We provide an implementation of the approach and a study for camera-based autonomous vehicles operating in simulation. The findings show that S3C outperforms existing techniques in discriminating among classes of inputs that cause failures, and offers spatial interpretations that can explain to what extent a dataset covers a test specification. Further exploration of S3C with open datasets complements the study findings, revealing the potential and shortcomings of deploying the approach in the wild.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {141},
numpages = {13},
keywords = {coverage, scene graph, autonomous vehicles, perception},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00115,
author = {Han, Zhilei and He, Fei},
title = {Data-Driven Recurrent Set Learning for Non-termination Analysis},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00115},
doi = {10.1109/ICSE48619.2023.00115},
abstract = {Termination is a fundamental liveness property for program verification. In this paper, we revisit the problem of non-termination analysis and propose the first data-driven learning algorithm for synthesizing recurrent sets, where the non-terminating samples are effectively speculated by a novel method. To ensure convergence of learning, we develop a learning algorithm which is guaranteed to converge to a valid recurrent set if one exists, and thus establish its relative completeness. The methods are implemented in a prototype tool, and experimental results on public benchmarks show its efficacy in proving non-termination as it outperforms state-of-the-art tools, both in terms of cases solved and performance. Evaluation on nonlinear programs also demonstrates its ability to handle complex programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1303–1315},
numpages = {13},
keywords = {black-box learning, data-driven approach, recurrent set, program termination},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3623325,
author = {Ren, Pengcheng and Zuo, Chaoshun and Liu, Xiaofeng and Diao, Wenrui and Zhao, Qingchuan and Guo, Shanqing},
title = {DEMISTIFY: Identifying On-device Machine Learning Models Stealing and Reuse Vulnerabilities in Mobile Apps},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623325},
doi = {10.1145/3597503.3623325},
abstract = {Mobile apps have become popular for providing artificial intelligence (AI) services via on-device machine learning (ML) techniques. Unlike accomplishing these AI services on remote servers traditionally, these on-device techniques process sensitive information required by AI services locally, which can mitigate the severe concerns of the sensitive data collection on the remote side. However, these on-device techniques have to push the core of ML expertise (e.g., models) to smartphones locally, which are still subject to similar vulnerabilities on the remote clouds and servers, especially when facing the model stealing attack. To defend against these attacks, developers have taken various protective measures. Unfortunately, we have found that these protections are still insufficient, and on-device ML models in mobile apps could be extracted and reused without limitation. To better demonstrate its inadequate protection and the feasibility of this attack, this paper presents DeMistify, which statically locates ML models within an app, slices relevant execution components, and finally generates scripts automatically to instrument mobile apps to successfully steal and reuse target ML models freely. To evaluate DeMistify and demonstrate its applicability, we apply it on 1,511 top mobile apps using on-device ML expertise for several ML services based on their install numbers from Google Play and DeMistify can successfully execute 1250 of them (82.73%). In addition, an in-depth study is conducted to understand the on-device ML ecosystem in the mobile application.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {41},
numpages = {13},
keywords = {android app, machine learning, on-device model reuse, program analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643788.3648019,
author = {Yuan, Yuan},
title = {ARJA-e for the First International Competition on Automated Program Repair},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648019},
doi = {10.1145/3643788.3648019},
abstract = {ARJA-e is an enhanced repair system based on ARJA. Here we briefly introduces ARJA-e and report the results of ARJA-e for the First International Competition on Automated Program Repair on two competition tracks, including the AI Generated Code track and the Functional Errors track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {50–52},
numpages = {3},
keywords = {program repair, genetic programming},
location = {Lisbon, Portugal},
series = {APR '24}
}

@proceedings{10.1145/3643659,
title = {SBFT '24: Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 17th edition of the International Workshop on Search-Based and Fuzz Testing (SBFT), formerly the International Workshop on Search-Based Software Testing. Search- Based Software Testing (SBST) applies search-based optimization algorithms to address various problems in software testing. The research in this area has proposed various SBST approaches that achieve different testing goals (e.g., structural, functional, non-functional, and state-based properties) across a range of application domains (e.g., traditional, web, enterprise, mobile applications, and Cyber-physical systems). Fuzz Testing also seeks automation to generate efficient tests that uncover issues in the systems under test (SUT). Fuzz Testing is usually applied at the system level and aims to generate unexpected inputs that would result in crashes of the SUT.The research endeavours in SBST and Fuzz Testing tackle similar testing problems and propose techniques grounded in similar principles (e.g., driving the test generation process by the achieved coverage). The recognition of this similarity has led to a decision to rename the workshop to Search-Based and Fuzz Testing starting in 2023. The primary objective of this workshop is to provide a platform for uniting together researchers and industrial practitioners from SBST, Fuzzing, and the wider Software Engineering community to exchange experiences and explore directions for future research on software testing automation. A second objective is to promote using search and fuzzing techniques to combine testing with other areas of software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639478.3643076,
author = {Huang, Tao and Sun, Zhihong and Jin, Zhi and Li, Ge and Lyu, Chen},
title = {KareCoder: A New Knowledge-Enriched Code Generation System},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643076},
doi = {10.1145/3639478.3643076},
abstract = {Large Language Models (LLMs) demonstrate proficiency in handling fundamental programming problems but struggle with complex programming in new types. The study presents KareCoder, integrating programming knowledge into code generation. Initial tests reveal KareCoder's significant success in the Pass@1 metric for complex competitive programming problems.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {270–271},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00045,
author = {Jiang, Ling and Yuan, Hengchen and Wu, Mingyuan and Zhang, Lingming and Zhang, Yuqun},
title = {Evaluating and Improving Hybrid Fuzzing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00045},
doi = {10.1109/ICSE48619.2023.00045},
abstract = {To date, various hybrid fuzzers have been proposed for maximal program vulnerability exposure by integrating the power of fuzzing strategies and concolic executors. While the existing hybrid fuzzers have shown their superiority over conventional coverage-guided fuzzers, they seldom follow equivalent evaluation setups, e.g., benchmarks and seed corpora. Thus, there is a pressing need for a comprehensive study on the existing hybrid fuzzers to provide implications and guidance for future research in this area. To this end, in this paper, we conduct the first extensive study on state-of-the-art hybrid fuzzers. Surprisingly, our study shows that the performance of existing hybrid fuzzers may not well generalize to other experimental settings. Meanwhile, their performance advantages over conventional coverage-guided fuzzers are overall limited. In addition, instead of simply updating the fuzzing strategies or concolic executors, updating their coordination modes potentially poses crucial performance impact of hybrid fuzzers. Accordingly, we propose CoFuzz to improve the effectiveness of hybrid fuzzers by upgrading their coordination modes. Specifically, based on the baseline hybrid fuzzer QSYM, CoFuzz adopts edge-oriented scheduling to schedule edges for applying concolic execution via an online linear regression model with Stochastic Gradient Descent. It also adopts sampling-augmenting synchronization to derive seeds for applying fuzzing strategies via the interval path abstraction and John walk as well as incrementally updating the model. Our evaluation results indicate that CoFuzz can significantly increase the edge coverage (e.g., 16.31% higher than the best existing hybrid fuzzer in our study) and expose around 2X more unique crashes than all studied hybrid fuzzers. Moreover, CoFuzz successfully detects 37 previously unknown bugs where 30 are confirmed with 8 new CVEs and 20 are fixed.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {410–422},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643663.3643966,
author = {Upadhyay, Abhinav and Dubey, Alpana and Sengupta, Shubhashis},
title = {RoMaViD: Learning Robotic Manipulation from Video Demonstrations},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643663.3643966},
doi = {10.1145/3643663.3643966},
abstract = {Large-scale manufacturing necessitates automation, and robotic automation has emerged as a primary solution. Traditionally, robotic systems are designed for fixed assembly lines dedicated to specific product sets. However, with an increasing demand for specialized and customized products, there is a growing need for more agile manufacturing processes. To address this, we introduce a robotic assembly framework capable of generating assembly plans directly from RGB-D video demonstrations. We develop a pose estimation model to capture changes in object poses. We demonstrate its effectiveness in the robotic assembly of IKEA furniture, emphasizing its precision in managing assembly tasks.},
booktitle = {Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
pages = {17–24},
numpages = {8},
keywords = {robotic assembly, learning from demonstration, plan generation},
location = {Lisbon, Portugal},
series = {RoSE '24}
}

@inproceedings{10.1145/3643991.3644930,
author = {Baral, Kesina and Johnson, John and Mahmud, Junayed and Salma, Sabiha and Fazzini, Mattia and Rubin, Julia and Offutt, Jeff and Moran, Kevin},
title = {Automating GUI-based Test Oracles for Mobile Apps},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644930},
doi = {10.1145/3643991.3644930},
abstract = {In automated testing, test oracles are used to determine whether software behaves correctly on individual tests by comparing expected behavior with actual behavior, revealing incorrect behavior. Automatically creating test oracles is a challenging task, especially in domains where software behavior is difficult to model. Mobile apps are one such domain, primarily due to their event-driven, GUI-based nature, coupled with significant ecosystem fragmentation. This paper takes a step toward automating the construction of GUI-based test oracles for mobile apps, first by characterizing common behaviors associated with failures into a behavioral taxonomy, and second by using this taxonomy to create automated oracles. Our taxonomy identifies and categorizes common GUI element behaviors, expected app responses, and failures from 124 reproducible bug reports, which allow us to better understand oracle characteristics. We use the taxonomy to create app-independent oracles and report on their generalizability by analyzing an additional dataset of 603 bug reports. We also use this taxonomy to define an app-independent process for creating automated test oracles, which leverages computer vision and natural language processing, and apply our process to automate five types of app-independent oracles. We perform a case study to assess the effectiveness of our automated oracles by exposing them to 15 real-world failures. The oracles reveal 11 of the 15 failures and report only one false positive. Additionally, we combine our oracles with a recent automated test input generation tool for Android, revealing two bugs with a low false positive rate. Our results can help developers create stronger automated tests that can reveal more problems in mobile apps and help researchers who can use the understanding from the taxonomy to make further advances in test automation.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {309–321},
numpages = {13},
keywords = {mobile apps, test oracles, software testing, UI analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3528227.3528565,
author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
title = {"If security is required": engineering and security practices for machine learning-based IoT devices},
year = {2023},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528227.3528565},
doi = {10.1145/3528227.3528565},
abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices.This research seeks to characterize engineering processes and security practices for ML-enabled IoT systems through the lens of the engineering lifecycle. We collected data from practitioners through a survey (N=25) and interviews (N=4). We found that security processes and engineering methods vary by company. Respondents emphasized the engineering cost of security analysis and threat modeling, and trade-offs with business needs. Engineers reduce their security investment if it is not an explicit requirement. The threats of IP theft and reverse engineering were a consistent concern among practitioners when deploying ML for IoT devices. Based on our findings, we recommend further research into understanding engineering cost, compliance, and security trade-offs.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
pages = {1–8},
numpages = {8},
keywords = {software engineering, security and privacy, machine learning, internet of things, embedded systems, cyber-physical systems},
location = {Pittsburgh, Pennsylvania},
series = {SERP4IoT '22}
}

@inproceedings{10.1145/3597503.3639096,
author = {Ahmad, Hammad and Endres, Madeline and Newman, Kaia and Santiesteban, Priscila and Shedden, Emma and Weimer, Westley},
title = {Causal Relationships and Programming Outcomes: A Transcranial Magnetic Stimulation Experiment},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639096},
doi = {10.1145/3597503.3639096},
abstract = {Understanding the relationship between cognition and programming outcomes is important: it can inform interventions that help novices become experts faster. Neuroimaging techniques can measure brain activity, but prior studies of programming report only correlations. We present the first causal neurological investigation of the cognition of programming by using Transcranial Magnetic Stimulation (TMS). TMS permits temporary and noninvasive disruption of specific brain regions. By disrupting brain regions and then measuring programming outcomes, we discover whether a true causal relationship exists. To the best of our knowledge, this is the first use of TMS to study software engineering.Where multiple previous studies reported correlations, we find no direct causal relationships between implicated brain regions and programming. Using a protocol that follows TMS best practices and mitigates for biases, we replicate psychology findings that TMS affects spatial tasks. We then find that neurostimulation can affect programming outcomes. Multi-level regression analysis shows that TMS stimulation of different regions significantly accounts for 2.2% of the variance in task completion time. Our results have implications for interventions in education and training as well as research into causal cognitive relationships.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {188},
numpages = {13},
keywords = {neurostimulation, spatial ability, code reading, data structures},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644924,
author = {Kola-Olawuyi, Ajiromola and Weeraddana, Nimmi Rashinika and Nagappan, Meiyappan},
title = {The Impact of Code Ownership of DevOps Artefacts on the Outcome of DevOps CI Builds},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644924},
doi = {10.1145/3643991.3644924},
abstract = {DevOps is a key element in sustaining the quality and efficiency of software development. Yet, the effectiveness of DevOps methodologies extends beyond just technological expertise. It is greatly affected by the manner in which teams handle and engage with DevOps artefacts. Grasping the intricacies of code ownership and contribution patterns within DevOps artefacts is vital for refining strategies and ensuring they deliver their full potential.There are two main strategies to manage DevOps artefacts as suggested in prior work: (1) all project developers need to contribute to DevOps artefacts, and (2) a dedicated group of developers needs to be authoring DevOps artefacts. To analyze which strategy works best for Open-Source Software (OSS) projects, we conduct an empirical analysis on a dataset of 892,193 CircleCI builds spanning 1,689 OSS projects. We employ a two-pronged approach to our study. First, we investigate the impact of chronological code ownership of DevOps artefacts on the outcome of a CI build on a build level. Second, we study the impact of the Skewness of DevOps contributions on the success rate of CI builds at the project level.Our findings reveal that, in general, larger chronological ownership and higher Skewness values of DevOps contributions are related to more successful build outcomes and higher rates of successful build outcomes, respectively. We further find that projects with low Skewness values could have high build success rates when the number of developers in the project is relatively small. Thus, our results suggest that while larger software organizations are better off having dedicated DevOps developers, smaller organizations would benefit from having all developers involved in DevOps.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {543–555},
numpages = {13},
keywords = {DevOps, code ownership, continuous integrations, empirical study},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643659.3643936,
author = {Zhu, Taohong and Newton, William and Embury, Suzanne and Sun, Youcheng},
title = {TAIiST CPS-UAV at the SBFT Tool Competition 2024},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643936},
doi = {10.1145/3643659.3643936},
abstract = {This paper presents an innovative approach to testing Cyber-Physical Systems (CPS) with a specific focus on Unmanned Aerial Vehicles (UAVs) using Large Language Models (LLMs). In the rapidly evolving field of UAV technology, ensuring the reliability and safety of these systems is of utmost importance. Traditional testing methods often fall short in addressing the complex, dynamic, and stateful environments in which UAVs operate. To bridge this gap, we propose the use of state-of-the-art LLMs.Our methodology leverages the capabilities of LLMs to "intelligently" simulate a wide range of real-world scenarios and interactions that UAVs may encounter. This includes interpreting and responding to dynamic environmental changes, unexpected obstacles, and real-time decision-making processes. By integrating LLMs into the testing framework, we can create more comprehensive, realistic, and efficient testing scenarios for CPS-UAVs.We demonstrate the effectiveness of our approach through a series of experiments using popular UAV platforms including PX4 and Ardupilot. Our code and implementation are made publicly available in our project page https://github.com/Trusted-AI-in-System-Test.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {51–52},
numpages = {2},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643656.3643899,
author = {Malmir, Samaneh and Rigby, Peter Christopher},
title = {Predicting the Lifetime of Flaky Tests on Chrome},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643899},
doi = {10.1145/3643656.3643899},
abstract = {Flaky tests not only disrupt the testing process but can also introduce significant delays in software release cycles, leading to increased costs and potential missed market opportunities. The ability to predict the lifetime class to which the test belongs to can empower development teams to allocate resources efficiently, prioritize bug fixes, and streamline the testing pipeline. Despite the critical role that this estimation plays in software development, there is a noticeable gap in the existing research literature. Bridging this gap is essential for enhancing testing efficiency and optimizing resource allocation and project planning in software development.In our investigation of the historical patterns of flaky tests in Chrome, we identified that 40% of flaky tests remain unresolved, while 38% are typically addressed within the initial 15 days of introduction. Subsequently, we developed a predictive model focused on identifying tests with quicker resolutions. Our model demonstrated a precision of 73% and a Matthews Correlation Coefficient (MCC) approaching 0.39 in forecasting the lifespan class of flaky tests.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {5–13},
numpages = {9},
keywords = {flaky tests, lifetime prediction, software testing, chrome, machine learning},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1145/3597503.3639122,
author = {Li, Haodong and Xu, Guosheng and Wang, Liu and Xiao, Xusheng and Luo, Xiapu and Xu, Guoai and Wang, Haoyu},
title = {MalCertain: Enhancing Deep Neural Network Based Android Malware Detection by Tackling Prediction Uncertainty},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639122},
doi = {10.1145/3597503.3639122},
abstract = {The long-lasting Android malware threat has attracted significant research efforts in malware detection. In particular, by modeling malware detection as a classification problem, machine learning based approaches, especially deep neural network (DNN) based approaches, are increasingly being used for Android malware detection and have achieved significant improvements over other detection approaches such as signature-based approaches. However, as Android malware evolve rapidly and the presence of adversarial samples, DNN models trained on early constructed samples often yield poor decisions when used to detect newly emerging samples. Fundamentally, this phenomenon can be summarized as the uncertainly in the data (noise or randomness) and the weakness in the training process (insufficient training data). Overlooking these uncertainties poses risks in the model predictions. In this paper, we take the first step to estimate the prediction uncertainty of DNN models in malware detection and leverage these estimates to enhance Android malware detection techniques. Specifically, besides training a DNN model to predict malware, we employ several uncertainty estimation methods to train a Correction Model that determines whether a sample is correctly or incorrectly predicted by the DNN model. We then leverage the estimated uncertainty output by the Correction Model to correct the prediction results, improving the accuracy of the DNN model. Experimental results show that our proposed MalCertain effectively improves the accuracy of the underlying DNN models for Android malware detection by around 21% and significantly improves the detection effectiveness of adversarial Android malware samples by up to 94.38%. Our research sheds light on the promising direction that leverages prediction uncertainty to improve prediction-based software engineering tasks.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {150},
numpages = {13},
keywords = {Android malware detection, uncertainty, DNN},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00096,
author = {Bhuiyan, Masudul Hasan Masud and Parthasarathy, Adithya Srinivas and Vasilakis, Nikos and Pradel, Michael and Staicu, Cristian-Alexandru},
title = {SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00096},
doi = {10.1109/ICSE48619.2023.00096},
abstract = {Npm is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js enables various applications, of which we explore three in this paper: (i) crosschecking SecBench.js against public security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1059–1070},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00020,
author = {Tian, Zhen and Yang, Yilong and Cheng, Sheng},
title = {RM2DM: A Tool for Automatic Generation of OO Design Models from Requirements Models},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00020},
doi = {10.1109/ICSE-Companion58688.2023.00020},
abstract = {Enterprise information systems focus on dealing with the complex business logic of collecting, filtering, processing, and distributing data to improve productivity and service in our daily lives. The successful development of enterprise information systems is a labor-intensive activity in software engineering, and it requires sophisticated human efforts for requirements validation and system design. Our previous work RM2PT can help to achieve a validated requirements model by automatically generating prototypes from requirements models to support incremental and rapid requirements validation. In this paper, we present a tool named RM2DM to further alleviate the problem of system development by supporting automatically generating a OO (Object-Oriented) design model of enterprise information system from the validated requirements model. We evaluate the tool through four case studies. The experimental result shows that all class diagram classes and 93.8% of sequence diagram messages can be correctly generated within 10 seconds. Overall, the results were satisfactory. The proposed approach can be further extended and applied for system development in the industry.The tool can be downloaded at http://rm2pt.com/advs/rm2dm, and a demo video casting its features is at https://www.youtube.com/watch?v=lrs57CjzmU8},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {36–40},
numpages = {5},
keywords = {model transformation, requirements, design model},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644032.3644460,
author = {Giamattei, Luca and Guerriero, Antonio and Malavolta, Ivano and Mascia, Cristian and Pietrantuono, Roberto and Russo, Stefano},
title = {Identifying Performance Issues in Microservice Architectures through Causal Reasoning},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644460},
doi = {10.1145/3644032.3644460},
abstract = {Evaluating the performance of Microservices Architectures (MSA) is essential to ensure their proper functioning and meet end-user satisfaction. For MSA performance analysts, one of the most challenging tasks is to determine the cause of any deviation of relevant metrics from the specified range.We introduce CAR-PT (CAusal-Reasoning-driven Performance Testing), a model-based technique for workload generation designed for the performance testing of MSA. CAR-PT leverages causal reasoning to effectively explore the space of operational conditions, with the goal of identifying those that lead to performance issues. Preliminary results show that CAR-PT is effective in generating configurations for discovering performance issues of an MSA.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {149–153},
numpages = {5},
keywords = {causal reasoning, microservices, testing},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3643991.3644876,
author = {Garijo, Daniel and Arroyo, Miguel and Gonzalez, Esteban and Treude, Christoph and Tarocco, Nicola},
title = {Bidirectional Paper-Repository Tracing in Software Engineering},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644876},
doi = {10.1145/3643991.3644876},
abstract = {While computer science papers frequently include their associated code repositories, establishing a clear link between papers and their corresponding implementations may be challenging due to the number of code repositories used in research publications. In this paper we describe a lightweight method for effectively identifying bidirectional links between papers and repositories from both LaTeX and PDF sources. We have used our approach to analyze more than 14000 PDF and Latex files in the Software Engineering category of Arxiv, generating a dataset of more than 1400 paper-code implementations and assessing current citation practices on it.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {642–646},
numpages = {5},
keywords = {research software, article analysis, software citation, open science},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/NSE66660.2025.00008,
author = {Herbold, Steffen and Knieke, Christoph and Rausch, Andreas and Schindler, Christian},
title = {Neurosymbolic Architectural Reasoning: Towards Formal Analysis through Neural Software Architecture Inference},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NSE66660.2025.00008},
doi = {10.1109/NSE66660.2025.00008},
abstract = {Formal analysis to ensure adherence of software to defined architectural constraints is not yet broadly used within software development, due to the effort involved in defining formal architecture models. Within this paper, we outline neural architecture inference to solve the problem of having a formal architecture definition for subsequent symbolic reasoning over these architectures, enabling neurosymbolic architectural reasoning. We discuss how this approach works in general and outline a research agenda based on six general research question that need to be addressed, to achieve this vision.},
booktitle = {2025 IEEE/ACM 1st International Workshop on Neuro-Symbolic Software Engineering (NSE)},
pages = {5–10},
numpages = {6},
location = {Ottawa, ON, Canada}
}

@inproceedings{10.1109/ICSE48619.2023.00050,
author = {Tan, Zeya and Song, Wei},
title = {PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00050},
doi = {10.1109/ICSE48619.2023.00050},
abstract = {Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {473–485},
numpages = {13},
keywords = {empirical study, taint analysis, violation detection, third-party entities, privacy policy, android app},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643916.3644419,
author = {Zhao, Yijun and Yu, Lingjing and Sun, Yong and Liu, Qingyun and Luo, Bo},
title = {No Source Code? No Problem! Demystifying and Detecting Mask Apps in iOS},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644419},
doi = {10.1145/3643916.3644419},
abstract = {The rise of malicious mobile applications poses a significant threat to users and app stores. While iOS apps have generally been considered more secure due to strict review processes and limited distribution avenues, developers have found ways to evade scrutiny by disguising malicious apps as benign "Mask Apps". Mask Apps activate hidden functionalities after the user downloads or with a trigger event. The malicious and potentially illegal hidden function poses significant risks, including privacy breaches, security vulnerabilities, and harm to legitimate businesses. However, existing defenses are ineffective against Mask Apps developed in web or hybrid models. In this paper, we propose Mask-Catcher, an automated approach that uses four filtering mechanisms to detect Mask Apps. Mask-Catcher leverages inconsistencies between app descriptions and user reviews, inter-app recommendation relationships, and code similarities to discover and identify Mask Apps. Experimental results show that Mask-Catcher achieves high recall and precision when applied to real-world datasets from the Apple App Store.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {358–369},
numpages = {12},
keywords = {mobile app security, Mask Apps, automated detection},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@proceedings{10.1145/3644033,
title = {FormaliSE '24: Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Historically, formal methods academic research and practical software development have had limited mutual interactions—except possibly in specialized domains such as safety-critical software. In recent times, the outlook has considerably improved: on the one hand, formal methods research has delivered more flexible techniques and tools that can support various aspects of the software development process—from user requirements elicitation, to design, implementation, verification and validation, as well as the creation of documentation. On the other hand, software engineering has developed a growing interest in rigorous techniques applied at scale.This evolution, and the desire to further improve it, motivated the creation of FormaliSE: a well-established annual conference whose main goal is to promote work at the intersection of the formal methods and software engineering communities, providing a venue to exchange ideas, experiences, techniques, and results. The collaboration between these two communities can be mutually beneficial by fostering the creation of formal methods that are practically useful and by helping develop higher-quality software.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3643796.3648451,
author = {Zharov, Yaroslav and Khudyakov, Yury and Fedotova, Evgeniia and Grigorenko, Evgeny and Bogomolov, Egor},
title = {Tool-augmented LLMs as a Universal Interface for IDEs},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648451},
doi = {10.1145/3643796.3648451},
abstract = {Modern-day Integrated Development Environments (IDEs) have come a long way from the early text editing utilities to the complex programs encompassing thousands of functions to help developers. However, with the increasing number of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated software with a steep learning curve. The rise of the Large Language Models (LLMs) capable of both natural language dialogue and code generation leads to a discourse on the obsolescence of the concept of IDE. In this work, we offer a view on the place of the LLMs in the IDEs as the universal interface wrapping the IDE facilities. We envision a model that is able to perform complex actions involving multiple IDE features upon user command, stripping the user experience of the tedious work involved in searching through options and actions. For the practical part of the work, we engage with the works exploring the ability of LLMs to call for external tools to expedite a given task execution. We showcase a proof-of-concept of such a tool.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {40–42},
numpages = {3},
keywords = {IDE, LLM, ToolFormer},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643991.3644936,
author = {Salma, Sabiha and Mansur, S M Hasan and Zhang, Yule and Moran, Kevin},
title = {GuiEvo: Automated Evolution of Mobile App UIs},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644936},
doi = {10.1145/3643991.3644936},
abstract = {With the increasing use of mobile applications in today's digital world, touch-based graphical user interfaces (GUIs) have become a crucial component of modern software by which end-users carry out computing tasks. As such, the tools involved in creating these GUIs are of fundamental importance. Due to the continuous pressure for frequent releases of mobile apps to keep pace with platform and device updates, the practice of evolving app GUIs is central to mobile app maintenance. Currently, developers manually introduce GUI changes to their apps as they evolve in a time-consuming process that involves creating mock-ups of updated GUIs and then implementing the changes stipulated by the mock-up.To help ease the burden of implementing GUI changes, and to help free mobile app developers to focus on fixing bugs or adding features, this paper introduces an automated approach for GUI evolution, called GuiEvo. This approach aims to assist developers in the process of GUI evolution by detecting changes in GUIs between existing releases and proposed mock-ups using computer vision techniques, and automatically generating updated GUI metadata for the new release. We evaluate our approach's performance based on accuracy, precision, recall, and F1-score in detecting the GUI changes, and tree edit distance to measure the correctness of generated UI hierarchies. Our evaluation demonstrates that GuiEvo can detect GUI changes with over 85% accuracy, and the generated GUI hierarchies closely match the expected structure with an average tree edit distance of 5.9. This work points toward the promise of automated tool support for assisting in the evolution of GUIs.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {335–347},
numpages = {13},
keywords = {mobile apps, UI analysis, UI generation, UI maintenance},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00148,
author = {Lee, Cheryl and Yang, Tianyi and Chen, Zhuangbin and Su, Yuxin and Yang, Yongqiang and Lyu, Michael R.},
title = {Heterogeneous Anomaly Detection for Software Systems via Semi-Supervised Cross-Modal Attention},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00148},
doi = {10.1109/ICSE48619.2023.00148},
abstract = {Prompt and accurate detection of system anomalies is essential to ensure the reliability of software systems. Unlike manual efforts that exploit all available run-time information, existing approaches usually leverage only a single type of monitoring data (often logs or metrics) or fail to make effective use of the joint information among different types of data. Consequently, many false predictions occur. To better understand the manifestations of system anomalies, we conduct a systematical study on a large amount of heterogeneous data, i.e., logs and metrics. Our study demonstrates that logs and metrics can manifest system anomalies collaboratively and complementarily, and neither of them only is sufficient. Thus, integrating heterogeneous data can help recover the complete picture of a system's health status. In this context, we propose Hades, the first end-to-end semi-supervised approach to effectively identify system anomalies based on heterogeneous data. Our approach employs a hierarchical architecture to learn a global representation of the system status by fusing log semantics and metric patterns. It captures discriminative features and meaningful interactions from heterogeneous data via a cross-modal attention module, trained in a semi-supervised manner. We evaluate Hades extensively on large-scale simulated data and datasets from Huawei Cloud. The experimental results present the effectiveness of our model in detecting system anomalies. We also release the code and the annotated dataset for replication and future research.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1724–1736},
numpages = {13},
keywords = {cross-modal learning, anomaly detection, software system},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00097,
author = {Sangaroonsilp, Pattaraporn and Dam, Hoa Khanh and Ghose, Aditya},
title = {On Privacy Weaknesses and Vulnerabilities in Software Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00097},
doi = {10.1109/ICSE48619.2023.00097},
abstract = {In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1071–1083},
numpages = {13},
keywords = {software, CVE, CWE, threats, vulnerabilities, privacy},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3641224,
author = {Babikian, Aren A.},
title = {Refining Abstract Specifications into Dangerous Traffic Scenarios},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641224},
doi = {10.1145/3639478.3641224},
abstract = {Safety assurance of autonomous vehicles (AVs) is particularly challenging when considering the infinite number of scenarios an AV may encounter. As such, existing scenario generation approaches optimize search to derive dangerous refinements of a (same) abstract scenario given as input. In this paper, we propose a scenario generation approach that derives dangerous (collision-inducing) concrete scenarios from arbitrary abstract scenarios (under reasonable assumptions). As added novelty, our approach allows to compare the level of danger offered by different abstract scenarios. We evaluate the collision avoidance capacity of the Transfuser AV controller by generating, then simulating, collision-inducing 2-actor scenarios at a road junction. Results show that distinctions at higher abstraction levels yield measurable differences in simulation.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {456–458},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639163,
author = {Yan, Yanfu and Cooper, Nathan and Chaparro, Oscar and Moran, Kevin and Poshyvanyk, Denys},
title = {Semantic GUI Scene Learning and Video Alignment for Detecting Duplicate Video-based Bug Reports},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639163},
doi = {10.1145/3597503.3639163},
abstract = {Video-based bug reports are increasingly being used to document bugs for programs centered around a graphical user interface (GUI). However, developing automated techniques to manage video-based reports is challenging as it requires identifying and understanding often nuanced visual patterns that capture key information about a reported bug. In this paper, we aim to overcome these challenges by advancing the bug report management task of duplicate detection for video-based reports. To this end, we introduce a new approach, called Janus, that adapts the scene-learning capabilities of vision transformers to capture subtle visual and textual patterns that manifest on app UI screens --- which is key to differentiating between similar screens for accurate duplicate report detection. Janus also makes use of a video alignment technique capable of adaptive weighting of video frames to account for typical bug manifestation patterns. In a comprehensive evaluation on a benchmark containing 7,290 duplicate detection tasks derived from 270 video-based bug reports from 90 Android app bugs, the best configuration of our approach achieves an overall mRR/mAP of 89.8%/84.7%, and for the large majority of duplicate detection tasks, outperforms prior work by ≈9% to a statistically significant degree. Finally, we qualitatively illustrate how the scene-learning capabilities provided by Janus benefits its performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {232},
numpages = {13},
keywords = {bug reporting, GUI learning, duplicate video retrieval},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00082,
author = {Huo, Yintong and Su, Yuxin and Lee, Cheryl and Lyu, Michael R.},
title = {SemParser: A Semantic Parser for Log Analytics},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00082},
doi = {10.1109/ICSE48619.2023.00082},
abstract = {Logs, being run-time information automatically generated by software, record system events and activities with their timestamps. Before obtaining more insights into the run-time status of the software, a fundamental step of log analysis, called log parsing, is employed to extract structured templates and parameters from the semi-structured raw log messages. However, current log parsers are all syntax-based and regard each message as a character string, ignoring the semantic information included in parameters and templates.Thus, we propose the first semantic-based parser SemParser to unlock the critical bottleneck of mining semantics from log messages. It contains two steps, an end-to-end semantics miner and a joint parser. Specifically, the first step aims to identify explicit semantics inside a single log, and the second step is responsible for jointly inferring implicit semantics and computing structural outputs according to the contextual knowledge base of the logs. To analyze the effectiveness of our semantic parser, we first demonstrate that it can derive rich semantics from log messages collected from six widely-applied systems with an average F1 score of 0.985. Then, we conduct two representative downstream tasks, showing that current downstream models improve their performance with appropriately extracted semantics by 1.2%-11.7% and 8.65% on two anomaly detection datasets and a failure identification dataset, respectively. We believe these findings provide insights into semantically understanding log messages for the log analysis community.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {881–893},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00064,
author = {Tan, Xin and Chen, Yiran and Wu, Haohua and Zhou, Minghui and Zhang, Li},
title = {Is it Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00064},
doi = {10.1109/ICSE48619.2023.00064},
abstract = {Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and successfully.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {653–664},
numpages = {12},
keywords = {good first issue, open source, mentoring, newcomer},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00100,
author = {Laudato, Gennaro and Scalabrino, Simone and Novielli, Nicole and Lanubile, Filippo and Oliveto, Rocco},
title = {Predicting Bugs by Monitoring Developers during Task Execution},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00100},
doi = {10.1109/ICSE48619.2023.00100},
abstract = {Knowing which parts of the source code will be defective can allow practitioners to better allocate testing resources. For this reason, many approaches have been proposed to achieve this goal. Most state-of-the-art predictive models rely on product and process metrics, i.e., they predict the defectiveness of a component by considering what developers did. However, there is still limited evidence of the benefits that can be achieved in this context by monitoring how developers complete a development task. In this paper, we present an empirical study in which we aim at understanding whether measuring human aspects on developers while they write code can help predict the introduction of defects. First, we introduce a new developer-based model which relies on behavioral, psychophysical, and control factors that can be measured during the execution of development tasks. Then, we run a controlled experiment involving 20 software developers to understand if our developer-based model is able to predict the introduction of bugs. Our results show that a developer-based model is able to achieve a similar accuracy compared to a state-of-the-art code-based model, i.e., a model that uses only features measured from the source code. We also observed that by combining the models it is possible to obtain the best results (~84% accuracy).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1110–1122},
numpages = {13},
keywords = {empirical software engineering, biometric sensors, human aspects of software engineering, bug prediction},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00154,
author = {Xia, Chunqiu Steven and Dutta, Saikat and Misailovic, Sasa and Marinov, Darko and Zhang, Lingming},
title = {Balancing Effectiveness and Flakiness of Non-Deterministic Machine Learning Tests},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00154},
doi = {10.1109/ICSE48619.2023.00154},
abstract = {Testing Machine Learning (ML) projects is challenging due to inherent non-determinism of various ML algorithms and the lack of reliable ways to compute reference results. Developers typically rely on their intuition when writing tests to check whether ML algorithms produce accurate results. However, this approach leads to conservative choices in selecting assertion bounds for comparing actual and expected results in test assertions. Because developers want to avoid false positive failures in tests, they often set the bounds to be too loose, potentially leading to missing critical bugs.We present FASER - the first systematic approach for balancing the trade-off between the fault-detection effectiveness and flakiness of non-deterministic tests by computing optimal assertion bounds. FASER frames this trade-off as an optimization problem between these competing objectives by varying the assertion bound. FASER leverages 1) statistical methods to estimate the flakiness rate, and 2) mutation testing to estimate the fault-detection effectiveness. We evaluate FASER on 87 non-deterministic tests collected from 22 popular ML projects. FASER finds that 23 out of 87 studied tests have conservative bounds and proposes tighter assertion bounds that maximizes the fault-detection effectiveness of the tests while limiting flakiness. We have sent 19 pull requests to developers, each fixing one test, out of which 14 pull requests have already been accepted.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1801–1813},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00172,
author = {Chen, Junjie and Suo, Chenyao and Jiang, Jiajun and Chen, Peiqi and Li, Xingjian},
title = {Compiler Test-Program Generation via Memoized Configuration Search},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00172},
doi = {10.1109/ICSE48619.2023.00172},
abstract = {To ensure compilers' quality, compiler testing has received more and more attention, and test-program generation is the core task. In recent years, some approaches have been proposed to explore test configurations for generating more effective test programs, but they either are restricted by historical bugs or suffer from the cost-effectiveness issue. Here, we propose a novel test-program generation approach (called MCS) to further improving the performance of compiler testing. MCS conducts memoized search via multi-agent reinforcement learning (RL) for guiding the construction of effective test configurations based on the memoization for the explored test configurations during the on-the-fly compiler-testing process. During the process, the elaborate coordination among configuration options can be also well learned by multi-agent RL, which is required for generating bug-triggering test programs. Specifically, MCS considers the diversity among test configurations to efficiently explore the input space and the testing results under each explored configuration to learn which portions of space are more bug-triggering. Our extensive experiments on GCC and LLVM demonstrate the performance of MCS, significantly outperforming the state-of-the-art test-program generation approaches in bug detection. Also, MCS detects 16 new bugs on the latest trunk revisions of GCC and LLVM, and all of them have been confirmed or fixed by developers. MCS has been deployed by a global IT company (i.e., Huawei) for testing their in-house compiler, and detects 10 new bugs (covering all the 5 bugs detected by the compared approaches), all of which have been confirmed.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2035–2047},
numpages = {13},
keywords = {configuration, reinforcement learning, test program generation, compiler testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00109,
author = {Motwani, Manish and Brun, Yuriy},
title = {Better Automatic Program Repair by Using Bug Reports and Tests Together},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00109},
doi = {10.1109/ICSE48619.2023.00109},
abstract = {Automated program repair is already deployed in industry, but concerns remain about repair quality. Recent research has shown that one of the main reasons repair tools produce incorrect (but seemingly correct) patches is imperfect fault localization (FL). This paper demonstrates that combining information from natural-language bug reports and test executions when localizing faults can have a significant positive impact on repair quality. For example, existing repair tools with such FL are able to correctly repair 7 defects in the Defects4J benchmark that no prior tools have repaired correctly.We develop, Blues, the first information-retrieval-based, statement-level FL technique that requires no training data. We further develop RAFL, the first unsupervised method for combining multiple FL techniques, which outperforms a supervised method. Using RAFL, we create SBIR by combining Blues with a spectrum-based (SBFL) technique. Evaluated on 815 real-world defects, SBIR consistently ranks buggy statements higher than its underlying techniques.We then modify three state-of-the-art repair tools, Arja, SequenceR, and SimFix, to use SBIR, SBFL, and Blues as their internal FL. We evaluate the quality of the produced patches on 689 real-world defects. Arja and SequenceR significantly benefit from SBIR: Arja using SBIR correctly repairs 28 defects, but only 21 using SBFL, and only 15 using Blues; SequenceR using SBIR correctly repairs 12 defects, but only 10 using SBFL, and only 4 using Blues. SimFix, (which has internal mechanisms to overcome poor FL), correctly repairs 30 defects using SBIR and SBFL, but only 13 using Blues. Our work is the first investigation of simultaneously using multiple software artifacts for automated program repair, and our promising findings suggest future research in this directions is likely to be fruitful.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1225–1237},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00195,
author = {Zhu, Hao-Nan and Rubio-Gonz\'{a}lez, Cindy},
title = {On the Reproducibility of Software Defect Datasets},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00195},
doi = {10.1109/ICSE48619.2023.00195},
abstract = {Software defect datasets are crucial to facilitating the evaluation and comparison of techniques in fields such as fault localization, test generation, and automated program repair. However, the reproducibility of software defect artifacts is not immune to breakage. In this paper, we conduct a study on the reproducibility of software defect artifacts. First, we study five state-of-the-art Java defect datasets. Despite the multiple strategies applied by dataset maintainers to ensure reproducibility, all datasets are prone to breakages. Second, we conduct a case study in which we systematically test the reproducibility of 1,795 software artifacts during a 13-month period. We find that 62.6% of the artifacts break at least once, and 15.3% artifacts break multiple times. We manually investigate the root causes of breakages and handcraft 10 patches, which are automatically applied to 1,055 distinct artifacts in 2,948 fixes. Based on the nature of the root causes, we propose automated dependency caching and artifact isolation to prevent further breakage. In particular, we show that isolating artifacts to eliminate external dependencies increases reproducibility to 95% or higher, which is on par with the level of reproducibility exhibited by the most reliable manually curated dataset.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2324–2335},
numpages = {12},
keywords = {software quality, software maintenance, software defects, software reproducibility},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3641822.3641865,
author = {Tanzil, Minaoar Hossain and Uddin, Gias and Barcomb, Ann},
title = {"How do people decide?": A Model for Software Library Selection},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641865},
doi = {10.1145/3641822.3641865},
abstract = {Modern-day software development is often facilitated by the reuse of third-party software libraries. Despite the significant effort to understand the factors contributing to library selection, it is relatively unknown how the libraries are selected and what tools are still needed to support the selection process. Using Straussian grounded theory, we conducted and analyzed the interviews of 24 professionals across the world and derived a model of library selection process which is governed by six selection patterns (i.e., rules). The model draws from marketing theory and lays the groundwork for the development of a library selection tool which captures the technical and non-technical aspects developers consider.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {third-party, software library, adoption process, decision pattern, open-source software, grounded theory, interview study, qualitative research},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00177,
author = {Weber, Max and Kaltenecker, Christian and Sattler, Florian and Apel, Sven and Siegmund, Norbert},
title = {Twins or False Friends? A Study on Energy Consumption and Performance of Configurable Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00177},
doi = {10.1109/ICSE48619.2023.00177},
abstract = {Reducing energy consumption of software is an increasingly important objective, and there has been extensive research for data centers, smartphones, and embedded systems. However, when it comes to software, we lack working tools and methods to directly reduce energy consumption. For performance, we can resort to configuration options for tuning response time or throughput of a software system. For energy, it is still unclear whether the underlying assumption that runtime performance correlates with energy consumption holds, especially when it comes to optimization via configuration. To evaluate whether and to what extent this assumption is valid for configurable software systems, we conducted the largest empirical study of this kind to date. First, we searched the literature for reports on whether and why runtime performance correlates with energy consumption. We obtained a mixed, even contradicting picture from positive to negative correlation, and that configurability has not been considered yet as a factor for this variance. Second, we measured and analyzed both the runtime performance and energy consumption of 14 real-world software systems. We found that, in many cases, it depends on the software system's configuration whether runtime performance and energy consumption correlate and that, typically, only few configuration options influence the degree of correlation. A fine-grained analysis at the function level revealed that only few functions are relevant to obtain an accurate proxy for energy consumption and that, knowing them, allows one to infer individual transfer factors between runtime performance and energy consumption.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2098–2110},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639094,
author = {Alecci, Marco and Samhi, Jordan and Bissyande, Tegawende F. and Klein, Jacques},
title = {Revisiting Android App Categorization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639094},
doi = {10.1145/3597503.3639094},
abstract = {Numerous tools rely on automatic categorization of Android apps as part of their methodology. However, incorrect categorization can lead to inaccurate outcomes, such as a malware detector wrongly flagging a benign app as malicious. One such example is the SlideIT Free Keyboard app, which has over 500 000 downloads on Google Play. Despite being a "Keyboard" app, it is often wrongly categorized alongside "Language" apps due to the app's description focusing heavily on language support, resulting in incorrect analysis outcomes, including mislabeling it as a potential malware when it is actually a benign app. Hence, there is a need to improve the categorization of Android apps to benefit all the tools relying on it.In this paper, we present a comprehensive evaluation of existing Android app categorization approaches using our new ground-truth dataset. Our evaluation demonstrates the notable superiority of approaches that utilize app descriptions over those solely relying on data extracted from the APK file, while also leaving space for potential improvement in the former category. Thus, we propose two innovative approaches that effectively outperform the performance of existing methods in both description-based and APK-based methodologies. Finally, by employing our novel description-based approach, we have successfully demonstrated that adopting a higher-performing categorization method can significantly benefit tools reliant on app categorization, leading to an improvement in their overall performance. This highlights the significance of developing advanced and efficient app categorization methodologies for improved results in software engineering tasks.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {204},
numpages = {12},
keywords = {android security, static analysis, app categorization},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3528227,
title = {SERP4IoT '22: Proceedings of the 4th International Workshop on Software Engineering Research and Practice for the IoT},
year = {2022},
isbn = {9781450393324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SERP4IoT begins to be recognised as an annual venue gathering researchers, industrials, and practitioners to share their vision, experience, and opinion on how to address the challenges of, find solutions for, and share experiences with the development, release, and testing of robust software systems for IoT devices.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3526072.3527532,
author = {Castellano, Ezequiel and Klikovits, Stefan and Cetinkaya, Ahmet and Arcaini, Paolo},
title = {FreneticV at the SBST 2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527532},
doi = {10.1145/3526072.3527532},
abstract = {FreneticV is a search-based testing tool based on an evolutionary approach that generates roads where an automated driving agent possibly fails the lane-keeping task. It uses a curvature-based road representation and, compared to its predecessor Frenetic, considers the validity of the generated roads. In particular, it tries to avoid generating roads with overly sharp turns, detects self-intersecting roads, and can rotate and relocate roads to fit them in a given map.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {47–48},
numpages = {2},
keywords = {search-based testing, frenet frame, autonomous driving, FreneticV},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3597503.3639228,
author = {He, Ziyao and Huq, Syed Fatiul and Malek, Sam},
title = {"I tend to view ads almost like a pestilence": On the Accessibility Implications of Mobile Ads for Blind Users},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639228},
doi = {10.1145/3597503.3639228},
abstract = {Ads are integral to the contemporary Android ecosystem, generating revenue for free-to-use applications. However, injected as third-party content, ads are displayed on native apps in pervasive ways that affect easy navigation. Ads can prove more disruptive for blind users, who rely on screen readers for navigating an app. While the literature has looked into either the accessibility of web advertisements or the privacy and security implications of mobile ads, a research gap on the accessibility of mobile ads remains, which we aim to bridge. We conduct an empirical study analyzing 500 ad screens in Android apps to categorize and examine the accessibility issues therein. Additionally, we conduct 15 qualitative user interviews with blind Android users to better understand the impact of those accessibility issues, how users interact with ads and their preferences. Based on our findings, we discuss the design and practical strategies for developing accessible ads.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {197},
numpages = {13},
keywords = {Android, accessibility, advertisement, screen reader},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643788.3648007,
author = {Meyer, Bertrand and Kananchuk, Viktoryia and Huang, Li},
title = {BUGFIX: towards a common language and framework for the Automatic Program Repair community},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648007},
doi = {10.1145/3643788.3648007},
abstract = {Techniques of Automatic Program Repair (APR) have the potential of thoroughly facilitating the task of producing quality software. After a promising start, however, progress in making APR practical has been hindered by the lack of a common framework to support the multiplicity of APR ideas and tools, and of target programming languages and environments. In this position paper we outline a general framework to enable the APR community to benefit from each other's advances, in particular through a standard language for describing bugs and their fixes. Such a common framework --- which is also applicable to work on fault seeding --- could be a tremendous benefit to researchers and developers of Interactive Development Environments (IDEs) who are working to make APR an effective part of the software developer's practical experience.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {9–13},
numpages = {5},
keywords = {automatic program repair, debugging, integrated development environments, software tools, program transformation, bug seeding, software quality},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643788.3648018,
author = {Mechtaev, Sergey and Tan, Shin Hwei},
title = {F1X at APR-COMP 2024},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648018},
doi = {10.1145/3643788.3648018},
abstract = {Automated program repair aims to generate patches for buggy programs, a task often hindered by the cost of test executions in large projects. F1X introduces a novel methodology relying on test-equivalence relations, defining if two programs yield indistinguishable results for a specific test. By leveraging two test-equivalence relations based on runtime values and dependencies, F1X' algorithm categorises patches into test-equivalence classes, which helps to significantly reduce the number of required test execution to generate a patch without any information loss. Experiments on real-world programs from the ManyBugs benchmark demonstrated a substantial reduction in test executions, leading to efficiency gains over the previous methods, while retaining the patch quality. The efficiency and effectiveness of F1X was further shown in APR-COMP 2024, where it received the highest score in the Functional-C track.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {56–57},
numpages = {2},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00041,
author = {Chen, Qihong and C\^{a}mara, R\'{u}ben and Campos, Jos\'{e} and Souto, Andr\'{e} and Ahmed, Iftekhar},
title = {The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00041},
doi = {10.1109/ICSE48619.2023.00041},
abstract = {Quantum Computing (QC) is a fast-growing field that has enhanced the emergence of new programming languages and frameworks. Furthermore, the increased availability of computational resources has also contributed to an influx in the development of quantum programs. Given that classical and QC are significantly different due to the intrinsic nature of quantum programs, several aspects of QC (e.g., performance, bugs) have been investigated, and novel approaches have been proposed. However, from a purely quantum perspective, maintenance, one of the major steps in a software development life-cycle, has not been considered by researchers yet. In this paper, we fill this gap and investigate the prevalence of code smells in quantum programs as an indicator of maintenance issues.We defined eight quantum-specific smells and validated them through a survey with 35 quantum developers. Since no tool specifically aims to detect quantum smells, we developed a tool called QSmell that supports the proposed quantum-specific smells. Finally, we conducted an empirical investigation to analyze the prevalence of quantum-specific smells in 15 open-source quantum programs. Our results showed that 11 programs (73.33%) contain at least one smell and, on average, a program has three smells. Furthermore, the long circuit is the most prevalent smell present in 53.33% of the programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {358–370},
numpages = {13},
keywords = {quantum-specific code smell, empirical study, quantum software engineering, quantum computing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643692.3648263,
author = {Craine, Benjamin and Rainford, Penn and Porter, Barry},
title = {Human Guidance Approaches for the Genetic Improvement of Software},
year = {2024},
isbn = {9798400705731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643692.3648263},
doi = {10.1145/3643692.3648263},
abstract = {Existing research on Genetic Improvement (GI) of source code to improve performance [10] has examined the mixed application of code synthesis and traditional GI mutation/crossover to gain higher-performing individuals that are tailored to particular deployment contexts, for examples such as hash tables or scheduling algorithms. While demonstrating successful improvements, this research presents a host of challenges [9], from search space size to fitness landscape shape, which raise questions on whether GI alone is able to present a complete solution. In this position paper we propose to augment GI processes with Human Guidance (HG) to offer a co-pilot paradigm which may overcome these challenges.},
booktitle = {Proceedings of the 13th ACM/IEEE International Workshop on Genetic Improvement},
pages = {21–22},
numpages = {2},
location = {Lisbon, Portugal},
series = {GI '24}
}

@inproceedings{10.1145/3643658.3643923,
author = {Kutzias, Damian and Von Mammen, Sebastian},
title = {Handling Interfaces for the Procedural Generation of Complete Buildings},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643658.3643923},
doi = {10.1145/3643658.3643923},
abstract = {This paper presents the challenge of interface handling for different building constituents in the context of PGB (Procedural Generation of Buildings). Alongside this challenge, the current state of research about interfaces and their handling in current implementations of (almost) complete buildings is presented. Afterwards, different means of integration are discussed including their expected impacts, implementation efforts, and control mechanisms. Handling the integration challenge for PGB can be understood as a meta challenge with several design choices, making several decisions for controlling complexity both computationally and regarding the implementation effort. One set of such decisions is presented, optimised to cover many common cases of buildings while utilising currently existing generators and their limited compatibility to procedurally generate complete buildings with manageable efforts.},
booktitle = {Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
pages = {9–14},
numpages = {6},
keywords = {procedural buildings, procedural content generation, PCG, procedural generation of buildings, PGB},
location = {Lisbon, Portugal},
series = {GAS '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00016,
author = {Tuli, Shreshth and Bojarczuk, Kinga and Gucevska, Natalija and Harman, Mark and Wang, Xiao-Yu and Wright, Graham},
title = {Simulation-Driven Automated End-to-End Test and Oracle Inference},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00016},
doi = {10.1109/ICSE-SEIP58684.2023.00016},
abstract = {This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {122–133},
numpages = {12},
keywords = {integrity testing, safety testing, test automation, automated oracle inference, oracle problem, automated test design},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3639478.3639814,
author = {Speth, Sandro},
title = {Architecture-Based Cross-Component Issue Management and Propagation Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639814},
doi = {10.1145/3639478.3639814},
abstract = {This paper addresses the challenge of issue management in complex, component-based software architectures. In these systems, issues in one component often propagate across the architecture along the call chains. Yet, traditional issue management systems (IMSs) are limited to the boundaries of a single component and lack mechanisms for managing issues concerning their architectural dependencies. We present Gropius, a novel method that enhances issue management by integrating issues in an architecture graph. Gropius allows semantically linking issues across different components, synchronizes changes with underlying IMSs like GitHub, and allows modeling the architecture ontologically by defining the components' semantics at runtime. We explore whether combining issue and architecture management improves the development of component-based architectures regarding issue management. We hypothesize that this method will improve the efficiency and effectiveness of identifying and resolving cross-component issues, maintaining a comprehensive view of the application's state.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {145–149},
numpages = {5},
keywords = {issue management, issue propagation analysis, component-based software architecture, model-based analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3640032,
author = {Urrico, Michael Ferdinando and Clerissi, Diego and Mariani, Leonardo},
title = {MutaBot: A Mutation Testing Approach for Chatbots},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640032},
doi = {10.1145/3639478.3640032},
abstract = {Mutation testing is a technique aimed at assessing the effectiveness of test suites by seeding artificial faults into programs. Although available for many platforms and languages, no mutation testing tool is currently available for conversational chatbots, which represent an increasingly popular solution to design systems that can interact with users through a natural language interface. Note that since conversations must be explicitly engineered by the developers of conversational chatbots, these systems are exposed to specific types of faults not supported by existing mutation testing tools.In this paper, we present MutaBot, a mutation testing tool for conversational chatbots. MutaBot addresses mutations at multiple levels, including conversational flows, intents, and contexts. We designed the tool to potentially target multiple platforms, while we implemented initial support for Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots and test cases generated with Botium, revealing weaknesses in the test suites.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {79–83},
numpages = {5},
keywords = {chatbot testing, mutation testing, botium, dialogflow},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00086,
author = {Lu, Chengjie},
title = {Test Scenario Generation for Autonomous Driving Systems with Reinforcement Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00086},
doi = {10.1109/ICSE-Companion58688.2023.00086},
abstract = {We have seen rapid development of autonomous driving systems (ADSs) in recent years. These systems place high requirements on safety and reliability for their mass adoption, and ADS testing is one of the crucial approaches to ensure the success of ADSs. To this end, this paper presents RLTester, a novel ADS testing approach, which adopts reinforcement learning (RL) to learn critical environment configurations (i.e., test scenarios) of the operating environment of ADSs that could reveal their unsafe behaviors. To generate diverse and critical test scenarios, we defined 142 environment configuration actions, and adopted the Time-To-Collision metric to construct the reward function. Our evaluation shows that RLTester discovered a total of 256 collisions, of which 192 are unique collisions, and took on average 11.59 seconds for each collision. Further, RLTester is effective in generating more diverse test scenarios compared to a state-of-the art approach, DeepCollision.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {317–319},
numpages = {3},
keywords = {reinforcement learning, critical scenario, autonomous driving system testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639229,
author = {Chiou, Paul T. and Winn, Robert and Alotaibi, Ali S. and Halfond, William G. J.},
title = {Automatically Detecting Reflow Accessibility Issues in Responsive Web Pages},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639229},
doi = {10.1145/3597503.3639229},
abstract = {Many web applications today use responsive design to adjust the view of web pages to match the screen size of end users. People with disabilities often use an alternative view either due to zooming on a desktop device to enlarge text or viewing within a smaller viewport when using assistive technologies. When web pages are not implemented to correctly adjust the page's content across different screen sizes, it can lead to both a loss of content and functionalities between the different versions. Recent studies show that these reflow accessibility issues are among the most prevalent modern web accessibility issues. In this paper, we present a novel automated technique to automatically detect reflow accessibility issues in web pages for keyboard users. The evaluation of our approach on real-world web pages demonstrated its effectiveness in detecting reflow accessibility issues, outperforming state-of-the-art techniques.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {147},
numpages = {13},
keywords = {web accessibility, WCAG, software testing, response web design, reflow, keyboard accessibility, inclusive design},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00049,
author = {Kurian, Elson and Braione, Pietro and Briola, Daniela and D'Avino, Dario and Modonato, Matteo and Denaro, Giovanni},
title = {Automated Test Case Generation for Safety-Critical Software in Scade},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00049},
doi = {10.1109/ICSE-SEIP58684.2023.00049},
abstract = {Software systems for automating safety-critical tasks in application domains like, for example, avionics, railways, automotive, industry 4.0 and healthcare, must be highly reliable. In this paper, we focus on safety-critical software written in Scade, a model-based programming language largely adopted in industry, and we specifically draw on our own experience in a joint industry-university project aimed at developing safety-critical Scade programs for the railways domain. We investigate automated test case generation for Scade programs. We leverage on state-of-the-art test generators based on either symbolic execution, bounded model checking or search-based testing, in order to define an original toolchain for generating test cases for Scade programs. We rely on the toolchain to explore the absolute and relative effectiveness of those mainstream test generation approaches on a benchmark of 37 Scade programs developed as part of an on-board signaling unit for high-speed railway systems.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {483–494},
numpages = {12},
keywords = {search-based testing, bounded model checking, symbolic execution, automated test generation, safety-critical scade programs},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00059,
author = {Tu, Haoxin},
title = {Boosting Symbolic Execution for Heap-Based Vulnerability Detection and Exploit Generation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00059},
doi = {10.1109/ICSE-Companion58688.2023.00059},
abstract = {Heap-based vulnerabilities such as buffer overflow and use after free are severe flaws in various software systems. Detecting heap-based vulnerabilities and demonstrating their severity via generating exploits for them are of critical importance. Existing symbolic execution-based approaches have shown their potential in the above tasks. However, they still have some fundamental limitations in path exploration, memory modeling, and environment modeling, which significantly impede existing symbolic execution engines from efficiently and effectively detecting and exploiting heap-based vulnerabilities. The objective of this thesis is to design and implement a boosted symbolic execution engine named HeapX to facilitate the automatic detection and exploitation of heap-based vulnerabilities. Specifically, a new path exploration strategy, a new memory model, and a new environment modeling solution are expected to be designed in HeapX, so that the new boosted symbolic execution engine can detect heap-based vulnerabilities and generate working exploits for them more efficiently and effectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {218–220},
numpages = {3},
keywords = {symbolic execution, automatic exploit generation, vulnerability detection, software reliability, software security},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00010,
author = {Pink, Sarah},
title = {Future Software for Life in Trusted Futures},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00010},
doi = {10.1109/ICSE48619.2023.00010},
abstract = {How will people, other species, software and hardware live together in as yet unknown futures? How can we work towards trusted and safe futures where human values and the environment are supported by emerging technologies? Research demonstrates that human values and everyday life priorities, ethics, routines and activities will shape our possible futures. I will draw on ethnographic research to outline how people anticipate and imagine everyday life futures with emerging technologies in their homes and neighbourhoods, and how technology workers envisage futures in their professional lives. If, as social science research shows, technologies cannot solve human and societal problems, what roles should they play in future life? What are the implications for future software? What values should underpin its design? Where should it be developed? By and in collaboration with whom? What role can software play in generating the circumstances for trusted futures?},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1},
numpages = {1},
keywords = {software engineering and society},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00216,
author = {Huai, Yuqi and Chen, Yuntianyi and Almanee, Sumaya and Ngo, Tuan and Liao, Xiang and Wan, Ziwen and Chen, Qi Alfred and Garcia, Joshua},
title = {Doppelg\"{a}nger Test Generation for Revealing Bugs in Autonomous Driving Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00216},
doi = {10.1109/ICSE48619.2023.00216},
abstract = {Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2591–2603},
numpages = {13},
keywords = {search-based software testing, autonomous driving systems, cyber-physical systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639581,
author = {Goldstein, Harrison and Cutler, Joseph W. and Dickstein, Daniel and Pierce, Benjamin C. and Head, Andrew},
title = {Property-Based Testing in Practice},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639581},
doi = {10.1145/3597503.3639581},
abstract = {Property-based testing (PBT) is a testing methodology where users write executable formal specifications of software components and an automated harness checks these specifications against many automatically generated inputs. From its roots in the QuickCheck library in Haskell, PBT has made significant inroads in mainstream languages and industrial practice at companies such as Amazon, Volvo, and Stripe. As PBT extends its reach, it is important to understand how developers are using it in practice, where they see its strengths and weaknesses, and what innovations are needed to make it more effective.We address these questions using data from 30 in-depth interviews with experienced users of PBT at Jane Street, a financial technology company making heavy and sophisticated use of PBT. These interviews provide empirical evidence that PBT's main strengths lie in testing complex code and in increasing confidence beyond what is available through conventional testing methodologies, and, moreover, that most uses fall into a relatively small number of high-leverage idioms. Its main weaknesses, on the other hand, lie in the relative complexity of writing properties and random data generators and in the difficulty of evaluating their effectiveness. From these observations, we identify a number of potentially high-impact areas for future exploration, including performance improvements, differential testing, additional high-leverage testing scenarios, better techniques for generating random input data, test-case reduction, and methods for evaluating the effectiveness of tests.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {187},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3643657,
title = {SATrends '24: Proceedings of the 1st International Workshop on New Trends in Software Architecture},
year = {2024},
isbn = {9798400705601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In this workshop, we aim at establishing a forum to collect practitioners' experiences and/or researchers' observations related to trends, and enable practitioners and researchers to exchange opinions, learn from each other, and progress the state of the art in the adoption of new trends.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3597503.3639175,
author = {Sun, Huijia and Poskitt, Christopher M. and Sun, Yang and Sun, Jun and Chen, Yuqi},
title = {ACAV: A Framework for Automatic Causality Analysis in Autonomous Vehicle Accident Recordings},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639175},
doi = {10.1145/3597503.3639175},
abstract = {The rapid progress of autonomous vehicles (AVs) has brought the prospect of a driverless future closer than ever. Recent fatalities, however, have emphasized the importance of safety validation through large-scale testing. Multiple approaches achieve this fully automatically using high-fidelity simulators, i.e., by generating diverse driving scenarios and evaluating autonomous driving systems (ADSs) against different test oracles. While effective at finding violations, these approaches do not identify the decisions and actions that caused them---information that is critical for improving the safety of ADSs. To address this challenge, we propose ACAV, an automated framework designed to conduct causality analyses for AV accident recordings in two stages. First, we apply feature extraction schemas based on the messages exchanged between ADS modules, and use a weighted voting method to discard frames of the recording unrelated to the accident. Second, we use safety specifications to identify safety-critical frames and deduce causal events by applying CAT---our causal analysis tool---to a station-time graph. We evaluated ACAV on the Apollo ADS, finding that it can identify five distinct types of causal events in 93.64% of 110 accident recordings generated by an AV testing engine. We further evaluated ACAV on 1206 accident recordings collected from versions of Apollo injected with specific faults, finding that it can correctly identify causal events in 96.44% of the accidents triggered by prediction errors, and 85.73% of the accidents triggered by planning errors.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {102},
numpages = {13},
keywords = {autonomous driving system, test reduction, causality},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3526073,
title = {SE4RAI '22: Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
year = {2022},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SE4RAI'22 is a forum where researchers, innovators, and leading professionals from both academia and industry can discuss the state and future of software engineering for responsible AI. SE4RAI'22 also aims to bring together researchers and practitioners from diverse disciplines such as software engineering, AI and social science to help tackle the end-to-end engineering challenges in developing AI systems responsibly. We hope that SE4RAI'22 will actively encourage a growing number of researchers to join this area.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3639477.3639756,
author = {Williams, David and Callan, James and Kirbas, Serkan and Mechtaev, Sergey and Petke, Justyna and Prideaux-Ghee, Thomas and Sarro, Federica},
title = {User-Centric Deployment of Automated Program Repair at Bloomberg},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639756},
doi = {10.1145/3639477.3639756},
abstract = {Automated program repair (APR) tools have unlocked the potential for the rapid rectification of codebase issues. However, to encourage wider adoption of program repair in practice, it is necessary to address the usability concerns related to generating irrelevant or out-of-context patches. When software engineers are presented with patches they deem uninteresting or unhelpful, they are burdened with more "noise" in their workflows and become less likely to engage with APR tools in future. This paper presents a novel approach to optimally time, target, and present auto-generated patches to software engineers. To achieve this, we designed, developed, and deployed a new tool dubbed B-Assist, which leverages GitHub's Suggested Changes interface to seamlessly integrate automated suggestions into active pull requests (PRs), as opposed to creating new, potentially distracting PRs. This strategy ensures that suggestions are not only timely, but also contextually relevant and delivered to engineers most familiar with the affected code. Evaluation among Bloomberg software engineers demonstrated their preference for this approach. From our user study, B-Assist's efficacy is evident, with the acceptance rate of patch suggestions being as high as 74.56%; engineers also found the suggestions valuable, giving usefulness ratings of at least 4 out of 5 in 78.2% of cases. Further, this paper sheds light on persisting usability challenges in APR and lays the groundwork for enhancing the user experience in future APR tools.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {81–91},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@proceedings{10.1145/3528226,
title = {WETSEB '22: Proceedings of the 5th International Workshop on Emerging Trends in Software Engineering for Blockchain},
year = {2022},
isbn = {9781450393317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 5th Workshop on Emerging Trends in Software Engineering for Blockchain gathers the interests of researchers and practitioneers, from both academia and industry, as well as Ph.D. students working in the field of Blockchain technology, to investigate on and to and tackle the new challenges defined by BOSE. The Workshop's goal is to discuss the progresses made by software engineering on the research and on the practical applications of blockchain technologies and smart contracts, focusing on software engineering principles and practices adopted to deal with such new software technology, and for the technologies relying on it.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3597503.3639124,
author = {Gao, Yi and Hu, Xing and Xu, Tongtong and Xia, Xin and Lo, David and Yang, Xiaohu},
title = {MUT: Human-in-the-Loop Unit Test Migration},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639124},
doi = {10.1145/3597503.3639124},
abstract = {Test migration, which enables the reuse of test cases crafted with knowledge and creativity by testers across various platforms and programming languages, has exhibited effectiveness in mobile app testing. However, unit test migration at the source code level has not garnered adequate attention and exploration. In this paper, we propose a novel cross-language and cross-platform test migration methodology, named MUT, which consists of four modules: code mapping, test case filtering, test case translation, and test case adaptation. MUT initially calculates code mappings to establish associations between source and target projects, and identifies suitable unit tests for migration from the source project. Then, MUT's code translation component generates a syntax tree by parsing the code to be migrated and progressively converts each node in the tree, ultima tely generating the target tests, which are compiled and executed in the target project. Moreover, we develop a web tool to assist developers in test migration. The effectiveness of our approach has been validated on five prevalent functional domain projects within the open-source community. We migrate a total of 550 unit tests and submitted pull requests to augment test code in the target projects on GitHub. By the time of this paper submission, 253 of these tests have already been merged into the projects (including 197 unit tests in the Luliyucoordinate-LeetCode project and 56 unit tests in the Rangerlee-HtmlParser project). Through running these tests, we identify 5 bugs, and 2 functional defects, and submitted corresponding issues to the project. The evaluation substantiates that MUT's test migration is both viable and beneficial across programming languages and different projects.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {229},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639477.3639749,
author = {Peng, Chao and Lv, Zhengwei and Fu, Jiarong and Liang, Jiayuan and Zhang, Zhao and Rajan, Ajitha and Yang, Ping},
title = {Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639749},
doi = {10.1145/3639477.3639749},
abstract = {Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events that focus on improving the test coverage of the entire App rather than prioritising updates and impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise change-impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. At ByteDance, our established model-based GUI testing tool, Fastbot2, has been in successful deployment for nearly three years. Fastbot2 leverages event-activity transition models derived from past explorations to achieve enhanced test coverage efficiently. A pivotal insight we gained is that the knowledge of event-activity transitions is equally valuable in effectively targeting changes introduced by updates. This insight propelled our proposal for directed testing of updates with Hawkeye. Hawkeye excels in prioritizing GUI actions associated with code changes through deep reinforcement learning from historical exploration data.In our empirical evaluation, we rigorously compared Hawkeye with state-of-the-art tools like Fastbot2 and ARES on 10 popular open-source Apps and a commercial App. The results showcased that Hawkeye consistently outperforms Fastbot2 and ARES in generating GUI event sequences that effectively target changed functions, both in open-source and commercial App contexts.In real-world industrial deployment, Hawkeye is seamlessly integrated into our development pipeline, performing smoke testing for merge requests in a complex commercial App. The positive feedback received from our App development teams further affirmed Hawkeye's ability in testing App updates effectively.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {298–308},
numpages = {11},
keywords = {software testing, deep reinforcement learning, Android, graphical user interface},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643787.3648038,
author = {Kallis, Rafael and Colavito, Giuseppe and Al-Kaswan, Ali and Pascarella, Luca and Chaparro, Oscar and Rani, Pooja},
title = {The NLBSE'24 Tool Competition},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648038},
doi = {10.1145/3643787.3648038},
abstract = {We report on the organization and results of the tool competition of the third International Workshop on Natural Language-based Software Engineering (NLBSE'24). As in prior editions, we organized the competition on automated issue report classification, with focus on small repositories, and on automated code comment classification, with a larger dataset. In this tool competition edition, six teams submitted multiple classification models to automatically classify issue reports and code comments. The submitted models were fine-tuned and evaluated on a benchmark dataset of 3 thousand issue reports or 82 thousand code comments, respectively. This paper reports details of the competition, including the rules, the teams and contestant models, and the ranking of models based on their average classification performance across issue report and code comment types.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {33–40},
numpages = {8},
keywords = {tool-competition, labeling, benchmark, issue reports, code comments},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3666015.3666016,
author = {Boudjemila, Chahrazed and Dagnat, Fabien and Mart\'{\i}nez, Salvador},
title = {Maintaining Security Consistency During System Development with Security-Oriented Model Federation},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666016},
doi = {10.1145/3666015.3666016},
abstract = {Multi-modeling is an approach within the MDE realm that promotes the development of complex systems by decomposing them in sets of heterogeneous models. These models are defined using different modeling languages and constructed using diverse tools. They represent different but often interdependent views. However, the models of a system are far from being static. They change to accommodate new requirements, functionality improvements, bug fixes, and other evolution events. These changes represent a challenge w.r.t. consistency. This is especially true in security-critical scenarios. Indeed, security information is often integrated within the systems models so that security requirements are met following what is called "security-by-design". In such scenarios, the security concern of the systems models must remain consistent across changes so that security properties continue to hold. In order to tackle this problem, we propose a methodology to enhance the (multi)model-based design phase of a system development process. It comprises the creation of a security federation in which security dependencies between the different models are reified and equipped with security rules expressing security consistency requirements. Then, whenever a model is changed, the security rules are evaluated to monitor the consistency of security across the system models. We evaluate the capabilities of this methodology by a prototype implementation and its application to different use cases.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {66–76},
numpages = {11},
keywords = {Model-driven engineering, model evolution., model federation, security by design},
location = {M\, Germany},
series = {ICSSP '24}
}

@inproceedings{10.1145/3644384.3644473,
author = {Riquet, Nicolas and Devroey, Xavier and Vanderose, Benoit},
title = {Debt Stories: Capturing Social and Technical Debt in the Industry},
year = {2024},
isbn = {9798400705908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644384.3644473},
doi = {10.1145/3644384.3644473},
abstract = {In today's organizations, software is mission-critical. However, the legacy of past decisions can make tasks related to artifacts increasingly inefficient or risky, creating debt. While most researchers and practitioners mainly focus on technical debt, some have investigated its social dimensions, known as social debt. We argue that organizations developing software need to tackle debt holistically, as it is intrinsically a socio-technical issue. In this short paper, we rely on a definition of socio-technical debt based on the existing literature to define Debt Stories: a tool based on the User Story format, that can help capture debt elements directly from the stakeholders involved in software development. A debt story includes information about the role of the stakeholder in the development process, the social or technical context, and the impact of the debt element on the different tasks performed by the stakeholder. We provide a first empirical evaluation of the usage of Debt Stories in an industrial context, demonstrating the relevance of Debt Stories to express and communicate socio-technical debt.},
booktitle = {Proceedings of the 7th ACM/IEEE International Conference on Technical Debt},
pages = {40–44},
numpages = {5},
keywords = {socio-technical debt, debt stories, empirical software engineering},
location = {Lisbon, Portugal},
series = {TechDebt '24}
}

@inproceedings{10.1145/3644032.3644446,
author = {Hufkens, Lianne V. and Pastor Ricos, Fernando and Marin, Beatriz and Vos, Tanja E. J.},
title = {Grammar-Based Action Selection Rules for Scriptless Testing},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644446},
doi = {10.1145/3644032.3644446},
abstract = {Scriptless testing at the GUI level involves generating test sequences on the fly. These test sequences mimic user interactions on the GUI. The creation of these sequences works through action selection rules, which is most commonly based on stochastic methods. Script-less tests are reliable because they work with the actual state of the System Under Test (SUT). However, the tests are less specific, harder to interpret, and it is difficult to test concrete use cases or workflows. We want to tackle this drawback of scriptless tests by introducing action selection rules that are easier to guide than pure stochastic methods. In this paper, a new approach based on a grammar for the action selection rules is proposed, enabling scriptless testing tools to mimic user behaviour when interacting with web applications. While grammars have been used in software testing to generate input data for test cases, the proposed approach uses grammars to specify action selection rules to generate test sequences that mimic testing strategies employed by human testers. An empirical study has been performed to evaluate the effectiveness and the efficiency of the grammar-based action selection rules to filing web forms in comparison with random action selection rules. In the study, two SUTs were used: WebformSUT and Parabank. The average success rate for the grammar-based approach was 95.9% against random's 57.0% for WebformSUT and 99.8% against 55.7% for Parabank. For the widget interaction grammar-based had an average deviation from the ideal ratio of 0.06165 (WebformSUT) and 0.0180 (Parabank), compared random's 0.4318 (WebformSUT) and 0.7774 (Parabank). The results demonstrate the effectiveness of the grammar-based approach and the improvement in the use of resources.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {56–65},
numpages = {10},
keywords = {GUI testing, scriptless testing, action selection, grammars},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1109/ICSE48619.2023.00121,
author = {Bose, Priyanka and Das, Dipanjan and Vasan, Saastha and Mariani, Sebastiano and Grishchenko, Ilya and Continella, Andrea and Bianchi, Antonio and Kruegel, Christopher and Vigna, Giovanni},
title = {Columbus: Android App Testing through Systematic Callback Exploration},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00121},
doi = {10.1109/ICSE48619.2023.00121},
abstract = {With the continuous rise in the popularity of Android mobile devices, automated testing of apps has become more important than ever. Android apps are event-driven programs. Unfortunately, generating all possible types of events by interacting with an app's interface is challenging for an automated testing approach. Callback-driven testing eliminates the need for event generation by directly invoking app callbacks. However, existing callback-driven testing techniques assume prior knowledge of Android callbacks, and they rely on a human expert, who is familiar with the Android API, to write stub code that prepares callback arguments before invocation. Since the Android API is very large and keeps evolving, prior techniques could only support a small fraction of callbacks present in the Android framework.In this work, we introduce Columbus, a callback-driven testing technique that employs two strategies to eliminate the need for human involvement: (i) it automatically identifies callbacks by simultaneously analyzing both the Android framework and the app under test; (ii) it uses a combination of under-constrained symbolic execution (primitive arguments), and type-guided dynamic heap introspection (object arguments) to generate valid and effective inputs. Lastly, Columbus integrates two novel feedback mechanisms---data dependency and crash-guidance---during testing to increase the likelihood of triggering crashes and maximizing coverage. In our evaluation, Columbus outperforms state-of-the-art model-driven, checkpoint-based, and callback-driven testing tools both in terms of crashes and coverage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1381–1392},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639080,
author = {Jia, Ang and Fan, Ming and Xu, Xi and Jin, Wuxia and Wang, Haijun and Liu, Ting},
title = {Cross-Inlining Binary Function Similarity Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639080},
doi = {10.1145/3597503.3639080},
abstract = {Binary function similarity detection plays an important role in a wide range of security applications. Existing works usually assume that the query function and target function share equal semantics and compare their full semantics to obtain the similarity. However, we find that the function mapping is more complex, especially when function inlining happens.In this paper, we will systematically investigate cross-inlining binary function similarity detection. We first construct a cross-inlining dataset by compiling 51 projects using 9 compilers, with 4 optimizations, to 6 architectures, with 2 inlining flags, which results in two datasets both with 216 combinations. Then we construct the cross-inlining function mappings by linking the common source functions in these two datasets. Through analysis of this dataset, we find that three cross-inlining patterns widely exist while existing work suffers when detecting cross-inlining binary function similarity. Next, we propose a pattern-based model named CI-Detector for cross-inlining matching. CI-Detector uses the attributed CFG to represent the semantics of binary functions and GNN to embed binary functions into vectors. CI-Detector respectively trains a model for these three cross-inlining patterns. Finally, the testing pairs are input to these three models and all the produced similarities are aggregated to produce the final similarity. We conduct several experiments to evaluate CI-Detector. Results show that CI-Detector can detect cross-inlining pairs with a precision of 81% and a recall of 97%, which exceeds all state-of-the-art works.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {223},
numpages = {13},
keywords = {cross-inlining, binary similarity detection, inlining pattern},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00016,
author = {Jia, Haoxiang and Wen, Ming and Xie, Zifan and Guo, Xiaochen and Wu, Rongxin and Sun, Maolin and Chen, Kang and Jin, Hai},
title = {Detecting JVM JIT Compiler Bugs via Exploring Two-Dimensional Input Spaces},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00016},
doi = {10.1109/ICSE48619.2023.00016},
abstract = {Java Virtual Machine (JVM) is the fundamental software system that supports the interpretation and execution of Java bytecode. To support the surging performance demands for the increasingly complex and large-scale Java programs, JustIn-Time (JIT) compiler was proposed to perform sophisticated runtime optimization. However, this inevitably induces various bugs, which are becoming more pervasive over the decades and can often cause significant consequences. To facilitate the design of effective and efficient testing techniques to detect JIT compiler bugs. This study first performs a preliminary study aiming to understand the characteristics of JIT compiler bugs and the corresponding triggering test cases. Inspired by the empirical findings, we propose JOpFuzzer, a new JVM testing approach with a specific focus on JIT compiler bugs. The main novelty of JOpFuzzer is embodied in three aspects. First, besides generating new seeds, JOpFuzzer also searches for diverse configurations along the new dimension of optimization options. Second, JOpFuzzer learns the correlations between various code features and different optimization options to guide the process of seed mutation and option exploration. Third, it leverages the profile data, which can reveal the program execution information, to guide the fuzzing process. Such novelties enable JOpFuzzer to effectively and efficiently explore the two-dimensional input spaces. Extensive evaluation shows that JOpFuzzer outperforms the state-of-the-art approaches in terms of the achieved code coverages. More importantly, it has detected 41 bugs in OpenJDK, and 25 of them have already been confirmed or fixed by the corresponding developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {43–55},
numpages = {13},
keywords = {JVM testing, JIT compiler, JVM},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639139,
author = {Pertseva, Elizaveta and Chang, Melinda and Zaman, Ulia and Coblenz, Michael},
title = {A Theory of Scientific Programming Efficacy},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639139},
doi = {10.1145/3597503.3639139},
abstract = {Scientists write and maintain software artifacts to construct, validate, and apply scientific theories. Despite the centrality of software in their work, their practices differ significantly from those of professional software engineers. We sought to understand what makes scientists effective at their work and how software engineering practices and tools can be adapted to fit their workflows. We interviewed 25 scientists and support staff to understand their work. Then, we constructed a theory that relates six factors that contribute to their efficacy in creating and maintaining software systems. We present the theory in the form of a cycle of scientific computing efficacy and identify opportunities for improvement based on the six contributing factors.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {192},
numpages = {12},
keywords = {scientific programming, qualitative study of programmers},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3528230.3529186,
author = {Oldfield, Noah and Yue, Tao and Ali, Shaukat},
title = {Investigating quantum cause-effect graphs},
year = {2023},
isbn = {9781450393355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528230.3529186},
doi = {10.1145/3528230.3529186},
abstract = {Cause-effect graphs have shown promising results in identifying relations among causes and effects of classical software systems, followed by designing effective test cases from them. Towards this end, we investigate the use of cause-effect graphs for quantum programs. Classical cause-effect graphs apply classical logic (e.g., AND, OR) to express these relations, which might not be practical for describing similar relations in quantum programs due to quantum superposition and entanglement. Thus, we propose an extension of cause-effect graphs, where quantum logic inspired functions (e.g., Hadamard) and their generalizations are defined and applied. Moreover, we present a metamodel describing various forms of cause-effect graphs. Finally, we demonstrate a possible method for generating test cases from a quantum cause-effect graph applied to a Bell state quantum program. Lastly, the design and utility of the resulting testing method is discussed, along with future prospects for general quantum cause-effect graphs.},
booktitle = {Proceedings of the 3rd International Workshop on Quantum Software Engineering},
pages = {8–15},
numpages = {8},
keywords = {quantum software testing, quantum software engineering, quantum program specification, cause-effect graphing},
location = {Pittsburgh, Pennsylvania},
series = {Q-SE '22}
}

@inproceedings{10.1109/ICSE48619.2023.00034,
author = {Wu, Jiahui and Xu, Zhengzi and Tang, Wei and Zhang, Lyuye and Wu, Yueming and Liu, Chengyue and Sun, Kairan and Zhao, Lida and Liu, Yang},
title = {OSSFP: Precise and Scalable C/C++ Third-Party Library Detection Using Fingerprinting Functions},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00034},
doi = {10.1109/ICSE48619.2023.00034},
abstract = {Third-party libraries (TPLs) are frequently used in software to boost efficiency by avoiding repeated developments. However, the massive using TPLs also brings security threats since TPLs may introduce bugs and vulnerabilities. Therefore, software composition analysis (SCA) tools have been proposed to detect and manage TPL usage. Unfortunately, due to the presence of common and trivial functions in the bloated feature dataset, existing tools fail to precisely and rapidly identify TPLs in C/C++ real-world projects. To this end, we propose OSSFP, a novel SCA framework for effective and efficient TPL detection in large-scale real-world projects via generating unique fingerprints for open source software. By removing common and trivial functions and keeping only the core functions to build the fingerprint index for each TPL project, OSSFP significantly reduces the database size and accelerates the detection process. It also improves TPL detection accuracy since noises are excluded from the fingerprints. We applied OSSFP on a large data set containing 23,427 C/C++ repositories, which included 585,683 versions and 90 billion lines of code. The result showed that it could achieve 90.84% of recall and 90.34% of precision, which outperformed the state-of-the-art tool by 35.31% and 3.71%, respectively. OSSFP took only 0.12 seconds on average to identify all TPLs per project, which was 22 times faster than the other tool. OSSFP has proven to be highly scalable on large-scale datasets.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {270–282},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643657.3643910,
author = {Cabrera, Christian and Paleyes, Andrei and Lawrence, Neil David},
title = {Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation},
year = {2024},
isbn = {9798400705601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643657.3643910},
doi = {10.1145/3643657.3643910},
abstract = {Software systems impact society at different levels as they pervasively solve real-world problems. Modern software systems are often so sophisticated that their complexity exceeds the limits of human comprehension. These systems must respond to changing goals, dynamic data, unexpected failures, and security threats, among other variable factors in real-world environments. Systems' complexity challenges their interpretability and requires autonomous responses to dynamic changes. Two main research areas explore autonomous systems' responses: evolutionary computing and autonomic computing. Evolutionary computing focuses on software improvement based on iterative modifications to the source code. Autonomic computing focuses on optimising systems' performance by changing their structure, behaviour, or environment variables. Approaches from both areas rely on feedback loops that accumulate knowledge from the system interactions to inform autonomous decision-making. However, this knowledge is often limited, constraining the systems' interpretability and adaptability. This paper proposes a new concept for interpretable and adaptable software systems: self-sustaining software systems (S4). S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability. This paper introduces and discusses the S4 concept.},
booktitle = {Proceedings of the 1st International Workshop on New Trends in Software Architecture},
pages = {5–9},
numpages = {5},
keywords = {autonomous systems, software engineering, knowledge graphs, data-oriented architectures, large language models},
location = {Lisbon, Portugal},
series = {SATrends '24}
}

@inproceedings{10.1145/3643796.3648447,
author = {Unterkalmsteiner, Michael and Badampudi, Deepika and Britto, Ricardo and Ali, Nauman Bin},
title = {Help Me to Understand this Commit! - A Vision for Contextualized Code Reviews},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648447},
doi = {10.1145/3643796.3648447},
abstract = {Background: Modern Code Review (MCR) is a key component for delivering high-quality software and sharing knowledge among developers. Effective reviews require an in-depth understanding of the code and demand from the reviewers to contextualize the change from different perspectives. Aim: While there is a plethora of research on solutions that support developers to understand changed code, we have observed that many provide only narrow, specialized insights and very few aggregate information in a meaningful manner. Therefore, we aim to provide a vision of improving code understanding in MCR. Method: We classified 53 research papers suggesting proposals to improve MCR code understanding. We use this classification, the needs expressed by code reviewers from previous research, and the information we have not found in the literature for extrapolation. Results: We identified four major types of support systems and suggest an environment for contextualized code reviews. Furthermore, we illustrate with a set of scenarios how such an environment would improve the effectiveness of code reviews. Conclusions: Current research focuses mostly on providing narrow support for developers. We outline a vision for how MCR can be improved by using context and reducing the cognitive load on developers. We hope our vision can foster future advancements in development environments.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {18–23},
numpages = {6},
keywords = {modern code reviews, code understanding, decision-making, support systems},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@proceedings{10.1145/3643655,
title = {SESoS '24: Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SESoS 2024 will provide a forum for researchers and practitioners with a forum to exchange ideas and experiences, analyze research and development issues, discuss promising solutions, and propose theoretical foundations for the development and evolution of complex software-intensive systems.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00013,
author = {Zhang, Mingrui and Zhou, Chijin and Liu, Jianzhong and Wang, Mingzhe and Liang, Jie and Zhu, Juan and Jiang, Yu},
title = {Daisy: Effective Fuzz Driver Synthesis with Object Usage Sequence Analysis},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00013},
doi = {10.1109/ICSE-SEIP58684.2023.00013},
abstract = {Fuzzing is increasingly used in industrial settings for vulnerability detection due to its scalability and effectiveness. Libraries require driver programs to feed the fuzzer-generated inputs into library-provided interfaces. Writing such drivers manually is tedious and error-prone, thus greatly hindering the widespread use of fuzzing in practical situations. Previous attempts at automatic driver synthesis perform static analysis on the libraries and their consumers. However, a lack of dynamic object usage information renders them ineffective at generating interface function calls with correct parameters and meaningful sequences. This severely limits fuzzing's bug-finding capabilities and can produce faulty drivers.In this paper, we propose Daisy, a driver synthesis framework, which extracts dynamic object usage sequences of library consumers to synthesize significantly more effective drivers. Daisy uses the following two steps to synthesize a fuzz driver for a library. First, it models each object's behaviors into an object usage sequence during the execution of its consumers. Next, it merges all the extracted sequences and constructs a series of interface calls with valid object usages based on the merged sequence. We implemented Daisy and evaluated its effectiveness on real-world libraries selected from both the Android Open Source Project (AOSP) and Google's FuzzBench. DAISY's synthesized drivers significantly outperform drivers produced by other state-of-the-art fuzz driver synthesizers. In addition, on applying Daisy to the latest versions of those extensively-fuzzed real-world libraries of the benchmark, e.g. libaom and freetype2, we also found 9 previously-unknown bugs with 3 CVEs assigned.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {87–98},
numpages = {12},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3639478.3639797,
author = {Siala, Hanan Abdulwahab},
title = {Enhancing Model-Driven Reverse Engineering Using Machine Learning},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639797},
doi = {10.1145/3639478.3639797},
abstract = {Organizations often rely on large applications that are classified as legacy systems due to their dependence on outdated programming languages or platforms. To modernize these systems, it is necessary to understand their architecture, functionality, and business rules. Our research aims to define a novel model-driven reverse engineering (MDRE) approach to extract Unified Modeling Language (UML) and Object Constraint Language (OCL) representations from source code using Large Language Models (LLMs).},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {173–175},
numpages = {3},
keywords = {application programs, model driven reverse engineering (MDRE), unified modeling language (UML), object constraint language (OCL), machine learning, large language models (LLMS), program comprehension},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3647632.3647993,
author = {Yu, Sheng and Nourzad, Narjes and Semple, Randye J. and Zhao, Yixue and Zhou, Emily and Krishnamachari, Bhaskar},
title = {CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3647993},
doi = {10.1145/3647632.3647993},
abstract = {The COVID-19 pandemic has intensified the urgency for effective and accessible mental health interventions in people's daily lives. Mobile Health (mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained traction as they expand beyond traditional clinical settings to support daily life. However, the effectiveness of current mHealth solutions is impeded by the lack of context-awareness, personalization, and modularity to foster their reusability. This paper introduces CAREForMe, a contextual multi-armed bandit (CMAB) recommendation framework for mental health. Designed with context-awareness, personalization, and modularity at its core, CAREForMe harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely, personalized recommendations. With its modular design, CAREForMe serves as both a customizable recommendation framework to guide future research, and a collaborative platform to facilitate interdisciplinary contributions in mHealth research. We showcase CAREForMe's versatility through its implementation across various platforms (e.g., Discord, Telegram) and its customization to diverse recommendation features.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {92–94},
numpages = {3},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1145/3643991.3644870,
author = {Liu, Kaibo and Han, Yudong and Liu, Yiyang and Chen, Zhenpeng and Zhang, Jie M. and Sarro, Federica and Huang, Gang and Ma, Yun},
title = {TrickyBugs: A Dataset of Corner-case Bugs in Plausible Programs},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644870},
doi = {10.1145/3643991.3644870},
abstract = {We call a program that passes existing tests but still contains bugs as a buggy plausible program. Bugs in such a program can bypass the testing environment and enter the production environment, causing unpredictable consequences. Therefore, discovering and fixing such bugs is a fundamental and critical problem. However, no existing bug dataset is purposed to collect this kind of bug, posing significant obstacles to relevant research. To address this gap, we introduce TrickyBugs, a bug dataset with 3,043 buggy plausible programs sourced from human-written submissions of 324 real-world competition coding tasks. We identified the buggy plausible programs from approximately 400,000 submissions, and all the bugs in TrickyBugs were not previously detected. We hope that TrickyBugs can effectively facilitate research in the fields of automated program repair, fault localization, test generation, and test adequacy.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {113–117},
numpages = {5},
keywords = {software testing, test generation, test adequacy, program repair, benchmark},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00196,
author = {Huang, Yuchao and Wang, Junjie and Liu, Zhe and Wang, Song and Chen, Chunyang and Li, Mingyang and Wang, Qing},
title = {Context-Aware Bug Reproduction for Mobile Apps},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00196},
doi = {10.1109/ICSE48619.2023.00196},
abstract = {Bug reports are vital for software maintenance that allow the developers being informed of the problems encountered in the software. Before bug fixing, developers need to reproduce the bugs which is an extremely time-consuming and tedious task, and it is highly expected to automate this process. However, it is challenging to do so considering the imprecise or incomplete natural language described in reproducing steps, and the missing or ambiguous single source of information in GUI components. In this paper, we propose a context-aware bug reproduction approach ScopeDroid which automatically reproduces crashes from textual bug reports for mobile apps. It first constructs a state transition graph (STG) and extracts the contextual information of components. We then design a multi-modal neural matching network to derive the fuzzy matching matrix between all candidate GUI events and reproducing steps. With the STG and matching information, it plans the exploration path for reproducing the bug, and enriches the initial STG iteratively. We evaluate the approach on 102 bug reports from 69 popular Android apps, and it successfully reproduces 63.7% of the crashes, outperforming the state-of-the-art baselines by 32.6% and 38.3%. We also evaluate the usefulness and robustness of ScopeDroid with promising results. Furthermore, to train the neural matching network, we develop a heuristic-based automated training data generation method, which can potentially motivate and facilitate other activities as user interface operations.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2336–2348},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643796.3648449,
author = {Santos, Andr\'{e} L. and Cancelinha, Alexandre and Batista, Fernando},
title = {Jasay: Towards Voice Commands in Projectional Editors},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648449},
doi = {10.1145/3643796.3648449},
abstract = {Permanent disabilities or temporary injuries (e.g., RSI) hinder the activity of writing code. The interaction modality of voice is a viable substitute or complement for typing on a keyboard. This paper describes the design of Jasay, a prototype tool that enables developers to write Java code using voice commands. Our implementation relies on a third-party speech-recognition system to convert the voice into text. In turn, such a text is translated into commands that transform the abstract syntax tree (AST) of the code being edited. Jasay works as an extension to a projectional editor, taking advantage of having the abstract syntax tree always available without parsing, a permanent well-formed structure of the code, and unambiguous editing locations (e.g., class member, statement, expression, etc). An early experiment with Jasay involving 5 programmers has shown encouraging results, as they were able to perform small program modifications within reasonable time.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {30–34},
numpages = {5},
keywords = {programming, voice, projectional editors, Java},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643665.3648049,
author = {Moon, Sae Young and Kerr, Gregor and Silavong, Fran and Moran, Sean},
title = {API-Miner: an API-to-API Specification Recommendation Engine},
year = {2024},
isbn = {9798400705687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643665.3648049},
doi = {10.1145/3643665.3648049},
abstract = {When designing a new API for a large project, developers need to make smart design choices so that their code base can grow sustainably. To ensure that new API components are well designed, developers can learn from existing API components. However, the lack of standardized methods for comparing API designs makes this learning process time-consuming and difficult. To address this gap we developed API-Miner. API-Miner retrieves relevant specification components written in OpenAPI. API-miner presents several significant contributions, including: (1) novel methods of processing and extracting key information from OpenAPI specifications, (2) innovative feature extraction techniques that are optimized for the highly technical API specification domain, and (3) a novel log-linear probabilistic model that combines multiple signals to retrieve relevant and high quality OpenAPI specification components given a query specification. We evaluate API-Miner in both quantitative and qualitative tasks and achieve an overall of 91.7% recall@1 and 56.2% F1, which surpasses baseline performance by 15.4 percentage points (pp) in recall@1 and 3.2 pp in F1. Code is available at: https://github.com/jpmorganchase/api-miner.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Software Engineering Challenges in Financial Firms},
pages = {9–16},
numpages = {8},
keywords = {API contract, API specification, structured document matching, recommendation system},
location = {Lisbon, Portugal},
series = {FinanSE '24}
}

@inproceedings{10.1145/3643655.3643878,
author = {Alhindi, Maysara and Hallett, Joseph},
title = {Sandboxing Adoption in Open Source Ecosystems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643878},
doi = {10.1145/3643655.3643878},
abstract = {Sandboxing mechanisms allow developers to limit how much access applications have to resources, following the least-privilege principle. However, it's not clear how much and in what ways developers are using these mechanisms. This study looks at the use of Seccomp, Landlock, Capsicum, Pledge, and Unveil in all packages of four open-source operating systems. We found that less than 1% of packages directly use these mechanisms, but many more indirectly use them. Examining how developers apply these mechanisms reveals interesting usage patterns, such as cases where developers simplify their sandbox implementation. It also highlights challenges that may be hindering the widespread adoption of sandboxing mechanisms.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {13–20},
numpages = {8},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3666015.3666023,
author = {Sutton, Stanley M.},
title = {Capability Modeling for Corporate Cognition},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666023},
doi = {10.1145/3666015.3666023},
abstract = {Several arguments support the proposition that corporations can be viewed as cognitive entities. But accepting that corporations perform cognitive functions raises questions about how we might describe and assess those functions. In this paper I propose that we can address such questions by viewing corporate cognition in terms of capabilities that are adapted from human cognition. To illustrate this idea, I use the concept of schemas, a human cognitive capability identified by Piaget, which I recast as a corporate cognitive capability. To develop this example, I review schemas as understood in psychology; use scenarios from corporate settings to motivate their relevance to corporations; and describe a hypothetical capability for corporate cognitive schemas using a template that addresses common concerns in enterprise capability modeling. This example gives support to the idea that cognitive capabilities are relevant on the corporate level. It shows that a corporate cognitive capabilities can be practically modeled like conventional business capabilities and that corporate cognitive capabilities can be related to business goals. This work points to a path for practical investigations and applications of a view of corporations as cognitive entities. It further opens a new domain for software process programming.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {24–35},
numpages = {12},
keywords = {Business Architecture, Capability Mapping, Capability Maturity, Capability Modeling, Cognitive Capabilities, Corporate Cognition, Distributed Cognition, Enterprise Architecture, Process Maturity},
location = {M\, Germany},
series = {ICSSP '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00107,
author = {Ivanov, Dmitry and Babushkin, Alexey and Grigoryev, Saveliy and Iatchenii, Pavel and Kalugin, Vladislav and Kichin, Egor and Kulikov, Egor and Misonizhnik, Aleksandr and Mordvinov, Dmitry and Morozov, Sergey and Naumenko, Olga and Pleshakov, Alexey and Ponomarev, Pavel and Shmidt, Svetlana and Utkin, Alexey and Volodin, Vadim and Volynets, Arseniy},
title = {UnitTestBot: Automated Unit Test Generation for C Code in Integrated Development Environments},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00107},
doi = {10.1109/ICSE-Companion58688.2023.00107},
abstract = {Symbolic execution (SE) is one of the most promising techniques for automated unit test generation, which is claimed to streamline the testing process and reduce developers' effort. There are symbolic execution engines working for Java, C, C#, C++, Python, .NET. The KLEE dynamic symbolic execution engine is one of the most elaborated ones --- it is built on top of the LLVM compiler infrastructure and can automatically generate inputs for C code unit testing. There are numerous attempts to apply KLEE to real-life software projects, while the industry experience still shows little transfer from research to practice. The extensions to popular integrated development environments (IDEs) are supposed to be breaking down this barrier. As far as there are not so many working tools like this, we share our experience of implementing the KLEE-based Visual Studio Code and CLion extensions for generating ready-to-use test cases --- UnitTestBot for C code --- and describe the challenges we had to rise to. We also share the solutions we came up with: without introducing "new" techniques, we made automated unit test generation really automated and supplemented it with the simple wizard interface. That was enough for turning an effective but demanding technology into a user-friendly tool, which is easy to adopt. Finally, we provide examples of running UnitTestBot on the open-source projects as well as Huawei nonpublic code.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {380–384},
numpages = {5},
keywords = {integrated development environment, KLEE, symbolic execution, automated unit test generation, software testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643796.3648445,
author = {Wasserman, Anthony},
title = {Lessons from a Pioneering Software Engineering Environment: Design Principles of Software through Pictures},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648445},
doi = {10.1145/3643796.3648445},
abstract = {This paper describes the historical background that led to the development of the innovative Software through Pictures multi-user development environment, and the principles for its integration with other software products to create a software engineering environment covering multiple tasks in the software development lifecycle.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {12–14},
numpages = {3},
keywords = {software engineering environments, software tools, software design, software architecture},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3639477.3639720,
author = {Gallagher, Shannon K. and Ratchford, Jasmine and Brooks, Tyler and Brown, Bryan P. and Heim, Eric and Nichols, William R. and Mcmillan, Scott and Rallapalli, Swati and Smith, Carol J. and Vanhoudnos, Nathan and Winski, Nick and Mellinger, Andrew O.},
title = {Assessing LLMs for High Stakes Applications},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639720},
doi = {10.1145/3639477.3639720},
abstract = {Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in reproducibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {103–105},
numpages = {3},
keywords = {large language models, TEVV, metrics, scaling, HCI, trust},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643796.3648461,
author = {Liu, Jiangshan and Liu, Shuang and Chen, Junjie},
title = {I3DE: An IDE for Inspecting Inconsistencies in PL/SQL Code},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648461},
doi = {10.1145/3643796.3648461},
abstract = {In this paper, we introduce I3DE (Inconsistency Inspecting IDE) --- an IDE plugin to inspect inconsistencies in PL/SQL code. We first observed the potential issues, e.g., misuses or bugs, that are introduced by the inconsistent understanding of PL/SQL semantics by PL/SQL programmers and DBMS developers, and propose a meta-morphic testing-based approach for inspecting such inconsistencies in PL/SQL code. We design and implement our approach in I3DE, a widely usable plugin for the IntelliJ Platform. We conducted a comparative user study involving 16 participants, and the findings indicate that I3DE is consistently effective and efficient in helping programmers identify and avoid inconsistencies across different programming difficulties.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {74–75},
numpages = {2},
keywords = {PL/SQL, inconsistency, IDE, plugin, code inspection},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3639478.3641227,
author = {Chen, Yang},
title = {Flakiness Repair in the Era of Large Language Models},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641227},
doi = {10.1145/3639478.3641227},
abstract = {Flaky tests can non-deterministically pass or fail regardless of any change to the code, which negatively impacts the effectiveness of the regression testing. Prior repair techniques for flaky tests mainly leverage program analysis techniques to mitigate test flakiness, which only focus on Order-Dependent (OD) and Implementation-Dependent (ID) flakiness with known flakiness patterns and root causes. In this paper, we propose an approach to repair flaky tests with the power of Large Language Models (LLMs). Our approach successfully repaired 79% of OD tests and 58% of ID tests in an extensive evaluation using 666 flaky tests from 222 projects. We submitted pull requests to fix 61 flaky tests; at the time of submission, 19 tests have already been accepted. However, we observed that currently LLMs are ineffective in adequately repairing Non-Order-Dependent (NOD) flaky tests by analyzing 118 of such tests from 11 projects.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {441–443},
numpages = {3},
keywords = {software testing, test flakiness, large language models},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3641226,
author = {Ibrahimzada, Ali Reza},
title = {Program Decomposition and Translation with Static Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3641226},
doi = {10.1145/3639478.3641226},
abstract = {The rising popularity of Large Language Models (LLMs) has motivated exploring their use in code-related tasks. Code LLMs with more than millions of parameters are trained on a massive amount of code in different Programming Languages (PLs). Such models are used for automating various Software Engineering (SE) tasks using prompt engineering. However, given the very large size of industry-scale project files, a major issue of these LLMs is their limited context window size, motivating the question of "Can these LLMs process very large files and can we effectively perform prompt engineering?". Code translation aims to convert source code from one PL to another. In this work, we assess the effect of method-level program decomposition on context window of LLMs and investigate how this approach can enable translation of very large files which originally could not be done due to out-of-context issue. Our observations from 20 well-known java projects and approximately 60K methods suggest that method-level program decomposition significantly improves the limited context window problem of LLMs by 99.5%. Furthermore, our empirical analysis indicate that with method-level decomposition, each input fragment on average only consumes 5% of the context window, leaving more context space for prompt engineering and the output. Finally, we investigate the effectiveness of a Call Graph (CG) approach for translating very large files when doing method-level program decomposition.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {453–455},
numpages = {3},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3644927,
author = {Menon, Harshitha and Nichols, Daniel and Bhatele, Abhinav and Gamblin, Todd},
title = {Learning to Predict and Improve Build Successes in Package Ecosystems},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644927},
doi = {10.1145/3643991.3644927},
abstract = {Software has become increasingly complex, with a typical application depending on tens or hundreds of packages. Finding compatible versions and build configurations of these packages is challenging. This paper presents a method to learn the likelihood of software build success, and techniques for leveraging this information to guide dependency solvers to better software configurations. We leverage the heavily parameterized package recipes from the Spack package manager to produce a training data set of builds, and we use Graph Neural Networks to learn whether a given package configuration will build successfully or not. We apply our tool to the U.S. Exascale Computing Project's software stack. We demonstrate its effectiveness in predicting whether a given package will build successfully. We show that our technique can be used to improve the solutions generated by dependency solvers, reducing the need for developers to find working builds by trial and error.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {531–542},
numpages = {12},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643788.3648017,
author = {Xie, Linna and Li, Chongmin and Pei, Yu and Zhang, Tian and Pan, Minxue},
title = {Automated Program Repair for Introductory Programming Assignments via Bidirectional Refactoring},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648017},
doi = {10.1145/3643788.3648017},
abstract = {The development of programming education has given rise to automated program repair techniques tailored for introductory programming assignments (IPAs). Despite the promising performance of mainstream automated feedback generation systems, they still struggle to handle scenarios where there is "no matching control-flow to generate repair" well. This paper presents Brafar, an innovative automated program repair tool for IPAs. Brafar tackles the core issue through a novel "bidirectional refactoring" algorithm which aligns the control-flow structures of the incorrect and reference programs without semantic changes. Additionally, Brafar incorporates a specification inference technique to further enhance the repair process, reducing the occurrence of unnecessary repairs. We have implemented the Brafar tool in Python and it is now publicly available. In comparative experiments, both the Brafar tool and the baseline Refactory tool were evaluated using 100 real-life incorrect programs from 5 different IPAs. The outcomes of these experiments demonstrated that our Brafar tool outperformed the baseline tool in terms of repair accuracy.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {53–55},
numpages = {3},
keywords = {feedback generation, program repair, software refactoring},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3639478.3639781,
author = {Duque-Torres, Alejandra},
title = {Selecting and Constraining Metamorphic Relations},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639781},
doi = {10.1145/3639478.3639781},
abstract = {Software testing is a critical aspect of ensuring the reliability and quality of software systems. However, it often poses challenges, particularly in determining the expected output of a System Under Test (SUT) for a given set of inputs, a problem commonly referred to as the test oracle problem. Metamorphic Testing (MT) offers a promising solution to the test oracle problem by examining the relations between input-output pairs in consecutive executions of the SUT. These relations, referred to as Metamorphic Relations (MRs), define the expected changes in the output when specific changes are made to the input. Our research is focused on developing methods and tools to assist testers in the selection of MRs, the definition of constraints, and providing explanations for MR outcomes. The research is divided in three parts. The first part focuses on MR collection and description, entailing the creation of a comprehensive repository of MRs from various sources. A standardised MR representation is devised to promote machine-readability and wide-ranging applicability. The second part introduces MetraTrimmer, a test-data-driven approach for systematically selecting and constraining MRs. This approach acknowledges that MRs may not be universally applicable to all test data space. The final part, evaluation and validation, encompasses empirical studies aimed at assessing the effectiveness of the developed methods and validating their suitability for real-world regression testing scenarios. Through this research, we aim to advance the automation of MR generation, enhance the understanding of MR violations, and facilitate their effective application in regression testing.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {212–216},
numpages = {5},
keywords = {test oracle, metamorphic testing, metamorphic relations, test data, pattern mining},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643091,
author = {Preda, Anamaria-Roberta and Mayr-Dorn, Christoph and Mashkoor, Atif and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Egyed, Alexander},
title = {Towards Leveraging Fine-Grained Dependencies to Check Requirements Traceability Correctness},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643091},
doi = {10.1145/3639478.3643091},
abstract = {Efficient software maintenance and evolution rely heavily on effective software traceability, which is crucial for understanding the relationships between code elements and their corresponding requirements. However, ensuring the accuracy of trace links, whether manually or automatically, is a significant challenge due to the labor-intensive and error-prone nature of traceability tasks. The granularity issue in traceability compounds this challenge, as most existing research focuses on class-level traceability, while fine-grained dependencies (e.g., method-level traces) are more pertinent in daily development practices.Our primary aim is to facilitate the checking of requirement-to-method traces. To this end, we investigate an approach that utilizes the method's calling information and textual embeddings of requirement-to-method traces to identify inaccuracies in trace links. Our preliminary results are promising. By leveraging a Random Forest (RF) classifier, we have achieved notable improvements in both precision (≈10%) and recall (≈30%) compared to existing methods. This advancement highlights the potential of our method in enhancing the accuracy and efficiency of traceability processes in software development.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {292–293},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/NSE66660.2025.00009,
author = {Sakizloglou, Lucas and Khakharova, Taisiya and Lambers, Leen},
title = {A Graph-Centric Neuro-Symbolic Architecture Applied to Personalized Sepsis Treatments},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NSE66660.2025.00009},
doi = {10.1109/NSE66660.2025.00009},
abstract = {Recent research on intelligent healthcare employs Deep Reinforcement Learning (DRL) to personalize treatments according to patients’ physiological characteristics and thus render treatments more effective. However, the majority of approaches rely on the relational data model, that struggles with the representation of the complex relationships within medical data. Moreover, the output of these approaches is typically a recommended action, e.g., a dosage; however, clinicians need the contextualization, i.e., the provision of supporting information, of such recommendations in order to decide whether to follow it.We present a neuro-symbolic architecture for personalized treatments based on a graph-centric foundation. The architecture is based on representing medical data as a knowledge graph and learning via graph neural networks; their combination enables the inherent capturing of relationships and their native integration into reasoning, which may thus render recommendations more effective. Moreover, the architecture employs formally specified graph queries over the knowledge graph to contextualize personalized treatments. We exemplify the architecture by an application to sepsis treatments and based on a widely-used medical dataset.},
booktitle = {2025 IEEE/ACM 1st International Workshop on Neuro-Symbolic Software Engineering (NSE)},
pages = {11–16},
numpages = {6},
location = {Ottawa, ON, Canada}
}

@inproceedings{10.1145/3643657.3643913,
author = {Li, Xiaozhou and Albano, Michele},
title = {A Framework for Microservice Organizational Structure Optimization},
year = {2024},
isbn = {9798400705601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643657.3643913},
doi = {10.1145/3643657.3643913},
abstract = {With the soaring popularity of microservices, practitioners have realized that the key to success lies more in the management of people than in the architecture itself. Due to the systems' inevitable evolution, the organization shifts towards such changes where a lack of proper optimization can result in dreadful efficiency, error proneness, and toxic collaboration environments. However, despite the criticality, limited studies have proposed concrete support for optimizing microservice organizational structure through system evolution. This short paper sketches the vision of a framework to facilitate the optimization of microservice organizational structure. With this work, we aim to attract the attention of the microservice community toward the benefits and issues of the microservice organizational structure and propose a promising direction to a continuous solution.},
booktitle = {Proceedings of the 1st International Workshop on New Trends in Software Architecture},
pages = {18–21},
numpages = {4},
keywords = {microservice, organizational structure, optimization, gamification, framework},
location = {Lisbon, Portugal},
series = {SATrends '24}
}

@inproceedings{10.1145/3639478.3643126,
author = {Nath, Sristy Sumana and Roy, Banani},
title = {Recovering Traceability Links between Release Notes and Related Software Artifacts},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643126},
doi = {10.1145/3639478.3643126},
abstract = {Inadequate traceability links between software artifacts can create challenges for developers in tracking the origin of bugs or issues and their corresponding code changes, leading to longer resolution times and the potential introduction of new bugs [5]. When changes are made without proper traceability links, inconsistencies and conflicts may arise between different artifacts [4], such as requirements, design documents, and code, resulting in software development that fails to meet user expectations or exhibits unexpected behavior. The lack of proper traceability links also poses challenges in maintaining software over time, making it difficult to upgrade, manage dependencies, and make changes to the software [3]. Additionally, the lack of traceability links can make it challenging to understand the software's evolution and developers' decision-making process, reducing transparency and hindering collaboration.Release notes are a document that includes details about new features, bug fixes, improvements, and known issues. They help users and developers understand the changes made to the software and their impact on workflows [1]. Traceability links of issues, pull requests (PRs), and commits are important in release notes as they provide context and understanding of changes made in a release [6]. In our dataset, 33% of release notes are not linked with the corresponding artifacts, highlighting the need for automated traceability link recovery in release notes.Additionally, limited traceability links can lead to duplicate bug reports, confusion, and wasted time and effort [7]. Traceability links are crucial for version controlling and back-porting to give a clear understanding of the dependencies within different versions. Without these links, managing releases and documenting changes accurately becomes challenging, potentially damaging the reputation of the software and causing confusion among stakeholders and customers [2].Our study begins by creating a benchmark to propose an automated traceability technique between release and related artifacts. To collect data, we use the GitHub API to gather information from 10 popular repositories, including release notes, pull-request titles, and commit messages. We analyze textual data and create a benchmark for recovering traceability links between releases and related artifacts such as commits, pull requests, and issues. Next, we investigate the feasibility of automated traceability approaches for software release notes in GitHub using rule-based and information retrieval (IR)-based classifier. To the best of our knowledge, our techniques are the first to automatically recover traceability links between software releases and related artifacts (i.e., commits, pull requests, and issues). This approach can help keep track of changes and improve the quality of release notes by collecting all useful information automatically in real-time. By using this approach, we aim to identify areas of improvement and refine our proposed technique to make it more usable and effective in practice.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {376–377},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00051,
author = {Krasniqi, Rrezarta},
title = {Detecting Scattered and Tangled Quality Concerns in Source Code to Aid Maintenance and Evolution Tasks},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00051},
doi = {10.1109/ICSE-Companion58688.2023.00051},
abstract = {Quality concerns, such as reliability, security, usability concerns, among others, are typically well-defined and prioritized at the requirement level with the set goal of achieving high quality, robust, user-friendly, and trustworthy systems. However, quality concerns are challenging to address at the implementation level. Often they are scattered across multiple modules in the codebase. In other instances, they are tangled with functional ones within a single module. Reasoning about quality concerns and their interactions with functional ones while being hindered by the effects of scattered and tangled code can only yield to more unseen problems. For example, developers can inadvertently retrofit new bugs or wrongly implement new features that deviate from original system requirement specifications. The goal of this thesis is twofold. First, we aim to detect quality concerns implemented at code level to differentiate them from functional ones when they are scattered across the codebase. Second, we aim to untangle quality concerns from unrelated changes to gain a detailed knowledge about the history of specific quality changes. This knowledge is crucial to support consistency between the requirements-and-design and to verify architecture conformance. From the practical stance, developers could gain a breadth of understanding about quality concerns and their relations with other artifacts. Thus, with more confidence, they could perform code modifications, improve module traceability, and provide a better holistic assessment of change impact analysis.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {184–188},
numpages = {5},
keywords = {software maintenance, scattered quality concerns, tangled quality concerns, quality bugs, quality concerns},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00036,
author = {Teo, Wei and Teoh, Ze and Arabi, Dayang Abang and Aboushadi, Morad and Lai, Khairenn and Ng, Zhe and Pant, Aastha and Hoda, Rashina and Tantithamthavorn, Chakkrit and Turhan, Burak},
title = {What Would You Do? An Ethical AI Quiz},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00036},
doi = {10.1109/ICSE-Companion58688.2023.00036},
abstract = {The resurgence of Artificial Intelligence (AI) has been accompanied by a rise in ethical issues. AI practitioners either face challenges in making ethical choices when designing AI-based systems or are not aware of such challenges in the first place. Increasing the level of awareness and understanding of the perceptions of those who develop AI systems is a critical step toward mitigating ethical issues in AI development. Motivated by these challenges, needs, and the lack of engaging approaches to address these, we developed an interactive, scenario-based ethical AI quiz. It allows AI practitioners, including software engineers who develop AI systems, to self-assess their awareness and perceptions about AI ethics. The experience of taking the quiz, and the feedback it provides, will help AI practitioners understand the gap areas, and improve their overall ethical practice in everyday development scenarios. To demonstrate these expected outcomes and the relevance of our tool, we also share a preliminary user study. The video demo can be found at https://zenodo.org/record/7601169#.Y9xgA-xBxhF.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {112–116},
numpages = {5},
keywords = {ethical AI quiz, self-assessment tools, AI practitioners, AI ethics, ethics},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528588.3528652,
author = {Devine, Peter and Blincoe, Kelly},
title = {Unsupervised extreme multi label classification of stack overflow posts},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528652},
doi = {10.1145/3528588.3528652},
abstract = {Knowing the topics of a software forum post, such as those on StackOverflow, allows for greater analysis and understanding of the large amounts of data that come from these communities. One approach to this problem is using extreme multi label classification (XMLC) to predict the topic (or "tag") of a post from a potentially very large candidate label set. While previous work has trained these models on data which has explicit text-to-tag information, we assess the classification ability of embedding models which have not been trained using such structured data (and are thus "unsupervised") to assess the potential applicability to other forums or domains in which tag data is not available.We evaluate 14 unsupervised pre-trained models on 0.1% of all StackOverflow posts against all 61,662 possible StackOverflow tags. We find that an MPNet model trained partially on unlabelled StackExchange data (i.e. without tag data) achieves the highest score overall for this task, with a recall score of 0.161 R@1. These results inform which models are most appropriate for use in XMLC of StackOverflow posts when supervised training is not feasible. This offers insight into these models' applicability in similar but not identical domains, such as software product forums. These results suggest that training embedding models using in-domain title-body or question-answer pairs can create an effective zero-shot topic classifier for situations where no topic data is available.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {1–8},
numpages = {8},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3643659.3643928,
author = {Olsthoorn, Mitchell and Stallenberg, Dimitri and Panichella, Annibale},
title = {Syntest-JavaScript: Automated Unit-Level Test Case Generation for JavaScript},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643928},
doi = {10.1145/3643659.3643928},
abstract = {Over the last decades, various tools (e.g., AUSTIN and EvoSuite) have been developed to automate the process of unit-level test case generation. Most of these tools are designed for statically-typed languages, such as C and Java. However, as is shown in recent Stack Overflow developer surveys, the popularity of dynamically-typed languages, such as JavaScript and Python, has been increasing and is dominating the charts. Only recently, tools for automated test case generation of dynamically-typed languages have started to emerge (e.g., Pynguin for Python). However, to the best of our knowledge, there is no tool that focuses on automated test case generation for server-side JavaScript. To this aim, we introduce SynTest-JavaScript, a user-friendly tool for automated unit-level test case generation for (server-side) JavaScript. To showcase the effectiveness of SynTest-JavaScript, we empirically evaluate it on five large open-source JavaScript projects and one artificial one.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {21–24},
numpages = {4},
keywords = {software testing, search-based software testing, test case generation, fuzzing, javascript, syntest},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@proceedings{10.1145/3643794,
title = {SERP4IoT '24: Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {SERP4IoT begins to be recognized as an annual venue gathering researchers, industrials, and practitioners to share their vision, experience, and opinion on how to address the challenges of, find solutions for, and share experiences with the development, release, and testing of robust software for IoT systems.Even today, there is no precise definition of what is software engineering for the IoT, because it encompasses many different aspects of software design, development, evolution, deployment, and operation, with varying and conflicting criteria such as success, longevity, growth, resilience, survival, diversity, sustainability, transparency, privacy, security, etc.Yet, software engineering is vital for IoT to design systems that are secure, interoperable, modifiable, and scalable. It is crucial to bring good practices for developing projects for IoT, to devise and study the best architectures, to understand and secure communication protocols, and, generally, to overcome the many challenges faced by practitioners and researchers.},
location = {Lisbon, Portugal}
}

@proceedings{10.5555/3623295,
title = {ICSE-SEET '23: Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
location = {Melbourne, Australia}
}

@inproceedings{10.1145/3597503.3639231,
author = {Pan, Zhiyuan and Hu, Xing and Xia, Xin and Zhan, Xian and Lo, David and Yang, Xiaohu},
title = {PPT4J: Patch Presence Test for Java Binaries},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639231},
doi = {10.1145/3597503.3639231},
abstract = {The number of vulnerabilities reported in open source software has increased substantially in recent years. Security patches provide the necessary measures to protect software from attacks and vulnerabilities. In practice, it is difficult to identify whether patches have been integrated into software, especially if we only have binary files. Therefore, the ability to test whether a patch is applied to the target binary, a.k.a. patch presence test, is crucial for practitioners. However, it is challenging to obtain accurate semantic information from patches, which could lead to incorrect results.In this paper, we propose a new patch presence test framework named Ppt4J (Patch Presence Test for Java Binaries). Ppt4J is designed for open-source Java libraries. It takes Java binaries (i.e. bytecode files) as input, extracts semantic information from patches, and uses feature-based techniques to identify patch lines in the binaries. To evaluate the effectiveness of our proposed approach Ppt4J, we construct a dataset with binaries that include 110 vulnerabilities. The results show that Ppt4J achieves an F1 score of 98.5% with reasonable efficiency, improving the baseline by 14.2%. Furthermore, we conduct an in-the-wild evaluation of Ppt4J on JetBrains IntelliJ IDEA. The results suggest that a third-party library included in the software is not patched for two CVEs, and we have reported this potential security problem to the vendor.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {225},
numpages = {12},
keywords = {patch presence test, binary analysis, software security},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3644033.3644373,
author = {Lorch, Robert and Meng, Baoluo and Siu, Kit and Moitra, Abha and Durling, Michael and Paul, Saswata and Varanasi, Sarat Chandra and Mcmillan, Craig},
title = {Formal Methods in Requirements Engineering: Survey and Future Directions},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644373},
doi = {10.1145/3644033.3644373},
abstract = {Requirements engineering plays a pivotal role in the development of safety-critical systems. However, the process is usually a manual one and can lead to errors and inconsistencies in the requirements that are not easily detectable. Formal methods are mathematically rigorous techniques that can aid engineers to detect errors and produce consistent and correct requirements. We survey a variety of requirements capture and analysis tools presented in the literature. Specifically, we focus on tools that incorporate formal methods techniques into their analyses. We discuss the various tools' strengths and weaknesses, identify current trends in requirements engineering research, and highlight open research questions.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {88–99},
numpages = {12},
keywords = {requirements engineering, requirements analysis, formal methods},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1145/3643991.3644884,
author = {Silva, Andr\'{e} and Saavedra, Nuno and Monperrus, Martin},
title = {GitBug-Java: A Reproducible Benchmark of Recent Java Bugs},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644884},
doi = {10.1145/3643991.3644884},
abstract = {Bug-fix benchmarks are essential for evaluating methodologies in automatic program repair (APR) and fault localization (FL). However, existing benchmarks, exemplified by Defects4J, need to evolve to incorporate recent bug-fixes aligned with contemporary development practices. Moreover, reproducibility, a key scientific principle, has been lacking in bug-fix benchmarks. To address these gaps, we present GitBug-Java, a reproducible benchmark of recent Java bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of 55 notable open-source repositories. The methodology for building GitBug-Java ensures the preservation of bug-fixes in fully-reproducible environments. We publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {118–122},
numpages = {5},
keywords = {software bugs, bug benchmark, reproducibility, bug database, Java benchmark, software testing, program analysis},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1109/ICSE48619.2023.00059,
author = {An, Gabin and Hong, Jingun and Kim, Naryeong and Yoo, Shin},
title = {Fonte: Finding Bug Inducing Commits from Failures},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00059},
doi = {10.1109/ICSE48619.2023.00059},
abstract = {A Bug Inducing Commit (BIC) is a commit that introduces a software bug into the codebase. Knowing the relevant BIC for a given bug can provide valuable information for debugging as well as bug triaging. However, existing BIC identification techniques are either too expensive (because they require the failing tests to be executed against previous versions for bisection) or inapplicable at the debugging time (because they require post hoc artefacts such as bug reports or bug fixes). We propose Fonte, an efficient and accurate BIC identification technique that only requires test coverage. Fonte combines Fault Localisation (FL) with BIC identification and ranks commits based on the suspiciousness of the code elements that they modified. Fonte reduces the search space of BICs using failure coverage as well as a filter that detects commits that are merely style changes. Our empirical evaluation using 130 real-world BICs shows that Fonte significantly outperforms state-of-the-art BIC identification techniques based on Information Retrieval as well as neural code embedding models, achieving at least 39% higher MRR. We also report that the ranking scores produced by Fonte can be used to perform weighted bisection, further reducing the cost of BIC identification. Finally, we apply Fonte to a large-scale industry project with over 10M lines of code, and show that it can rank the actual BIC within the top five commits for 87% of the studied real batch-testing failures, and save the BIC inspection cost by 32% on average.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {589–601},
numpages = {13},
keywords = {batch testing, weighted bisection, git, fault localisation, bug inducing commit},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3639793,
author = {Wallner, Felix},
title = {Learning Models of Cyber-Physical Systems with Discrete and Continuous Behaviour for Digital Twin Synthesis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639793},
doi = {10.1145/3639478.3639793},
abstract = {Digital twins are used to simulate (cyber-physical) systems and offer great benefits for testing and verification. The importance of quickly and efficiently constructing digital twins increases with the appearance of devices of greater complexity. Furthermore, the more (varied) behaviour the digital twin captures of the simulated device the more use cases it can be used for. In the presented thesis we investigate methods from automata learning and machine learning to automatically synthesise digital twins from cyber-physical systems, capturing both discrete and continuous behaviour. Our aim hereby is to combine methods from both fields and utilize their respective strengths to build better digital twins from cyber-physical systems in practice. We already developed an algorithm that learns discrete behavioural models even in the presence of noisy data.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {192–194},
numpages = {3},
keywords = {digital twin, hybrid system, automata learning, machine learning, cyber-physical system},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643662.3643957,
author = {Xia, Boming and Zhang, Dawen and Liu, Yue and Lu, Qinghua and Xing, Zhenchang and Zhu, Liming},
title = {Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643957},
doi = {10.1145/3643662.3643957},
abstract = {The robustness of critical infrastructure systems is contingent upon the integrity and transparency of their software supply chains. A Software Bill of Materials (SBOM) is pivotal in this regard, offering an exhaustive inventory of components and dependencies crucial to software development. However, prevalent challenges in SBOM sharing, such as data tampering risks and vendors' reluctance to fully disclose sensitive information, significantly hinder its effective implementation. These challenges pose a notable threat to the security of critical infrastructure and systems where transparency and trust are paramount, underscoring the need for a more secure and flexible mechanism for SBOM sharing. To bridge the gap, this study introduces a blockchain-empowered architecture for SBOM sharing, leveraging verifiable credentials to allow for selective disclosure. This strategy not only heightens security but also offers flexibility. Furthermore, this paper broadens the remit of SBOM to encompass AI systems, thereby coining the term AI Bill of Materials (AIBOM). The advent of AI and its application in critical infrastructure necessitates a nuanced understanding of AI software components, including their origins and interdependencies. The evaluation of our solution indicates the feasibility and flexibility of the proposed SBOM sharing mechanism, positing a solution for safeguarding (AI) software supply chains, which is essential for the resilience and reliability of modern critical infrastructure systems.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {12–19},
numpages = {8},
keywords = {software bill of materials, verifiable credential, selective disclosure},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1145/3643794.3648283,
author = {Trabelsi, Imen and Popa, Bianca and Pereyrol, Jeremie and Beaulieu, Pier-Olivier and Moha, Naouel},
title = {MicroMatic: Fully Automated Microservices Identification Approach From Monolithic Systems},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648283},
doi = {10.1145/3643794.3648283},
abstract = {The Internet of Things (IoT) revolution is transforming system interactions and functionalities, necessitating more adaptable, scalable, and responsive systems architectures. These IoT systems build on recent advances in software architectures, particularly Microservices Architecture (MSA), enabling scalability, facilitating cloud deployment, and supporting seamless integration with DevOps practices. While new IoT applications can seamlessly integrate Microservices Architecture from their design, the migration of existing monolithic IoT systems to MSA is essential to leverage its benefits yet it remains a challenging and costly process. To facilitate this migration, we propose MicroMatic, a tooled fully automated microservices identification approach that is based on static-relationship analyses between code elements as well as semantic analyses of the source code. Our approach relies on Machine Learning (ML) techniques and uses service types to guide the identification of microservices from IoT monolithic systems. We validate the effectiveness of our tool through a detailed case study, comparing our results with established ground truths. This process included a quantitative evaluation of the microservices generated, focusing on their business capabilities. Our findings demonstrate the efficiency of MicroMatic in automating one of the most labour-intensive aspects of migrating legacy systems to a microservices framework, successfully identifying architecturally significant microservices with 62.5% precision and 45.5% recall.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {7–13},
numpages = {7},
keywords = {migration, microservices, service types, machine learning, semantic analysis, automation, tool},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@proceedings{10.1145/3643658,
title = {GAS '24: Proceedings of the ACM/IEEE 8th International Workshop on Games and Software Engineering},
year = {2024},
isbn = {9798400705618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GAS is an annual workshop that brings together researchers and practitioners who are keen on exchanging ideas and progressing techniques in the intersection of game engineering and software engineering.GAS explores how advanced technologies can be used to benefit the engineering of gameful systems, including entertainment games, serious games, and gamified applications. The goal of this one-day workshop is to bring together the greater community of software engineers and game engineers to encourage discussions from an interdisciplinary perspective, on the emerging research challenges around game and software engineering.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639478.3639802,
author = {Qiu, Ketai},
title = {Autonomic Testing: Testing with Scenarios from Production},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639802},
doi = {10.1145/3639478.3639802},
abstract = {My PhD addresses the problem of detecting field failures with a new approach to test software systems under conditions that emerge only in production. Ex-vivo approaches detect field failures by executing the software system in the testbed with data extracted from the production environment. In-vivo approaches execute the available test suites in the production environment. We will define autonomic testing that detects conditions that emerge only in production scenarios, generates test cases for the new conditions, and executes the generated test cases in the new scenarios, to detect failures before they occur in production.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {156–158},
numpages = {3},
keywords = {autonomic testing, failure detection, test generation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00028,
author = {Barr, Earl and Bell, Jonathan and Hilton, Michael and Mechtaev, Sergey and Timperley, Christopher},
title = {Continuously Accelerating Research},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00028},
doi = {10.1109/ICSE-NIER58687.2023.00028},
abstract = {Science is facing a software reproducibility crisis. Software powers experimentation, and fuels insights, yielding new scientific contributions. Yet, the research software is often difficult for other researchers to reproducibly run. Beyond reproduction, research software that is truly reusable will speed science by allowing other researchers to easily build upon and extend prior work. As software engineering researchers, we believe that it is our duty to create tools and processes that instill reproducibility, reusability, and extensibility into research software. This paper outlines a vision for a community infrastructure that will bring the benefits of continuous integration to scientists developing research software. To persuade researchers to adopt this infrastructure, we will appeal to their self-interest by making it easier for them to develop and evaluate research prototypes. Building better research software is a complex socio-technical problem that requires stakeholders to join forces to solve this problem for the software engineering community, and the greater scientific community. This vision paper outlines an agenda for realizing a world where the reproducibility and reusability barriers in research software are lifted, continuously accelerating research.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {123–128},
numpages = {6},
keywords = {containers, scientific software, continuous integration, artifact evaluation, reproducibility},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE48619.2023.00193,
author = {Liu, Jinyang and He, Shilin and Chen, Zhuangbin and Li, Liqun and Kang, Yu and Zhang, Xu and He, Pinjia and Zhang, Hongyu and Lin, Qingwei and Xu, Zhangwei and Rajmohan, Saravan and Zhang, Dongmei and Lyu, Michael R.},
title = {Incident-Aware Duplicate Ticket Aggregation for Cloud Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00193},
doi = {10.1109/ICSE48619.2023.00193},
abstract = {In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871~0.935 and outperforming state-of-the-art methods by 12.4%~31.2%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2299–2311},
numpages = {13},
keywords = {cloud systems, incidents, duplicate tickets},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00049,
author = {Shen, Kedi and Zhang, Yun and Bao, Lingfeng and Wan, Zhiyuan and Li, Zhuorong and Wu, Minghui},
title = {Patchmatch: A Tool for Locating Patches of Open Source Project Vulnerabilities},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00049},
doi = {10.1109/ICSE-Companion58688.2023.00049},
abstract = {With the rapid development of open source projects, the continuous emergence of vulnerabilities in the project brings great challenges to the security of the project. Security patches are one of the best ways to deal with vulnerabilities, but are not well applied currently. Although there are sites like CVE/NVD that provide information about vulnerabilities, many of the vulnerabilities disclosed by CVE/NVD are not accompanied by security patches. This makes it difficult for developers to apply patches. In the present study, a sorting method based on extracting multidimensional features from auxiliary information in CVE/NVD was proposed. And we made a further step, we proposed VCmatch, a model for mining semantic information in vulnerability description and code commit messages, which has good recall rate and applicability across projects. On this basis, we established Patchmatch, a tool for helping developers to quickly locate patches. Given a vulnerability, Patchmatch can forecast the implicit patches in the code repository's commits. Patchmatch also has a visual webpage for information statistics and a display web page to help developers manage all kinds of information in the code repository. A demo video of Patch-match is at https://www.youtube.com/watch?v=nOBSMFtZV8A. Patchmatch is in https://github.com/Sklud1456/patchmatch.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {175–179},
numpages = {5},
keywords = {manage tool, model application, vulnerability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3643530,
author = {Monjezi, Verya and Kumar, Ashish and Tan, Gang and Trivedi, Ashutosh and Tizpaz-Niari, Saeid},
title = {Causal Graph Fuzzing for Fair ML Sofware Development},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643530},
doi = {10.1145/3639478.3643530},
abstract = {Machine learning (ML) is increasingly used in high-stakes areas like autonomous driving, finance, and criminal justice. However, it often unintentionally perpetuates biases against marginalized groups. To address this, the software engineering community has developed fairness testing and debugging methods, establishing best practices for fair ML software. These practices focus on training model design, including the selection of sensitive and non-sensitive attributes and hyperparameter configuration. However, the application of these practices across different socio-economic and cultural contexts is challenging, as societal constraints vary.Our study proposes a search-based software engineering approach to evaluate the robustness of these fairness practices. We formulate these practices as the first-order logic properties and search for two neighborhood datasets where the practice satisfies in one dataset, but fail in the other one. Our key observation is that these practices should be general and robust to various uncertainty such as noise, faulty labeling, and demographic shifts. To generate datasets, we sift to the causal graph representations of datasets and apply perturbations over the causal graphs to generate neighborhood datasets. In this short paper, we show our methodology using an example of predicting risks in the car insurance application.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {402–403},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643796.3648455,
author = {Trofimov, Artem and Kostyukov, Mikhail and Ugdyzhekov, Sergei and Ponomareva, Natalia and Naumov, Igor and Melekhovets, Maksim},
title = {JetTrain: IDE-Native Machine Learning Experiments},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648455},
doi = {10.1145/3643796.3648455},
abstract = {Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {59–61},
numpages = {3},
keywords = {integrated development environment, machine learning, MLOps},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3643664.3648206,
author = {Runeson, Per and Soderberg, Emma and Host, Martin},
title = {A Conceptual Framework and Recommendations for Open Data and Artifacts in Empirical Software Engineering},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648206},
doi = {10.1145/3643664.3648206},
abstract = {Background. Open science aims to improve research accessibility, replicability, and consequently its quality. Empirical software engineering entails both data and artifacts, which may be shared more or less openly, to support transparency. However, the trade-offs involved in balancing the openness against integrity and secrecy concerns need methodological guidance. Aim. We aim to derive such advice, based on our own experiences from a research project, in the field of gaze-assisted code reviews - the Gander case. Method. We draw on literature about open data and artifacts in socio-technical research. Next, we describe our case project and derive a conceptual framework of steps in research data analysis and artifact development, using our data and artifacts as illustrating examples. Results. The conceptual framework contains 1) a categorization of humans involved as participants and their concerns, 2) four steps for data analysis, each resulting in corresponding data and meta-data, and 3) three steps of artifact distribution, matching different levels of openness. We derive a preliminary set of recommendations for open science practices for data and artifacts. Conclusion. The conceptual framework has proven useful in summarizing and discussing data and artifacts in the studied case project. We envision that the framework and recommendations will provide a foundation for further advancement of open science research practices in empirical, socio-technical software engineering.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {68–75},
numpages = {8},
keywords = {open science, open research, open data, FAIR data, open artifacts, socio-technical software engineering},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@inproceedings{10.1145/3643659.3643931,
author = {Khatiri, Sajad and Saurabh, Prasun and Zimmermann, Timothy and Munasinghe, Charith and Birchler, Christian and Panichella, Sebastiano},
title = {SBFT Tool Competition 2024 - CPS-UAV Test Case Generation Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643931},
doi = {10.1145/3643659.3643931},
abstract = {While simulation-based testing is critical for ensuring the safety of autonomous Unmanned Aerial Vehicles (UAVs), it has not been adequately researched yet. The UAV Testing Competition organized by the Search-Based and Fuzz Testing (SBFT) workshop is an initiative designed to inspire and encourage the software testing Community to direct their attention toward UAVs as a rapidly emerging and crucial domain. It provides a simple software platform and case study to facilitate their onboarding in the UAV domain and help them develop their first test generation tools for UAVs.In this first edition of the competition, 7 tools were submitted, evaluated, and compared extensively against each other and the baseline approach. We evaluated their test generation performance for 6 different case studies using our novel benchmarking infrastructure. The generated test suites were scored and ranked based on the number and severity of the revealed faults, and the complexity, diversity, and execution time of the test cases. This paper describes the competition context, its platform, the competing tools, and the evaluation process and results.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {29–32},
numpages = {4},
keywords = {tool competition, software testing, test case generation, unmanned aerial vehicles, search based software engineering},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3639477.3639727,
author = {Zmigrod, Ran and Alamir, Salwa and Liu, Xiaomo},
title = {Translating between SQL Dialects for Cloud Migration},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639727},
doi = {10.1145/3639477.3639727},
abstract = {Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate 100% of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and so more innovative solutions are required. We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {189–191},
numpages = {3},
keywords = {cloud migration, SQL, code translation},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/SVM66695.2025.00006,
author = {Basu, Srijita and Staron, Miroslaw},
title = {Understanding the Changing Landscape of Automotive Software Vulnerabilities: Insights from a Seven-Year Analysis},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00006},
doi = {10.1109/SVM66695.2025.00006},
abstract = {The automotive industry has experienced a drastic transformation in the past few years when vehicles got connected to the internet. Nowadays, connected vehicles require complex architecture and interdependent functionalities, facilitating modern lifestyles and their needs. As a result, automotive software has shifted from "just embedded system/SoC (System on Chip)" to a more hybrid platform, which includes software for web/mobile applications, cloud, simulation, infotainment, etc. Automatically, the security concerns for automotive software have also developed accordingly. This paper presents a study on automotive vulnerabilities from 2018 to September 2024, i.e., the last seven years, intending to understand and report the noticeable changes in their pattern/trend. 1,663 automotive software vulnerabilities were found to have been reported in the studied time frame. The study reveals the Common Weakness Enumeration (CWE) associated with these vulnerabilities develop over time and how different parts of the automotive ecosystem are exposed to these CWEs. Our study provides the platform to understand the automotive software weaknesses and loopholes and paves the way for identifying the phases in the software development lifecycle where the vulnerability was introduced. Our findings are a step forward to support vulnerability management in automotive software across its entire life cycle.},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {9–16},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1145/3528588.3528662,
author = {Izadi, Maliheh},
title = {CatIss: an intelligent tool for categorizing issues reports using transformers},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528662},
doi = {10.1145/3528588.3528662},
abstract = {Users use Issue Tracking Systems to keep track and manage issue reports in their repositories. An issue is a rich source of software information that contains different reports including a problem, a request for new features, or merely a question about the software product. As the number of these issues increases, it becomes harder to manage them manually. Thus, automatic approaches are proposed to help facilitate the management of issue reports. This paper describes CatIss, an automatic Categorizer of Issue reports which is built upon the Transformer-based pre-trained RoBERTa model. CatIss classifies issue reports into three main categories of Bug report, Enhancement/feature request, and Question. First, the datasets provided for the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on about 80 thousand issue reports from GitHub, indicates that it performs very well surpassing the competition baseline, TicketTagger, and achieving 87.2% F1-score (micro average). Additionally, as CatIss is trained on a wide set of repositories, it is a generic prediction model, hence applicable for any unseen software project or projects with little historical data. Scripts for cleaning the datasets, training CatIss and evaluating the model are publicly available. 1},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {44–47},
numpages = {4},
keywords = {transformers, repositories, natural language processing, machine learning, issue report management, classification},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3528231.3528357,
author = {Tisha, Sirazum Munira and Oregon, Rufino A. and Baumgartner, Gerald and Alegre, Fernando and Moreno, Juana},
title = {An automatic grading system for a high school-level computational thinking course},
year = {2023},
isbn = {9781450393362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528231.3528357},
doi = {10.1145/3528231.3528357},
abstract = {Automatic grading systems help lessen the load of manual grading. Most existent autograders are based on unit testing, which focuses on the correctness of the code, but has limited scope for judging code quality. Moreover, it is cumbersome to implement unit testing for evaluating graphical output code. We propose an autograder that can effectively judge the code quality of the visual output codes created by students enrolled in a high school-level computational thinking course. We aim to provide suggestions to teachers on an essential aspect of their grading, namely the level of student competency in using abstraction within their codes. A dataset from five different assignments, including open-ended problems, is used to evaluate the effectiveness of our autograder. Our initial experiments show that our method can classify the students' submissions even for open-ended problems, where existing autograders fail to do so. Additionally, survey responses from course teachers support the importance of our work.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Education for the Next Generation},
pages = {20–27},
numpages = {8},
keywords = {code quality, lexical analysis, machine learning, open-ended problems},
location = {Pittsburgh, Pennsylvania},
series = {SEENG '22}
}

@inproceedings{10.1145/3639478.3643524,
author = {Kumar, Jahnavi and Chimalakonda, Sridhar},
title = {On the Need for Empirically Investigating Fast-Growing Programming Languages},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643524},
doi = {10.1145/3639478.3643524},
abstract = {The research community has extensively explored various aspects of programming languages (PLs) such as developer sentiment analysis, ecosystem dynamics, pull request trends, energy consumption, and more. Existing research has predominantly focused on "Popular PLs" with the largest unique user bases which considers users since the language's inception. However, our approach shifts to examining "Fast Growing Languages" (FGLs) - those experiencing significant growth in their user base in recent years. We see this perspective as an essential exploration for understanding the current developer interest in both established and emerging languages. In this short note, we discuss (i) An empirical study on understanding how developers perceive these FGLs, analyzing their emotions through 1.86M Github comments (ii) Discuss potential future research on fast-growing PLs at the cross-section of programming languages and software engineering.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {372–373},
numpages = {2},
keywords = {developer emotion analysis, programming languages},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00066,
author = {Yin, Likang and Zhang, Xiyu and Filkov, Vladimir},
title = {On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00066},
doi = {10.1109/ICSE48619.2023.00066},
abstract = {Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability?From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {678–689},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639478.3640023,
author = {Saavedra, Nuno and Silva, Andr\'{e} and Monperrus, Martin},
title = {GitBug-Actions: Building Reproducible Bug-Fix Benchmarks with GitHub Actions},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640023},
doi = {10.1145/3639478.3640023},
abstract = {Bug-fix benchmarks are fundamental in advancing various sub-fields of software engineering such as automatic program repair (APR) and fault localization (FL). A good benchmark must include recent examples that accurately reflect technologies and development practices of today. To be executable in the long term, a benchmark must feature test suites that do not degrade overtime due to, for example, dependencies that are no longer available. Existing benchmarks fail in meeting both criteria. For instance, Defects4J, one of the foremost Java benchmarks, last received an update in 2020. Moreover, full-reproducibility has been neglected by the majority of existing benchmarks. In this paper, we present GitBug-Actions: a novel tool for building bug-fix benchmarks with modern and fully-reproducible bug-fixes. GitBug-Actions relies on the most popular CI platform, GitHub Actions, to detect bug-fixes and smartly locally execute the CI pipeline in a controlled and reproducible environment. To the best of our knowledge, we are the first to rely on GitHub Actions to collect bug-fixes. To demonstrate our toolchain, we deploy GitBug-Actions to build a proof-of-concept Go bug-fix benchmark containing executable, fully-reproducible bug-fixes from different repositories. A video demonstrating GitBug-Actions is available at: https://youtu.be/aBWwa1sJYBs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {software bugs, bug benchmark, bug database, reproducibility, software testing, program analysis, github actions},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643915.3644105,
author = {Marda, Arya and Kulkarni, Shubham and Vaidhyanathan, Karthik},
title = {SWITCH: An Exemplar for Evaluating Self-Adaptive ML-Enabled Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644105},
doi = {10.1145/3643915.3644105},
abstract = {Addressing runtime uncertainties in Machine Learning-Enabled Systems (MLS) is crucial for maintaining Quality of Service (QoS). The Machine Learning Model Balancer is a concept that addresses these uncertainties by facilitating dynamic ML model switching, showing promise in improving QoS in MLS. Leveraging this concept, this paper introduces SWITCH, an exemplar developed to enhance self-adaptive capabilities in such systems through dynamic model switching in runtime. SWITCH is designed as a comprehensive web service catering to a broad range of ML scenarios, with its implementation demonstrated through an object detection use case. SWITCH provides researchers with a flexible platform to apply and evaluate their ML model switching strategies, aiming to enhance QoS in MLS. SWITCH features advanced input handling, real-time data processing, and logging for adaptation metrics supplemented with an interactive real-time dashboard for enhancing system observability. This paper details SWITCH's architecture, self-adaptation strategies through ML model switching, and its empirical validation through a case study, illustrating its potential to improve QoS in MLS. By enabling a hands-on approach to explore adaptive behaviors in ML systems, SWITCH contributes a valuable tool to the SEAMS community for research into self-adaptive mechanisms for MLS and their practical applications.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {143–149},
numpages = {7},
keywords = {self-adaptation, self adaptive systems, machine learning-enabled systems, exemplar, web service, object detection},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3597503.3608136,
author = {Niu, Changan and Li, Chuanyi and Ng, Vincent and Lo, David and Luo, Bin},
title = {FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608136},
doi = {10.1145/3597503.3608136},
abstract = {While the majority of existing pre-trained models from code learn source code features such as code tokens and abstract syntax trees, there are some other works that focus on learning from compiler intermediate representations (IRs). Existing IR-based models typically utilize IR features such as instructions, control and data flow graphs (CDFGs), call graphs, etc. However, these methods confuse variable nodes and instruction nodes in a CDFG and fail to distinguish different types of flows, and the neural networks they use fail to capture long-distance dependencies and have over-smoothing and over-squashing problems. To address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained model for IR that involves employing (1) a novel input representation of IR programs; (2) Graph Transformer to address over-smoothing, over-squashing and long-dependencies problems; and (3) five pre-training tasks that we specifically propose to enable FAIR to learn the semantics of IR tokens, flow type information, and the overall representation of IR. Experimental results show that FAIR can achieve state-of-the-art results on four code-related downstream tasks.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {33},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639151,
author = {Sun, Yang and Poskitt, Christopher M. and Zhang, Xiaodong and Sun, Jun},
title = {REDriver: Runtime Enforcement for Autonomous Vehicles},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639151},
doi = {10.1145/3597503.3639151},
abstract = {Autonomous driving systems (ADSs) integrate sensing, perception, drive control, and several other critical tasks in autonomous vehicles, motivating research into techniques for assessing their safety. While there are several approaches for testing and analysing them in high-fidelity simulators, ADSs may still encounter additional critical scenarios beyond those covered once they are deployed on real roads. An additional level of confidence can be established by monitoring and enforcing critical properties when the ADS is running. Existing work, however, is only able to monitor simple safety properties (e.g., avoidance of collisions) and is limited to blunt enforcement mechanisms such as hitting the emergency brakes. In this work, we propose REDriver, a general and modular approach to runtime enforcement, in which users can specify a broad range of properties (e.g., national traffic laws) in a specification language based on signal temporal logic (STL). REDriver monitors the planned trajectory of the ADS based on a quantitative semantics of STL, and uses a gradient-driven algorithm to repair the trajectory when a violation of the specification is likely. We implemented REDriver for two versions of Apollo (i.e., a popular ADS), and subjected it to a benchmark of violations of Chinese traffic laws. The results show that REDriver significantly improves Apollo's conformance to the specification with minimal overhead.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {176},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3648505,
title = {ExEn '24: Proceedings of the 2024 Workshop on Explainability Engineering},
year = {2024},
isbn = {9798400705960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the International Workshop on Explainability Engineering (ExEn). This workshop is the successor of the International Workshop on Requirements Engineering for Explainable Systems (RE4ES), which was hosted at the IEEE International Requirements Engineering Conference 2021-2023.We aim to advance software engineering for explainable systems, foster interdisciplinary exchange, and build a community. This workshop edition consists of a mix of paper presentations from authors of different domains, one keynote as well as interactive activities to stimulate lively discussions.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1109/SVM66695.2025.00007,
author = {Li, Tianyu and Lu, Chaomeng and Lagaisse, Bert},
title = {A Multi-Dimensional Visual Analytics Tool for the Security Posture of Open-Source Software},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00007},
doi = {10.1109/SVM66695.2025.00007},
abstract = {Open-source software is widely used by developers and businesses, but assessing its security posture is challenging due to the lack of time and specialized expertise. Existing visual security analysis tools for open-source projects primarily focus on vulnerabilities within the source code, lacking a comprehensive assessment of the project’s overall security posture. To address these issues, we propose a multi-dimensional visual analytics tool for evaluating the security posture of open-source projects. Our tool integrates data from code commits, contributor activity, and historical vulnerability duration, providing a holistic view of project security.Our tool integrates data from multiple sources, including the National Vulnerability Database (NVD) and GitHub commit histories, and applies the SZZ algorithm to identify both vulnerability-fixing and inducing commits. We tested the dashboard on two popular GitHub projects, each containing thousands of commits and hundreds of vulnerabilities, allowing users to easily track development and vulnerability management within each project. An evaluation study with experienced developers confirmed the dashboard’s effectiveness in helping users quickly understand developer interactions and the project’s overall approach to security management. Our contributions include a diverse vulnerability dataset and a visual dashboard that offers a multi-dimensional perspective on open-source project security, meeting the needs of various stakeholders.},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {17–24},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1109/ICSE48619.2023.00014,
author = {Liu, Zhongxin and Tang, Zhijie and Xia, Xin and Yang, Xiaohu},
title = {CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00014},
doi = {10.1109/ICSE48619.2023.00014},
abstract = {Representing code changes as numeric feature vectors, i.e., code change representations, is usually an essential step to automate many software engineering tasks related to code changes, e.g., commit message generation and just-in-time defect prediction. Intuitively, the quality of code change representations is crucial for the effectiveness of automated approaches. Prior work on code changes usually designs and evaluates code change representation approaches for a specific task, and little work has investigated code change encoders that can be used and jointly trained on various tasks. To fill this gap, this work proposes a novel Code Change Representation learning approach named CCRep, which can learn to encode code changes as feature vectors for diverse downstream tasks. Specifically, CCRep regards a code change as the combination of its before-change and after-change code, leverages a pre-trained code model to obtain high-quality contextual embeddings of code, and uses a novel mechanism named query back to extract and encode the changed code fragments and make them explicitly interact with the whole code change. To evaluate CCRep and demonstrate its applicability to diverse code-change-related tasks, we apply it to three tasks: commit message generation, patch correctness assessment, and just-in-time defect prediction. Experimental results show that CCRep outperforms the state-of-the-art techniques on each task.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {17–29},
numpages = {13},
keywords = {just-in-time defect prediction, patch correctness assessment, commit message generation, representation learning, code change},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00084,
author = {Santos, Fabio},
title = {Skill Recommendation for New Contributors in Open-Source Software},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00084},
doi = {10.1109/ICSE-Companion58688.2023.00084},
abstract = {Selecting an appropriate task is challenging for newcomers to Open Source Software (OSS) projects. Therefore, researchers and OSS projects have proposed strategies to label tasks (a.k.a. issues). Several approaches relying on machine learning techniques, historical information, and textual analysis have been submitted. However, the results vary, and these approaches are still far from mainstream adoption, possibly because of a lack of good predictors. Inspired by previous research, we advocate that the prediction models might benefit from leveraging social metrics.In this research, we investigate how to assist the new contributors in finding a task when onboarding a new project. To achieve our goal, we predict the skills needed to solve an open issue by labeling them with the categories of APIs declared in the source code (API-domain labels) that should be updated or implemented. Starting from a case study using one project and an empirical experiment, we found the API-domain labels were relevant to select an issue for a contribution. In the sequence, we investigated employing interviews and a survey of what strategies maintainers the strategies believe the communities have to adopt to assist the new contributors in finding a task. We also studied how maintainers think about new contributors' strategies to pick a task. We found maintainers, frequent contributors, and new contributors diverge about the importance of the communities and new contributors' strategies.The ongoing research works in three directions: 1) generalization of the approach, 2) Use of conversation data metrics for predictions, 3) Demonstration of the approach, and 4) Matching contributors and tasks skills.By addressing the lack of knowledge about the skills in tasks, we hope to assist new contributors in picking tasks with more confidence.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {311–313},
numpages = {3},
keywords = {ontology matching, machine learning, open-source software, social network analysis, mining software repositories, skills, labelling},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639165,
author = {Huang, Jianjun and Nie, Jianglei and Gong, Yuanjun and You, Wei and Liang, Bin and Bian, Pan},
title = {Raisin: Identifying Rare Sensitive Functions for Bug Detection},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639165},
doi = {10.1145/3597503.3639165},
abstract = {Mastering the knowledge about the bug-prone functions (i.e., sensitive functions) is important to detect bugs. Some automated techniques have been proposed to identify the sensitive functions in large software systems, based on machine learning or natural language processing. However, the existing statistics-based techniques are not directly applicable to a special kind of sensitive functions, i.e., the rare sensitive functions, which have very few invocations even in large systems. Unfortunately, the rare ones can also introduce bugs. Therefore, how to effectively identify such functions is a problem deserving attention.This study is the first to explore the identification of rare sensitive functions. We propose a context-based analogical reasoning technique to automatically infer rare sensitive functions. A 1+context scheme is devised, where a function and its context are embedded into a pair of vectors, enabling pair-wise analogical reasoning. Considering that the rarity of the functions may lead to low-quality embedding vectors, we propose a weighted subword embedding method that can highlight the semantics of the key subwords to facilitate effective embedding. In addition, frequent sensitive functions are utilized to filter out reasoning candidates. We implement a prototype called Raisin and apply it to identify the rare sensitive functions and detect bugs in large open-source code bases. We successfully discover thousands of previously unknown rare sensitive functions and detect 21 bugs confirmed by the developers. Some of the rare sensitive functions cause bugs even with a solitary invocation in the kernel. It is demonstrated that identifying them is necessary to enhance software reliability.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {175},
numpages = {12},
keywords = {rare sensitive function, bug detection, analogical reasoning, embedding},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643690.3648239,
author = {Turhan, Yagmur and Buehrle, Deborah and Herzwurm, Georg},
title = {Developing a Taxonomy for Agile Scaling Frameworks},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648239},
doi = {10.1145/3643690.3648239},
abstract = {In the last decade, various scalable agile frameworks have emerged to scale the benefits of agility to wider organizational levels and have become increasingly popular in practice. However, the variety of frameworks has not been widely understood yet. The aim of this paper is to provide a well-organized understanding of the primary dimensions and characteristics of these frameworks. To this end, a taxonomy for scaling agile frameworks was created utilising a systematic method for taxonomy development. The basis for the formal and content-related creation of the taxonomy is a systematic and focused literature review, followed by a qualitative text analysis to extract key framework characteristics, concentrating on the four agile scaling frameworks Scaled Agile Framework, Large-Scale Scrum, Disciplined Agile Delivery and Scrum@Scale. For the validation of the proposed taxonomy, an exemplary categorisation of 2 frameworks in the taxonomy was performed.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {40–47},
numpages = {8},
keywords = {scaled agile, scaled agile software development, scaled agile frameworks, SAFe, LeSS, S@S, DAD, taxonomy},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1145/3639477.3639729,
author = {Alshahwan, Nadia and Blasi, Arianna and Bojarczuk, Kinga and Ciancone, Andrea and Gucevska, Natalija and Harman, Mark and Krolikowski, Michal and Rojas, Rubmary and Martac, Dragos and Schellaert, Simon and Ustiuzhanina, Kate and Harper, Inna and Jia, Yue and Lewis, Will},
title = {Enhancing Testing at Meta with Rich-State Simulated Populations},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639729},
doi = {10.1145/3639477.3639729},
abstract = {This paper reports the results of the deployment of Rich-State Simulated Populations at Meta for both automated and manual testing. We use simulated users (aka test users) to mimic user interactions and acquire state in much the same way that real user accounts acquire state. For automated testing, we present empirical results from deployment on the Facebook, Messenger, and Instagram apps for iOS and Android Platforms. These apps consist of tens of millions of lines of code, communicating with hundreds of millions of lines of backend code, and are used by over 2 billion people every day. Our results reveal that rich state increases average code coverage by 38%, and endpoint coverage by 61%. More importantly, it also yields an average increase of 115% in the faults found by automated testing. The rich-state test user populations are also deployed in a (continually evolving) Test Universe; a web-enabled simulation platform for privacy-safe manual testing, which has been used by over 21,000 Meta engineers since its deployment in November 2022.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {1–12},
numpages = {12},
keywords = {software testing, cyber cyber digital twins, simulation-based testing, machine learning},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643663.3643964,
author = {Mey, Johannes and Podlubne, Ariel and Sch\"{o}ne, Ren\'{e} and Gottschaldt, Paul and G\"{o}hringer, Diana and A\ss{}mann, Uwe},
title = {Systematic Testing of a ROS Interface Specification Backend},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643663.3643964},
doi = {10.1145/3643663.3643964},
abstract = {Code generators are frequently used when language-independent specifications are compiled into client libraries to support multiple languages. One example is the message definition specification of the Robot Operating System (ROS). This work discusses how a configurable code generator for reconfigurable hardware built using a model-based toolchain based on attribute grammars is tested during development. It supports multiple input and output variants for different source and target languages. To ensure the correctness of all potentially generatable code, a modular test toolchain is provided that can be extended to support different client libraries. Using it, we can identify bugs concerning specification divergence of the tool under test for all current ROS distributions. In this work, we present insights obtained during the design and execution of the test system.},
booktitle = {Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
pages = {25–30},
numpages = {6},
keywords = {robot operating system, specification testing, code generation},
location = {Lisbon, Portugal},
series = {RoSE '24}
}

@inproceedings{10.1145/3643690.3648595,
author = {Kedziora, Damian and Siemon, Dominik and Elshan, Edona and So\'{n}ta, Monika},
title = {Towards stability, predictability, and quality of intelligent automation services: ECIT product journey from on-premise to as-a-service},
year = {2024},
isbn = {9798400705717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643690.3648595},
doi = {10.1145/3643690.3648595},
abstract = {The intensive transformations of software products have been widely discussed in academia and business from various perspectives, yet with limited reference to low-code technologies, such as robotic process automation (RPA). The intensive growth in the size and importance of RPA and low-code industry puts their offering transitions from traditional on-premise to modern software-as-a-service models at the core of its paradigm evolution. Our single case study presents the transformation from 'On-Prem' to 'RaaS-P product for intelligent automation (IA) services, conducted by the leading Nordic consultancy ECIT Group, elaborating on its triggers, journey, stakeholders, as well as implications on cost, quality, and customer satisfaction. Triggered by internal experiences, cost, and market pressures, the case allowed us to discover that while RPA technology has rapidly grown to become a commodity at a wide amount of business organizations, the RPA service providers need to aim at resolving the issues and addressing challenges of their customers, not technology itself. It brings forward the 'Robot as a Service - Process Automation' product, with a novel approach to SLA, focused on availability, job stability, recovery, and transaction quality.},
booktitle = {Proceedings of the 7th ACM/IEEE International Workshop on Software-Intensive Business},
pages = {15–23},
numpages = {9},
keywords = {robotic process automation, low code, business models},
location = {Lisbon, Portugal},
series = {IWSiB '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00025,
author = {Sun, Jiamou and Xing, Zhenchang and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
title = {A Multi-Faceted Vulnerability Searching Website Powered by Aspect-Level Vulnerability Knowledge Graph},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00025},
doi = {10.1109/ICSE-Companion58688.2023.00025},
abstract = {Vulnerabilities can cause damages to users. With heavy dependencies among software, it is particularly important to safely select the dependent libraries and maintain security of software in a targeted manner, which require deep understanding of potential weakness of third-party libraries. Current vulnerability advisories only support rough-level description-based vulnerability information searching, which cannot cater the needs of in-depth investigation and understanding of vulnerabilities. Driven by the real needs, we propose a vulnerability aspect-level vulnerability knowledge graph integrating diversified vulnerability key aspect information from heterogeneous vulnerability databases. Based on the knowledge graph, we implement a multi-faceted vulnerability searching website for statistics and details acquiring of vulnerabilities. Our use cases demonstrate the usefulness of our knowledge graph and website to the software security.Demo Video: https://youtu.be/vYSy7MYIU48Source Code: https://github.com/sjmsjmdsg/Multi_faceted_WebWebsite: see GitHub repository.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {60–63},
numpages = {4},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIS58686.2023.00019,
author = {Tizpaz-Niari, Saeid and Monjezi, Verya and Wagner, Morgan and Darian, Shiva and Reed, Krystia and Trivedi, Ashutosh},
title = {Metamorphic Testing and Debugging of Tax Preparation Software},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS58686.2023.00019},
doi = {10.1109/ICSE-SEIS58686.2023.00019},
abstract = {This paper presents a data-driven debugging framework to improve the trustworthiness of US tax preparation software systems. Given the legal implications of bugs in such software on its users, ensuring compliance and trustworthiness of tax preparation software is of paramount importance. The key barriers in developing debugging aids for tax preparation systems are the unavailability of explicit specifications and the difficulty of obtaining oracles. We posit that, since the US tax law adheres to the legal doctrine of precedent, the specifications about the outcome of tax preparation software for an individual taxpayer must be viewed in comparison with individuals that are deemed similar. Consequently, these specifications are naturally available as properties on the software requiring similar inputs provide similar outputs. Inspired by the metamorphic testing paradigm, we dub these relations metamorphic relations as they relate to structurally modified inputs.In collaboration with legal and tax experts, we explicated metamorphic relations for a set of challenging properties from various US Internal Revenue Services (IRS) publications including Form 1040 (U.S. Individual Income Tax Return), Publication 596 (Earned Income Tax Credit), Schedule 8812 (Qualifying Children and Other Dependents), and Form 8863 (Education Credits). While we focus on an open-source tax preparation software for our case study, the proposed framework can be readily extended to other commercial software. We develop a randomized test-case generation strategy to systematically validate the correctness of tax preparation software guided by metamorphic relations. We further aid this test-case generation by visually explaining the behavior of software on suspicious instances using easy-to-interpret decision-tree models. Our tool uncovered several accountability bugs with varying severity ranging from nonrobust behavior in corner-cases (unreliable behavior when tax returns are close to zero) to missing eligibility conditions in the updated versions of software.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
pages = {138–149},
numpages = {12},
location = {Melbourne, Australia},
series = {ICSE-SEIS '23}
}

@inproceedings{10.1145/3639474.3640055,
author = {Leist, Eleanor and Lee, Jaejoon},
title = {Adopting an Agile Approach for Reflective Learning and Teaching},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640055},
doi = {10.1145/3639474.3640055},
abstract = {Software engineering is concerned with how best to create software in ways that promote sustainable development and maximise quality. We have been largely successful at transferring software engineering knowledge into the industry, however, many challenges in software engineering training remain. A key amongst these is how best to teach practical engineering approaches along with the theoretical concepts behind them.This paper describes our experience of adopting an agile approach for reflective learning and teaching within the context of our Software Systems Engineering module, aimed at addressing challenges identified with previous efforts to promote reflective practice. Our study attempts to strengthen the use of reflective learning approaches for our current cohort, as well as introducing reflective teaching practices, whereby we examine our teaching approach in order to improve its efficiency and effectiveness. Our analysis of student response to the module shows that it was very well-received by the students, and we were able to collect ample evidence from feedback to support this. Most of our approaches resulted in positive feedback and contributed to improvements in teaching quality, however, we also identified some key aspects in our method that could still benefit from refinement, such as the need for explicit links between learning outcomes and workshop activities, and intuitive design of feedback questions, along with feedback collection frequency. We plan to incorporate these additional updates into the revision of the module for the next academic year, and to continue collecting and analysing feedback data for further enhancement.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {46–55},
numpages = {10},
keywords = {reflection, learning, teaching, Agile, feedback, gamification},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00046,
author = {Liu, Yu and Thurston, Zachary and Han, Alan and Nie, Pengyu and Gligoric, Milos and Legunsen, Owolabi},
title = {pytest-Inline: An Inline Testing Tool for Python},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00046},
doi = {10.1109/ICSE-Companion58688.2023.00046},
abstract = {We present pytest-inline, the first inline testing framework for Python. We recently proposed inline tests to make it easier to test individual program statements. But, there is no framework-level support for developers to write inline tests in Python. To fill this gap, we design and implement pytest-inline as a plugin for pytest, the most popular Python testing framework. Using pytest-inline, a developer can write an inline test by assigning test inputs to variables in a target statement and specifying the expected test output. Then, pytest-inline runs each inline test and fails if the target statement's output does not match the expected output. In this paper, we describe our design of pytest-inline, the testing features that it provides, and the intended use cases. Our evaluation on inline tests that we wrote for 80 target statements from 31 open-source Python projects shows that using pytest-inline incurs negligible overhead, at 0.012x. pytest-inline is integrated into the pytest-dev organization, and a video demo is at https://www.youtube.com/watch?v=pZgiAxR_uJg.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {161–164},
numpages = {4},
keywords = {pytest, Python, software testing, inline tests},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643655.3643875,
author = {Li, Jialong and Manzano, Wallace and Yamauchi, Takuto and Matsuyama, Nobuhiro and Nakagawa, Elisa Yumi and Tei, Kenji},
title = {Employing Discrete Controller Synthesis for Developing Systems-of-Systems Controllers},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643875},
doi = {10.1145/3643655.3643875},
abstract = {The increasing complexity and interconnectedness of diverse services have driven the integration of software systems across various sectors, leading to complex, interconnected systems referred to as Systems-of-Systems (SoS). The coordination of SoS behavior is crucial for their harmonious and efficient operations, ensuring the achievement of SoS missions. Current solutions for SoS coordination are often domain-specific, so lacking a domain-agnostic approach for adequately coordinating SoS and ensuring the SoS mission execution. Addressing this gap, our paper employs Discrete Controller Synthesis (DCS) to automatically generate SoS behavioral specifications, which can be used to the SoS control. The primary advantage of DCS lies in its formal guarantees and automated generation, offering a reliable and flexible control solution for complex and evolving SoS. Our contributions include formalizing procedures for employing DCS tailored to two distinct types of SoS and evaluating the applicability and scalability of DCS using three applications: flood monitoring, smart building, and space SoS.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {1–8},
numpages = {8},
keywords = {discrete controller synthesis, systems-of-systems, discrete event systems, linear temporal logic},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3639477.3639713,
author = {Ahmad, Mak and Geewax, J. J. and Macvean, Andrew and Karger, David and Ma, Kwan-Liu},
title = {API Governance at Scale},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639713},
doi = {10.1145/3639477.3639713},
abstract = {API Governance, the process of applying standardized sets of policies and guardrails to the design and development of APIs, has only grown in importance and prominence given the continued growth in APIs being produced. In this paper, we present an Action Research style approach to investigate and understand the utility of a multi-faceted API Governance process being adopted inside Google. We first reflect on past research around API Governance, and then introduce three new components, 1. API Improvement Proposals (AIPs) the documented source of truth for API design rules, 2. API Linter, an automated analysis tool which checks for adherence to / violations of AIPs, and 3. API Readability, a program to educate and certify API design experts. These three components are designed to build upon pre-existing processes to scale and improve API design. Through a mixed-methods research strategy, containing both a survey and a series of interviews, we evaluate the utility of these approaches in supporting API Producers. Our research shows that API Producers have positive sentiment towards API Governance, validating the general direction of the program. Specifically, our study participants highlighted the positive impact of API Governance on the quality of the APIs they produced, via consistency in both the outcome and approach. This paper also discusses future research opportunities to enhance API Governance, specifically with regards to newer API Producers, who reported worse sentiment towards the program than their more experienced peers.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {430–440},
numpages = {11},
keywords = {API governance, API design},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3641822.3641880,
author = {Alami, Adam and Ernst, Neil},
title = {Understanding the building blocks of accountability in software engineering},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641880},
doi = {10.1145/3641822.3641880},
abstract = {In the social and organizational sciences, accountability has been linked to the efficient operation of organizations. However, it has received limited attention in software engineering (SE) research, in spite of its central role in the most popular software development methods (e.g., Scrum). In this article, we explore the mechanisms of accountability in SE environments. We investigate the factors that foster software engineers' individual accountability within their teams through an interview study with 12 people. Our findings recognize two primary forms of accountability shaping software engineers individual senses of accountability: institutionalized and grassroots. While the former is directed by formal processes and mechanisms, like performance reviews, grassroots accountability arises organically within teams, driven by factors such as peers' expectations and intrinsic motivation. This organic form cultivates a shared sense of collective responsibility, emanating from shared team standards and individual engineers' inner commitment to their personal, professional values, and self-set standards. While institutionalized accountability relies on traditional "carrot and stick" approaches, such as financial incentives or denial of promotions, grassroots accountability operates on reciprocity with peers and intrinsic motivations, like maintaining one's reputation in the team.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {153–163},
numpages = {11},
keywords = {accountability, human aspects of software engineering, qualitative methods, interviews},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1145/3597503.3639097,
author = {Murphy-Hill, Emerson and Elizondo, Alberto and Murillo, Ambar and Harbach, Marian and Vasilescu, Bogdan and Carlson, Delphine and Dessloch, Florian},
title = {GenderMag Improves Discoverability in the Field, Especially for Women: An Multi-Year Case Study of Suggest Edit, a Code Review Feature},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639097},
doi = {10.1145/3597503.3639097},
abstract = {Prior research shows that the GenderMag method can help identify and address usability barriers that are more likely to affect women software users than men. However, the evidence for the effectiveness of GenderMag is limited to small lab studies. In this case study, by combining self-reported gender data from tens of thousands of users of an internal code review tool with software logs data gathered over a five-year period, we quantitatively show that GenderMag helped a team at Google (a) correctly identify discoverability as a usability barrier more likely to affect women than men, and (b) increase discoverability by 2.4x while also achieving gender parity. That is, compared to men using the original code review tool, women and men using the system redesigned with GenderMag were both 2.4x more likely to discover the "Suggest Edit" feature at any given time. Thus, this paper contributes the first large-scale evidence of the effectiveness of GenderMag in the field.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {189},
numpages = {12},
keywords = {software features, feature discovery, UX design, gender, inclusion},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643915.3644095,
author = {Carwehl, Marc and Imrie, Calum and Vogel, Thomas and Rodrigues, Gena\'{\i}na and Calinescu, Radu and Grunske, Lars},
title = {Formal Synthesis of Uncertainty Reduction Controllers},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644095},
doi = {10.1145/3643915.3644095},
abstract = {In its quest for approaches to taming uncertainty in self-adaptive systems (SAS), the research community has largely focused on solutions that adapt the SAS architecture or behaviour in response to uncertainty. By comparison, solutions that reduce the uncertainty affecting SAS (other than through the blanket monitoring of their components and environment) remain underexplored. Our paper proposes a more nuanced, adaptive approach to SAS uncertainty reduction. To that end, we introduce a SAS architecture comprising an uncertainty reduction controller that drives the adaptive acquisition of new information within the SAS adaptation loop, and a tool-supported method that uses probabilistic model checking to synthesise such controllers. The controllers generated by our method deliver optimal trade-offs between SAS uncertainty reduction benefits and new information acquisition costs. We illustrate the use and evaluate the effectiveness of our approach for mobile robot navigation and server infrastructure management SAS.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {2–13},
numpages = {12},
keywords = {controller synthesis, uncertainty, self-adaptive systems},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00034,
author = {Luo, Linghui and Mukherjee, Rajdeep and Tripp, Omer and Sch\"{a}f, Martin and Zhou, Qiang and Sanchez, Daniel},
title = {Long-Term Static Analysis Rule Quality Monitoring Using True Negatives},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00034},
doi = {10.1109/ICSE-SEIP58684.2023.00034},
abstract = {Static application security testing (SAST) tools have found broad adoption in modern software development workflows. These tools employ a variety of static analysis rules to generate recommendations on how to improve the code of an application.Every recommendation consumes the time of the engineer that is investigating it, so it is important to measure how useful these rules are in the long term. But what is a good metric for monitoring rule quality over time? Counting the number of recommendations rewards noisy rules and ignores developers' reactions. Measuring fix rate is not ideal either, because it overemphasizes rules that are easy to fix.In this paper, we report on an experiment where we use the frequency of true negatives to quantify if developers are able to learn a static analysis rule. We consider a static analysis rule to be ideal if its recommendations are not only addressed, but also internalized by the developer in a way that prevents the bug from recurring. That is, the rule contributes to code quality not only at present, but also in the future. We measure how often developers produce true negatives, that is, code changes that are relevant to a rule but do not trigger a recommendation, and we compare true-negative rate against other metrics. Our results show that measuring true negatives provides insights that cannot be provided by metrics such as fix rate or developer feedback.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {315–326},
numpages = {12},
keywords = {static analysis, software security},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3643916.3644433,
author = {Aldndni, Waad and Servant, Francisco and Meng, Na},
title = {Understanding the Impact of Branch Edit Features for the Automatic Prediction of Merge Conflict Resolutions},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644433},
doi = {10.1145/3643916.3644433},
abstract = {Developers regularly have to resolve merge conflicts, i.e., two conflicting sets of changes to the same files in different branches, which can be tedious and error-prone. To resolve conflicts, developers typically: keep the local version (KL) or the remote version (KR) of the code. They also sometimes manually edit both versions into a single one (ME). However, most existing techniques only support merging the local and remote versions (the ME strategy).We recently proposed RPredictor, a machine learning-based approach to support developers in choosing how to resolve a conflict (by KL, KR, or ME), by predicting their resolution strategy. In its original design, RPredictor uses a set of Evolution History Features (EHFs) that capture: the magnitude of the changes in conflict, their evolution, and the experience of the developers involved.In this paper, we proposed and evaluated a new set of Branch Edit Features (BEFs), that capture the fine-grained edits that were performed on each branch of the conflict. We learned multiple lessons. First, BEFs provided lower effectiveness (F-score) than the original EHFs. Second, combining BEFs with EHFs still did not improve the effectiveness of EHFs, it provided the same f-score. Third, the feature set that provided highest effectiveness in our experiments was the combination of EHFs with a subset of BEFs that captures the number of insertions performed in the local branch, but this combination only improved EHFs by 3 pp. f-score. Finally, our experiments also share the lesson that some feature sets provided higher C-score (i.e., the safety of the technique's mistakes) as a trade-off for lower f-scores. This may be valued by developers and we believe that it should be studied in the future.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {149–160},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@proceedings{10.1145/3644384,
title = {TechDebt '24: Proceedings of the 7th ACM/IEEE International Conference on Technical Debt},
year = {2024},
isbn = {9798400705908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {TechDebt 2024 brings together leading software engineering researchers, practitioners and educators to explore the theoretical and practical techniques for managing Technical Debt and to share experiences, challenges, and best practices to address open issues in both the industry and research and bridge the gap between them. In addition, a unique aspect of the Technical Debt community is its emphasis on using existing tools to manage Technical Debt as well as developing new ones.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3528226.3528379,
author = {Pinelli, Nicholas and Pareschi, Remo},
title = {Social news aggregations and the bitcoin: mapping their correlation through the three dimensions of content, sentiment and time},
year = {2023},
isbn = {9781450393317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528226.3528379},
doi = {10.1145/3528226.3528379},
abstract = {Social media and financial markets are ecosystems bound to interact and overlap more and more. For example, cryptocurrencies, Bitcoin in the first place, have long been among the hottest topics on social media. Here we illustrate a methodology for correlating Bitcoin price trends and social media that operates on the aggregation of news from multiple social networks as typically occurs on dedicated channels in sites such as Reddit. For this purpose, we define general laws to map the financial fluctuations of Bitcoin in a space defined on three dimensions: content volume, sentiment and time. These laws provide the foundation upon which to build effective methodologies for social media-based prediction of Bitcoin performance.},
booktitle = {Proceedings of the 5th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {33–38},
numpages = {6},
keywords = {social media, sentiment analysis, reddit, news aggregation, content analysis, bitcoin},
location = {Pittsburgh, Pennsylvania},
series = {WETSEB '22}
}

@inproceedings{10.1145/3647632.3647991,
author = {Inayoshi, Hiroki and Kakei, Shohei and Saito, Shoichi},
title = {Detection of Inconsistencies between Guidance Pages and Actual Data Collection of Third-party SDKs in Android Apps},
year = {2024},
isbn = {9798400705946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647632.3647991},
doi = {10.1145/3647632.3647991},
abstract = {Major app stores have introduced privacy labels (e.g., Google Play's data safety section since July 2022), requiring app developers to provide their privacy disclosures, including data types collected and shared by their apps and third-party SDKs they use. Third-party SDK providers have published guidance pages instructing app developers what data types their SDKs use and thus must be declared to the data safety section. Availability and correctness of the guidance pages are critical issues but have yet to receive any attention. This paper presents the first study of the guidance pages. We first attempted to collect the guidance pages of 175 commercial SDKs widely used in Android apps and did not obtain them for 63% of the SDKs, suggesting that the majority of them have not provided guidance pages. Further, we develop a system that detects inconsistencies between the guidance pages and the actual data collection of SDKs. It uses machine learning and dynamic taint analysis to extract privacy practices from the guidance pages and SDKs and analyzes the outcomes to detect the critical gap. We construct datasets of 47 guidance pages and 43 SDKs' 159 sample apps and evaluate the system. The system uncovered discrepancies related to location and identifiers in the guidance pages of eight SDKs. We also evaluate the machine learning model's accuracy for unknown guidance page contents. The results show that the model performs satisfactorily for updated guidance pages, and the accuracy for newly posted ones increases as the model learns more. This study exposes the critical issues of the guidance pages and also contributes to tools and datasets for facilitating further research on guidance pages and privacy labels.},
booktitle = {Proceedings of the IEEE/ACM 11th International Conference on Mobile Software Engineering and Systems},
pages = {43–53},
numpages = {11},
keywords = {Android third-party SDK, data safety section, consistency analysis},
location = {Lisbon, Portugal},
series = {MOBILESoft '24}
}

@inproceedings{10.1109/ICSE48619.2023.00076,
author = {Li, Jiawei and Ahmed, Iftekhar},
title = {Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00076},
doi = {10.1109/ICSE48619.2023.00076},
abstract = {Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {806–817},
numpages = {12},
keywords = {empirical analysis, software defect proneness, commit message quality},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643659.3643927,
author = {Dobslaw, Felix and Feldt, Robert},
title = {Automated Boundary Identification for Machine Learning Classifiers},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643927},
doi = {10.1145/3643659.3643927},
abstract = {AI and Machine Learning (ML) models are increasingly used as (critical) components in software systems, even safety-critical ones. This puts new demands on the degree to which we need to test them and requires new and expanded testing methods. Recent boundary-value identification methods have been developed and shown to automatically find boundary candidates for traditional, non-ML software: pairs of nearby inputs that result in (highly) differing outputs. These can be shown to developers and testers, who can judge if the boundary is where it is supposed to be.Here, we explore how this method can identify decision boundaries of ML classification models. The resulting ML Boundary Spanning Algorithm (ML-BSA) is a search-based method extending previous work in two main ways. We empirically evaluate ML-BSA on seven ML datasets and show that it better spans and thus better identifies the entire classification boundary(ies). The diversity objective helps spread out the boundary pairs more broadly and evenly. This, we argue, can help testers and developers better judge where a classification boundary actually is, compare to expectations, and then focus further testing, validation, and even further training and model refinement on parts of the boundary where behaviour is not ideal.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {1–8},
numpages = {8},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3666015.3666019,
author = {Bilal, Anmol and Mayr-Dorn, Christoph and Egyed, Alexander},
title = {Supporting Engineering Process Compliance via Generation of Detailed Guidance Actions},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666019},
doi = {10.1145/3666015.3666019},
abstract = {In regulation-intensive domains, software engineering organizations need to demonstrate compliance with process and traceability guidelines. To this end, novel approaches have emerged that support these activities via the automatic checking of constraints. Yet, engineers still need to decide how to fix violated constraints. While some general-purpose state-of-the-art constraint-checking approaches provide basic support for fixing constraint violations, the provided fixing recommendations often lack crucial details. The approaches typically do not analyze the overall constraint to identify which constraint sub-expressions put a restriction on the possible fixing action. For example, a fix suggests “set the parent of requirement R1 to an issue” rather than additionally stating that the “issue needs to be of type ’Change Request’ and in state ’Released” ’. Engineers, therefore, require mental effort to identify such restrictions by analyzing the constraint in detail or require extra time to try out which action completely fixes the constraint violation. In this paper, we propose a mechanism that determines restrictions automatically. We assessed the relevance of our mechanism by inspecting historical engineering data at our industry partner ACME-ATC and found that 92% of actions that engineers executed to fix a violation were non-trivial, i.e., were subject to a restriction. In a controlled experiment, we then obtained preliminary confirmation that our produced restrictions are readable and helpful: on average, participants could complete tasks with restriction details quicker than tasks without restriction details.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {87–97},
numpages = {11},
keywords = {Constraint Consistency, Controlled Experiment, Detailed Guidance Actions, Industrial Data Analysis, Process Constraints, Software Process},
location = {M\, Germany},
series = {ICSSP '24}
}

@proceedings{10.1145/3643663,
title = {RoSE '24: Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Software engineering is a crucial enabler for successful deployment of robotic applications. However, much of the research that is advancing the state of the art in robotics software engineering is dispersed across numerous conferences that are either primarily attended by robotics researchers and practitioners (e.g., ICRA, IROS, SIMPAR) or attended mostly by software engineering researchers and practitioners (e.g., ICSE, FSE, MODELS). At robotics conferences, software engineering lacks visibility and vice versa.RoSE brings together researchers and practitioners from both domains at a prominent conference to foster crossfertilization between the two domains. Through a combination of presentations, papers, and discussions, RoSE helps researchers within the budding field of robotics software engineering to learn more about the challenges faced by robotics practitioners that (i) require further research from the software engineering community or (ii) are already solved but solutions have not yet been widely adopted by practitioners.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3639478.3643527,
author = {Kaniyur, Mira Bhagirathi and Cavalcante-Studart, Ana and Yang, Yihan and Park, Sangeon and Chen, David and Lam, Duy and Bang, Lucas},
title = {Path Complexity Analysis for Interprocedural Code},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643527},
doi = {10.1145/3639478.3643527},
abstract = {Symbolic execution's path explosion is a critical issue in software testing, quantified by Asymptotic Path Complexity (APC) [3]. APC, more precise than cyclomatic [6] or NPATH [7] complexities, measures the effort to cover paths in code analysis [1]. It's vital for testing, setting limits on path growth for tools like Klee [4], focusing previously on intraprocedural code [2, 8]. Our advancement, APC-IP, extends APC to interprocedural analysis, enhancing scalability and encompassing earlier models.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {404–405},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639200,
author = {Mang, Qiuyang and Fang, Aoyang and Yu, Boxi and Chen, Hanfei and He, Pinjia},
title = {Testing Graph Database Systems via Equivalent Query Rewriting},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639200},
doi = {10.1145/3597503.3639200},
abstract = {Graph Database Management Systems (GDBMS), which utilize graph models for data storage and execute queries via graph traversals, have seen ubiquitous usage in real-world scenarios such as recommendation systems, knowledge graphs, and social networks. Much like Relational Database Management Systems (RDBMS), GDBMS are not immune to bugs. These bugs typically manifest as logic errors that yield incorrect results (e.g., omitting a node that should be included), performance bugs (e.g., long execution time caused by redundant graph scanning), and exception issues (e.g., unexpected or missing exceptions).This paper adapts Equivalent Query Rewriting (EQR) to GDBMS testing. EQR rewrites a GDBMS query into equivalent ones that trigger distinct query plans, and checks whether they exhibit discrepancies in system behaviors. To facilitate the realization of EQR, we propose a general concept called Abstract Syntax Graph (ASG). Its core idea is to embed the semantics of a base query into the paths of a graph, which can be utilized to generate new queries with customized properties (e.g., equivalence). Given a base query, an ASG is constructed and then an equivalent query can be generated by finding paths collectively carrying the complete semantics of the base query. To this end, we further design Random Walk Covering (RWC), a simple yet effective path covering algorithm. As a practical implementation of these ideas, we develop a tool GRev, which has successfully detected 22 previously unknown bugs across 5 popular GDBMS, with 15 of them being confirmed. In particular, 14 of the detected bugs are related to improper implementation of graph data retrieval in GDBMS, which is challenging to identify for existing techniques.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {143},
numpages = {12},
keywords = {graph databases, metamorphic testing, query rewriting},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639208,
author = {Li, Shuxin and Rigger, Manuel},
title = {Finding XPath Bugs in XML Document Processors via Differential Testing},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639208},
doi = {10.1145/3597503.3639208},
abstract = {Extensible Markup Language (XML) is a widely used file format for data storage and transmission. Many XML processors support XPath, a query language that enables the extraction of elements from XML documents. These systems can be affected by logic bugs, which are bugs that cause the processor to return incorrect results. In order to tackle such bugs, we propose a new approach, which we realized as a system called XPress. As a test oracle, XPress relies on differential testing, which compares the results of multiple systems on the same test input, and identifies bugs through discrepancies in their outputs. As test inputs, XPress generates both XML documents and XPath queries. Aiming to generate meaningful queries that compute non-empty results, XPress selects a so-called targeted node to guide the XPath expression generation process. Using the targeted node, XPress generates XPath expressions that reference existing context related to the targeted node, such as its tag name and attributes, while also guaranteeing that a predicate evaluates to true before further expanding the query. We tested our approach on six mature XML processors, BaseX, eXist-DB, Saxon, PostgreSQL, libXML2, and a commercial database system. In total, we have found 27 unique bugs in these systems, of which 25 have been verified by the developers, and 20 of which have been fixed. XPress is efficient, as it finds 12 unique bugs in BaseX in 24 hours, which is 2\texttimes{} as fast as naive random generation. We expect that the effectiveness and simplicity of our approach will help to improve the robustness of many XML processors.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {145},
numpages = {12},
keywords = {XML processors, XPath generation, differential testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3644032.3644451,
author = {Otto, Bj\"{o}rn and Kleinert, Tobias},
title = {Fences: Systematic Sample Generation for JSON Schemas using Boolean Algebra and Flow Graphs},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644451},
doi = {10.1145/3644032.3644451},
abstract = {In this work, we present Fences, a tool to generate sample data for arbitrary JSON Schemas. Fences generates these samples in three basic steps: First, the schema is simplified and normalized using boolean algebra. Second, the schema is mapped to a flow graph. And third, the flow graph is analyzed to obtain the final sets of JSON data. Besides valid samples, Fences also generates invalid samples it knows to be rejected by the schema. By combining these sets, all degrees of freedom offered by the schema are covered systematically. We assess the feasibility and correctness of our approach using synthetic schemas from the official JSON Schema test suite and a real-world use case from the Industry 4.0 domain. Additionally, we introduce schema coverage as metric to assess the completeness of our approach.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {66–75},
numpages = {10},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3644032.3644457,
author = {Crespo-Rodriguez, Victor and Neelofar and Aleti, Aldeida},
title = {PAFOT: A Position-Based Approach for Finding Optimal Tests of Autonomous Vehicles},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644457},
doi = {10.1145/3644032.3644457},
abstract = {Autonomous Vehicles (AVs) are prone to revolutionise the transportation industry. However, they must be thoroughly tested to avoid safety violations. Simulation testing plays a crucial role in finding safety violations of Automated Driving Systems (ADSs). This paper proposes PAFOT, a position-based approach testing framework, which generates adversarial driving scenarios to expose safety violations of ADSs. We introduce a 9-position grid which is virtually drawn around the Ego Vehicle (EV) and modify the driving behaviours of Non-Playable Characters (NPCs) to move within this grid. PAFOT utilises a single-objective genetic algorithm to search for adversarial test scenarios. We demonstrate PAFOT on a well-known high-fidelity simulator, CARLA. The experimental results show that PAFOT can effectively generate safety-critical scenarios to crash ADSs and is able to find collisions in a short simulation time. Furthermore, it outperforms other search-based testing techniques by finding more safety-critical scenarios under the same driving conditions within less effective simulation time.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {159–170},
numpages = {12},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3643915.3644097,
author = {Sanabria, Mateo and Dusaric, Ivana and Cardozo, Nicol\'{a}s},
title = {Learning Recovery Strategies for Dynamic Self-healing in Reactive Systems},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644097},
doi = {10.1145/3643915.3644097},
abstract = {Self-healing systems depend on following a set of predefined instructions to recover from a known failure state. Failure states are generally detected based on domain specific specialized metrics. Failure fixes are applied at predefined application hooks that are not sufficiently expressive to manage different failure types. Self-healing is usually applied in the context of distributed systems, where the detection of failures is constrained to communication problems, and resolution strategies often consist of replacing complete components. However, current complex systems may reach failure states at a fine granularity not anticipated by developers (for example, value range changes for data streaming in IoT systems), making them unsuitable for existing self-healing techniques. To counter these problems, in this paper we propose a new self-healing framework that learns recovery strategies for healing fine-grained system behavior at run time. Our proposal targets complex reactive systems, defining monitors as predicates specifying satisfiability conditions of system properties. Such monitors are functionally expressive and can be defined at run time to detect failure states at any execution point. Once failure states are detected, we use a Reinforcement Learning-based technique to learn a recovery strategy based on users' corrective sequences. Finally, to execute the learned strategies, we extract them as Context-oriented Programming variations that activate dynamically whenever the failure state is detected, overwriting the base system behavior with the recovery strategy for that state. We validate the feasibility and effectiveness of our framework through a prototypical reactive application for tracking mouse movements, and the DeltaIoT exemplar for self-healing systems. Our results demonstrate that with just the definition of monitors, the system is effective in detecting and recovering from failures between 55% -- 92% of the cases in the first application, and at par with the predefined strategies in the second application.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {133–142},
numpages = {10},
keywords = {self-healing systems, context-oriented programming, functional-reactive programming, RL},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1109/SVM66695.2025.00010,
author = {Garcia, Derek and Mirakorhli, Mehdi Tarrit and Dillon, Schuyler and Laporte, Kevin and Morrison, Matthew and Lu, Henry and Koscinski, Viktoria and Enoch, Christopher and Fazelnia, Mohamad and Chen, Roger},
title = {A Landscape Study of Open-Source Tools for Software Bill of Materials (SBOM) and Supply Chain Security},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00010},
doi = {10.1109/SVM66695.2025.00010},
abstract = {Modern software applications heavily rely on diverse third-party components, libraries, and frameworks sourced from various vendors and open source repositories. This presents a complex challenge for securing the software supply chain. To address this complexity, the adoption of a Software Bill of Materials (SBOM) has emerged as a promising solution, offering a unifying standard that inventories all third-party components and dependencies used in an application. Recent supply chain breaches, exemplified by the SolarWinds attack, underscore the urgent need to enhance software security and mitigate vulnerability risks. SBOMs play a pivotal role in this endeavor by revealing potential vulnerabilities, outdated components, and unsupported elements. This research paper conducts an extensive empirical analysis to assess the current landscape of open-source tools related to SBOM. We investigate emerging use cases in software supply chain security and identify gaps in SBOM technologies. Our analysis encompasses 84 tools, providing a snapshot of the current market and highlighting areas for improvement.},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {37–45},
numpages = {9},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1145/3597503.3639152,
author = {Wu, Shuohan and Li, Zihao and Yan, Luyi and Chen, Weimin and Jiang, Muhui and Wang, Chenxu and Luo, Xiapu and Zhou, Hao},
title = {Are We There Yet? Unraveling the State-of-the-Art Smart Contract Fuzzers},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639152},
doi = {10.1145/3597503.3639152},
abstract = {Given the growing importance of smart contracts in various applications, ensuring their security and reliability is critical. Fuzzing, an effective vulnerability detection technique, has recently been widely applied to smart contracts. Despite numerous studies, a systematic investigation of smart contract fuzzing techniques remains lacking. In this paper, we fill this gap by: 1) providing a comprehensive review of current research in contract fuzzing, and 2) conducting an in-depth empirical study to evaluate state-of-the-art contract fuzzers' usability. To guarantee a fair evaluation, we employ a carefully-labeled benchmark and introduce a set of pragmatic performance metrics, evaluating fuzzers from five complementary perspectives. Based on our findings, we provide direction for the future research and development of contract fuzzers.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {127},
numpages = {13},
keywords = {smart contract, fuzzing, evaluation},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3640026,
author = {Sigurdson, Brian and Flint, Samuel W. and Dyer, Robert},
title = {Boidae: Your Personal Mining Platform},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640026},
doi = {10.1145/3639478.3640026},
abstract = {Mining software repositories is a useful technique for researchers and practitioners to see what software developers actually do when developing software. Tools like Boa provide users with the ability to easily mine these open-source software repositories at a very large scale, with datasets containing hundreds of thousands of projects. The trade-off is that users must use the provided infrastructure, query language, runtime, and datasets and this might not fit all analysis needs. In this work, we present Boidae: a family of Boa installations controlled and customized by users. Boidae uses automation tools such as Ansible and Docker to facilitate the deployment of a customized Boa installation. In particular, Boidae allows the creation of custom datasets generated from any set of Git repositories, with helper scripts to aid in finding and cloning repositories from GitHub and SourceForge. In this paper, we briefly describe the architecture of Boidae and how researchers can utilize the infrastructure to generate custom datasets. Boidae's scripts and all infrastructure it builds upon are open-sourced. A video demonstration of Boidae's installation and extension is available at https://go.unl.edu/boidae.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {40–43},
numpages = {4},
keywords = {boa, mining software repositories, scalable, open source},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639102,
author = {Xu, Zhiwu and Wu, Bohao and Wen, Cheng and Zhang, Bin and Qin, Shengchao and He, Mengda},
title = {RPG: Rust Library Fuzzing with Pool-based Fuzz Target Generation and Generic Support},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639102},
doi = {10.1145/3597503.3639102},
abstract = {Rust libraries are ubiquitous in Rust-based software development. Guaranteeing their correctness and reliability requires thorough analysis and testing. Fuzzing is a popular bug-finding solution, yet it requires writing fuzz targets for libraries. Recently, some automatic fuzz target generation methods have been proposed. However, two challenges remain: (1) how to generate diverse API sequences that prioritize unsafe code and interactions to reveal bugs in Rust libraries; (2) how to provide support for the generic APIs and verify both syntactic and semantic validity of the fuzz targets to enable more comprehensive testing of Rust libraries. In this paper, we propose RPG, an automatic fuzz target synthesis technique to support Rust library fuzzing. RPG uses a pool-based search to generate diverse and unsafe API sequences, and synthesizes fuzz targets with generic support and validity check. The experimental results demonstrate that RPG enhances both the quality of the generated fuzz targets and the bug-finding ability through pool-based generation and generic support, substantially outperforming the state-of-the-art. Moreover, RPG has discovered 25 previously unknown bugs from 50 well-known Rust libraries available on Crates.io.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {124},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3640022,
author = {Cao, Clinton and Schneider, Simon and Ferreyra, Nicolas E. Diaz and Verwer, Sicco and Panichella, Annibale and Scandariato, Riccardo},
title = {CATMA: Conformance Analysis Tool For Microservice Applications},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640022},
doi = {10.1145/3639478.3640022},
abstract = {The microservice architecture allows developers to divide the core functionality of their software system into multiple smaller services. However, this architectural style also makes it harder for them to debug and assess whether the system's deployment conforms to its implementation. We present CATMA, an automated tool that detects non-conformances between the system's deployment and implementation. It automatically visualizes and generates potential interpretations for the detected discrepancies. Our evaluation of CATMA shows promising results in terms of performance and providing useful insights. CATMA is available at https://cyber-analytics.nl/catma.github.io/, and a demonstration video is available at https://youtu.be/WKP1hG-TDKc.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {59–63},
numpages = {5},
keywords = {microservices, static analysis, dynamic analysis, software testing, empirical software engineering},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643991.3644932,
author = {Serbout, Souhaila and Pautasso, Cesare},
title = {APIstic: A Large Collection of OpenAPI Metrics},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644932},
doi = {10.1145/3643991.3644932},
abstract = {In the rapidly evolving landscape of web services, the significance of efficiently designed and well-documented APIs is paramount. In this paper, we present APIstic an API analytics dataset and exploration tool to navigate and segment APIs based on an extensive set of pre-computed metrics extracted from OpenAPI specifications, sourced from GitHub, SwaggerHub, BigQuery and APIs.guru. These pre-computed metrics are categorized into structure, data model, natural language description, and security metrics. The extensive dataset of varied API metrics provides crucial insights into API design and documentation for both researchers and practitioners. Researchers can use APIstic as an empirical resource to extract refined samples, analyze API design trends, best practices, smells, and patterns. For API designers, it serves as a benchmarking tool to assess, compare, and improve API structures, data models, and documentation using metrics to select points of references among 1,275,568 valid OpenAPI specifications. The paper discusses potential use cases of the collected data and presents a descriptive analysis of selected API analytics metrics.The dataset available at: http://openapi.inf.usi.ch/},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {265–277},
numpages = {13},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643667.3648219,
author = {Pontolillo, Gabriel Joseph and Mousavi, Mohammad Reza},
title = {Delta Debugging for Property-Based Regression Testing of Quantum Programs},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643667.3648219},
doi = {10.1145/3643667.3648219},
abstract = {Manually debugging quantum programs is a difficult and time-intensive process. In this paper, we introduce an automated debugging technique, based on delta debugging and property-based testing, for quantum programs. Our technique automatically identifies the changes made within an update to a quantum program that cause a property-based regression test to fail. To evaluate our technique, we inject faults and semantic preserving changes into three quantum algorithms. We discuss the viability and efficacy of our approach after measuring the percentage of faults and semantic preserving changes. Our results indicate that our method has a high true positive (called sensitivity) and true negative rate (called specificity) and is robust in terms of the amount of changes introduced to the program. Moreover, the sensitivity of the method increases significantly with the number of properties. While the specificity remains stable when increasing the number of properties and inputs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
pages = {1–8},
numpages = {8},
location = {Lisbon, Portugal},
series = {Q-SE 2024}
}

@inproceedings{10.1145/3644032.3644464,
author = {Gala, Viraj Rohit and Schneider, Martin and Vogt, Marvin},
title = {Towards an Empirical Robustness Assessment Through Measuring Adversarial Subspaces},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644464},
doi = {10.1145/3644032.3644464},
abstract = {Machine learning (ML) paves the way for innovative applications in various domains. However, adversarial examples pose a significant threat to their robustness, which hinders their usage, for instance, in safety-critical applications. The arms race of attacks and defenses against adversarial examples has received much attention, whilst the analysis on measuring the robustness received little. Robustness scores provide a means to estimate safe regions in the input space for which no adversarial examples exist for a given model. However, these methods often do not scale. On the other hand, empirical investigations have brought the insight that adversarial examples are not isolated examples in the input space, but form contiguous subspaces.In this paper, we contribute to these investigations with methods for the empirical analysis on identifying the extent of adversarial subspaces through analyzing their boundaries. To that aim, we apply two methods to measure their boundaries and draw conclusions on the shape, extent, and distribution of adversarial subspaces within the input space. The presented results are a first step towards an efficient and scalable empirical measurement of adversarial subspaces, aiming to quantify the robustness of an ML model in cases where a formal verification is not feasible. To the best of our knowledge, this is the first empirical investigation of the extent of adversarial subspaces. We illustrate our results on the OpenSky dataset and identify the challenges in assessing the robustness of ML models.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {120–124},
numpages = {5},
keywords = {artificial intelligence, machine learning, adversarial attacks, adversarial subspaces, robustness assessment},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3597503.3639112,
author = {Liang, Jie and Wu, Zhiyong and Fu, Jingzhou and Wang, Mingzhe and Sun, Chengnian and Jiang, Yu},
title = {Mozi: Discovering DBMS Bugs via Configuration-Based Equivalent Transformation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639112},
doi = {10.1145/3597503.3639112},
abstract = {Testing database management systems (DBMSs) is a complex task. Traditional approaches, such as metamorphic testing, need a precise comprehension of the SQL specification to create diverse inputs with equivalent semantics. The vagueness and intricacy of the SQL specification make it challenging to accurately model query semantics, thereby posing difficulties in testing the correctness and performance of DBMSs. To address this, we propose Mozi, a framework that finds DBMS bugs via configuration-based equivalent transformation. The key idea behind Mozi is to compare the results of equivalent DBMSs with different configurations, rather than between semantically equivalent queries. The framework involves analyzing the query plan, changing configurations to transform the DBMS to an equivalent one, and re-executing the query to compare the results using various test oracles. For example, detecting differences in query results indicates correctness bugs, while observing faster execution times on the optimization-closed DBMS suggests performance bugs.We demonstrate the effectiveness of Mozi by evaluating it on four widely used DBMSs, namely MySQL, MariaDB, Clickhouse, and PostgreSQL. In the continuous testing, Mozi found a total of 101 previously unknown bugs, including 49 correctness and 52 performance bugs in four DBMSs. Among them, 90 bugs are confirmed and 57 bugs have been fixed. In addition, Mozi can be extended to other DBMS fuzzers for testing various types of bugs. With Mozi, testing DBMSs becomes simpler and more effective, potentially saving time and effort that would otherwise be spent on precisely modeling SQL specifications for testing purposes.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {135},
numpages = {12},
keywords = {DBMS testing, configuration, test oracle},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643659.3648551,
author = {Zohdinasab, Tahereh and Doreste, Andrea},
title = {DeepHyperion-UAV at the SBFT Tool Competition 2024 - CPS-UAV Test Case Generation Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648551},
doi = {10.1145/3643659.3648551},
abstract = {Unmanned Aerial Vehicles (UAV) have showcased the potential for autonomous flights in real-world settings, sparking considerable interest across various application domains. Ensuring the robustness and safety of UAV operations in real-world scenarios necessitates a more comprehensive approach to testing, an aspect that has yet to receive adequate attention. In this paper, we proposed an automated test input generator for testing Unmanned Aerial Vehicles, named DeepHyperion-UAV that leverages the key advantages of Illumination search. DeepHyperion-UAV searches for diverse misbehaviour-inducing inputs spread across the cells of a map representing the feature space of the system.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {49–50},
numpages = {2},
keywords = {tool competition, software testing, test case generation},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643659.3648558,
author = {Sadykov, Rustam and Abdullin, Azat and Akhin, Marat},
title = {Evokex at the SBFT 2024 Tool Competition},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648558},
doi = {10.1145/3643659.3648558},
abstract = {EvoKex is a test generation tool for Java that seamlessly integrates genetic testing from EvoSuite and concolic analysis from Kex. This integration is achieved through the dynamic sharing of states between these two powerful approaches. By combining genetic and concolic testing, EvoKex aims to overcome individual tools' limitations and provides a synergistic solution for enhanced automated test generation. This paper summarizes EvoKex's performance and experiences in the Java unit testing tool competition at the International Workshop on Search-Based and Fuzz Testing (SBFT) 2024.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {67–68},
numpages = {2},
keywords = {automatic test generation, symbolic execution, concolic testing, genetic testing, search-based test generation, software testing},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643659.3648566,
author = {David, Robin and Heitman, Christian},
title = {PASTIS: A Framework for Distributed Ensemble Fuzzing},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648566},
doi = {10.1145/3643659.3648566},
abstract = {The fuzzing research field experienced outstanding advances over the past decade, making it a very effective approach for software testing. Dynamic Symbolic Execution (DSE) also called whitebox fuzzing is another approach that also evolved significantly and has the advantage of being able to solve very complex path conditions. We propose a combination of fuzzing and DSE into an ensemble fuzzing framework called PASTIS that helps in circumventing engines inner-working discrepancies.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {59–60},
numpages = {2},
keywords = {software testing, greybox fuzzing, dynamic symbolic execution, ensemble fuzzing},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643659.3648552,
author = {Humeniuk, Dmytro and Khomh, Foutse},
title = {AmbieGen at the SBFT 2024 Tool Competition - CPS-UAV Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648552},
doi = {10.1145/3643659.3648552},
abstract = {Simulation based testing of autonomous systems prior to their deployment in the real world is of big importance. AmbieGen is a tool for automatic generation of virtual test cases for autonomous cyber-physical systems (CPS). In the context of SBFT 2024 CPS-UAV tool competition, we adopted it to the generation of scenes with obstacles for testing an unmanned aerial vehicle (UAV) obstacle avoidance system. AmbieGen leverages a genetic algorithm guided by a path planner to prioritize test scenarios to be evaluated in the simulation. It could reveal some critical failures in all the 6 use cases considered in the competition.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {69–70},
numpages = {2},
keywords = {test cases, UAVs, tool competition, genetic algorithm, path planning},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3639478.3643537,
author = {Ou\'{e}draogo, Wendk\^{u}uni C. and Plein, Laura and Kabore, Kader and Habib, Andrew and Klein, Jacques and Lo, David and Bissyande, Tegawende F.},
title = {Extracting Relevant Test Inputs from Bug Reports for Automatic Test Case Generation},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643537},
doi = {10.1145/3639478.3643537},
abstract = {The pursuit of automating software test case generation, particularly for unit tests, has become increasingly important due to the labor-intensive nature of manual test generation [6]. However, a significant challenge in this domain is the inability of automated approaches to generate relevant inputs, which compromises the efficacy of the tests [6].},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {406–407},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3643659.3648562,
author = {She, Dongdong and Storek, Adam and Xie, Yuchong and Kweon, Seoyoung and Srivastava, Prashast and Jana, Suman},
title = {FOX: Coverage-guided Fuzzing as Online Stochastic Control},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648562},
doi = {10.1145/3643659.3648562},
abstract = {Fuzzing large and complex programs remains challenging due to difficulties in uncovering deeply hidden vulnerabilities. This paper addresses the limitations of existing coverage-guided fuzzers, focusing on the scheduler and mutator components. We propose an end-to-end online stochastic control formulation for coverage-guided fuzzing. Our approach incorporates a custom scheduler and mutator that can adapt to branch logic, maximizing edge coverage. The scheduler utilizes fine-grained branch distance measures to identify frontier branches, where new coverage is likely to be achieved. The mutator leverages branch distance information to perform efficient and targeted seed mutations, leading to robust progress with minimal overhead. We present FOX, a proof-of-concept implementation of our control-theoretic approach.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {57–58},
numpages = {2},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3643659.3648561,
author = {Babikian, Aren A. and Varr\'{o}, D\'{a}niel},
title = {OptAngle at the SBFT 2024 Tool Competition - Cyber-Physical Systems Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648561},
doi = {10.1145/3643659.3648561},
abstract = {OptAngle is a test generator for autonomous vehicles that leverages meta-heuristic search over a road representation based on relative angles between fixed-size road segments. It derives virtual roads by optimizing for (1) road structure validity, (2) failures during test execution, and (3) test case diversity. We have submitted OptAngle for participation to the Cyber-physical systems testing tool competition at SBFT 2024. Results show that test suites produced by OptAngle often provide high failure rates (wrt. its competitors), despite providing low diversity in detected failures.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {73–74},
numpages = {2},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3648505.3648510,
author = {Butt, Hallah Shahid and Schafer, Benjamin},
title = {Why Reinforcement Learning in Energy Systems Needs Explanations},
year = {2024},
isbn = {9798400705960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648505.3648510},
doi = {10.1145/3648505.3648510},
abstract = {With economic development, the complexity of infrastructure has increased drastically. Similarly, with the shift from fossil fuels to renewable sources of energy, there is a dire need for such systems that not only predict and forecast with accuracy but also help in understanding the process of predictions. Artificial intelligence and machine learning techniques have helped in finding out well-performing solutions to different problems in the energy sector. However, the usage of state-of-the-art techniques like reinforcement learning is not surprisingly convincing. This paper discusses the application of reinforcement techniques in energy systems and how explanations of these models can be helpful.},
booktitle = {Proceedings of the 2024 Workshop on Explainability Engineering},
pages = {26–30},
numpages = {5},
keywords = {explainable artificial intelligence (XAI), reinforcement learning, machine learning, interpretations, energy systems},
location = {Lisbon, Portugal},
series = {ExEn '24}
}

@inproceedings{10.1145/3643662.3643959,
author = {Zhang, Xiangwei and Wang, Junjie and Du, Xiaoning and Liu, Shuang},
title = {WasmCFuzz: Structure-aware Fuzzing for Wasm Compilers},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643959},
doi = {10.1145/3643662.3643959},
abstract = {WebAssembly (Wasm) has emerged as a pivotal technology for web applications, offering near-native execution speeds and bolstered security through sandboxed execution. Despite its widespread adoption in major browsers, the rapid evolution of Wasm introduces novel attack surfaces, particularly in Wasm compilers. The challenge of Wasm compiler testing lies in producing semi-valid Wasm samples that are structurally sound enough to bypass initial checks yet sufficiently unique to probe for vulnerabilities. In response, we introduce WasmCFuzz, an innovative fuzzing approach that utilizes AFL-generated random bytes to create semi-valid Wasm formats. This method effectively balances structural validity with the potential to uncover compiler corner cases. Our comprehensive evaluation demonstrates that WasmCFuzz not only outperforms existing methods like Wasm-smith and WAfuzzer but also uncovers 13 previously unidentified bugs in mainstream browsers within just a week. These findings highlight WasmCFuzz's capability in enhancing the security of Wasm compilers, marking a significant step forward in Wasm compiler testing.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {1–5},
numpages = {5},
keywords = {fuzzing, WebAssembly, browser},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00081,
author = {Smytzek, Marius},
title = {From Input to Failure: Explaining Program Behavior via Cause-Effect Chains},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00081},
doi = {10.1109/ICSE-Companion58688.2023.00081},
abstract = {Debugging a fault in a program is an error-prone and resource-intensive process that requires considerable work. My doctoral research aims at supporting developers during this process by integrating test generation as a feedback loop into a novel fault diagnosis to narrow down the causality by validating or disproving suggested hypotheses. I will combine input, output, and state to detect relevant relations for an immersive fault diagnosis. Further, I want to introduce an approach for a targeted test that leverages statistical fault localization to extract oracles based on execution features to identify failing tests.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {302–304},
numpages = {3},
keywords = {diagnostics, debugging aids, testing and debugging, software engineering, software/software engineering},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/GREENS66463.2025.00008,
author = {Peslalz, Tobias Leonhard Joschka and Katz, Bastian},
title = {Educated Energy Efficiency Optimization of Distributed Software: Measure, Monitor, Mitigate},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00008},
doi = {10.1109/GREENS66463.2025.00008},
abstract = {As distributed systems scale, optimizing energy efficiency becomes increasingly critical yet challenging. This paper introduces a continuous process for improving energy efficiency that leverages three iterative steps — measure, monitor, and mitigate — forming a feedback loop. Utilizing state-of-the-art tools and best practices, we empirically demonstrate targeted optimizations. Our experiments highlight common pitfalls and optimization opportunities, providing practical guidance in using our process. Additionally, we present an extensible monitoring platform designed to collect, analyze, and visualize energy consumption and performance metrics, facilitating the selection of informed optimization techniques. This approach is tailored for engineers and researchers seeking efficient and adaptable energy efficiency optimization strategies. By implementing this structured process, significant improvements in energy efficiency can be achieved, maximizing optimization impact relative to invested resources while maintaining system performance and reliability.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {20–27},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@proceedings{10.1145/3528231,
title = {SEENG '22: Proceedings of the 4th International Workshop on Software Engineering Education for the Next Generation},
year = {2022},
isbn = {9781450393362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This workshop, the fourth in the series since ICSE 2017, brings together scholars, educators, and other stakeholders to discuss the unique needs and challenges of software engineering education for the next generation. Building on its predecessors, the workshop employs a highly interactive format, structured around short presentations to generate discussion topics, an activity to select the most interesting topics, and structured breakout sessions to allow participants to address those topics.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3597503.3623307,
author = {Jiang, Yuancheng and Liu, Jiahao and Ba, Jinsheng and Yap, Roland H. C. and Liang, Zhenkai and Rigger, Manuel},
title = {Detecting Logic Bugs in Graph Database Management Systems via Injective and Surjective Graph Query Transformation},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623307},
doi = {10.1145/3597503.3623307},
abstract = {Graph Database Management Systems (GDBMSs) store graphs as data. They are used naturally in applications such as social networks, recommendation systems and program analysis. However, they can be affected by logic bugs, which cause the GDBMSs to compute incorrect results and subsequently affect the applications relying on them. In this work, we propose injective and surjective Graph Query Transformation (GQT) to detect logic bugs in GDBMSs. Given a query Q, we derive a mutated query Q', so that either their result sets are: (i) semantically equivalent; or (ii) variant based on the mutation to be either a subset or superset of each other. When the expected relationship between the results does not hold, a logic bug in the GDBMS is detected. The key insight to mutate Q is that the graph pattern in graph queries enables systematic query transformations derived from injective and surjective mappings of the directed edge sets between Q and Q'. We implemented injective and surjective Graph Query Transformation (GQT) as a tool called GraphGenie and evaluated it on 6 popular and mature GDBMSs. GraphGenie has found 25 unknown bugs, comprising 16 logic bugs, 3 internal errors, and 6 performance issues. Our results demonstrate the practicality and effectiveness of GraphGenie in detecting logic bugs in GDBMSs which has the potential for improving the reliability of applications relying on these GDBMSs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {46},
numpages = {12},
keywords = {graph databases, logic bugs, metamorphic testing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639192,
author = {Sa\u{g}lam, Timur and Br\"{o}del, Moritz and Schmid, Larissa and Hahner, Sebastian},
title = {Detecting Automatic Software Plagiarism via Token Sequence Normalization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639192},
doi = {10.1145/3597503.3639192},
abstract = {While software plagiarism detectors have been used for decades, the assumption that evading detection requires programming proficiency is challenged by the emergence of automated plagiarism generators. These generators enable effortless obfuscation attacks, exploiting vulnerabilities in existing detectors by inserting statements to disrupt the matching of related programs. Thus, we present a novel, language-independent defense mechanism that leverages program dependence graphs, rendering such attacks infeasible. We evaluate our approach with multiple real-world datasets and show that it defeats plagiarism generators by offering resilience against automated obfuscation while maintaining a low rate of false positives.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {113},
numpages = {13},
keywords = {software plagiarism detection, plagiarism obfuscation, obfuscation attacks, code normalization, PDG, tokenization},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3608138,
author = {Gruber, Martin and Roslan, Muhammad Firhard and Parry, Owain and Scharnb\"{o}ck, Fabian and McMinn, Phil and Fraser, Gordon},
title = {Do Automatic Test Generation Tools Generate Flaky Tests?},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3608138},
doi = {10.1145/3597503.3608138},
abstract = {Non-deterministic test behavior, or flakiness, is common and dreaded among developers. Researchers have studied the issue and proposed approaches to mitigate it. However, the vast majority of previous work has only considered developer-written tests. The prevalence and nature of flaky tests produced by test generation tools remain largely unknown. We ask whether such tools also produce flaky tests and how these differ from developer-written ones. Furthermore, we evaluate mechanisms that suppress flaky test generation. We sample 6 356 projects written in Java or Python. For each project, we generate tests using EvoSuite (Java) and Pynguin (Python), and execute each test 200 times, looking for inconsistent outcomes. Our results show that flakiness is at least as common in generated tests as in developer-written tests. Nevertheless, existing flakiness suppression mechanisms implemented in EvoSuite are effective in alleviating this issue (71.7 % fewer flaky tests). Compared to developer-written flaky tests, the causes of generated flaky tests are distributed differently. Their non-deterministic behavior is more frequently caused by randomness, rather than by networking and concurrency. Using flakiness suppression, the remaining flaky tests differ significantly from any flakiness previously reported, where most are attributable to runtime optimizations and EvoSuite-internal resource thresholds. These insights, with the accompanying dataset, can help maintainers to improve test generation tools, give recommendations for developers using these tools, and serve as a foundation for future research in test flakiness or test generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {47},
numpages = {12},
keywords = {test generation, flaky tests, empirical study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3623301,
author = {Hu, Jie and Duan, Yue and Yin, Heng},
title = {Marco: A Stochastic Asynchronous Concolic Explorer},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623301},
doi = {10.1145/3597503.3623301},
abstract = {Concolic execution is a powerful program analysis technique for code path exploration. Despite recent advances that greatly improved the efficiency of concolic execution engines, path constraint solving remains a major bottleneck of concolic testing. An intelligent scheduler for inputs/branches becomes even more crucial. Our studies show that the previously under-studied branch-flipping policy adopted by state-of-the-art concolic execution engines has several limitations. We propose to assess each branch by its potential for new code coverage from a global view, concerning the path divergence probability at each branch. To validate this idea, we implemented a prototype Marco and evaluated it against the state-of-the-art concolic executor on 30 real-world programs from Google's Fuzzbench, Binutils, and UniBench. The result shows that Marco can outperform the baseline approach and make continuous progress after the baseline approach terminates.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {59},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00041,
author = {Yang, Rui and Zheng, Yingying and Tang, Lei and Dou, Wensheng and Wang, Wei and Wei, Jun},
title = {Randomized Differential Testing of RDF Stores},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00041},
doi = {10.1109/ICSE-Companion58688.2023.00041},
abstract = {As a special kind of graph database systems, RDF stores have been widely used in many applications, e.g., knowledge graphs and semantic web. RDF stores utilize SPARQL as their standardized query language to store and retrieve RDF graphs. Incorrect implementations of RDF stores can introduce logic bugs that cause RDF stores to return incorrect query results. These logic bugs can lead to severe consequences and are likely to go unnoticed by developers. However, no available tools can detect logic bugs in RDF stores.In this paper, we propose RD2, a Randomized Differential testing approach of RDF stores, to reveal discrepancies among RDF stores, which indicate potential logic bugs in RDF stores. The core idea of RD2 is to build an equivalent RDF graph for multiple RDF stores, and verify whether they can return the same query result for a given SPARQL query. Guided by the SPARQL syntax and the generated RDF graph, we automatically generate syntactically valid SPARQL queries, which can return non-empty query results with high probability. We further unify the formats of SPARQL query results from different RDF stores and find discrepancies among them. We evaluate RD2 on three popular and widely-used RDF stores. In total, we have detected 5 logic bugs in them. A video demonstration of RD2 is available at https://youtu.be/da7XlsdbRR4.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {136–140},
numpages = {5},
keywords = {SPARQL, differential testing, RDF store},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639477.3639717,
author = {Olewicki, Doriane and Habchi, Sarra and Nayrolles, Mathieu and Faramarzi, Mojtaba and Chandar, Sarath and Adams, Bram},
title = {On the Costs and Benefits of Adopting Lifelong Learning for Software Analytics - Empirical Study on Brown Build and Risk Prediction},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639717},
doi = {10.1145/3639477.3639717},
abstract = {Nowadays, software analytics tools using machine learning (ML) models to, for example, predict the risk of a code change are well established. However, as the goals of a project shift over time, and developers and their habits change, the performance of said models tends to degrade (drift) over time. Current retraining practices typically require retraining a new model from scratch on a large updated dataset when performance decay is observed, thus incurring a computational cost; also there is no continuity between the models as the past model is discarded and ignored during the new model training. Even though the literature has taken interest in online learning approaches, those have rarely been integrated and evaluated in industrial environments.This paper evaluates the use of lifelong learning (LL) for industrial use cases at Ubisoft, evaluating both the performance and the required computational effort in comparison to the retraining-from-scratch approaches commonly used by the industry. LL is used to continuously build and maintain ML-based software analytics tools using an incremental learner that progressively updates the old model using new data. To avoid so-called "catastrophic forgetting" of important older data points, we adopt a replay buffer of older data, which still allows us to drastically reduce the size of the overall training dataset, and hence model training time.Empirical evaluation of our LL approach on two industrial use cases, i.e., a brown build detector and a just-in-time risk prediction tool, shows how LL in practice manages to at least match traditional retraining-from-scratch performance in terms of F1-score, while using 3.3-13.7x less data at each update, thus considerably speeding up the model updating process. Considering both the computational effort of updates and the time between model updates, the LL setup needs 2-40x less computational effort than retraining-from-scratch setups, thus clearly showing the potential of LL setups in the industry.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {275–286},
numpages = {12},
keywords = {software analytics, brown build detection, just-in-time risk prediction, lifelong learning, online learning},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/ICSE48619.2023.00187,
author = {Kim, Jongwook and So, Sunbeom and Oh, Hakjoo},
title = {DIVER: Oracle-Guided SMT Solver Testing with Unrestricted Random Mutations},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00187},
doi = {10.1109/ICSE48619.2023.00187},
abstract = {We present DIVER, a novel technique for effectively finding critical bugs in SMT solvers. Ensuring the correctness of SMT solvers is becoming increasingly important as many applications use solvers as a foundational basis. In response, several approaches for testing SMT solvers, which are classified into differential testing and oracle-guided approaches, have been proposed until recently. However, they are still unsatisfactory in that (1) differential testing approaches cannot validate unique yet important features of solvers, and (2) oracle-guided approaches cannot generate diverse tests due to their reliance on limited mutation rules. DIVER aims to complement these shortcomings, particularly focusing on finding bugs that are missed by existing approaches. To this end, we present a new testing technique that performs oracle-guided yet unrestricted random mutations. We have used DIVER to validate the most recent versions of three popular SMT solvers: CVC5, Z3 and dReal. In total, DIVER found 25 new bugs, of which 21 are critical and directly affect the reliability of the solvers. We also empirically prove DIVER's own strength by showing that existing tools are unlikely to find the bugs discovered by DIVER.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2224–2236},
numpages = {13},
keywords = {SMT solver, fuzzing, software testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3650105.3652302,
author = {Blyth, Scott and Treude, Christoph and Wagner, Markus},
title = {Creative and Correct: Requesting Diverse Code Solutions from AI Foundation Models},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652302},
doi = {10.1145/3650105.3652302},
abstract = {AI foundation models have the capability to produce a wide array of responses to a single prompt, a feature that is highly beneficial in software engineering to generate diverse code solutions. However, this advantage introduces a significant trade-off between diversity and correctness. In software engineering tasks, diversity is key to exploring design spaces and fostering creativity, but the practical value of these solutions is heavily dependent on their correctness. Our study systematically investigates this trade-off using experiments with HumanEval tasks, exploring various parameter settings and prompting strategies. We assess the diversity of code solutions using similarity metrics from the code clone community. The study identifies combinations of parameters and strategies that strike an optimal balance between diversity and correctness, situated on the Pareto front of this trade-off space. These findings offer valuable insights for software engineers on how to effectively use AI foundation models to generate code solutions that are diverse and accurate.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {119–123},
numpages = {5},
keywords = {foundation models, correctness, creativity},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@inproceedings{10.1145/3644033.3644378,
author = {Cort\'{e}s, David and Ortiz, James and Basile, Davide and Aranda, Jesus and Perrouin, Gilles and Schobbens, Pierre Yves},
title = {Time for Networks: Mutation Testing for Timed Automata Networks},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644378},
doi = {10.1145/3644033.3644378},
abstract = {Mutation Testing (MT) is a technique employed to assess the efficacy of tests by introducing artificial faults, known as mutations, into the system. The goal is to evaluate how well the tests can detect these mutations. These artificial faults are generated using mutation operators, which produce a set of mutations derived from the original system. Mutation operators and frameworks exist for a variety of programming languages, and model-based mutation testing is gaining traction, particularly for timed safety-critical systems. This paper focuses on extending MT to Networks of Timed Automata (NTAs), an area that has not been extensively explored. We introduce mutation operators designed for NTAs specified in UPPAAL, aiming to create temporal interaction faults. We assess the effectiveness of these operators on five UPPAAL NTAs sourced from the literature, specifically examining the generation of equivalent and duplicate mutants. Our results demonstrate a varied prevalence of equivalent mutants (from 12% to 71%) while the number of duplicates is less. In all cases, timed bisimulation was able to process each mutant pair in less than one second.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {44–54},
numpages = {11},
keywords = {model-based mutation testing, UPPAAL, bisimulation},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1145/3643655.3643876,
author = {Rossi, Maria Teresa and Tundo, Alessandro and Mariani, Leonardo},
title = {Towards Model-Driven Dashboard Generation for Systems-of-Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643876},
doi = {10.1145/3643655.3643876},
abstract = {Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {9–12},
numpages = {4},
keywords = {automatic dashboard generation, model-driven engineering, model-based dashboard, systems of systems, monitoring dashboard},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3639478.3640048,
author = {Pan, Hongyue and Yang, Yilong},
title = {ValidGen: A Tool for Automatic Generation of Validation Scripts to Support Rapid Requirements Validation},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640048},
doi = {10.1145/3639478.3640048},
abstract = {Rapid prototyping is an effective way for requirement validation in the earliest stages of software development. Our previous work, RM2PT, can automatically generate software prototypes from requirements models to support incremental and rapid requirements validation. This paper proposes a CASE tool named ValidGen based on RM2PT, which can automatically generate validation scripts to execute the prototype. Thanks to these validation scripts, the stakeholder only needs to monitor the execution process without selecting the system operation and typing the input parameters, which significantly reduces the time and effort needed for validating requirements. We adopted three case studies to evaluate the tool, and the results show that the tool requires only about 60% of the time for requirements validation compared to traditional methods. Overall, the results were satisfactory. The proposed tool can be further extended and applied for requirements validation in the software industry.The tool can be downloaded at https://rm2pt.com/advs/validgen/, and a demo video is at https://youtu.be/AP9Ymg1ewIA.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {124–128},
numpages = {5},
keywords = {requirements model, software prototype, code generation, requirements validation},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3623315,
author = {Li, Junqiang and Li, Senyi and Li, Keyao and Luo, Falin and Yu, Hongfang and Li, Shanshan and Li, Xiang},
title = {ECFuzz: Effective Configuration Fuzzing for Large-Scale Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623315},
doi = {10.1145/3597503.3623315},
abstract = {A large-scale system contains a huge configuration space because of its large number of configuration parameters. This leads to a combination explosion among configuration parameters when exploring the configuration space. Existing configuration testing techniques first use fuzzing to generate different configuration parameters, and then directly inject them into the program under test to find configuration-induced bugs. However, they do not fully consider the complexity of large-scale systems, resulting in low testing effectiveness. In this paper, we propose ECFuzz, an effective configuration fuzzer for large-scale systems. Our core approach consists of (i) Multi-dimensional configuration generation strategy. ECFuzz first designs different mutation strategies according to different dependencies and selects multiple configuration parameters from the candidate configuration parameters to effectively generate configuration parameters; (ii) Unit-testing-oriented configuration validation strategy. ECFuzz introduces unit testing into configuration testing techniques to filter out configuration parameters that are unlikely to yield errors before executing system testing, and effectively validate generated configuration parameters. We have conducted extensive experiments in real-world large-scale systems including HCommon, HDFS, HBase, ZooKeeper and Alluxio. Our evaluation shows that ECFuzz is effective in finding configuration-induced crash bugs. Compared with the state-of-the-art configuration testing tools including ConfTest, ConfErr and ConfDiagDetector, ECFuzz finds 60.3--67 more unexpected failures when the same 1000 testcases are injected into the system with an increase of 1.87x--2.63x. Moreover, ECFuzz has exposed 14 previously unknown bugs, and 5 of them have been confirmed.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {48},
numpages = {12},
keywords = {configuration, large-scale systems, testing, fuzzing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639475.3640107,
author = {Liebel, Grischa and Langlois, Noah and Gama, Kiev},
title = {Challenges, Strengths, and Strategies of Software Engineers with ADHD: A Case Study},
year = {2024},
isbn = {9798400704994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639475.3640107},
doi = {10.1145/3639475.3640107},
abstract = {Neurodiversity describes brain function variation in individuals, including Attention deficit hyperactivity disorder (ADHD) and Autism spectrum disorder. Neurodivergent individuals both experience challenges and exhibit strengths in the workplace. As an important disorder included under the neurodiversity term, an estimated 5.0% to 7.1% of the world population have ADHD. However, existing studies involving ADHD in the workplace are of general nature and do not focus on software engineering (SE) activities. To address this gap, we performed an exploratory qualitative case study on the experiences of people with ADHD working in SE. We find that people with ADHD struggle with several important SE-related activities, e.g., task organisation and estimation, attention to work, relation to others. Furthermore, they experience issues with physical and mental health. In terms of strengths, they exhibit, e.g., increased creative skills, perform well when solving puzzles, and have the capability to think ahead. Our findings align with clinical ADHD research, having important implications to SE practice. Lay Abstract - Neurodiversity describes brain function variation in individuals, such as Attention deficit hyperactivity disorder (ADHD) and Autism spectrum disorder. People included under this term often experience problems in their work, e.g., due to differences in communication or behaviour, but also exhibit strengths compared to people without these disorders. To better include them, it is essential that we understand how these challenges and strengths manifest in different professions. There is limited research on how neurodiversity affects professionals in software engineering (SE), an environment characterised by a rapid pace, frequent change, and intense collaborative work. Therefore, we studied the strengths, challenges, and strategies of SE professionals with ADHD, a disorder affecting approximately 5.0% to 7.1% of the world population. We find that these professionals perceive many common SE activities as challenging, e.g., estimating how long tasks take, or maintaining focus. Interestingly, they also exhibit highly relevant strengths to the SE industry, such as increased creativity and systems thinking. Based on our findings, we provide several recommendations on how SE companies can better support employees with ADHD.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
pages = {57–68},
numpages = {12},
keywords = {neurodiversity, ADHD, inclusion, diversity, case study},
location = {Lisbon, Portugal},
series = {ICSE-SEIS'24}
}

@inproceedings{10.1145/3597503.3639113,
author = {Liu, Jiakun and Zhang, Zicheng and Hu, Xing and Thung, Ferdian and Maoz, Shahar and Gao, Debin and Toch, Eran and Zhao, Zhipeng and Lo, David},
title = {MiniMon: Minimizing Android Applications with Intelligent Monitoring-Based Debloating},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639113},
doi = {10.1145/3597503.3639113},
abstract = {The size of Android applications is getting larger to fulfill the requirements of various users. However, not all the features of the applications are needed and desired by a specific user. The unnecessary and non-desired features can increase the attack surface and consume system resources such as storage and memory. To address this issue, we propose a framework, MiniMon, to debloat unnecessary features from an Android app based on the logs of specific users' interactions with the app.However, rarely used features may not be recorded during the data collection, and users' preferences may change slightly over time. To address these challenges, we embed several solutions in our framework that can uncover user-desired features by learning and generalizing from the logs of how users interact with an application. MiniMon first collects the application methods that are executed when users interact with it. Then, given the collected executed methods and the call graph of the application, MiniMon applies 10 techniques to generalize from logs. These include three program analysis-based techniques, two graph clustering-based techniques, and five graph embedding-based techniques to identify the additional methods in an app that are similar to the logged executed methods. Finally, MiniMon generates a debloated application by removing methods that are not similar to the executed methods. To evaluate the performance of variants of MiniMon that use different generalization techniques, we create a benchmark for a controlled experiment. The results show that the graph embedding-based generalization technique that considers the information of all nodes in the call graph is the best, and can correctly uncover 75.5% of the unobserved but desired behaviors and still debloat more than half of the app. We also conducted a user study that uncovers that the use of the intelligent (generalization) method of MiniMon boosts the overall user satisfaction rate by 37.6%.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {206},
numpages = {13},
keywords = {android, software debloating, log analysis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00023,
author = {Su, Yanqi and Han, Zheming and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Lu, Qinghua},
title = {SoapOperaTG: A Tool for System Knowledge Graph Based Soap Opera Test Generation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00023},
doi = {10.1109/ICSE-Companion58688.2023.00023},
abstract = {Exploratory testing is an effective testing approach for the system-level testing from the end user's perspective, which is widely practiced and appreciated in the software industry. Although many concrete principles and guidelines for performing exploratory testing have been proposed, there are no effective tools for automatic generation of exploratory test scenarios (a.k.a soap opera tests). In this paper, we propose a tool named SoapOperaTG for automatic soap opera test generation by leveraging the scenario and oracle knowledge in bug reports. We first construct a system knowledge graph (KG) of user tasks and failures from the preconditions, steps to reproduce (S2Rs), expected behavior (EB) and observed behavior (OB) in bug reports. Then, we create soap opera tests by combining the scenarios of relevant bugs based on the system knowledge graph. SoapOperaTG is implemented as a web tool to present the generated test scenarios. In our user study, 5 users find 18 bugs in Mozilla Firefox (a mature, well-maintained software system) in 2 hours using SoapOperaTG, while the control group finds only 5 bugs based on the recommended similar bugs. SoapOperaTG can be found at https://github.com/SuYanqi/SYS-KG. Demo video can be found at https://youtu.be/xcXmY8qGDSc.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {51–54},
numpages = {4},
keywords = {user tasks and failures, exploratory testing, knowledge graph},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00202,
author = {Paltenghi, Matteo and Pradel, Michael},
title = {MorphQ: Metamorphic Testing of the Qiskit Quantum Computing Platform},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00202},
doi = {10.1109/ICSE48619.2023.00202},
abstract = {As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2413–2424},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00062,
author = {Wang, Teng and Jia, Zhouyang and Li, Shanshan and Zheng, Si and Yu, Yue and Xu, Erci and Peng, Shaoliang and Liao, Xiangke},
title = {Understanding and Detecting On-the-Fly Configuration Bugs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00062},
doi = {10.1109/ICSE48619.2023.00062},
abstract = {Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short.In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented Parachute, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. Parachute generates tests for on-the-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated Parachute on 7 real-world software systems. The results show that Parachute detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {628–639},
numpages = {12},
keywords = {metamorphic testing, bug detection, on-the-fly configuration updates},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639476.3639772,
author = {Vorobyov, Kostyantyn and Gauthier, Francois and Krishnan, Padmanabhan},
title = {Synthesis of Allowlists for Runtime Protection against SQLi},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639772},
doi = {10.1145/3639476.3639772},
abstract = {Data is the new oil. This metaphor is commonly used to highlight the fact that data is a highly valuable commodity. Nowadays, much of worldwide data sits in SQL databases and transits through various web-based applications. As the value of data increases and attracts more attention from malicious actors, application protections against SQL injections need to become more sophisticated. Although SQL injections have been known for many years, they are still one of the top security vulnerabilities. For example, in 2022 more than 1000 CVEs related to SQL injection were reported. We propose a runtime application protection approach that infers and constrains the information that can be disclosed by database-backed applications. Where existing approaches use syntax or hand-crafted features as a proxy for information disclosure, we propose a lightweight, but precise, information disclosure model that faithfully captures the semantics of SQL and achieves finer-grain security.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {16–20},
numpages = {5},
keywords = {SQLi, synthesis, generalisation},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1109/ICSE48619.2023.00219,
author = {Xia, Boming and Bi, Tingting and Xing, Zhenchang and Lu, Qinghua and Zhu, Liming},
title = {An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00219},
doi = {10.1109/ICSE48619.2023.00219},
abstract = {The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into three topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2630–2642},
numpages = {13},
keywords = {empirical study, responsible AI, bill of materials, SBOM, software bill of materials},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643788.3648008,
author = {Al-Bataineh, Omar and Moonen, Leon},
title = {Towards Developing Effective Fault localization Technique for Termination Bugs in Loop Programs},
year = {2024},
isbn = {9798400705779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643788.3648008},
doi = {10.1145/3643788.3648008},
abstract = {This paper describes a fault localization (FL) framework for termination bugs, which are programming errors that cause the program to run indefinitely. Due to the high degree of complexity involved in identifying the root causes of a termination bug, a framework for automated debugging of termination bugs needs to integrate the strengths of multiple technologies: termination provers, static analysis, and slicing techniques. Termination provers are used to identify the non-terminating loops in the examined loop program and to generate counter-examples for the non-terminating cases, static analysis is used to obtain useful syntactic information about the analyzed loop program, whereas program slicing is used to reduce the complexity of the analyzed multiple-loop programs. We demonstrate how to combine the technologies to produce a feasible list of suspicious statements for termination bugs. We also conduct an empirical analysis to evaluate the effectiveness of current automated program repair (APR) tools in handling termination bugs using dynamic FL techniques, and identify the challenges and limitations that FL tools face when handling termination bugs.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Automated Program Repair},
pages = {5–8},
numpages = {4},
location = {Lisbon, Portugal},
series = {APR '24}
}

@inproceedings{10.1145/3643991.3644915,
author = {Bernardo, Jo\~{a}o Helis and Da Costa, Daniel Alencar and Medeiros, S\'{e}rgio Queiroz de and Kulesza, Uir\'{a}},
title = {How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644915},
doi = {10.1145/3643991.3644915},
abstract = {Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build duration, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualitative analysis illuminates the discussions around CI in both ML and non-ML projects, encompassing themes like CI Build Execution and Status, CI Testing, and CI Infrastructure. These insights shed light on the unique challenges faced by ML projects in adopting CI practices effectively.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {665–676},
numpages = {12},
keywords = {continuous integration, machine learning, GitHub actions, mining software repositories},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639477.3639755,
author = {Kapel, Eileen and Cruz, Luis and Spinellis, Diomidis and Van Deursen, Arie},
title = {On the Difficulty of Identifying Incident-Inducing Changes},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639755},
doi = {10.1145/3639477.3639755},
abstract = {Effective change management is crucial for businesses heavily reliant on software and services to minimise incidents induced by changes. Unfortunately, in practice it is often difficult to effectively use artificial intelligence for IT Operations (AIOps) to enhance service management, primarily due to inadequate data quality. Establishing reliable links between changes and the induced incidents is crucial for identifying patterns, improving change deployment, identifying high-risk changes, and enhancing incident response. In this research, we investigate the enhancement of traceability between changes and incidents through AIOps methods. Our approach involves a close examination of incident-inducing changes, the replication of methods linking incidents to the changes that caused them, introducing an adapted method, and demonstrating its results using historical data and practical evaluations. Our findings reveal that incident-inducing changes exhibit different characteristics dependent on context. Furthermore, a significant disparity exists between assessments based on historical data and real-world observation, with an increased occurrence of false positives when identifying links between unlabeled changes and incidents. This study highlights the complex nature of identifying links between changes and incidents, emphasising the contextual influence on AIOps method effectiveness. While we are actively working on improving the quality of current data through AIOps approaches, it remains apparent that further measures are necessary to address issues like data imbalances and promote a postmortem culture that brings attention to the value of properly administrating tickets. A better overview of change failure rates contributes to improved risk compliance and reliable change management.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {36–46},
numpages = {11},
keywords = {change management, incident management, traceability},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3639476.3639765,
author = {Klimis, Vasileios and Donaldson, Alastair F. and Vafeiadis, Viktor and Wickerson, John and Raad, Azalea},
title = {Challenges in Empirically Testing Memory Persistency Models},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639765},
doi = {10.1145/3639476.3639765},
abstract = {Memory persistency models provide the foundational rules for software engineers to develop applications that take advantage of non-volatile memory (NVM), dictating which (and when) writes to NVM are deemed persistent. Though formalised for Intel-x86 and Arm architectures, these models remain empirically unvalidated on actual machines. Conventional validation methods for memory consistency models fall short as test programs cannot differentiate between volatile cache reads and those from NVM. To address this, we employed a commercial device designed to intercept and log data on a system's memory bus in their order of arrival. We used this device to conduct a campaign using litmus tests---small programs designed to assess specific memory persistency behaviours---aimed at empirically validating Intel-x86 and Arm machine persistency guarantees.We noted out-of-order memory writes and ensured they were not merely artifacts of our test setup. Analysis revealed Intel-x86's architecture cannot be validated via memory bus interception due to legitimate early subsystem reordering. Intel engineers confirmed the absence of dependable validation methods for the persistency claims of their architectures. Meanwhile, an expert-recommended Arm machine did not align with the formal persistency model due to a specification loophole, and further investigation suggests that no market-available Arm machine fully supports NVM.Our finding for Intel highlights a major concern for software developers wishing to take advantage of NVM: currently there is, to our knowledge, no viable way to confirm the persistency guarantees claimed by Intel. Our results for Arm suggest that our interception-based approach is viable for reliably detecting reorderings in the memory subsystem, which will be valuable for empirical validation once NVM-supporting machines become available.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {82–86},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00052,
author = {Charoenwet, Wachiraphan},
title = {Complementing Secure Code Review with Automated Program Analysis},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00052},
doi = {10.1109/ICSE-Companion58688.2023.00052},
abstract = {Code review is an important activity in software engineering process to reduce software defects before the production phase. It is crucial that software defects are identified as soon as they are introduced because their impact can be amplified if they are discovered in the later stages. However, previous studies have observed that vulnerability, a software weakness that could be exploited by an attacker, can slip through the code review process because of the limited resources and security awareness of reviewers. Approaches such as automated program analysis have been recommended to assist this problem. Yet, it is unclear about the capability of automated program analysis to augment human reviewers on the security aspects. This research project aims to investigate to what extent, and how, can different automated program analysis approaches complement human reviewers in the code review process.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {189–191},
numpages = {3},
keywords = {automated program analysis, code review assistant, secure code review, modern code review},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00077,
author = {Dai, Hetong and Tang, Yiming and Li, Heng and Shang, Weiyi},
title = {PILAR: Studying and Mitigating the Influence of Configurations on Log Parsing},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00077},
doi = {10.1109/ICSE48619.2023.00077},
abstract = {The significance of logs has been widely acknowledged with the adoption of various log analysis techniques that assist in software engineering tasks. Many log analysis techniques require structured logs as input while raw logs are typically unstructured. Automated log parsing is proposed to convert unstructured raw logs into structured log templates. Some log parsers achieve promising accuracy, yet they rely on significant efforts from the users to tune the parameters to achieve optimal results. In this paper, we first conduct an empirical study to understand the influence of the configurable parameters of six state-of-the-art log parsers on their parsing results on three aspects: 1) varying the parameters while using the same dataset, 2) keeping the same parameters while using different datasets, and 3) using different samples from the same dataset. Our results show that all these parsers are sensitive to the parameters, posing challenges to their adoption in practice. To mitigate such challenges, we propose PILAR (Parameter Insensitive Log Parser), an entropy-based log parsing approach. We compare PILAR with the existing log parsers on the same three aspects and find that PILAR is the most parameter-insensitive one. In addition, PILAR achieves the second highest parsing accuracy and efficiency among all the state-of-the-art log parsers. This paper paves the road for easing the adoption of log analysis in software engineer practices.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {818–829},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00085,
author = {Deng, Wenjing},
title = {AIGROW: A Feedback-Driven Test Generation Framework for Hardware Model Checkers},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00085},
doi = {10.1109/ICSE-Companion58688.2023.00085},
abstract = {This research abstract introduces an effective and efficient approach to automatically generate high-quality hardware model checker benchmarks. The key contribution of this work is to model the input format of hardware model checkers using a tree-based structure named ARTree and build an effective feedback-driven test generation framework based on ARTree named AIGROW. The evaluation shows that AIGROW generates very small but high-quality benchmarks for coverage-oriented and performance-oriented testing and outperforms the existing generation-based testing tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {314–316},
numpages = {3},
keywords = {hardware model checker, test generation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/WETSEB66605.2025.00014,
author = {Chen, Zhiyang and Kemper, Phillip and Liu, Yi and Gorzny, Jan and Siqueira, Diego and Li, Yuekang and Pellegrino, Donato and Derka, Martin},
title = {A Methodology for Replicating Historical Exploits on EVM-Compatible Blockchains},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/WETSEB66605.2025.00014},
doi = {10.1109/WETSEB66605.2025.00014},
abstract = {Replicating historical exploits on blockchain platforms is essential for testing emerging real-time defense mechanisms. However, existing tools primarily fork blockchain states and replay original exploit transactions rather than replicating these hacks in new blocks. This paper introduces a methodology for replicating exploit transactions across EVM-compatible blockchains, enabling testing of new security measures. By addressing key challenges in address mapping, contract deployment and storage configuration, our approach successfully replicates 18 security challenge exploit transactions and 12 real-world exploit transactions. This evaluation results confirm the methodology’s effectiveness in recreating both controlled challenges and real-world hacks, marking a significant advancement in smart contract security research and testing.},
booktitle = {Proceedings of the 2025 IEEE/ACM 7th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {57–60},
numpages = {4},
location = {Ottawa, ON, Canada},
series = {WETSEB '25}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00083,
author = {Pregerson, Eli},
title = {Path Complexity of Recursive Functions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00083},
doi = {10.1109/ICSE-Companion58688.2023.00083},
abstract = {Path coverage is of critical importance in software testing and verification, and further, path explosion is a well-known challenge for automatic software analysis techniques like symbolic execution [7]. Asymptotic Path Complexity (APC), a code complexity metric developed in my research lab, formalizes the quantitative measurement of path explosion.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {308–310},
numpages = {3},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00101,
author = {Dou, Wensheng and Cui, Ziyu and Dai, Qianwang and Song, Jiansen and Wang, Dong and Gao, Yu and Wang, Wei and Wei, Jun and Chen, Lei and Wang, Hanmo and Zhong, Hua and Huang, Tao},
title = {Detecting Isolation Bugs via Transaction Oracle Construction},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00101},
doi = {10.1109/ICSE48619.2023.00101},
abstract = {Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, e.g., incorrect query results and database states.In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, i.e., MySQL, MariaDB, and TiDB. We have detected 5 previously-unknown isolation bugs in the latest versions of these DBMSs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1123–1135},
numpages = {13},
keywords = {oracle, isolation, transaction, database system},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644032.3644449,
author = {Leu, Benjamin and Volken, Jonas and Kropp, Martin and Dogru, Nejdet and Anslow, Craig and Biddle, Robert},
title = {Reducing Workload in Using AI-based API REST Test Generation},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644449},
doi = {10.1145/3644032.3644449},
abstract = {Modern software applications, notably those utilizing microservices architectures, rely heavily on REST API technology for communication. Testing these APIs is challenging, time-consuming, and prone to errors. This paper introduces Pulse-UI, an AI-supported tool designed to enhance test sequence generation for REST APIs, aiming to reduce the workload involved in managing test sequences efficiently and improve overall test quality.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {147–148},
numpages = {2},
keywords = {software testing, AI-based testing, REST API, automation, teaching AI system, test case scenarios, human support},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3597503.3639172,
author = {Liu, Xuwei and You, Wei and Ye, Yapeng and Zhang, Zhuo and Huang, Jianjun and Zhang, Xiangyu},
title = {FuzzInMem: Fuzzing Programs via In-memory Structures},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639172},
doi = {10.1145/3597503.3639172},
abstract = {In recent years, coverage-based greybox fuzzing has proven to be an effective and practical technique for discovering software vulnerabilities. The availability of American Fuzzy Loop (AFL) has facilitated numerous advances in overcoming challenges in fuzzing. However, the issue of mutating complex file formats, such as PDF, remains unresolved due to strict constraints. Existing fuzzers often produce mutants that fail to parse by applications, limited by bit/byte mutations performed on input files. Our observation is that most in-memory representations of file formats are simple, and well-designed applications have built-in printer functions to emit these structures as files. Thus, we propose a new technique that mutates the in-memory structures of inputs and utilizes printer functions to regenerate mutated files. Unlike prior approaches that require complex analysis to learn file format constraints, our technique leverages the printer function to preserve format constraints. We implement a prototype called FuzzInMem and compare it with AFL as well as other state-of-the-art fuzzers, including AFL++, Mopt, Weizz, and FormatFuzzer. The results show that FuzzInMem is scalable and substantially outperforms general-purpose fuzzers in terms of valid seed generation and path coverage. By applying FuzzInMem to real-world applications, we found 29 unique vulnerabilities and were awarded 5 CVEs.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {131},
numpages = {13},
keywords = {fuzzing, software testing, program synthesis},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643796.3648464,
author = {Johnson, Oshando and Piskachev, Goran and Krishnamurthy, Ranjith and Bodden, Eric},
title = {Detecting Security-Relevant Methods using Multi-label Machine Learning},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648464},
doi = {10.1145/3643796.3648464},
abstract = {To detect security vulnerabilities, static analysis tools need to be configured with security-relevant methods. Current approaches can automatically identify such methods using binary relevance machine learning approaches. However, they ignore dependencies among security-relevant methods, over-generalize and perform poorly in practice. Additionally, users have to nevertheless manually configure static analysis tools using the detected methods. Based on feedback from users and our observations, the excessive manual steps can often be tedious, error-prone and counter-intuitive.In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects security-relevant methods using a multi-label machine learning approach that considers dependencies among labels. The plugin can automatically generate configurations for static analysis tools, run the static analysis, and show the results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine learning approach has a higher F1-Measure than related approaches. Moreover, the plugin reduces and simplifies the manual effort required when configuring and using static analysis tools.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {101–106},
numpages = {6},
keywords = {static analysis, software security, machine learning, vulnerability detection, multi-label learning, IntelliJ plugin development},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00078,
author = {Ahmad, Fozail},
title = {Graph Solver as a Service},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00078},
doi = {10.1109/ICSE-Companion58688.2023.00078},
abstract = {Graphs can be a key abstraction for formal verification challenges. As such, graph solvers are essential tools for synthesizing scalable domain-specific consistent graph models, which are both realistic and diverse. The main goal of this doctoral research plan is to develop a graph solver framework based on a state-of-the-art graph solver in order to provide a graph solver as a service. We expect this will improve the overall scalability of graph solvers whilst increasing the usage and adoption of such tools. The scalability of the framework will be investigated in several case studies of different complexity.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {291–293},
numpages = {3},
keywords = {software as a service, partial graph models, consistent graph generation, graph solver},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3644033.3644382,
author = {Mahe, Erwan and Bannour, Boutheina and Gaston, Christophe and Lapitre, Arnault and Le Gall, Pascale},
title = {Finite Automata synthesis from Interactions},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644382},
doi = {10.1145/3644033.3644382},
abstract = {Interactions are graphical models representing communication flows between actors. Well-known interaction languages include UML Sequence Diagrams or Message Sequence Charts. Even though interactions allow for concise and intuitive specifications, their use remains limited in formal verification, partly because the subsets of formalized languages often lack expressiveness. As many verification methods, such as model-checking or runtime verification, are routinely available for finite automata, we propose a new approach to generate finite automata from an expressive interaction language with operators such as the concurrent region. Our approach leverages an operational semantics to compute derivatives of an interaction and assimilate them to states of a finite automata. In addition, we use term rewriting to merge states on-the-fly so as to obtain small automata without relying on costly a-posteriori minimization techniques.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {12–22},
numpages = {11},
keywords = {interaction language, sequence diagram, message sequence chart, non-deterministic finite automaton, operational semantics, term rewriting},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00044,
author = {Cao, Sicong and Sun, Xiaobing and Wu, Xiaoxue and Bo, Lili and Li, Bin and Wu, Rongxin and Liu, Wei and He, Biao and Ouyang, Yu and Li, Jiajia},
title = {Improving Java Deserialization Gadget Chain Mining via Overriding-Guided Object Generation},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00044},
doi = {10.1109/ICSE48619.2023.00044},
abstract = {Java (de)serialization is prone to causing security-critical vulnerabilities that attackers can invoke existing methods (gadgets) on the application's classpath to construct a gadget chain to perform malicious behaviors. Several techniques have been proposed to statically identify suspicious gadget chains and dynamically generate injection objects for fuzzing. However, due to their incomplete support for dynamic program features (e.g., Java runtime polymorphism) and ineffective injection object generation for fuzzing, the existing techniques are still far from satisfactory.In this paper, we first performed an empirical study to investigate the characteristics of Java deserialization vulnerabilities based on our manually collected 86 publicly known gadget chains. The empirical results show that 1) Java deserialization gadgets are usually exploited by abusing runtime polymorphism, which enables attackers to reuse serializable overridden methods; and 2) attackers usually invoke exploitable overridden methods (gadgets) via dynamic binding to generate injection objects for gadget chain construction. Based on our empirical findings, we propose a novel gadget chain mining approach, GCMiner, which captures both explicit and implicit method calls to identify more gadget chains, and adopts an overriding-guided object generation approach to generate valid injection objects for fuzzing. The evaluation results show that GCMiner significantly outperforms the state-of-the-art techniques, and discovers 56 unique gadget chains that cannot be identified by the baseline approaches.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {397–409},
numpages = {13},
keywords = {exploit generation, method overriding, gadget chain, java deserialization vulnerability},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00028,
author = {Marchezan, Luciano and Assun\c{c}\~{a}o, Wesley K. G. and Herac, Edvin and Keplinger, Felix and Egyed, Alexander and Lauwerys, Christophe},
title = {Fulfilling Industrial Needs for Consistency among Engineering Artifacts},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00028},
doi = {10.1109/ICSE-SEIP58684.2023.00028},
abstract = {Maintaining the consistency of engineering artifacts is a challenge faced by several engineering companies. This is more evident when the engineering artifacts are created using different tools and have different formats. This is the context of a company that builds agricultural machines, where components are developed using a decentralized iterative process. In this study, we present an approach developed in collaboration with an industry partner to address the issues and requirements of a real engineering scenario. These issues include the manual execution of consistency checking, without guidelines that formalize the activity. Furthermore, the industry partner aims at a flexible solution that can be applied without disrupting the current development process significantly. The proposed approach applies consistency rules (CR) defined to automatically detect and provide inconsistency feedback to engineers in real-time. The approach presented in this work also allows the customization of the CRs, giving flexibility to how the consistency checking is applied. The feasibility of our approach is demonstrated in such an industrial scenario, with a discussion about how the issues were addressed and the limitations of the current solution. We also perform a scalability evaluation showing that the approach can be applied in large systems (up to 21,061 elements) in a reasonable amount of time, taking less than 0.25 milliseconds to apply a CR, in the worst cases.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {246–257},
numpages = {12},
keywords = {consistency flexibility, trace generation, consistency checking, model-driven engineering},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00215,
author = {Poskitt, Christopher M. and Chen, Yuqi and Sun, Jun and Jiang, Yu},
title = {Finding Causally Different Tests for an Industrial Control System},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00215},
doi = {10.1109/ICSE48619.2023.00215},
abstract = {Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully different' tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2578–2590},
numpages = {13},
keywords = {causality, equivalence classes, test diversity, fuzzing, cyber-physical systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3526072.3527530,
author = {Yan, Songyang and Fan, Ming},
title = {AdaFrenetic at the SBST 2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527530},
doi = {10.1145/3526072.3527530},
abstract = {AdaFrenetic is a test generation tool for testing Autonomous Driving System (ADS). It extends the genetic algorithm-based testing tool Frenetic by adjusting the road points to reduce the number of invalid test cases. This paper provides a brief overview of the tool and analyzes the results of AdaFrenetic's performance in the Cyber-physical systems (CPS) testing tool competition at SBST 2022.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {41–42},
numpages = {2},
keywords = {virtual roads, simulation testing, search-based software testing, cyber-physical systems},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3639478.3643107,
author = {Heo, Jinseok and Lee, Eunseok},
title = {Analyzing the Impact of Context Representation and Scope in Code Infilling},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643107},
doi = {10.1145/3639478.3643107},
abstract = {Existing studies solve software engineering tasks using code infilling through LLMC. They utilize context information, which refers to data near the target code of infilling, as input prompts. Although prompts are essential for infilling the target code, current studies use them without analyzing the impact of the representation and scope of context on code infilling. In this study, we analyzed how context representation and scope affect the performance of code infilling. We used XLCost, which contains code, comments, and a function comment for various programming languages. The combination of code and a function comment for context representation yielded the best code infilling performance. Furthermore, we found that the context scope is proportional to performance. Our analysis results can be applied in various tasks that involve code infilling in the future.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {333–334},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00186,
author = {Gao, Yu and Dou, Wensheng and Wang, Dong and Feng, Wenhan and Wei, Jun and Zhong, Hua and Huang, Tao},
title = {Coverage Guided Fault Injection for Cloud Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00186},
doi = {10.1109/ICSE48619.2023.00186},
abstract = {To support high reliability and availability, modern cloud systems are designed to be resilient to node crashes and reboots. That is, a cloud system should gracefully recover from node crashes/reboots and continue to function. However, node crashes/reboots that occur under special timing can trigger crash recovery bugs that lie in incorrect crash recovery protocols and their implementations. To ensure that a cloud system is free from crash recovery bugs, some fault injection approaches have been proposed to test whether a cloud system can correctly recover from various crash scenarios. These approaches are not effective in exploring the huge crash scenario space without developers' knowledge.In this paper, we propose CrashFuzz, a fault injection testing approach that can effectively test crash recovery behaviors and reveal crash recovery bugs in cloud systems. CrashFuzz mutates the combinations of possible node crashes and reboots according to runtime feedbacks, and prioritizes the combinations that are prone to increase code coverage and trigger crash recovery bugs for smart exploration. We have implemented CrashFuzz and evaluated it on three popular open-source cloud systems, i.e., ZooKeeper, HDFS and HBase. CrashFuzz has detected 4 unknown bugs and 1 known bug. Compared with other fault injection approaches, CrashFuzz can detect more crash recovery bugs and achieve higher code coverage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2211–2223},
numpages = {13},
keywords = {fuzzing, bug detection, fault injection, crash recovery bug, cloud system},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643659.3648556,
author = {Moon, Seokhyeon and Jhi, Yoonchan},
title = {EvoFuzz at the SBFT 2024 Tool Competition},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648556},
doi = {10.1145/3643659.3648556},
abstract = {EvoFuzz is an automated fuzzing tool that integrates fuzzing techniques into EvoSuite to improve code coverage. It first uses EvoSuite to generate a test suite, which is then utilized for fuzzing. During this process, EvoFuzz ensures the generated test suite is executable by performing strict code validity checks. Additionally, EvoFuzz actively explores new variables/values to achieve more code coverage. Our experimental results on the SBFT2024 benchmark demonstrate that EvoFuzz outperforms EvoSuite in both code coverage and mutation kill ratio.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {63–64},
numpages = {2},
keywords = {software testing, fuzzing, test case generation, code validity},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3526072.3527523,
author = {Tr\"{u}benbach, Daniel and M\"{u}ller, Sebastian and Grunske, Lars},
title = {A comparative evaluation on the quality of manual and automatic test case generation techniques for scientific software: a case study of a python project for material science workflows},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527523},
doi = {10.1145/3526072.3527523},
abstract = {Writing software tests is essential to ensure a high quality of the software project under test. However, writing tests manually is time consuming and expensive. Especially in research fields of the natural sciences, scientists do not have a formal education in software engineering. Thus, automatic test case generation is particularly promising to help build good test suites.In this case study, we investigate the efficacy of automated test case generation approaches for the Python project Atomic Simulation Environment (ASE) used in the material sciences. We compare the branch and mutation coverages reached by both the automatic approaches, as well as a manually created test suite. Finally, we statistically evaluate the measured coverages by each approach against those reached by any of the other approaches.We find that while all evaluated approaches are able to improve upon the original test suite of ASE, none of the automated test case generation algorithms manage to come close to the coverages reached by the manually created test suite. We hypothesize this may be due to the fact that none of the employed test case generation approaches were developed to work on complex structured inputs. Thus, we conclude that more work may be needed if automated test case generation is used on software that requires this type of input.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {6–13},
numpages = {8},
keywords = {test case generation, search based testing, scientific software, scientific computing},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1145/3639478.3643534,
author = {Minna, Francesco and Blaise, Agathe and Massacci, Fabio and Tuma, Katja},
title = {Automated Security Repair for Helm Charts},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643534},
doi = {10.1145/3639478.3643534},
abstract = {We aim to evaluate and compare open-source static analyzers for Helm Charts, a package manager to deploy applications on Kubernetes (K8s). Specifically, we developed a pipeline to measure what misconfigurations are found by each tool, to provide automatic misconfiguration repair, and whether this latter breaks application functionalities. To evaluate our approach, we analyzed the 60 most common Helm Charts available on Artifact Hub, seven open-source Helm Charts analyzers, and generated functionality profiles for each chart application. We found several bugs and inconsistency issues with the tools, which we reported on respective tool repositories, and concluded that such tools that should provide automatic security repair still require significant manual intervention.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {412–413},
numpages = {2},
keywords = {helm charts, automated security repair, kubernetes, misconfigurations},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643093,
author = {Schulz-Rosengarten, Alexander and Ahmad, Akash and Clement, Malte and von Hanxleden, Reinhard and Asch, Benjamin and Lohstroh, Marten and Lee, Edward A. and Quiros, Gustavo and Shukla, Ankit},
title = {Behavior Trees with Dataflow: Coordinating Reactive Tasks in Lingua Franca},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643093},
doi = {10.1145/3639478.3643093},
abstract = {Behavior Trees (BTs) provide a lean set of control flow elements that are easily composable in a modular tree structure. They are well established for modeling the high-level behavior of non-player characters in computer games and recently gained popularity in other areas such as industrial automation.While BTs nicely express control, data handling aspects so far must be provided separately, e. g. in the form of blackboards. This may hamper reusability and can be a source of nondeterminism.We here propose a dataflow extension to BTs that explicitly models data relations and communication. We realize and validate that approach in the recently introduced polyglot coordination language Lingua Franca (LF).},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {304–305},
numpages = {2},
keywords = {behavior trees, reactive systems, coordination languages},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643531,
author = {Yin, Zhipeng and Wang, Zichong and Zhang, Wenbin},
title = {Improving Fairness in Machine Learning Software via Counterfactual Fairness Thinking},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643531},
doi = {10.1145/3639478.3643531},
abstract = {Machine Learning (ML) software is increasingly influencing decisions that impact individuals' lives. However, some of these decisions show discrimination and thus introduce algorithmic biases against certain social subgroups defined by sensitive attributes (e.g., gender or race). This has elevated software fairness bugs to an increasingly significant concern for software engineering (SE). However, most existing bias mitigation works enhance software fairness, a non-functional software property, at the cost of software performance. To this end, we proposed a novel framework, namely Group Equality Counterfactual Fairness (GECF), which aims to mitigate sensitive attribute bias and labeling bias using counterfactual fairness while reducing the resulting performance loss based on ensemble learning. Experimental results on 6 real-world datasets show the superiority of our proposed framework from different aspects.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {420–421},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643518,
author = {Kim, Myeongsoo and Pande, Santosh and Orso, Alessandro},
title = {Improving Program Debloating with 1-DU Chain Minimality},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643518},
doi = {10.1145/3639478.3643518},
abstract = {Modern software often struggles with bloat, leading to increased memory consumption and security vulnerabilities from unused code. In response, various program debloating techniques have been developed, typically utilizing test cases that represent functionalities users want to retain. These methods range from aggressive approaches, which prioritize maximal code reduction but may overfit to test cases and potentially reintroduce past security issues, to conservative strategies that aim to preserve all influenced code, often at the expense of less effective bloat reduction and security improvement. In this research, we present RLDebloatDU, an innovative debloating technique that employs 1-DU chain minimality within abstract syntax trees. Our approach maintains essential program data dependencies, striking a balance between aggressive code reduction and the preservation of program semantics. We evaluated RLDebloatDU on ten Linux kernel programs, comparing its performance with two leading debloating techniques: Chisel, known for its aggressive debloating approach, and Razor, recognized for its conservative strategy. RLDebloatDU significantly lowers the incidence of Common Vulnerabilities and Exposures (CVEs) and improves soundness compared to both, highlighting its efficacy in reducing security issues without reintroducing resolved security issues.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {384–385},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643058,
author = {Noureddine, Adel},
title = {Analyzing Software Energy Consumption},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643058},
doi = {10.1145/3639478.3643058},
abstract = {Analyzing the energy consumption of applications is a crucial step in building energy-efficient software. In this technical briefing, we detail software energy measurements, starting from hardware components all down towards measuring source code. In particular, we showcase how practitioners can diagnose the energy consumption of individual methods and execution branches on runtime. We show how this diagnosis helps in identifying energy hotspots and guiding practitioners in optimizing software energy.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {424–425},
numpages = {2},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3643118,
author = {De Jesus, Galileu Santos and Borba, Paulo and Bonif\'{a}cio, Rodrigo and De Oliveira, Matheus Barbosa},
title = {Lightweight Semantic Conflict Detection with Static Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3643118},
doi = {10.1145/3639478.3643118},
abstract = {Version control system tools empower developers to independently work on their development tasks. These tools also facilitate the integration of changes through merging operations, and report textual conflicts. However, during the integration of changes, developers might encounter other types of conflicts that are not detected by current merge tools. In this paper, we focus on dynamic semantic conflicts, which occur when merging reports no textual conflicts but results in undesired interference---causing unexpected program behavior at runtime. To address this issue, we propose a technique that explores the use of static analysis to detect interference when merging contributions from two developers. We evaluate our technique using a dataset of 99 experimental units extracted from merge scenarios. The results provide evidence that our technique presents significant interference detection capability (F1 Score of 0.50 and Accuracy of 0.60).},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {343–345},
numpages = {3},
keywords = {merge conflicts, configuration management, software evolution, static analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00091,
author = {Dilhara, Malinda and Dig, Danny and Ketkar, Ameya},
title = {PyEvolve: Automating Frequent Code Changes in Python ML Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00091},
doi = {10.1109/ICSE48619.2023.00091},
abstract = {Because of the naturalness of software and the rapid evolution of Machine Learning (ML) techniques, frequently repeated code change patterns (CPATs) occur often. They range from simple API migrations to changes involving several complex control structures such as for loops. While manually performing CPATs is tedious, the current state-of-the-art techniques for inferring transformation rules are not advanced enough to handle unseen variants of complex CPATs, resulting in a low recall rate. In this paper we present a novel, automated workflow that mines CPATs, infers the transformation rules, and then transplants them automatically to new target sites. We designed, implemented, evaluated and released this in a tool, PyEvolve. At its core is a novel data-flow, control-flow aware transformation rule inference engine. Our technique allows us to advance the state-of-the-art for transformation-by-example tools; without it, 70% of the code changes that PyEvolve transforms would not be possible to automate. Our thorough empirical evaluation of over 40,000 transformations shows 97% precision and 94% recall. By accepting 90% of CPATs generated by PyEvolve in famous open-source projects, developers confirmed its changes are useful.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {995–1007},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639087,
author = {Carvalho, Luiz and Degiovanni, Renzo and Cordy, Maxime and Aguirre, Nazareno and Le Traon, Yves and Papadakis, Mike},
title = {SpecBCFuzz: Fuzzing LTL Solvers with Boundary Conditions},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639087},
doi = {10.1145/3597503.3639087},
abstract = {LTL solvers check the satisfiability of Linear-time Temporal Logic (LTL) formulas and are widely used for verifying and testing critical software systems. Thus, potential bugs in the solvers' implementations can have a significant impact. We present SpecBCFuzz, a fuzzing method for finding bugs in LTL solvers, that is guided by boundary conditions (BCs), corner cases whose (un)satisfiability depends on rare traces. SpecBCFuzz implements a search-based algorithm that fuzzes LTL formulas giving relevance to BCs. It integrates syntactic and semantic similarity metrics to explore the vicinity of the seeded formulas with BCs. We evaluate SpecBCFuzz on 21 different configurations (including the latest and past releases) of four mature and state-of-the-art LTL solvers (NuSMV, Black, Aalta, and PLTL) that implement a diverse set of satisfiability algorithms. SpecBCFuzz produces 368,716 bug-triggering formulas, detecting bugs in 18 out of the 21 solvers' configurations we study. Overall, SpecBCFuzz reveals: soundness issues (wrong answers given by a solver) in Aalta and PLTL; crashes, e.g., segmentation faults, in NuSMV, Black and Aalta; flaky behaviors (different responses across re-runs of the solver on the same formula) in NuSMV and Aalta; performance bugs (large time performance degradation between successive versions of the solver on the same formula) in Black, Aalta and PLTL; and no bug in NuSMV BDD (all versions), suggesting that the latter is currently the most robust solver.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {123},
numpages = {13},
keywords = {fuzzing, search-based software engineering, linear-time temporal logic},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00042,
author = {Kuepper, Joel and Wu, David and Erbsen, Andres and Gross, Jason and Conoly, Owen and Sun, Chuyue and Tian, Samuel and Chlipala, Adam and Chuengsatiansup, Chitchanok and Genkin, Daniel and Wagner, Markus and Yarom, Yuval},
title = {CryptOpt: Automatic Optimization of Straightline Code},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00042},
doi = {10.1109/ICSE-Companion58688.2023.00042},
abstract = {Manual engineering of high-performance implementations typically consumes many resources and requires in-depth knowledge of the hardware. Compilers try to address these problems; however, they are limited by design in what they can do. To address this, we present CryptOpt, an automatic optimizer for long stretches of straightline code. Experimental results across eight hardware platforms show that CryptOpt achieves a speedup factor of up to 2.56 over current off-the-shelf compilers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {141–145},
numpages = {5},
keywords = {elliptic curve cryptography, local search, search based software engineering, automatic performance optimization},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643659.3648602,
author = {Tang, Shuncheng and Zhang, Zhenya and Cetinkaya, Ahmet and Arcaini, Paolo},
title = {TUMB at the SBFT 2024 Tool Competition - CPS-UAV Test Case Generation Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3648602},
doi = {10.1145/3643659.3648602},
abstract = {TUMB is a generator of scenarios for UAV testing, that participated in the UAV Testing Competition at SBFT 2024. TUMB relies on Monte Carlo Tree Search (MCTS) to search for different placements of obstacles of different sizes in the mission environment. Increasing the tree depth corresponds to adding a new obstacle to the environment, while adding a new node in the current tree level corresponds to the optimisation of the placement and of the dimension of the last added obstacle.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {53–54},
numpages = {2},
keywords = {unmanned aerial vehicle, test generation, simulation-based testing, Monte Carlo Tree Search},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1145/3639477.3639714,
author = {Zhang, Ying and Li, Peng and Ding, Yu and Wang, Lingxiang and Williams, Dan and Meng, Na},
title = {Broadly Enabling KLEE to Effortlessly Find Unrecoverable Errors in Rust},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639714},
doi = {10.1145/3639477.3639714},
abstract = {Rust is a general-purpose programming language designed for performance and safety. Unrecoverable errors (e.g., Divide by Zero) in Rust programs are critical, as they signal bad program states and terminate programs abruptly. Previous work has contributed to utilizing KLEE, a dynamic symbolic test engine, to verify the program would not panic. However, it is difficult for engineers who lack domain expertise to write test code correctly. Besides, the effectiveness of KLEE in finding panics in production Rust code has not been evaluated. We created an approach, called PanicCheck, to hide the complexity of verifying Rust programs with KLEE. Using PanicCheck, engineers only need to annotate the function-to-verify with #[panic_check]. The annotation guides PanicCheck to generate test code, compile the function together with tests, and execute KLEE for verification. After applying PanicCheck to 21 open-source and 2 closed-source projects, we found 61 test inputs that triggered panics; 59 of the 61 panics have been addressed by developers so far. Our research shows promising verification results by KLEE, while revealing technical challenges in using KLEE. Our experience will shed light on future practice and research in program verification.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {441–451},
numpages = {11},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1109/GREENS66463.2025.00005,
author = {Ailane, Mohamed Toufik and Rubner, Carolin and Rausch, Andreas},
title = {Specification Completion for Sustainable Software Development via Sustainability-Driven Mining},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GREENS66463.2025.00005},
doi = {10.1109/GREENS66463.2025.00005},
abstract = {In an era where digital transformation intersects with environmental sustainability, the software development industry must integrate practices that minimize ecological impacts. This paper proposes a comprehensive framework for specification completion through specification mining, aimed at embedding sustainability into software development processes. Utilizing an Extended Abstraction Refinement Model (EARM), we enhance the DevOps lifecycle by mapping observed behavior to development artifacts, ensuring sustainability metrics are accessible and actionable for stakeholders such as requirements engineers, software architects, and developers. Our approach leverages emergent behavior analysis to identify sustainability impacts that manifest during runtime, enabling targeted, energy-efficient interventions without incurring the rebound effect of exhaustive evaluations. By incorporating natural language processing (NLP) techniques for automated specification mapping, the framework refines software models iteratively, integrating real-world sustainability insights. This methodology supports the reduction of the carbon footprint in software products while preserving performance and quality, contributing to the alignment of software engineering with global sustainability objectives.},
booktitle = {Proceedings of the 2025 IEEE/ACM 9th International Workshop on Green and Sustainable Software},
pages = {1–7},
numpages = {7},
location = {Ottawa, ON, Canada},
series = {GREENS '25}
}

@inproceedings{10.1145/3643667.3648221,
author = {Ammermann, Joshua and Brenneisen, Fabian Jakob and Bittner, Tim and Schaefer, Ina},
title = {Quantum Solution for Configuration Selection and Prioritization},
year = {2024},
isbn = {9798400705700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643667.3648221},
doi = {10.1145/3643667.3648221},
abstract = {The analyses of highly configurable systems, as applied in software or automotive domains, yield hard problems due to the exponentially increasing number of possible product configurations. Current research identified that such combinatorial optimization problems, e.g. configuration selection and prioritization, are ideal targets for expected exponential quantum speedups. However, empirical evidence about the applicability of quantum computing to these problems is still missing. In this paper, we investigate how the constraint satisfaction and optimization problems of configuration selection and prioritization can be addressed using quantum computing. We propose a method to transform the configuration selection and prioritization problems encoded in attributed feature models into a quantum mechanical formulation suitable for optimization problems. We provide a Python library to automatically perform this transformation and apply the Quantum Approximate Optimization Algorithm (QAOA), such that configuration selection and prioritization are solved with quantum computers. Our approach is evaluated regarding feasibility, solution quality, and scalability. We show that QAOA obtains good results regarding configuration selection, but for configuration prioritization, the approach needs further improvement.},
booktitle = {Proceedings of the 5th ACM/IEEE International Workshop on Quantum Software Engineering},
pages = {21–28},
numpages = {8},
keywords = {configuration selection, configuration prioritization, QAOA},
location = {Lisbon, Portugal},
series = {Q-SE 2024}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00106,
author = {Voggenreiter, Markus and Sch\"{o}pp, Ulrich},
title = {Prioritizing Industrial Security Findings in Agile Software Development Projects},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00106},
doi = {10.1109/ICSE-Companion58688.2023.00106},
abstract = {Automating repetitive activities is a key principle in most software development approaches employed in the industry. This implies that security activities and all related processes should be investigated for automation capabilities, particularly the management of security findings and vulnerabilities. Considering the limited time available for each release and the vast flood of findings by automated security testing, prioritizing security finding responses is essential.In this paper, we present a partially automated process to prioritize security findings in industrial software development projects. We utilize a two-staged calculation process to produce a prioritization score, representing the finding's severity and factors like stakeholder input alike. This process was evaluated by conducting structured interviews with security professionals while also integrating the approach in ongoing industrial software development projects. The results indicate the potential of the process in terms of usefulness and correctness for agile software development projects.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {375–379},
numpages = {5},
keywords = {prioritization, software engineering, security findings, agile},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643656.3643896,
author = {Hoang, Minh and Berding, Adrian},
title = {Presubmit Rescue: Automatically Ignoring FlakyTest Executions},
year = {2024},
isbn = {9798400705588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643656.3643896},
doi = {10.1145/3643656.3643896},
abstract = {Flaky tests often give misleading signals and slow down the speed at which Software Engineers can develop. Traditional ways to help mitigate the developer time loss revolve around identifying tests which are flaky and choosing not to run them or adding additional attempts to reduce the chance of them flakily failing. We present Google's newest approach to the problem that tries to maintain the usefulness of flaky tests without being as time and resource wasteful as executing additional test attempts (deflaking). Presubmit Rescue is a machine learning model that predicts if a failed test execution was caused by a flake. Predicted flaky failures can be automatically ignored to save developer time without spending machine time for retries or developer time on debugging.},
booktitle = {Proceedings of the 1st International Workshop on Flaky Tests},
pages = {1–2},
numpages = {2},
location = {Lisbon, Portugal},
series = {FTW '24}
}

@inproceedings{10.1145/3597503.3639189,
author = {Kwon, Seungwan and Kwon, Jaeseong and Kang, Wooseok and Lee, Juneyoung and Heo, Kihong},
title = {Translation Validation for JIT Compiler in the V8 JavaScript Engine},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639189},
doi = {10.1145/3597503.3639189},
abstract = {We present TurboTV, a translation validator for the JavaScript (JS) just-in-time (JIT) compiler of V8. While JS engines have become a crucial part of various software systems, their emerging adaption of JIT compilation makes it increasingly challenging to ensure their correctness. We tackle this problem with an SMT-based translation validation (TV) that checks whether a specific compilation is semantically correct. We formally define the semantics of IR of TurboFan (JIT compiler of V8) as SMT encoding. For efficient validation, we design a staged strategy for JS JIT compilers. This allows us to decompose the whole correctness checking into simpler ones. Furthermore, we utilize fuzzing to achieve practical TV. We generate a large number of JS functions using a fuzzer to trigger various optimization passes of TurboFan and validate their compilation using TurboTV. Lastly, we demonstrate that TurboTV can also be used for cross-language TV. We show that TurboTV can validate the translation chain from LLVM IR to TurboFan IR, collaborating with an off-the-shelf TV tool for LLVM. We evaluated TurboTV on various sets of JS and LLVM programs. TurboTV effectively validated a large number of compilations of TurboFan with a low false positive rate and discovered a new miscompilation in LLVM.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {178},
numpages = {12},
keywords = {translation validation, Javascript engine, JIT compiler, IR, semantics, fuzzing},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00167,
author = {Yandrapally, Rahulkrishna and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali},
title = {Carving UI Tests to Generate API Tests and API Specification},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00167},
doi = {10.1109/ICSE48619.2023.00167},
abstract = {Modern web applications make extensive use of API calls to update the UI state in response to user events or server-side changes. For such applications, API-level testing can play an important role, in-between unit-level testing and UI-level (or end-to-end) testing. Existing API testing tools require API specifications (e.g., OpenAPI), which often may not be available or, when available, be inconsistent with the API implementation, thus limiting the applicability of automated API testing to web applications. In this paper, we present an approach that leverages UI testing to enable API-level testing for web applications. Our technique navigates the web application under test and automatically generates an API-level test suite, along with an OpenAPI specification that describes the application's server-side APIs (for REST-based web applications). A key element of our solution is a dynamic approach for inferring API endpoints with path parameters via UI navigation and directed API probing. We evaluated the technique for its accuracy in inferring API specifications and the effectiveness of the "carved" API tests. Our results on seven open-source web applications show that the technique achieves 98% precision and 56% recall in inferring endpoints. The carved API tests, when added to test suites generated by two automated REST API testing tools, increase statement coverage by 52% and 29% and branch coverage by 99% and 75%, on average. The main benefits of our technique are: (1) it enables API-level testing of web applications in cases where existing API testing tools are inapplicable and (2) it creates API-level test suites that cover server-side code efficiently while exercising APIs as they would be invoked from an application's web UI, and that can augment existing API test suites.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1971–1982},
numpages = {12},
keywords = {API specification inference, test carving, end-to-end testing, UI testing, test generation, API testing, web application testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00076,
author = {Bangash, Abdul Ali},
title = {Cost-Effective Strategies for Building Energy Efficient Mobile Applications},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00076},
doi = {10.1109/ICSE-Companion58688.2023.00076},
abstract = {Smartphone users rely on applications to perform various functionalities through their phones, but these functionalities may cause a significant drain on the device's battery. To ensure that an app does not consume unnecessary energy, app developers measure and optimize the energy consumption of their apps before releasing them to the end users. However, current optimization and measurement techniques have several limitations. The energy optimization techniques only focus on refactoring energy-greedy patterns related to system events, such as garbage collection and process switching, and on providing recommendation models for API usage. Despite the fact that the energy consumption of a single API can vary depending on its configuration, and API events account for 85% of energy consumption in smartphone apps, existing optimization techniques do not provide guidance on how to configure APIs for energy-efficient usage. Moreover, energy measurement techniques are cumbersome because they require developers to generate test cases and execute them on expensive, sophisticated hardware. My thesis argues that we can develop a general methodology that researchers may follow to extract energy-efficient guidelines pertaining to an API, and developers may use such guidelines to develop energy-efficient apps. Additionally, it argues that we can use static analysis to estimate an app's energy consumption. Such methodology will elevate the need for a physical smartphone and test case generation and execution. The insights and techniques that my thesis presents are particularly useful within the context of an Integrated Development Environment (IDE) or a Continuous Integration/Continuous Deployment (CI/CD) pipeline, where developers require results within a matter of milliseconds. Using our technique, developers would quickly receive warnings about high energy consumption caused by their code modifications, specifically those related to API usage.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {281–285},
numpages = {5},
keywords = {mobile application, static analysis, energy estimation, mining software repositories},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00210,
author = {Ouyang, Yicheng and Shao, Kailai and Chen, Kunqiu and Shen, Ruobing and Chen, Chao and Xu, Mingze and Zhang, Yuqun and Zhang, Lingming},
title = {MirrorTaint: Practical Non-Intrusive Dynamic Taint Tracking for JVM-Based Microservice Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00210},
doi = {10.1109/ICSE48619.2023.00210},
abstract = {Taint analysis, i.e., labeling data and propagating the labels through data flows, has been widely used for analyzing program information flows and ensuring system/data security. Due to its important applications, various taint analysis techniques have been proposed, including static and dynamic taint analysis. However, existing taint analysis techniques can be hardly applied to the rising microservice systems for industrial applications. To address such a problem, in this paper, we proposed the first practical non-intrusive dynamic taint analysis technique MirrorTaint for extensively supporting microservice systems on JVMs. In particular, by instrumenting the microservice systems, MirrorTaint constructs a set of data structures with their respective policies for labeling/propagating taints in its mirrored space. Such data structures are essentially non-intrusive, i.e., modifying no program meta-data or runtime system. Then, during program execution, MirrorTaint replicates the stack-based JVM instruction execution in its mirrored space on-the-fly for dynamic taint tracking. We have evaluated MirrorTaint against state-of-the-art dynamic and static taint analysis systems on various popular open-source microservice systems. The results demonstrate that MirrorTaint can achieve better compatibility, quite close precision and higher recall (97.9%/100.0%) than state-of-the-art Phosphor (100.0%/9.9%) and FlowDroid (100%/28.2%). Also, MirrorTaint incurs lower runtime overhead than Phosphor (although both are dynamic techniques). Moreover, we have performed a case study in Ant Group, a global billion-user FinTech company, to compare MirrorTaint and their mature developer-experience-based data checking system for automatically generated fund documents. The result shows that the developer experience can be incomplete, causing the data checking system to only cover 84.0% total data relations, while MirrorTaint can automatically find 99.0% relations with 100.0% precision. Lastly, we also applied MirrorTaint to successfully detect a recently wide-spread Log4j2 security vulnerability.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2514–2526},
numpages = {13},
keywords = {JVM, microservice, dynamic taint analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643659.3643925,
author = {Kraus, Roman and Nguyen, Hoang Lam and Schneider, Martin A.},
title = {Generator-based Fuzzing with Input Features},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643925},
doi = {10.1145/3643659.3643925},
abstract = {Generator-based fuzzing is a capable technique for testing semantic processing stages of a system under test (SUT). The idea is to use format-specific input generators, which can guarantee that inputs will be syntactically valid. One open question however is how to create inputs with generator-based fuzzing whose content exhibits particular qualities (or input features). This is a downside, as previous research suggests the importance of input features for triggering otherwise rarely reached functionalities of an SUT. We propose an approach to identify input features for rarely visited code by performing sequential pattern mining on the tree model of generated inputs. These features are regenerated by splicing (i.e., inserting) them into the model of newly generated inputs. We evaluate our approach on Ant, Maven, Closure and Rhino. The results indicate an increased diversity in the exploration of rarely executed code in most benchmarks. Significant improvements in valid rare branch hits were observed in half of the SUTs. JavaScript benchmarks tend to benefit more in terms of overall coverage but no statistically significant difference was found.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {13–20},
numpages = {8},
keywords = {structure-aware fuzzing, generator-based fuzzing, random testing},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00017,
author = {Malyala, Aniketh and Zhou, Katelyn and Ray, Baishakhi and Chakraborty, Saikat},
title = {On ML-Based Program Translation: Perils and Promises},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00017},
doi = {10.1109/ICSE-NIER58687.2023.00017},
abstract = {With the advent of new and advanced programming languages, it becomes imperative to migrate legacy software to new programming languages. Unsupervised Machine Learning-based Program Translation could play an essential role in such migration, even without a sufficiently sizeable reliable corpus of parallel source code. However, these translators are far from perfect due to their statistical nature. This work investigates unsupervised program translators and where and why they fail. With in-depth error analysis of such failures, we have identified that the cases where such translators fail follow a few particular patterns. With this insight, we develop a rule-based program mutation engine, which pre-processes the input code if the input follows specific patterns and post-process the output if the output follows certain patterns. We show that our code processing tool, in conjunction with the program translator, can form a hybrid program translator and significantly improve the state-of-the-art. In the future, we envision an end-to-end program translation tool where programming domain knowledge can be embedded into an ML-based translation pipeline using pre- and post-processing steps.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {60–65},
numpages = {6},
keywords = {program transformation, code translation, code generation},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE48619.2023.00174,
author = {Ba, Jinsheng and Rigger, Manuel},
title = {Testing Database Engines via Query Plan Guidance},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00174},
doi = {10.1109/ICSE48619.2023.00174},
abstract = {Database systems are widely used to store and query data. Test oracles have been proposed to find logic bugs in such systems, that is, bugs that cause the database system to compute an incorrect result. To realize a fully automated testing approach, such test oracles are paired with a test case generation technique; a test case refers to a database state and a query on which the test oracle can be applied. In this work, we propose the concept of Query Plan Guidance (QPG) for guiding automated testing towards "interesting" test cases. SQL and other query languages are declarative. Thus, to execute a query, the database system translates every operator in the source language to one of the potentially many so-called physical operators that can be executed; the tree of physical operators is referred to as the query plan. Our intuition is that by steering testing towards exploring a variety of unique query plans, we also explore more interesting behaviors---some of which are potentially incorrect. To this end, we propose a mutation technique that gradually applies promising mutations to the database state, causing the DBMS to create potentially unseen query plans for subsequent queries. We applied our method to three mature, widely-used, and extensively-tested database systems---SQLite, TiDB, and CockroachDB---and found 53 unique, previously unknown bugs. Our method exercises 4.85--408.48\texttimes{} more unique query plans than a naive random generation method and 7.46\texttimes{} more than a code coverage guidance method. Since most database systems---including commercial ones---expose query plans to the user, we consider QPG a generally applicable, black-box approach and believe that the core idea could also be applied in other contexts (e.g., to measure the quality of a test suite).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2060–2071},
numpages = {12},
keywords = {test case generation, automated testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00018,
author = {Sun, Maolin and Yang, Yibiao and Wen, Ming and Wang, Yongcong and Zhou, Yuming and Jin, Hai},
title = {Validating SMT Solvers via Skeleton Enumeration Empowered by Historical Bug-Triggering Inputs},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00018},
doi = {10.1109/ICSE48619.2023.00018},
abstract = {SMT solvers check the satisfiability of logic formulas over first-order theories, which have been utilized in a rich number of critical applications, such as software verification, test case generation, and program synthesis. Bugs hidden in SMT solvers would severely mislead those applications and further cause severe consequences. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Although many approaches have been proposed to test SMT solvers, it is still a challenge to discover bugs effectively. To tackle such a challenge, we conduct an empirical study on the historical bug-triggering formulas in SMT solvers' bug tracking systems. We observe that the historical bug-triggering formulas contain valuable skeletons (i.e., core structures of formulas) as well as associated atomic formulas which can cast significant impacts on formulas' ability in triggering bugs. Therefore, we propose a novel approach that utilizes the skeletons extracted from the historical bug-triggering formulas and enumerates atomic formulas under the guidance of association rules derived from historical formulas. In this study, we realized our approach as a practical fuzzing tool HistFuzz and conducted extensive testing on the well-known SMT solvers Z3 and cvc5. To date, HistFuzz has found 111 confirmed new bugs for Z3 and cvc5, of which 108 have been fixed by the developers. More notably, out of the confirmed bugs, 23 are soundness bugs and invalid model bugs found in the solvers' default mode, which are essential for SMT solvers. In addition, our experiments also demonstrate that HistFuzz outperforms the state-of-the-art SMT solver fuzzers in terms of achieved code coverage and effectiveness.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {69–81},
numpages = {13},
keywords = {bug detection, association rules, skeleton enumeration, fuzzing, SMT solver},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643659.3643930,
author = {Erni, Nicolas and Al-Ameen, Mohammed and Birchler, Christian and Derakhshanfar, Pouria and Lukasczyk, Stephan and Panichella, Sebastiano},
title = {SBFT Tool Competition 2024 - Python Test Case Generation Track},
year = {2024},
isbn = {9798400705625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643659.3643930},
doi = {10.1145/3643659.3643930},
abstract = {Test case generation (TCG) for Python poses distinctive challenges due to the language's dynamic nature and the absence of strict type information. Previous research has successfully explored automated unit TCG for Python, with solutions outperforming random test generation methods. Nevertheless, fundamental issues persist, hindering the practical adoption of existing test case generators. To address these challenges, we report on the organization, challenges, and results of the first edition of the Python Testing Competition. Four tools, namely UTBotPython, Klara, Hypothesis Ghostwriter, and Pynguin were executed on a benchmark set consisting of 35 Python source files sampled from 7 open-source Python projects for a time budget of 400 seconds. We considered one configuration of each tool for each test subject and evaluated the tools' effectiveness in terms of code and mutation coverage. This paper describes our methodology, the analysis of the results together with the competing tools, and the challenges faced while running the competition experiments.},
booktitle = {Proceedings of the 17th ACM/IEEE International Workshop on Search-Based and Fuzz Testing},
pages = {37–40},
numpages = {4},
keywords = {tool competition, software testing, test case generation, python, search based software engineering},
location = {Lisbon, Portugal},
series = {SBFT '24}
}

@inproceedings{10.1109/ICSE48619.2023.00027,
author = {Huang, Sunzhou and Wang, Xiaoyin},
title = {PExReport: Automatic Creation of Pruned Executable Cross-Project Failure Reports},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00027},
doi = {10.1109/ICSE48619.2023.00027},
abstract = {Modern software development extensively depends on existing libraries written by other developer teams from the same or a different organization. When a developer executes the software, the execution trace may go across the boundaries of multiple software products and create cross-project failures (CPFs). Existing studies show that a stand-alone executable failure report may enable the most effective communication, but creating such a report is often challenging due to the complicated files and dependencies interactions in the software ecosystems. In this paper, to solve the CPF report trilemma, we developed PExReport, which automatically creates stand-alone executable CPF reports. PExReport leverages build tools to prune source code and dependencies, and further analyzes the build process to create a pruned build environment for reproducing the CPF. We performed an evaluation on 74 software project issues with 198 CPFs, and the evaluation results show that PExReport can create executable CPF reports for 184 out of 198 test failures in our dataset, with an average reduction of 72.97% on source classes and the classes in internal JARs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {184–195},
numpages = {12},
keywords = {debloating, build environment, build tool, failure reproduction, executable failure report, cross-project failure},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3639474.3640081,
author = {Venson, Elaine and Alfayez, Reem},
title = {Bridging Theory to Practice in Software Testing Teaching through Team-based Learning (TBL) and Open Source Software (OSS) Contribution},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640081},
doi = {10.1145/3639474.3640081},
abstract = {Curricula recommendation for undergraduate Software Engineering courses underscore the importance of transcending from traditional lecture format to actively involving students in time-limited, iterative development practices. This paper presents a teaching approach for a software testing course that integrates theory and practical experience through the utilization of both TBL and active contributions to OSS projects. The paper reports on our experience implementing the pedagogical approach over four consecutive semesters of a Software Testing course within an undergraduate Software Engineering program. The experience encompassed both online and in-person classes, involving a substantial cohort of over 300 students spanning four semesters. Students' perceptions regarding the course are analyzed and compared with previous, related studies. Our results are positively aligned with the existing literature of software engineering teaching, confirming the effectiveness of combining TBL with OSS contributions. Additionally, our survey has shed light on the challenges that students encounter during their first contribution to OSS projects, highlighting the need for targeted solutions. Overall, the experience demonstrates that the proposed pedagogical structure can effectively facilitate the transition from theoretical knowledge to real-world practice in the domain of Software Testing.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {72–81},
numpages = {10},
keywords = {software engineering education, team-based learning, open source software, software testing},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3639477.3640328,
author = {Serebryany, Kostya and Kennelly, Chris and Phillips, Mitch and Denton, Matt and Elver, Marco and Potapenko, Alexander and Morehouse, Matt and Tsyrklevich, Vlad and Holler, Christian and Lettner, Julian and Kilzer, David and Brandt, Lander},
title = {GWP-ASan: Sampling-Based Detection of Memory-Safety Bugs in Production},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3640328},
doi = {10.1145/3639477.3640328},
abstract = {Despite the recent advances in pre-production bug detection, heap-use-after-free and heap-buffer-overflow bugs remain the primary problem for security, reliability, and developer productivity for applications written in C or C++, across all major software ecosystems. Memory-safe languages solve this problem when they are used, but the existing code bases consisting of billions of lines of C and C++ continue to grow, and we need additional bug detection mechanisms.This paper describes a family of tools that detect these two classes of memory-safety bugs, while running in production, at near-zero overhead. These tools combine page-granular guarded allocation and low-rate sampling. In other words, we added an "if" statement to a 36-year-old idea and made it work at scale.We describe the basic algorithm, several of its variants and implementations, and the results of multi-year deployments across mobile, desktop, and server applications.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {168–177},
numpages = {10},
keywords = {memory safety, dynamic program analysis, programming languages, software engineering},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643660.3643938,
author = {Radosky, Lukas and Polasek, Ivan},
title = {Executable Multi-Layered Software Models},
year = {2024},
isbn = {9798400705632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643660.3643938},
doi = {10.1145/3643660.3643938},
abstract = {This paper introduces a novel software visualisation and animation method, manifested in a prototype software tool - AnimArch. The introduced method is based on model fusion of static and dynamic models. The static model is represented by class diagram while the dynamic model is represented by source code written in high-level Object Action Language from xUML (executable UML). The class diagram defines architecture that is animated in response to real-time execution of the source code. Moreover, additional object diagram layer represents all object instances present in runtime. The AnimArch also features source code generation to Python, to bridge the gap from design to implementation. This paper provides detailed description of the modelling method and screenshots of the accompanying software tool.},
booktitle = {Proceedings of the 1st International Workshop on Designing Software},
pages = {46–51},
numpages = {6},
keywords = {software modelling, visualisation, animation, executable model, class diagram, object diagram, source code generation},
location = {Lisbon, Portugal},
series = {Designing '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00088,
author = {Rajan, Sai Sathiesh and Soremekun, Ezekiel and Chattopadhyay, Sudipta and Traon, Yves Le},
title = {Poster: Distribution-Aware Fairness Test Generation},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00088},
doi = {10.1109/ICSE-Companion58688.2023.00088},
abstract = {This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images and systematically mutates objects in the images to become OOD using three semantic-preserving image mutations - object deletion, object insertion and object rotation. We evaluate DistroFair with two well-known datasets (CityScapes and MS-COCO) and three commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision) and find that at least 21% of images generated by DistroFair result in class-level fairness violations. DistroFair is up to 2.3x more effective than the baseline (generation of images within the observed distribution). Finally, we evaluated the semantic validity of our approach via a user study with 81 participants, using 30 real images and 30 corresponding mutated images generated by DistroFair and found that the generated images are 80% as realistic as the original images.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {322–323},
numpages = {2},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00067,
author = {Bettscheider, Leon},
title = {Learning Test Input Constraints from Branch Conditions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00067},
doi = {10.1109/ICSE-Companion58688.2023.00067},
abstract = {Precise input specifications are the holy grail of blackbox test generation. In order to test programs that process structured inputs effectively, inputs should match the expected input format. Otherwise, they are likely to be rejected during initial input validation, and fail to reach the main application logic. While the structure and constraints of widely used data formats such as XML are known, the input constraints imposed by application logic are vast, unstructured, and encoded in branch conditions. Hence, they are rarely specified manually, leaving large parts of the program unexplored by blackbox techniques. We propose to address this issue by dynamically externalizing local constraints and exposing them to system-level test generators. These could combine such constraints with an existing input specification in order to find global solutions. This could provide a means to explore application logic systematically.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {248–250},
numpages = {3},
keywords = {symbolic execution, context-free grammars, dynamic program analysis, fuzzing, software testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643991.3644874,
author = {Tabosa, Davi and Pinheiro, Oton and Rocha, Lincoln and Viana, Windson},
title = {A Dataset of Atoms of Confusion in the Android Open Source Project},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644874},
doi = {10.1145/3643991.3644874},
abstract = {Ensuring the readability and comprehension of source code is key for effective software maintenance and evolution, particularly in tasks involving bug fixing, refactoring, and optimization. Previous studies highlight that challenges in maintenance and the emergence of certain bugs can be traced back to small code fragments called "Atoms of Confusion" (AC). While initial investigations identified these snippets in C++ code bases, subsequent studies have identified analogous structures in languages such as Java. Although numerous studies have delved into observing ACs in Java projects, there is a lack of studies focused on the Android ecosystem. This paper aims to address this gap by constructing a comprehensive dataset, which catalogs ACs and assesses their prevalence in the Android Open Source Project (AOSP). After analyzing over 125,000 source code files across 370 Git repositories, our findings reveal that more than 30% of Java files within the AOSP contain at least one AC, totaling over 320,000 recorded instances. This equates to one AC for approximately every 91 lines of code. Particularly, this dataset can be used to support further studies on shedding light on the ACs prevalence in the Android ecosystem, alongside their relation with traditional object-oriented software metrics (e.g., size, complexity, cohesion, and coupling), also recorded in the dataset. Using this dataset, researchers may provide valuable insights for developers and software engineers, emphasizing the need for strategies to mitigate the possible impact of ACs on software maintenance.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {520–524},
numpages = {5},
keywords = {Android, atoms of confusion, AOSP},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3643991.3644912,
author = {Oliver, Philip and Dietrich, Jens and Anslow, Craig and Homer, Michael},
title = {CrashJS: A NodeJS Benchmark for Automated Crash Reproduction},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644912},
doi = {10.1145/3643991.3644912},
abstract = {Software bugs often lead to software crashes, which cost US companies upwards of $2.08 trillion annually. Automated Crash Reproduction (ACR) aims to generate unit tests that successfully reproduce a crash. The goal of ACR is to aid developers with debugging, providing them with another tool to locate where a bug is in a program. The main approach ACR currently takes is to replicate a stack trace from an error thrown within a program. Currently, ACR has been developed for C, Java, and Python, but there are no tools targeting JavaScript programs. To aid the development of JavaScript ACR tools, we propose CrashJS: a benchmark dataset of 453 Node.js crashes from several sources. CrashJS includes a mix of real-world and synthesised tests, multiple projects, and different levels of complexity for both crashes and target programs.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {75–87},
numpages = {13},
keywords = {automated crash reproduction, benchmark, data collection, dataset, software testing, test generation},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3639477.3639726,
author = {Sun, Gengyi and Meidani, Mehran and Habchi, Sarra and Nayrolles, Mathieu and Mcintosh, Shane},
title = {Code Impact Beyond Disciplinary Boundaries: Constructing a Multidisciplinary Dependency Graph and Analyzing Cross-Boundary Impact},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639726},
doi = {10.1145/3639477.3639726},
abstract = {To produce a video game, engineers and artists must iterate on the same project simultaneously. In such projects, a change to the work products of any of the teams can impact the work of other teams. As a result, any analytics tasks should consider intra- and inter-dependencies within and between artifacts produced by different teams. For instance, the focus of quality assurance teams on changes that are local to a team differs from one that impacts others. To extract and analyze such cross-disciplinary dependencies, we propose the multidisciplinary dependency graph. We instantiate our idea by developing tools that extract dependencies and construct the graph at Ubisoft---a multinational video game organization with more than 18,000 employees.Our analysis of a recently launched video game project reveals that code files only make up 2.8% of the dependency graph, and code-to-code dependencies only make up 4.3% of all dependencies. We also observe that 44% of the studied source code changes impact the artifacts that are developed by other teams, highlighting the importance of analyzing inter-artifact dependencies. A comparative analysis of cross-boundary changes with changes that do not cross boundaries indicates that cross-boundary changes are: (1) impacting a median of 120,368 files; (2) with a 51% probability of causing build failures; and (3) a 67% likelihood of introducing defects. All three measurements are larger than changes that do not cross boundaries to statistically significant degrees.We also find that cross-boundary changes are: (4) more commonly associated with gameplay functionality and feature additions that directly impact the game experience than changes that do not cross boundaries, and (5) disproportionately produced by the same team (74% of the contributors are associated with that team).},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {122–133},
numpages = {12},
keywords = {interdisciplinary dependencies, build systems, impact analysis},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3643991.3644891,
author = {Birchler, Christian and Rohrbach, Cyrill and Kehrer, Timo and Panichella, Sebastiano},
title = {SensoDat: Simulation-based Sensor Dataset of Self-driving Cars},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644891},
doi = {10.1145/3643991.3644891},
abstract = {Developing tools in the context of autonomous systems [22, 24], such as self-driving cars (SDCs), is time-consuming and costly since researchers and practitioners rely on expensive computing hardware and simulation software. We propose SensoDat, a dataset of 32,580 executed simulation-based SDC test cases generated with state-of-the-art test generators for SDCs. The dataset consists of trajectory logs and a variety of sensor data from the SDCs (e.g., rpm, wheel speed, brake thermals, transmission, etc.) represented as a time series. In total, SensoDat provides data from 81 different simulated sensors. Future research in the domain of SDCs does not necessarily depend on executing expensive test cases when using SensoDat. Furthermore, with the high amount and variety of sensor data, we think SensoDat can contribute to research, particularly for AI development, regression testing techniques for simulation-based SDC testing, flakiness in simulation, etc. Link to the dataset: https://doi.org/10.5281/zenodo.10307479},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {510–514},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3644032.3644463,
author = {Masserini, Elena and Ginelli, Davide and Micucci, Daniela and Briola, Daniela and Mariani, Leonardo},
title = {Anonymizing Test Data in Android: Does It Hurt?},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644463},
doi = {10.1145/3644032.3644463},
abstract = {Failure data collected from the field (e.g., failure traces, bug reports, and memory dumps) represent an invaluable source of information for developers who need to reproduce and analyze failures. Unfortunately, field data may include sensitive information and thus cannot be collected indiscriminately. Privacy-preserving techniques can address this problem anonymizing data and reducing the risk of disclosing personal information. However, collecting anonymized information may harm reproducibility, that is, the anonymized data may not allow the reproduction of a failure observed in the field. In this paper, we present an empirical investigation about the impact of privacy-preserving techniques on the reproducibility of failures. In particular, we study how five privacy-preserving techniques may impact reproducibilty for 19 bugs in 17 Android applications. Results provide insights on how to select and configure privacy-preserving techniques.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {88–98},
numpages = {11},
keywords = {data anonymization, privacy-preserving, privacy, bug reproduction, mobile applications, debugging, testing},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3643915.3644090,
author = {Chu, Simon and Koe, Justin and Garlan, David and Kang, Eunsuk},
title = {Integrating Graceful Degradation and Recovery through Requirement-driven Adaptation},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644090},
doi = {10.1145/3643915.3644090},
abstract = {Cyber-physical systems (CPS) are subject to environmental uncertainties such as adverse operating conditions, malicious attacks, and hardware degradation. These uncertainties may lead to failures that put the system in a sub-optimal or unsafe state. Systems that are resilient to such uncertainties rely on two types of operations: (1) graceful degradation, for ensuring that the system maintains an acceptable level of safety during unexpected environmental conditions and (2) recovery, to facilitate the resumption of normal system functions. Typically, mechanisms for degradation and recovery are developed independently from each other, and later integrated into a system, requiring the designer to develop an additional, ad-hoc logic for activating and coordinating between the two operations.In this paper, we propose a self-adaptation approach for improving system resiliency through automated triggering and coordination of graceful degradation and recovery. The key idea behind our approach is to treat degradation and recovery as requirement-driven adaptation tasks: Degradation can be thought of as temporarily weakening original (i.e., ideal) system requirements to be achieved by the system, and recovery as strengthening the weakened requirements when the environment returns within an expected operating boundary. Furthermore, by treating weakening and strengthening as dual operations, we argue that a single requirement-based adaptation method is sufficient to enable coordination between degradation and recovery. Given system requirements specified in signal temporal logic (STL), we propose a run-time adaptation framework that performs degradation and recovery in response to environmental changes. We describe a prototype implementation of our framework and demonstrate the feasibility of the proposed approach using a case study in unmanned underwater vehicles.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {122–132},
numpages = {11},
keywords = {graceful degradation, recovery, self-adaptive systems, requirement-driven adaptation, signal temporal logic},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1109/ICSE48619.2023.00037,
author = {Zheng, Peilin and Luo, Xiapu and Zheng, Zibin},
title = {BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00037},
doi = {10.1109/ICSE48619.2023.00037},
abstract = {Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {307–318},
numpages = {12},
keywords = {smart contract, blockchain, bitcoin},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643794.3648279,
author = {Kassab, Mohamad and Arbex, Wagner and Graciano Neto, Valdemar Vicente and Gomes, Jonas and Braga, Regina and Maria David, Jose and Oliveira, Roberto},
title = {Livestock IoT: Precision Livestock Management in Agribusiness},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648279},
doi = {10.1145/3643794.3648279},
abstract = {This paper introduces Livestock IoT (LIoT), a Software Ecosystem (SECO) tailored for precision livestock management within the broader Internet of Things (IoT) concept in agribusiness. In response to the challenges posed by the Fourth Industrial Revolution and the need for enhanced productivity in global food production, the paper highlights the transformative emergence of IoT in elevating precision agribusiness. LIoT is designed to capture, store, and interpret data, providing an integrated platform for intelligent decision-making in animal treatment and automated events. The SECO is structured into five layers, including data streaming, processing, integration, external sources, and visualization, offering a holistic view of information related to confined livestock farming. The paper presents an initial evaluation of the SECO LIoT platform, demonstrating its efficacy in handling compost barn data and supporting decision-making in agriculture. Future work is outlined to optimize the architecture, explore novel applications, and enhance its capacity for supporting emerging technologies in precision livestock contexts.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {52–56},
numpages = {5},
keywords = {livestock, farming, cows, agriculture, animals, sensors},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1109/ICSE48619.2023.00035,
author = {Kim, Taeyoung and Jang, Yunhee and Lee, Chanjong and Koo, Hyungjoon and Kim, Hyoungshick},
title = {SmartMark: Software Watermarking Scheme for Smart Contracts},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00035},
doi = {10.1109/ICSE48619.2023.00035},
abstract = {A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed SmartMark, aiming to protect the ownership of a smart contract against a pirate activity. SmartMark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying SmartMark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that SmartMark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that SmartMark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {283–294},
numpages = {12},
keywords = {software copyrights, blockchain, software watermarking, smart contract},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643663.3643969,
author = {Hamid, Ahmad Rzgar and Kj\ae{}rgaard, Mikkel Baun},
title = {An Edge Computing Sizing Tool for Robotic Workloads},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643663.3643969},
doi = {10.1145/3643663.3643969},
abstract = {Robots of today are equipped with lightweight computing resources used merely to make the robot function. However, proportional advancements in associated data processing and algorithms are needed, given the significant advances in robots' sensing and programming capabilities and the increasingly complex tasks they must complete.Yet, such advancements require additional hardware resources to function as intended. In many robotic applications, cloud computing is not an option; therefore, edge computing must be embraced.This paper proposes a sizing tool for benchmarking workloads against pre-written tasks to determine optimal edge computing hardware candidates used to deploy said workloads efficiently without wasting resources.Preliminary results show that the right combination of hardware resources has an impact on workload execution.},
booktitle = {Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
pages = {43–46},
numpages = {4},
keywords = {edge computing, workload benchmarking, hardware comparison},
location = {Lisbon, Portugal},
series = {RoSE '24}
}

@inproceedings{10.1145/3643916.3644397,
author = {Njoku, Anthonia and Amini, Mahta and Sharafi, Zohreh},
title = {Innovating Coding: Evaluating the Impact of Innovative Thinking in Programming},
year = {2024},
isbn = {9798400705861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643916.3644397},
doi = {10.1145/3643916.3644397},
abstract = {The software engineering research community has begun recognizing the importance of human factors in the field. Yet, there remains a limited focus on the role of creativity in software development beyond requirement engineering. Given that software development fundamentally entails problem solving that requires creativity, this innovative paper explores the relationship between coding approaches and developer creativity, particularly in functionality development tasks.In our study involving 77 participants, we aimed to evaluate the impact of creativity on developers' innovative coding approaches. The creativity was evaluated using well-established psychometric tests. The task for the participants was to create a scoring function for an online JavaScript game. We analyzed their methods, particularly noting the use of visual-auditory elements like visual modifications and sound effects. Hierarchical clustering and the gap statistic were employed to identify distinct approaches. The results showed that participants with higher creativity scores used a diverse approach of utilizing a more comprehensive range of visual-auditory elements, including incorporating additional sounds and multiple animations into the game. These preliminary results shed some light on the role and importance of creativity in programming. It paves the way for future software engineering education and training initiatives, suggesting ways to nurture creative thinking and enhance developer performance and engagement.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension},
pages = {241–245},
numpages = {5},
keywords = {creativity in software engineering, coding strategies, clustering},
location = {Lisbon, Portugal},
series = {ICPC '24}
}

@inproceedings{10.1145/3526073.3527592,
author = {Wozniak, Anne-Laure and Segura, Sergio and Mazo, Ra\'{u}l and Leroy, Sarah},
title = {Robustness testing of a machine learning-based road object detection system: an industrial case},
year = {2023},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526073.3527592},
doi = {10.1145/3526073.3527592},
abstract = {With the increasing development of critical systems based on artificial intelligence (AI), methods have been proposed and evaluated in academia to assess the reliability of these systems. In the context of computer vision, some approaches use the generation of images altered by common perturbations and realistic transformations to assess the robustness of systems. To better understand the strengths and limitations of these approaches, we report the results obtained on an industrial case of a road object detection system. By comparing these results with those of reference models, we identify areas for improvement regarding the robustness of the system and the metrics used for this evaluation.},
booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
pages = {9–12},
numpages = {4},
keywords = {robustness testing, object detection, machine learning, industrial case},
location = {Pittsburgh, Pennsylvania},
series = {SE4RAI '22}
}

@inproceedings{10.1145/3639478.3640039,
author = {Naghipour Vijouyeh, Lyla and Bruno, Rodrigo and Ferreira, Paulo},
title = {Emulation Tool For Android Edge Devices},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640039},
doi = {10.1145/3639478.3640039},
abstract = {The number of mobile devices has surpassed the global population, making them the primary means of communication and data sharing. However, existing applications still heavily rely on centralized networks for communicating and sharing data due to the lack of tools for developers to create and test distributed applications in edge environments. To address this issue, we present EdgeEmu, an Android-based distributed emulation tool. EdgeEmu allows a considerable number of Android emulators to participate remotely in the emulation, making it an appropriate tool for testing large networks. Unlike the standard Android SDK, EdgeEmu is not restricted to local emulators. Thus, it eliminates scalability issues present in the current Android testing infrastructure. Evaluations show that EdgeEmu outperforms the standard Android SDK by approximately 59.1% in terms of emulation startup time when ten Android emulators are used. Additionally, it exhibits low latency and negligible overhead during message exchanges between different emulators. A demo video of EdgeEmu is available at https://youtu.be/6jT9KXiUmQM.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {109–113},
numpages = {5},
keywords = {Android, edge networks, wi-fi direct, emulation, peer-to-peer, bluetooth, edge},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639478.3640028,
author = {Attaoui, Mohammed and Pastore, Fabrizio and Briand, Lionel},
title = {SAFE: Safety Analysis and Retraining of DNNs},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640028},
doi = {10.1145/3639478.3640028},
abstract = {We present SAFE, a tool based on a black-box approach to automatically characterize the root causes of Deep Neural Network (DNN) failures. SAFE relies on VGGNet-16, a transfer learning model pre-trained on ImageNet, to extract the features from error-inducing images. After feature extraction, SAFE applies a density-based clustering algorithm to discover arbitrarily shaped clusters of images modeling plausible causes of failures. By relying on the identified clusters, SAFE can select a set of additional images to be used to retrain and improve the DNN efficiently. Empirical results show the potential of SAFE in identifying different root causes of DNN failures based on case studies in the automotive domain. It also yields significant improvements in DNN accuracy after retraining while saving considerable execution time and memory compared to alternatives. A demo video of SAFE is available at https://youtu.be/8QD-PPFTZxs.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {74–78},
numpages = {5},
keywords = {DNN explanation, functional safety analysis, DNN debugging},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE48619.2023.00046,
author = {Zhang, Changjian and Saluja, Tarang and Meira-G\'{o}es, R\^{o}mulo and Bolton, Matthew and Garlan, David and Kang, Eunsuk},
title = {Robustification of Behavioral Designs against Environmental Deviations},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00046},
doi = {10.1109/ICSE48619.2023.00046},
abstract = {Modern software systems are deployed in a highly dynamic, uncertain environment. Ideally, a system that is robust should be capable of establishing its most critical requirements even in the presence of possible deviations in the environment. We propose a technique called behavioral robustification, which involves systematically and rigorously improving the robustness of a design against potential deviations. Given behavioral models of a system and its environment, along with a set of user-specified deviations, our robustification method produces a redesign that is capable of satisfying a desired property even when the environment exhibits those deviations. In particular, we describe how the robustification problem can be formulated as a multi-objective optimization problem, where the goal is to restrict the deviating environment from causing a violation of a desired property, while maximizing the amount of existing functionality and minimizing the cost of changes to the original design. We demonstrate the effectiveness of our approach on case studies involving the robustness of an electronic voting machine and safety-critical interfaces.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {423–434},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643655.3643879,
author = {Sakizloglou, Lucas and Khakharova, Taisiya and Ruehs, Florian and Lambers, Leen},
title = {On an Exemplar Supporting Model-based Quality Assurance Research for Healthcare Systems-of-Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643879},
doi = {10.1145/3643655.3643879},
abstract = {As healthcare is being massively digitized, it increasingly relies on the harmonious interaction of autonomous and interconnected software systems. In this context, it becomes crucial to ensure that the emergent behavior that occurs when these systems interact remains beneficial to the overall quality goals. This task requires that the interacting systems are conceptualized and further studied as a System-of-Systems (SoS). We present the design and prototypical implementation of an exemplar for such a Healthcare SoS (HSoS) which consists of different types of basic healthcare systems. Its typical quality goals, e.g., safety and interoperability, and selected technical characteristics, e.g., cloud-nativity and support for integration with robotics, render the exemplar representative of modern healthcare solutions. The exemplar simulates patient journeys which rely on the interaction of the constituent systems and thereby may capture the emergent behavior of the HSoS. Our aim is that the exemplar supports future research on quality assurance for HSoSs using model-based techniques, which are suitable for developing both SoSs and safety-critical systems. For each quality goal, we outline relevant challenges as well as our research plans.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {57–60},
numpages = {4},
keywords = {healthcare SoS, exemplar, model-based quality assurance},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3643796.3648459,
author = {Garaccione, Giacomo and Fulcini, Tommaso and Stefanut Bodnarescul, Paolo and Coppola, Riccardo and Ardito, Luca},
title = {Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin},
year = {2024},
isbn = {9798400705809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643796.3648459},
doi = {10.1145/3643796.3648459},
abstract = {Software testing is a crucial phase in software development, enabling the detection of issues and defects that may arise during the development process. Addressing these issues enhances software applications' quality, reliability, user experience, and performance. Graphical User Interface (GUI) testing, one such technique, involves mimicking a regular user's interactions with an application to identify defects. However, GUI testing is often underutilized due to its perceived repetitiveness, error-proneness, and lack of immediate feedback on test quality. In recent years, gamification---incorporating game elements in non-game contexts to boost interest, motivation, and engagement---has gained traction in various fields, including software engineering and education. This paper presents GIPGUT: a prototype of a gamification plugin for IntelliJ IDEA, an Integrated Development Environment (IDE) that supports scripted GUI testing. The plugin enhances testers' engagement with typically monotonous and tedious tasks through achievements, rewards, and profile customization. A preliminary prototype evaluation was conducted with a small group of users to assess its usability and the impact of gamification on the GUI testing process. The results indicate high usability and positive reception of the gamification elements. However, due to the limited sample size of participants, further research is necessary to understand the plugin's effectiveness fully.},
booktitle = {Proceedings of the 1st ACM/IEEE Workshop on Integrated Development Environments},
pages = {76–80},
numpages = {5},
keywords = {software testing, graphical user interface testing, gamification, integrated development environment, web testing},
location = {Lisbon, Portugal},
series = {IDE '24}
}

@inproceedings{10.1145/3639476.3639759,
author = {Wang, Zerui and Liu, Yan and Arumugam Thiruselvi, Abishek and Hamou-Lhadj, Abdelwahab},
title = {XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639759},
doi = {10.1145/3639476.3639759},
abstract = {In this study, we propose the early adoption of Explainable AI (XAI) with a focus on three properties: Quality of explanation, the explanation summaries should be consistent across multiple XAI methods; Architectural Compatibility, for effective integration in XAI, the architecture styles of both the XAI methods and the models to be explained must be compatible with the framework; Configurable operations, XAI explanations are operable, akin to machine learning operations. Thus, an explanation for AI models should be reproducible and tractable to be trustworthy. We present XAIport, a framework of XAI microservices encapsulated into Open APIs to deliver early explanations as observation for learning model quality assurance. XAIport enables configurable XAI operations along with machine learning development. We quantify the operational costs of incorporating XAI with three cloud computer vision services on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and Amazon Rekognition. Our findings show comparable operational costs between XAI and traditional machine learning, with XAIport significantly improving both cloud AI model performance and explanation stability.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {67–71},
numpages = {5},
keywords = {XAI, MLOps, operational cost analysis, deployment strategy},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3639478.3640027,
author = {Sorokin, Lev and Munaro, Tiziano and Safin, Damir and Liao, Brian Hsuan-Cheng and Molin, Adam},
title = {OpenSBT: A Modular Framework for Search-based Testing of Automated Driving Systems},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640027},
doi = {10.1145/3639478.3640027},
abstract = {Search-based software testing (SBST) is an effective and efficient approach for testing automated driving systems (ADS). However, testing pipelines for ADS testing are particularly challenging as they involve integrating complex driving simulation platforms and establishing communication protocols and APIs with the desired search algorithm. This complexity prevents a wide adoption of SBST and thorough empirical comparative experiments with different simulators and search approaches. We present OpenSBT, an open-source, modular and extensible framework to facilitate the SBT of ADS. With OpenSBT, it is possible to integrate simulators with an embedded system under test, search algorithms and fitness functions for testing. We describe the architecture and show the usage of our framework by applying different search algorithms for testing Automated Emergency Braking Systems in CARLA as well as in the industrial and high-fidelity simulator Prescan in collaboration with our industrial partner DENSO. OpenSBT is available at https://git.fortiss.org/opensbt. A demo video is provided here: https://www.youtube.com/watch?v=qi_CTTzrk5s.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {94–98},
numpages = {5},
keywords = {search-based software testing, metaheuristics, scenario-based testing, autonomous driving, automated driving},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3639476.3639766,
author = {Amini, Mahta and Olson, Jay and Sharafi, Zohreh},
title = {Coding with a Creative Twist: Investigating the Link Between Creativity Scores and problem-solving Strategies},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639766},
doi = {10.1145/3639476.3639766},
abstract = {Creative ideation and its adaptive value in reacting to new events are critical to advancing scientific and technological innovation. Software development, at its core, is a problem-solving endeavor that inherently demands creativity. Yet, the available research on creativity in software engineering is fragmented and limited.We present the first empirical evaluation of the role of creativity in software engineering (SE) tasks. We conducted an empirical study with 77 participants to objectively assess the effect of creativity---quantified via established psychometric tests---on developers' performance and behavior through the problem-solving strategies they used during programming. We find that participants with higher creativity scores employed diverse strategies with significant variations, adding extra game features and multiple animations. Additionally, we report a notable correlation between task time and divergent creativity scores, as participants with higher DAT scores exhibit extended task times. Our findings can inform educational and training strategies in SE, fostering innovative approaches and boosting developer performance and engagement.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {21–25},
numpages = {5},
keywords = {human factors, software engineering, creativity, and problem-solving strategies},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3597503.3639140,
author = {Zhong, Zhijie and Zheng, Zibin and Dai, Hong-Ning and Xue, Qing and Chen, Junjia and Nan, Yuhong},
title = {PrettySmart: Detecting Permission Re-delegation Vulnerability for Token Behaviors in Smart Contracts},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639140},
doi = {10.1145/3597503.3639140},
abstract = {As an essential component in Ethereum and other blockchains, token assets have been interacted with by diverse smart contracts. Effective permission policies of smart contracts must prevent token assets from being manipulated by unauthorized adversaries. Recent efforts have studied the accessibility of privileged functions or state variables to unauthorized users. However, little attention is paid to how publicly accessible functions of smart contracts can be manipulated by adversaries to steal users' digital assets. This attack is mainly caused by the permission re-delegation (PRD) vulnerability. In this work, we propose PrettySmart, a bytecode-level Permission re-delegation vulnerability detector for Smart contracts. Our study begins with an empirical study on 0.43 million open-source smart contracts, revealing that five types of widely-used permission constraints dominate 98% of the studied contracts. Accordingly, we propose a mechanism to infer these permission constraints, as well as an algorithm to identify constraints that can be bypassed by unauthorized adversaries. Based on the identification of permission constraints, we propose to detect whether adversaries could manipulate the privileged token management functionalities of smart contracts. The experimental results on real-world datasets demonstrate the effectiveness of the proposed PrettySmart, which achieves the highest precision score and detects 118 new PRD vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {168},
numpages = {12},
keywords = {smart contract, permission control, vulnerability detection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/ICSE48619.2023.00032,
author = {Wang, Xiaoke and Zhao, Lei},
title = {APICad: Augmenting API Misuse Detection through Specifications from Code and Documents},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00032},
doi = {10.1109/ICSE48619.2023.00032},
abstract = {Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICad to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICad can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICad, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {245–256},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597503.3639209,
author = {Na, Yoonjong and Woo, Seunghoon and Lee, Joomyeong and Lee, Heejo},
title = {CNEPS: A Precise Approach for Examining Dependencies among Third-Party C/C++ Open-Source Components},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639209},
doi = {10.1145/3597503.3639209},
abstract = {The rise in open-source software (OSS) reuse has led to intricate dependencies among third-party components, increasing the demand for precise dependency analysis. However, owing to the presence of reused files that are difficult to identify the originating components (i.e., indistinguishable files) and duplicated components, precisely identifying component dependencies is becoming challenging.In this paper, we present Cneps, a precise approach for examining dependencies in reused C/C++ OSS components. The key idea of Cneps is to use a novel granularity called a module, which represents a minimum unit (i.e., set of source files) that can be reused as a library from another project. By examining dependencies based on modules instead of analyzing single reused files, Cneps can precisely identify dependencies in the target projects, even in the presence of indistinguishable files. To differentiate duplicated components, Cneps examines the cloned paths and originating projects of each component, enabling precise identification of dependencies associated with them. Experimental results on top 100 C/C++ software show that Cneps outperforms a state-of-the-art approach by identifying twice as many dependencies. Cneps could identify 435 dependencies with 89.9% precision and 93.2% recall in less than 10 seconds per application on average, whereas the existing approach hardly achieved 63.5% precision and 42.5% recall.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {236},
numpages = {12},
keywords = {open source software reuse, supply chain security, third-party library dependency, software bill of materials (SBOM)},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3648505.3648508,
author = {Bairy, Akhila and Fr\"{a}nzle, Martin},
title = {What if Autonomous Systems had a Game Master? Targeted Explaining with the help of a Supervisory Control System},
year = {2024},
isbn = {9798400705960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648505.3648508},
doi = {10.1145/3648505.3648508},
abstract = {In the era of increasing automation, systems are making more autonomous decisions than ever before, often leading to conflicts with other systems. In situations where humans are entangled in such conflicts without proper explanation, they experience frustration. The consideration of the human's frustration level in such scenarios therefore is a key aspect of solving these conflicts. Our approach involves synthesizing an appropriate supervisory control system (SCS) within a gaming context. The main focus of this paper is to seamlessly blend a SCS with conflict resolution capabilities, capitalizing on the explanatory potential. The central objective is to explore how this integration can be accomplished by employing the SCS as a game master or as a moderator within a game-theoretic framework. In this capacity, the system becomes a central authority equipped with comprehensive information relevant to the game, possessing the knowledge and insights required to navigate intricate scenarios. Employing a game-theoretic approach allows us to harness the system's in-depth understanding of the game's dynamics to strategically resolve conflicts and optimize outcomes. The game master utilises an explanation model designed to provide understandable and situationally aware explanations, effectively mitigating frustration levels for humans involved in the process.},
booktitle = {Proceedings of the 2024 Workshop on Explainability Engineering},
pages = {15–19},
numpages = {5},
keywords = {game theory, explanations, user models, frustration models, explanation game, game master},
location = {Lisbon, Portugal},
series = {ExEn '24}
}

@inproceedings{10.1145/3639478.3640040,
author = {Zang, Zhiqiang and Thimmaiah, Aditya and Gligoric, Milos},
title = {JOG: Java JIT Peephole Optimizations and Tests from Patterns},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3640040},
doi = {10.1145/3639478.3640040},
abstract = {We present JOG, a framework for developing peephole optimizations and accompanying tests for Java compilers. JOG allows developers to write a peephole optimization as a pattern in Java itself. Such a pattern contains code before and after the desired transformation defined by the peephole optimization, with any necessary preconditions, and the pattern can be written in the same way that tests for the optimization are already written in OpenJDK. JOG automatically translates each pattern into C/C++ code as a JIT optimization pass, and generates tests for the optimization. Also, JOG automatically analyzes the shadow relation between a pair of optimizations where the effect of the shadowed optimization is overridden by the other. We used JOG to write 162 patterns, including many patterns found in OpenJDK and LLVM, as well as some that we proposed. We opened ten pull requests (PRs) for OpenJDK, on introducing new optimizations, removing shadowed optimizations, and adding generated tests for optimizations; nine of PRs have already been integrated into the master branch of OpenJDK. The demo video for JOG can be found at https://youtu.be/z2q6dhOiqgw.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {11–15},
numpages = {5},
keywords = {just-in-time compilers, code generation, peephole optimizations},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3597503.3639095,
author = {Eladawy, Hadeel and Le Goues, Claire and Brun, Yuriy},
title = {Automated Program Repair, What Is It Good For? Not Absolutely Nothing!},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639095},
doi = {10.1145/3597503.3639095},
abstract = {Industrial deployments of automated program repair (APR), e.g., at Facebook and Bloomberg, signal a new milestone for this exciting and potentially impactful technology. In these deployments, developers use APR-generated patch suggestions as part of a human-driven debugging process. Unfortunately, little is known about how using patch suggestions affects developers during debugging. This paper conducts a controlled user study with 40 developers with a median of 6 years of experience. The developers engage in debugging tasks on nine naturally-occurring defects in real-world, open-source, Java projects, using Recoder, SimFix, and TBar, three state-of-the-art APR tools. For each debugging task, the developers either have access to the project's tests, or, also, to code suggestions that make all the tests pass. These suggestions are either developer-written or APR-generated, which can be correct or deceptive. Deceptive suggestions, which are a common APR occurrence, make all the available tests pass but fail to generalize to the intended specification. Through a total of 160 debugging sessions, we find that access to a code suggestion significantly increases the odds of submitting a patch. Access to correct APR suggestions increase the odds of debugging success by 14,000% as compared to having access only to tests, but access to deceptive suggestions decrease the odds of success by 65%. Correct suggestions also speed up debugging. Surprisingly, we observe no significant difference in how novice and experienced developers are affected by APR, suggesting that APR may find uses across the experience spectrum. Overall, developers come away with a strong positive impression of APR, suggesting promise for APR-mediated, human-driven debugging, despite existing challenges in APR-generated repair quality.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {84},
numpages = {13},
keywords = {automated program repair, debugging, human factors, user study},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639478.3639801,
author = {Brizzio, Mat\'{\i}as},
title = {Resolving Goal-Conflicts and Scaling Synthesis through Mode-Based Decomposition},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639801},
doi = {10.1145/3639478.3639801},
abstract = {Reactive synthesis, with its roots in the work of A. Church, presents a transformative approach for the formal methods community. It seeks to translate system behaviors expressed in Linear-Time Temporal Logic (LTL) into correct-by-construction models using synthesis tools. However, this approach faces substantial challenges. Among these challenges is the high computational complexity of LTL synthesis, which constrains its application to large-scale systems. Additionally, unrealizable specifications present a significant obstacle as they act as barriers, impeding the synthesis process. Furthermore, the presence of goal-conflicts within requirements introduces contradictions and ambiguity, further complicating the synthesis process. These issues collectively make synthesis demanding, often resulting in suboptimal or unviable systems.Our research is dedicated to establishing a robust framework that systematically addresses these challenges, effectively bridging the gap between high-level requirements and dependable system realization. We prioritize refining requirement precision and advancing scalable synthesis techniques, offering advanced tools and methodologies to practitioners and researchers.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {207–211},
numpages = {5},
keywords = {search-based software engineering, formal methods, requirements engineering, evolutionary computation, reactive synthesis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00024,
author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
title = {DocToModel: Automated Authoring of Models from Diverse Requirements Specification Documents},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
doi = {10.1109/ICSE-SEIP58684.2023.00024},
abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {199–210},
numpages = {12},
keywords = {pattern interpreter, meta-model pattern, NLP, document parser, model extraction, automated model authoring, meta-model},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE-SEIS58686.2023.00011,
author = {de Souza Santos, Ronnie and de Magalh\~{a}es, Cleyton V. C. and Ralph, Paul},
title = {Benefits and Limitations of Remote Work to LGBTQIA+ Software Professionals},
year = {2023},
isbn = {9798350322613},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS58686.2023.00011},
doi = {10.1109/ICSE-SEIS58686.2023.00011},
abstract = {Background. The mass transition to remote work amid the COVID-19 pandemic profoundly affected software professionals, who abruptly shifted into ostensibly temporary home offices. The effects of this transition on these professionals are complex, depending on the particularities of the context and individuals. Recent studies advocate for remote structures to create opportunities for many equity-deserving groups; however, remote work can also be challenging for some individuals, such as women and individuals with disabilities. As the discussions on equity, diversity, and inclusion increase in software engineering, it is important to explore the realities and perspectives of different equity-deserving groups to develop strategies that can support them post-pandemic. Objective. This study aims to investigate the effects of remote work on LGBTQIA+ software professionals. Method. Grounded theory methodology was applied based on information collected from two main sources: a survey questionnaire with a sample of 57 LGBTQIA+ software professionals and nine follow-up interviews with individuals from this sample. This sample included professionals of different genders, ethnicities, sexual orientations, and levels of experience. Consistent with grounded theory methodology, the process of data collection and analysis was conducted iteratively using three stages of coding: line-by-line, focused, and theoretical. Member checking was used to validate the findings obtained from interpreting the experiences commented on by LGBTQIA+ software professionals. Findings. Our findings demonstrate that (1) remote work benefits LGBTQIA+ people by increasing security and visibility; (2) remote work harms LGBTQIA+ software professionals through isolation and invisibility; (3) the benefits outweigh the drawbacks; (4) the drawbacks can be mitigated by supportive measures developed by software companies. Conclusion. This paper investigated how remote work can affect LGBTQIA+ software professionals and presented a set of recommendations on how software companies can address the benefits and limitations associated with this work model. In summary, we concluded that remote work is crucial in increasing diversity and inclusion in the software industry.Remote work is here to stay. There is no denying it, as some software professionals would rather quit their jobs than return to the office full-time. Therefore, software companies want to understand how the remote working model can be successfully used without causing major issues. The problem is that the effects of remote work are complex because they depend on individual and group characteristics that require careful evaluation. In this scenario, one thing has been extremely positive: remote work is helping to increase diversity in software engineering by fostering new opportunities and better work conditions for individuals from equity-deserving groups, for instance, LGBTQIA+ software professionals. The software industry is overly homogeneous, most of the professionals who work in this area are heterosexual men (a reflection of the university courses on computer science and software engineering), but diversity can only be good for an area that strongly depends on creativity and innovation. What better way to innovate than putting several individuals from different backgrounds and with various experiences to work together? Remote work plays an important role in improving equity, diversity, and inclusion in the software industry. In this paper, we discuss how remote work is affecting software professionals from the LGBTQIA+ community and provide a list of recommendations to support software companies in dealing with this work model.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society},
pages = {48–57},
numpages = {10},
keywords = {LGBTQIA+, software professionals, inclusion, diversity, equity, EDI},
location = {Melbourne, Australia},
series = {ICSE-SEIS '23}
}

@inproceedings{10.1145/3597503.3639115,
author = {Rahman, Shanto and Shi, August},
title = {FlakeSync: Automatically Repairing Async Flaky Tests},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639115},
doi = {10.1145/3597503.3639115},
abstract = {Regression testing is an important part of the development process but suffers from the presence of flaky tests. Flaky tests nondeterministically pass or fail when run on the same code, misleading developers about the correctness of their changes. A common type of flaky tests are async flaky tests that flakily fail due to timing-related issues such as asynchronous waits that do not return in time or different thread interleavings during execution. Developers commonly try to repair async flaky tests by inserting or increasing some wait time, but such repairs are unreliable.We propose FlakeSync, a technique for automatically repairing async flaky tests by introducing synchronization for a specific test execution. FlakeSync works by identifying a critical point, representing some key part of code that must be executed early w.r.t. other concurrently executing code, and a barrier point, representing the part of code that should wait until the critical point has been executed. FlakeSync can modify code to check when the critical point is executed and have the barrier point keep waiting until the critical point has been executed, essentially synchronizing these two parts of code for the specific test execution. Our evaluation of FlakeSync on known flaky tests from prior work shows that FlakeSync can automatically repair 83.75% of async flaky tests, and the resulting changes add a median overhead of only 1.00X the original test runtime. We submitted 10 pull requests with our changes to developers, with 3 already accepted and none rejected.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {136},
numpages = {12},
keywords = {flaky test repair, async flaky tests},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644865,
author = {Islam, Anisha and Eng, Kalvin and Hindle, Abram},
title = {Opening the Valve on Pure-Data: Usage Patterns and Programming Practices of a Data-Flow Based Visual Programming Language},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644865},
doi = {10.1145/3643991.3644865},
abstract = {Pure Data (PD), a data-flow based visual programming language utilized for music and sound synthesis, remains underexplored in software engineering research. Existing literature fails to address the nuanced programming practices within PD, prompting the need to investigate how end-users manipulate nodes and edges in this visual language. This paper systematically extracts and analyzes 6,534 publicly available PD projects from GitHub. Employing source code parsing, pattern matching, and statistical analysis, we unveil usage patterns of PD by the end-user programmers. We found that most revisions of the PD files are small and simple, with fewer than 64 nodes, 51 connections, and 3 revisions. Most PD projects have less than 17 PD files, 31 commits, and only 1 author working on the PD files. The median differences in the number of nodes and edges between each commit and its parents, modifying the same file, are 3 and 0, respectively, implying small changes across various revisions of a PD file. Our findings contribute a valuable dataset for future studies, addressing the dearth of research in PD. By unraveling usage patterns, we provide insights that empower scholars and practitioners to optimize the programming experience for end-users in the realm of visual programming languages.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {492–497},
numpages = {6},
keywords = {visual programming language, pure data, end-user programmers},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3623331,
author = {Deiner, Adina and Fraser, Gordon},
title = {NuzzleBug: Debugging Block-Based Programs in Scratch},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623331},
doi = {10.1145/3597503.3623331},
abstract = {While professional integrated programming environments support developers with advanced debugging functionality, block-based programming environments for young learners often provide no support for debugging at all, thus inhibiting debugging and preventing debugging education. In this paper we introduce NuzzleBug, an extension of the popular block-based programming environment Scratch that provides the missing debugging support. NuzzleBug allows controlling the executions of Scratch programs with classical debugging functionality such as stepping and breakpoints, and it is an omniscient debugger that also allows reverse stepping. To support learners in deriving hypotheses that guide debugging, NuzzleBug is an interrogative debugger that enables to ask questions about executions and provides answers explaining the behavior in question. In order to evaluate NuzzleBug, we survey the opinions of teachers, and study the effects on learners in terms of debugging effectiveness and efficiency. We find that teachers consider NuzzleBug to be useful, and children can use it to debug faulty programs effectively. However, systematic debugging requires dedicated training, and even when NuzzleBug can provide correct answers learners may require further help to comprehend faults and necessary fixes, thus calling for further research on improving debugging techniques and the information they provide.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {22},
numpages = {13},
keywords = {debugging tools, omniscient debugging, interrogative debugging, scratch, computer science education},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3643991.3644866,
author = {Samhi, Jordan and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
title = {AndroLibZoo: A Reliable Dataset of Libraries Based on Software Dependency Analysis},
year = {2024},
isbn = {9798400705878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643991.3644866},
doi = {10.1145/3643991.3644866},
abstract = {Android app developers extensively employ code reuse, integrating many third-party libraries into their apps. While such integration is practical for developers, it can be challenging for static analyzers to achieve scalability and precision when libraries account for a large part of the code. As a direct consequence, it is common practice in the literature to consider developer code only during static analysis -with the assumption that the sought issues are in developer code rather than the libraries. However, analysts need to distinguish between library and developer code. Currently, many static analyses rely on white lists of libraries. However, these white lists are unreliable, inaccurate, and largely non-comprehensive.In this paper, we propose a new approach to address the lack of comprehensive and automated solutions for the production of accurate and "always up to date" sets of libraries. First, we demonstrate the continued need for a white list of libraries. Second, we propose an automated approach to produce an accurate and up-to-date set of third-party libraries in the form of a dataset called AndroLibZoo. Our dataset, which we make available to the community, contains to date 34 813 libraries and is meant to evolve.},
booktitle = {Proceedings of the 21st International Conference on Mining Software Repositories},
pages = {32–36},
numpages = {5},
location = {Lisbon, Portugal},
series = {MSR '24}
}

@inproceedings{10.1145/3597503.3639132,
author = {Wu, Rongxin and He, Yuxuan and Huang, Jiafeng and Wang, Chengpeng and Tang, Wensheng and Shi, Qingkai and Xiao, Xiao and Zhang, Charles},
title = {LibAlchemy: A Two-Layer Persistent Summary Design for Taming Third-Party Libraries in Static Bug-Finding Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639132},
doi = {10.1145/3597503.3639132},
abstract = {Despite the benefits of using third-party libraries (TPLs), the misuse of TPL functions raises quality and security concerns. Using traditional static analysis to detect bugs caused by TPL function is non-trivial. One promising solution would be to automatically generate and persist the summaries of TPL functions offline and then reuse these summaries in compositional static analysis online. However, when dealing with millions of lines of TPL code, the summaries designed by existing studies suffer from an unresolved paradox. That is, a highly precise form of summary leads to an unaffordable space and time overhead, while an imprecise one seriously hurts its precision or recall.To address the paradox, we propose a novel two-layer summary design. The first layer utilizes a line-sized program representation known as the program dependence graph to compactly encode path conditions, while the second layer encodes bug-type-specific properties. We implemented our idea as a tool called LibAlchemy and evaluated it on fifteen mature and extensively checked open-source projects. Experimental results show that LibAlchemy can check over ten million lines of code within ten hours. LibAlchemy has detected 55 true bugs with a high precision of 90.16%, eleven of which have been assigned CVE IDs. Compared to whole-program analysis and the conventional design of path-sensitively precise summaries, LibAlchemy achieves an 18.56x and 12.77x speedup and saves 91.49% and 90.51% of memory usage, respectively.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {105},
numpages = {13},
keywords = {static bug-finding, function summary, third-party library},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1109/SVM66695.2025.00008,
author = {Meneely, Andrew and Green, Aiden and Jaafari, Tyler and Fluet, Matthew and Keller, Brandon},
title = {"Just Use Rust": A Best-Case Historical Study of Open Source Vulnerabilities in C},
year = {2025},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SVM66695.2025.00008},
doi = {10.1109/SVM66695.2025.00008},
abstract = {Since its inception in 1972, the C programming language has employed a design philosophy that gives developers free reign and fine-grained control. Unfortunately, despite multiple generations of new engineers, C projects still suffer from chronic human errors that lead to vulnerabilities. Newer languages like Rust have taken a different approach with the stated goal of mitigating these errors via enforcing types and borrow checking at compile-time. These guarantees are promising, and indeed much work is being committed to "translating" C to Rust. But how much of an impact would Rust have had? The goal of this work is to inform the discussion around secure programming language designs by historically analyzing vulnerabilities in C with respect to Rust. We made a comprehensive mapping of the entire Common Weakness Enumeration vulnerability taxonomy to those that Rust would (and would not) be able to mitigate. We identified 68 prominent open source projects with C code, collected their vulnerability history data, and used their CWE designations to explore a best-case speculation on how many vulnerabilities might have been prevented with Rust. We estimate that 58.2% of historical vulnerabilities in our selection of open source C projects would have been virtually impossible in Rust. Depending on one’s expectations, this number might be surprisingly high or surprisingly low, so we hope this study will help ground the discussion about the decision for C projects to "oxidize."},
booktitle = {Proceedings of the 2025 IEEE/ACM 3rd International Workshop on Software Vulnerability Management},
pages = {25–32},
numpages = {8},
location = {Ottawa, ON, Canada},
series = {SVM '25}
}

@inproceedings{10.1145/3644033.3644375,
author = {Stratan, Cristina and Dawes, Joshua Heneage and Bianculli, Domenico},
title = {Diagnosing Violations of Time-based Properties Captured in iCFTL},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644375},
doi = {10.1145/3644033.3644375},
abstract = {Runtime Verification (RV) dynamically analyses the sequence of events recorded during system execution, typically stored in traces, and provides a verdict on system behavior. RV has tended to use Boolean, or sometimes quantitative, verdicts to express whether an execution satisfied some specification. However, engineers often want to know the reason for the verdict, which can be found by carrying out diagnostics.In this paper, we develop a diagnostics approach for a time-based fragment of iCFTL, a specification language designed for capturing properties concerning inter-procedural, source code-level behaviour of programs. We begin by developing an instrumentation scheme that builds on iCFTL's original scheme, enabling the construction of more informative traces. These traces are then used to determine a point of no return, which is an event past which a specification can never be satisfied. Our diagnostics approach then highlights a section of the trace in question that leads to the point of no return. We conclude the paper by presenting an evaluation of a prototype tool. Across 21 diverse programs, we observe that our approach is effective, efficient, and induces low time and space overhead.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {33–43},
numpages = {11},
keywords = {runtime verification, diagnostics, static analysis, temporal logic},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

@inproceedings{10.1145/3643666.3648580,
author = {Miranda, Darliane and Ara\'{u}jo, Jo\~{a}o and Liebel, Grischa},
title = {A Conceptual Model For Web Accessibility Requirements In Agile Development},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643666.3648580},
doi = {10.1145/3643666.3648580},
abstract = {Accessibility is the practice of making content and functionality accessible to all users, regardless of their abilities. Although accessibility is a highly relevant quality attribute, it is often treated as an afterthought in software development, unfortunately excluding people with disabilities from using many web-based systems. Specifically in agile development, sprints focus on new features and quality attributes, such as accessibility, are often not considered sufficiently. In these cases, using conceptual models to understand and analyze requirements that developers have formulated as a set of related user stories is a research opportunity. To increase agile professionals' focus on accessibility, we built a conceptual model for web accessibility, identifying artifacts and concepts used in agile development to specify accessibility. We discuss how this model can be used as a guide to better integrate accessibility considerations into agile software development. Researchers can use the result to define resources that are not currently covered or improve underutilized practices. We plan to use the conceptual model in the next steps to adapt existing agile artifacts and create support tools for web accessibility in agile development.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Multi-Disciplinary, Open, and RElevant Requirements Engineering},
pages = {15–21},
numpages = {7},
keywords = {accessibility requirements, agile development, conceptual model, requirements engineering},
location = {Lisbon, Portugal},
series = {MO2RE 2024}
}

@inproceedings{10.1145/3597503.3639093,
author = {Feng, Nick and Marsso, Lina and Getir Yaman, Sinem and Baatartogtokh, Yesugen and Ayad, Reem and De Mello, Victoria Oldemburgo and Townsend, Beverley and Standen, Isobel and Stefanakos, Ioannis and Imrie, Calum and Rodrigues, Genaina Nunes and Cavalcanti, Ana and Calinescu, Radu and Chechik, Marsha},
title = {Analyzing and Debugging Normative Requirements via Satisfiability Checking},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639093},
doi = {10.1145/3597503.3639093},
abstract = {As software systems increasingly interact with humans in application domains such as transportation and healthcare, they raise concerns related to the social, legal, ethical, empathetic, and cultural (SLEEC) norms and values of their stakeholders. Normative non-functional requirements (N-NFRs) are used to capture these concerns by setting SLEEC-relevant boundaries for system behavior. Since N-NFRs need to be specified by multiple stakeholders with widely different, non-technical expertise (ethicists, lawyers, regulators, end users, etc.), N-NFR elicitation is very challenging. To address this difficult task, we introduce N-Check, a novel tool-supported formal approach to N-NFR analysis and debugging. N-Check employs satisfiability checking to identify a broad spectrum of N-NFR well-formedness issues, such as conflicts, redundancy, restrictiveness, and insufficiency, yielding diagnostics that pinpoint their causes in a user-friendly way that enables non-technical stakeholders to understand and fix them. We show the effectiveness and usability of our approach through nine case studies in which teams of ethicists, lawyers, philosophers, psychologists, safety analysts, and engineers used N-Check to analyse and debug 233 N-NFRs, comprising 62 issues for the software underpinning the operation of systems, such as, assistive-care robots and tree-disease detection drones to manufacturing collaborative robots.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {214},
numpages = {12},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3639476.3639761,
author = {Sato, Naoto and Katsube, Ryota},
title = {Locating Buggy Segments in Quantum Program Debugging},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639761},
doi = {10.1145/3639476.3639761},
abstract = {When a bug is detected by testing a quantum program on a quantum computer, we want to determine its location to fix it. To locate the bug, the quantum program is divided into several segments, and each segment is tested. However, to prepare a quantum state that is input to a segment, it is necessary to execute all the segments ahead of that segment in a quantum computer. This means that the cost of testing each segment depends on its location. We can also locate a buggy segment only if it is confirmed that there are no bugs in all segments ahead of that buggy segment. Since a quantum program is tested statistically on the basis of measurement results, there is a tradeoff between testing accuracy and cost. Although these characteristics are unique to quantum programs and complicate locating bugs, they have not been investigated. We suggest for the first time that these characteristics should be considered to efficiently locate bugs. We are also the first to propose a bug-locating method that takes these characteristics into account. The results from experiments indicate that the bug-locating cost, represented as the number of executed quantum gates, can be reduced with the proposed method compared with naive methods.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {26–31},
numpages = {6},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3639476.3639767,
author = {Malka, Julien and Zacchiroli, Stefano and Zimmermann, Th\'{e}o},
title = {Reproducibility of Build Environments through Space and Time},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639767},
doi = {10.1145/3639476.3639767},
abstract = {Modern software engineering builds up on the composability of software components, that rely on more and more direct and transitive dependencies to build their functionalities. This principle of reusability however makes it harder to reproduce projects' build environments, even though reproducibility of build environments is essential for collaboration, maintenance and component lifetime. In this work, we argue that functional package managers provide the tooling to make build environments reproducible in space and time, and we produce a preliminary evaluation to justify this claim. Using historical data, we show that we are able to reproduce build environments of about 7 million Nix packages, and to rebuild 99.94% of the 14 thousand packages from a 6-year-old Nixpkgs revision.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–101},
numpages = {5},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00047,
author = {Arno, Alisa and Iwama, Futoshi and Takeuchi, Mikio},
title = {Automated Metamorphic Testing using Transitive Relations for Specializing Stance Detection Models},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00047},
doi = {10.1109/ICSE-SEIP58684.2023.00047},
abstract = {In machine-learning-based natural language processing, methods with high accuracy have been proposed for stance detection tasks. However, when they are applied to specific domains, they are often inaccurate due to domain-specific expressions. We propose an automated metamorphic testing method using transitive relations for creating training data that specializes stance detection in a specific domain. By specializing IBM Debater's stance detection in currency exchange domain, we confirmed our proposed method can improve the accuracy of judging the currency exchange-related sentences.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {467–470},
numpages = {4},
keywords = {model specialization, stance detection, transitive relation, metamorphic testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3639476.3639763,
author = {Tu, Haoxin and Jiang, Lingxiao and Gao, Debin and Jiang, He},
title = {Beyond a Joke: Dead Code Elimination Can Delete Live Code},
year = {2024},
isbn = {9798400705007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639476.3639763},
doi = {10.1145/3639476.3639763},
abstract = {Dead Code Elimination (DCE) is a fundamental compiler optimization technique that removes dead code (e.g., unreachable or reachable but whose results are unused) in the program to produce smaller or faster executables. However, since compiler optimizations are typically aggressively performed and there are complex relationships/interplay among a vast number of compiler optimizations (including DCE), it is not known whether DCE is indeed correctly performed and will only delete dead code in practice. In this study, we open a new research problem to investigate: can DCE happen to erroneously delete live code? To tackle this problem, we design a new approach named Xdead, which leverages differential testing, static binary analysis, and dynamic symbolic execution techniques, to detect miscompilation bugs caused by the erroneously deleted live code. Preliminary evaluation shows that Xdead can identify many divergent portions indicating erroneously deleted live code and finally detect two such miscompilation bugs in LLVM compilers. Our findings call for more attention to the potential issues in existing DCE implementations and more conservative strategies when designing new DCE-related compiler optimizations.},
booktitle = {Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {32–36},
numpages = {5},
keywords = {reliability, software testing, program analysis, symbolic execution},
location = {Lisbon, Portugal},
series = {ICSE-NIER'24}
}

@inproceedings{10.1145/3643794.3648275,
author = {Yefi, Peter and Menon, Ramanunni and Eicker, Ursula},
title = {Evaluation of APIs for Data Exchange with Building Management Systems},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648275},
doi = {10.1145/3643794.3648275},
abstract = {Access to the systems that manage buildings, building management systems (BMS), and the data they generate can enable applications and research work to achieve important goals, like minimising building energy consumption. The management layer of BMSs has human device interfaces (HDI) or human-machine interfaces (HMI), which include the building management software used by facility managers to monitor and control these systems. We explore the application programming interfaces (API) various BMS software provide to facilitate data exchange with third-party applications and other interested stakeholders. We focus on the BMS of five vendors: Siemens, Honeywell, Schneider Electric, Johnson Controls, and Delta Controls. We broadly evaluate their API support for five aspects of BMS, including alarms and events, locations (or structure), device and equipment, points and trended data.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {1–6},
numpages = {6},
keywords = {building management system (BMS), application programming interface (API), middleware, integration, data exchange},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00021,
author = {Almeida, Lu\'{\i}s and Gonzaga, Miguel and Santos, Jos\'{e} Fragoso and Abreu, Rui},
title = {RexStepper: A Reference Debugger for JavaScript Regular Expressions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00021},
doi = {10.1109/ICSE-Companion58688.2023.00021},
abstract = {Regular expressions are notoriously difficult to get right, with developers often having to resort to trial-and-error approaches. Even so, little attention has been given by the research community to the development of effective debugging tools for regular expressions. We present RexStepper, a reference debugger for troubleshooting JavaScript regular expressions in the browser. RexStepper is implemented on top of RexRef, a trusted reference implementation of JavaScript (ECMAScript 5) regular expressions, which works by transpiling the given regular expression to a JavaScript function that recognises its expansions. We demonstrate the usefulness of RexStepper by successfully using it to troubleshoot a benchmark of 18 faulty regular expressions obtained from the Stack Overflow and Stack Exchange websites.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {41–45},
numpages = {5},
keywords = {debuggers, regular expressions, JavaScript},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00102,
author = {Cui, Siwei and Gao, Yifei and Unterguggenberger, Rainer and Pichler, Wilfried and Livingstone, Sean and Huang, Jeff},
title = {SmallRace: Static Race Detection for Dynamic Languages - A Case on Smalltalk},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00102},
doi = {10.1109/ICSE48619.2023.00102},
abstract = {Smalltalk, one of the first object-oriented programming languages, has had a tremendous influence on the evolution of computer technology. Due to the simplicity and productivity provided by the language, Smalltalk is still in active use today by many companies with large legacy codebases and with new code written every day.A crucial problem in Smalltalk programming is the race condition. Like in any other parallel language, debugging race conditions is inherently challenging, but in Smalltalk, it is even more challenging due to its dynamic nature. Being a purely dynamically-typed language, Smalltalk allows assigning any object to any variable without type restrictions, and allows forking new threads to execute arbitrary anonymous code blocks passed as objects. In Smalltalk, race conditions can be introduced easily, but are difficult to prevent at runtime.We present SmallRace, a novel static race detection framework designed for multithreaded dynamic languages, with a focus on Smalltalk. A key component of SmallRace is SmallIR, a subset of LLVM IR, in which all variables are declared with the same type---a generic pointer i8*. This allows SmallRace to design an effective interprocedural thread-sensitive pointer analysis to infer the concrete types of dynamic variables. SmallRace automatically translates Smalltalk source code into SmallIR, supports most of the modern Smalltalk syntax in Visual Works, and generates actionable race reports with detailed debugging information. Importantly, SmallRace has been used to analyze a production codebase in a large company with over a million lines of code, and it has found tens of complex race conditions in the production code.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1136–1147},
numpages = {12},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3526072.3527537,
author = {Diller, Abigail C. and Fredericks, Erik M.},
title = {Towards run-time search for real-world multi-agent systems},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527537},
doi = {10.1145/3526072.3527537},
abstract = {Multi-agent systems (MAS) may encounter uncertainties in the form of unexpected environmental conditions, sub-optimal system configurations, and unplanned interactions between autonomous agents. The number of combinations of such uncertainties may be innumerable, however run-time testing may reduce the issues impacting such a system. We posit that search heuristics can augment a run-time testing process, in-situ, for a MAS. To support our position we discuss our in-progress experimental testbed to realize this goal and highlight challenges we anticipate for this domain.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {14–15},
numpages = {2},
keywords = {search-based software testing, multi-agent systems, cyber-physical systems},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00050,
author = {Xu, Weiwei and Wu, Xin and He, Runzhi and Zhou, Minghui},
title = {LicenseRec: Knowledge Based Open Source License Recommendation for OSS Projects},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00050},
doi = {10.1109/ICSE-Companion58688.2023.00050},
abstract = {Open Source license is a prerequisite for open source software, which regulates the use, modification, redistribution, and attribution of the software. Open source license is crucial to the community development and commercial interests of an OSS project, yet choosing a proper license from hundreds of licenses remains challenging. Tools assisting developers to understand the terms and pick the right license have been emerging, while inferring license compatibility on the dependency tree and satisfying the complex needs of developers are beyond the capability of most of them. Thus we propose LicenseRec, an open source license recommendation tool that helps to bridge the gap. LicenseRec performs fine-grained license compatibility checks on OSS projects' code and dependencies, and assists developers to choose the optimal license through an interactive wizard with guidelines of three aspects: personal open source style, business pattern, and community development. The usefulness of LicenseRec is confirmed by the consistent positive feedback from 10 software developers with academic and industrial backgrounds. Our tool is accessible at https://licenserec.com and a video showcasing the tool is available at https://video.licenserec.com.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {180–183},
numpages = {4},
keywords = {open source license recommendation, open source license, open source software},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3526073.3527590,
author = {Lewis, Grace A. and Echeverr\'{\i}a, Sebasti\'{a}n and Pons, Lena and Chrabaszcz, Jeffrey},
title = {Augur: a step towards realistic drift detection in production ML systems},
year = {2023},
isbn = {9781450393195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526073.3527590},
doi = {10.1145/3526073.3527590},
abstract = {The inference quality of deployed machine learning (ML) models degrades over time due to differences between training and production data, typically referred to as drift. While large organizations rely on periodic training to evade drift, the reality is that not all organizations have the data and the resources required to do so. We propose a process for drift behavior analysis at model development time that determines the set of metrics and thresholds to monitor for runtime drift detection. Better understanding of how models will react to drift before they are deployed, combined with a mechanism for how to detect this drift in production, is an important aspect of Responsible AI. The toolset and experiments reported in this paper provide an initial demonstration of (1) drift behavior analysis as a part of the model development process, (2) metrics and thresholds that need to be monitored for drift detection in production, and (3) libraries for drift detection that can be embedded in production monitoring infrastructures.},
booktitle = {Proceedings of the 1st Workshop on Software Engineering for Responsible AI},
pages = {37–44},
numpages = {8},
keywords = {software engineering, responsible AI, model monitoring, machine learning, drift detection},
location = {Pittsburgh, Pennsylvania},
series = {SE4RAI '22}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00033,
author = {Fabijan, Aleksander and Dmitriev, Pavel and Arai, Benjamin and Drake, Andy and Kohlmeier, Sebastian and Kwong, April},
title = {A/B Integrations: 7 Lessons Learned from Enabling A/B Testing as a Product Feature},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00033},
doi = {10.1109/ICSE-SEIP58684.2023.00033},
abstract = {A/B tests are the gold standard for evaluating product changes. At Microsoft, for example, we run tens of thousands of A/B tests every year to understand how users respond to new designs, new features, bug fixes, or any other ideas we might have on what will deliver value to users. In addition to testing product changes, however, A/B testing is starting to gain momentum as a differentiating feature of platforms or products whose primary purpose may not be A/B testing. As we describe in this paper, organizations such as Azure PlayFab and Outreach have integrated experimentation platforms and offer A/B testing to their customers as one of the many features in their product portfolio. In this paper and based on multiple-case studies, we present the lessons learned from enabling A/B integrations - integrating A/B testing into software products. We enrich each of the learnings with a motivating example, share the trade-offs made along this journey, and provide recommendations for practitioners. Our learnings are most applicable for engineering teams developing experimentation platforms, integrators considering embedding A/B testing into their products, and for researchers working in the A/B testing domain.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {304–314},
numpages = {11},
keywords = {platform design, A/B integrations, A/B testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3526072.3527529,
author = {Ivanov, Dmitry and Menshutin, Alexey and Fokin, Denis and Kamenev, Yury and Pospelov, Sergey and Kulikov, Egor and Stroganov, Nikita},
title = {UTBot Java at the SBST2022 tool competition},
year = {2023},
isbn = {9781450393188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526072.3527529},
doi = {10.1145/3526072.3527529},
abstract = {UTBotCpp and UTBot Java [3] are automatic white-box test generators for C/C++ and Java programs correspondingly. The tools were developed by Huawei and are based on symbolic and concrete execution. They try to cover as many branches as possible using program bytecode. For this purpose, UTBot tools analyze paths in the control flow graph of a given method, construct constraints for them, and try to find satisfying input values using SMT-solver to cover corresponding branches. In this paper, we report the results of UTBot Java at the tenth edition of the SBST 2022 tool competition.},
booktitle = {Proceedings of the 15th Workshop on Search-Based Software Testing},
pages = {39–40},
numpages = {2},
location = {Pittsburgh, Pennsylvania},
series = {SBST '22}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00104,
author = {Stradowski, Szymon and Madeyski, Lech},
title = {Can We Knapsack Software Defect Prediction? Nokia 5G Case},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00104},
doi = {10.1109/ICSE-Companion58688.2023.00104},
abstract = {As software products become larger and more complex, the test infrastructure needed for quality assurance grows similarly, causing a constant increase in operational and maintenance costs. Although rising in popularity, most Artificial Intelligence (AI) and Machine Learning (ML) Software Defect Prediction (SDP) solutions address singular test phases. In contrast, the need to address the whole Software Development Life Cycle (SDLC) is rarely explored. Therefore in this paper, we define the problem of extending the SDP concept to the entire SDLC, as this may be one of the significant next steps for the field. Furthermore, we explore the similarity between the defined challenge and the widely known Multidimensional Knapsack Problem (MKP). We use Nokia's 5G wireless technology test process to illustrate the proposed concept. Resulting comparison validates the applicability of MKP to optimize the overall test cycle, which can be similarly relevant to any large-scale industrial software development process.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {365–369},
numpages = {5},
keywords = {Nokia 5G, software development life cycle, continuous integration, software testing, software defect prediction, artificial intelligence},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00028,
author = {Shariffdeen, Ridwan and Mirchev, Martin and Noller, Yannic and Roychoudhury, Abhik},
title = {Cerberus: A Program Repair Framework},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00028},
doi = {10.1109/ICSE-Companion58688.2023.00028},
abstract = {Automated Program Repair (APR) represents a suite of emerging technologies which attempt to automatically fix bugs and vulnerabilities in programs. APR is a rapidly growing field with new tools and benchmarks being added frequently. Yet a language agnostic repair framework is not available. We introduce Cerberus, a program repair framework integrated with 20 program repair tools and 9 repair benchmarks, coexisting in the same framework. Cerberus is capable of executing diverse set of program repair tasks, using multitude of program repair tools and benchmarks.Video: https://www.youtube.com/watch?v=bYtShpsGL68},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {73–77},
numpages = {5},
keywords = {repair platform, automated program repair},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643664.3648209,
author = {Rodr\'{\i}guez, Pilar},
title = {Grounded Theory in Software Engineering: Challenges and Lessons Learned from the Trenches},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648209},
doi = {10.1145/3643664.3648209},
abstract = {Context: Grounded Theory (GT) is a research method that facilitates theory development. Its application in Software Engineering (SE) often raises concerns among researchers. Objectives: This paper aims to highlight challenges in applying GT in SE and provide practical insights to overcome them. Method: I outline the top five challenges I faced while applying GT to develop a theory of value for value-based feature selection. The theory was developed taking a positivist stand. I also reflect on the lessons that I learned along the way. Results: The top five challenges are: 1) defining research questions, 2) keeping away from the related literature, 3) assuring the trustworthiness of "ground" data, 4) specifying the theory with a proper granularity level, and 5) reporting a GT study in a limited-length research paper. These challenges led to 17 lessons learned. Conclusions: While my experience with GT in SE has been positive, certain aspects of the method need thoughtful consideration, particularly when the research is conducted from a positivist stand. I hope that the experience I share in this paper is valuable for others in the SE community attempting to use GT for their studies.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {21–26},
numpages = {6},
keywords = {grounded theory, research method, software engineering},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@inproceedings{10.1145/3643915.3644110,
author = {Riccio, Vincenzo and Sorrentino, Giancarlo and Zamponi, Ettore and Camilli, Matteo and Mirandola, Raffaela and Scandurra, Patrizia},
title = {RAMSES: An Artifact Exemplar for Engineering Self-Adaptive Microservice Applications},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644110},
doi = {10.1145/3643915.3644110},
abstract = {This paper introduces RAMSES, an exemplar tailored for both practitioners and researchers working on self-adaptive microservice applications. By emphasizing a clear separation of concerns between the application and its adaptation logic, RAMSES realizes a reusable autonomic manager that implements a MAPE-K feedback loop whose components are microservices themselves. Its primary focus lies in addressing user-defined QoS attributes at runtime, like availability and performance. To illustrate its usage, we provide a practical example showing its mechanics in an e-food microservice application. Initial experiments indicate the advantages of utilizing RAMSES, as shown by a comparative analysis of the quality properties of a microservice application with and without self-adaptation.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {161–167},
numpages = {7},
keywords = {microservice applications, self-adaptation, MAPE-K, exemplar},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3641822.3641883,
author = {S\'{a}nchez-Gord\'{o}n, Mary and Colomo-Palacios, Ricardo and Sanchez Gordon, Alex},
title = {Characterizing Role Models in Software Practitioners' Career: An Interview Study},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641883},
doi = {10.1145/3641822.3641883},
abstract = {A role model is a person who serves as an example for others to follow, especially in terms of values, behavior, achievements, and personal characteristics. In this paper, authors study how role models influence software practitioners' careers, an aspect not studied in the literature before. By means of this study, authors aim to understand if there are any salient role model archetypes and what characteristics are valued by participants in their role models. To do so, authors use a thematic coding approach to analyze the data collected from interviewing ten Latin American software practitioners. Findings reveal that role models were perceived as sources of knowledge, yet the majority of participants, regardless of their career stage, displayed a stronger interest in the human side and the moral values that their role models embodied. This study also shows that any practitioner can be viewed as a role model.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {164–169},
numpages = {6},
keywords = {role model, software developers, software engineering},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00048,
author = {Zhao, Zelin and Wang, Xizao and Xu, Zhaogui and Tang, Zhenhao and Li, Yongchao and Di, Peng},
title = {Incremental Call Graph Construction in Industrial Practice},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00048},
doi = {10.1109/ICSE-SEIP58684.2023.00048},
abstract = {Interprocedural program analysis is critical in finding hidden program defects and vulnerabilities in CI/CD pipelines. A pre-constructed call graph is a prerequisite for interprocedural analysis. However, the exhaustive call graph construction, i.e., analyzing the target program as a whole and constructing from scratch, often takes too much time. We made a scalable empirical study on both industrial and open-source projects and observed that most program updates only involve a very limited part of the code. The observation inspires an efficient approach that not wholely re-constructs a call graph but incrementally patches the old one with the partial graph affected by the update. We propose a sound incremental call graph construction algorithm that works in a reset-recompute way: first, prune invalid nodes and edges from the old call graph, then analyze the new code to patch it to construct the new one. We implemented the algorithm and built a benchmark suite consisting of 20 industrial and 10 open-source projects. The experimental evaluation shows that the efficiency improvement is encouraging. Compared with the exhaustive construction algorithm, the incremental way can speed up the construction by 20.0 times and reduce the memory and storage consumption to 58.1% and 10.4%, respectively.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {471–482},
numpages = {12},
keywords = {CI/CD, class hierarchy analysis, incremental construction, call graph},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE48619.2023.00020,
author = {Guo, Suyue and Wan, Xinyu and You, Wei and Liang, Bin and Shi, Wenchang and Zhang, Yiwei and Huang, Jianjun and Zhang, Jian},
title = {Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00020},
doi = {10.1109/ICSE48619.2023.00020},
abstract = {Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {95–107},
numpages = {13},
keywords = {fuzzing, type reasoning, PDF reader, binding call},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00053,
author = {Gill, Waris and Anwar, Ali and Gulzar, Muhammad Ali},
title = {FedDebug: Systematic Debugging for Federated Learning Applications},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00053},
doi = {10.1109/ICSE48619.2023.00053},
abstract = {In Federated Learning (FL), clients independently train local models and share them with a central aggregator to build a global model. Impermissibility to access clients' data and collaborative training make FL appealing for applications with data-privacy concerns, such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, identifying the responsible rounds and clients is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the global model's accuracy or let future FL rounds retune the model, which are time-consuming and costly.We design a systematic fault localization framework, FedDebug, that advances the FL debugging on two novel fronts. First, FedDebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to construct a simulation that mirrors live FL. FedDebug's breakpoint can help inspect an FL state (round, client, and global model) and move between rounds and clients' models seamlessly, enabling a fine-grained step-by-step inspection. Second, FedDebug automatically identifies the client(s) responsible for lowering the global model's performance without any testing data and labels---both are essential for existing debugging techniques. FedDebug's strengths come from adapting differential testing in conjunction with neuron activations to determine the client(s) deviating from normal behavior. FedDebug achieves 100% accuracy in finding a single faulty client and 90.3% accuracy in finding multiple faulty clients. FedDebug's interactive debugging incurs 1.2% overhead during training, while it localizes a faulty client in only 2.1% of a round's training time. With FedDebug, we bring effective debugging practices to federated learning, improving the quality and productivity of FL application developers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {512–523},
numpages = {12},
keywords = {CNN, neural networks, fault localization, client, testing, federated learning, software debugging},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00023,
author = {AlOmar, Eman Abdullah and AlOmar, Salma Abdullah and Mkaouer, Mohamed Wiem},
title = {On the Use of Static Analysis to Engage Students with Software Quality Improvement: An Experience with PMD},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00023},
doi = {10.1109/ICSE-SEET58685.2023.00023},
abstract = {Static analysis tools are frequently used to scan the source code and detect deviations from the project coding guidelines. Given their importance, linters are often introduced to classrooms to educate students on how to detect and potentially avoid these code anti-patterns. However, little is known about their effectiveness in raising students' awareness, given that these linters tend to generate a large number of false positives. To increase the awareness of potential coding issues that violate coding standards, in this paper, we aim to reflect on our experience with teaching the use of static analysis for the purpose of evaluating its effectiveness in helping students with respect to improving software quality. This paper discusses the results of an experiment in the classroom, over a period of 3 academic semesters, involving 65 submissions that carried out code review activity of 690 rules using PMD. The results of the quantitative and qualitative analysis show that the presence of a set of PMD quality issues influences the acceptance or rejection of the issues, design, and best practices-related categories that take longer time to be resolved, and students acknowledge the potential of using static analysis tools during code review. Through this experiment, code review can turn into a vital part of the educational computing plan. We envision our findings enabling educators to support students with code review strategies in order to raise students' awareness about static analysis tools and scaffold their coding skills.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {179–191},
numpages = {13},
keywords = {quality, education, static analysis tool},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1109/ICSE48619.2023.00151,
author = {Yu, Guangba and Chen, Pengfei and Li, Pairui and Weng, Tianjun and Zheng, Haibing and Deng, Yuetang and Zheng, Zibin},
title = {LogReducer: Identify and Reduce Log Hotspots in Kernel on the Fly},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00151},
doi = {10.1109/ICSE48619.2023.00151},
abstract = {Modern systems generate a massive amount of logs to detect and diagnose system faults, which incurs expensive storage costs and runtime overhead. After investigating real-world production logs, we observe that most of the logging overhead is due to a small number of log templates, referred to as log hotspots. Therefore, we conduct a systematical study about log hotspots in an industrial system WeChat, which motivates us to identify log hotspots and reduce them on the fly. In this paper, we propose LogReducer, a non-intrusive and language-independent log reduction framework based on eBPF (Extended Berkeley Packet Filter), consisting of both online and offline processes. After two months of serving the offline process of LogReducer in WeChat, the log storage overhead has dropped from 19.7 PB per day to 12.0 PB (i.e., about a 39.08% decrease). Practical implementation and experimental evaluations in the test environment demonstrate that the online process of LogReducer can control the logging overhead of hotspots while preserving logging effectiveness. Moreover, the log hotspot handling time can be reduced from an average of 9 days in production to 10 minutes in the test with the help of LogReducer.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1763–1775},
numpages = {13},
keywords = {log parsing, log reduction, eBPF, log hotspot},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00014,
author = {Zang, Zhiqiang and Yu, Fu-Yao and Wiatrek, Nathan and Gligoric, Milos and Shi, August},
title = {JAttack: Java JIT Testing Using Template Programs},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00014},
doi = {10.1109/ICSE-Companion58688.2023.00014},
abstract = {We present JAttack, a framework that enables compiler testing using templates. JAttack allows compiler developers to write a template program that describes a set of concrete programs to be used to test compilers. Such a template-based approach leverages developers' intuition on testing compilers, by allowing developers to write a template program in the host programming language (Java), which contains a basic program structure while provides an opportunity to express variants of specific language constructs in holes. Each hole, written in a domain-specific language embedded in the host language, is used to construct an extended abstract syntax tree (eAST), which defines the search space of a language construct, e.g., a set of numbers, expressions, statements, etc. JAttack executes the template program to fill every hole by randomly choosing a number, expression, or statement within the search space defined by the hole, and it generates concrete programs with all holes filled. We used JAttack to test Java just-in-time (JIT) compilers, and we have found seven critical bugs in Oracle JDK JIT compiler. Oracle developers confirmed and fixed all seven bugs, five of which were previously unknown, including two CVEs (Common Vulnerabilities and Exposures). JAttack blends developers' intuition via templates with random testing to detect bugs in compilers. The demo video for JAttack can be found at https://www.youtube.com/watch?v=meCFPxucqk4.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {6–10},
numpages = {5},
keywords = {templates, compiler, program generation, test generation, testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00019,
author = {Petrovi\'{c}, Goran and Ivankovi\'{c}, Marko and Fraser, Gordon and Just, Ren\'{e}},
title = {Please Fix This Mutant: How Do Developers Resolve Mutants Surfaced during Code Review?},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00019},
doi = {10.1109/ICSE-SEIP58684.2023.00019},
abstract = {Mutation testing has been demonstrated to motivate developers to write more tests when presented with undetected, actionable mutants. To facilitate this effect, modern mutation systems aim to generate and surface only actionable mutants---few in numbers but highly valuable to the developer. This requires a deeper understanding of the extent to which developers resolve surfaced mutants and how: If they decide not to resolve an undetected mutant, why not? On the other hand, if they do resolve a mutant, do they simply add a test that detects it, or do they also improve the code?In order to answer these questions we compiled and analyzed a dataset of 1,538 merge requests with corresponding mutants surfaced during the code review phase. Our analysis reveals that determining whether a mutant is indeed resolved during code review is actually a non-trivial problem: for 64% of mutants, the mutated code changes as the merge request evolves, requiring dedicated techniques to precisely resurface the same mutants and to discover which of them remain unresolved after a code change. Overall, our analysis demonstrates that 38% of all surfaced mutants are resolved via code changes or test additions. Out of all mutants that are endorsed by a reviewer, 60% are resolved and result in additional tests, code refactorings, and improved documentation. Unresolved, yet endorsed, mutants stem from developers questioning the value of adding tests for surfaced mutants, later resolving mutants in deferred code changes (atomicity of merge requests), and false positives (mutants being resolved by tests not considered when creating the mutants, e.g., in integration test suites).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {150–161},
numpages = {12},
keywords = {mutant resolution, code review, code quality, test efficacy, mutation testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00009,
author = {van Meerten, Martijn and Ozkan, Burcu Kulahcioglu and Panichella, Annibale},
title = {Evolutionary Approach for Concurrency Testing of Ripple Blockchain Consensus Algorithm},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00009},
doi = {10.1109/ICSE-SEIP58684.2023.00009},
abstract = {Blockchain systems are prone to concurrency bugs due to the nondeterminism in the delivery order of messages between the distributed nodes. These bugs are hard to detect since they can only be triggered by a specific order or timing of concurrent events in the execution. Systematic concurrency testing techniques, which explore all possible delivery orderings of messages to uncover concurrency bugs, are not scalable to large distributed systems such as blockchains. Random concurrency testing methods search for bugs in a randomly generated set of executions and offer a practical testing method.In this paper, we investigate the effectiveness of random concurrency testing on blockchain systems using a case study on the XRP Ledger of the Ripple blockchain, which maintains one of the most popular cryptocurrencies in the market today. We test the Ripple consensus algorithm of the XRP Ledger by exploring different delivery orderings of consensus protocol messages. Moreover, we design an evolutionary algorithm to guide the random test case generation toward certain system behaviors to discover concurrency bugs more efficiently. Our case study shows that random concurrency testing is effective at detecting concurrency bugs in blockchains, and the evolutionary approach for test generation improves test efficiency. Our experiments could successfully detect the bugs we seeded in the Ripple source code. Moreover, we discovered a previously unknown concurrency bug in the production implementation of Ripple.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {36–47},
numpages = {12},
keywords = {software testing, evolutionary algorithms, concurrency, consensus, distributed systems, blockchains, ripple},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3605760.3623763,
author = {Sadeghpour, Shadi and Vlajic, Natalija},
title = {RanABD: MTD-Based Technique for Detection of Advanced Session-Replay Web Bots},
year = {2023},
isbn = {9798400702563},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605760.3623763},
doi = {10.1145/3605760.3623763},
abstract = {In the current digital landscape, cyberattacks have become increasingly sophisticated in their attempts to evade detection. One such example is the session-replay web bot attack, where hackers use previously recorded human mouse movements (i.e., sessions) to emulate human behavior on the target web sites and apps. With the emergence of advanced AI, hackers are further expected to utilize these programs to generate carefully randomized session-replay bots that still exhibit human-like behavior but without replaying/repeating identical mouse trajectories, as was previously the case. Detecting such advanced bots in the traditionally designed web pages and sites is exceptionally hard if not impossible. In this paper, we propose RanABD, a novel defensive web page randomization technique that is built on the concepts of moving-target defence (MTD) and is designed to counter advanced session-replay web bots. RanABD performs randomized micro modifications in the alignment of select visual HTML elements and element attributes in the target web page, while causing minimal disturbances in the page's overall appearance and functionality. By doing so, the technique ensures that the distances between trajectories of genuine human-visitors, as well as trajectories of repeat visits by the same human user, are sufficiently separated in the Feature Space. For session-replay bot operators, the only way to bypass this defence is by increasing the degree of randomization in replay sessions, but this approach is likely to backfire as it inevitably results in outlier-like trajectories that are even easier to detect. According to our knowledge, this is the first research paper that explicitly addresses the issue of advanced session-replay bots as well as proposes a novel technique that can effectively detect these specific types of bots.},
booktitle = {Proceedings of the 10th ACM Workshop on Moving Target Defense},
pages = {17–23},
numpages = {7},
keywords = {randomization, session-replay bots, web defences},
location = {Copenhagen, Denmark},
series = {MTD '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00024,
author = {Mobilio, Marco and Clerissi, Diego and Denaro, Giovanni and Mariani, Leonardo},
title = {GUI Testing to the Power of Parallel Q-Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00024},
doi = {10.1109/ICSE-Companion58688.2023.00024},
abstract = {Q-learning is an attractive option for GUI testing, allowing for sophisticated test generation strategies that learn and exploit effective GUI interactions. However, learning comprehensive models requires long test sessions. This issue is exacerbated by the needs of both testers, who might want to run multiple testing sessions to fine-tune the test strategy to their applications under test, and researchers, who might want to experiment with multiple alternative approaches. To address these concerns, this paper presents GTPQL, a testing tool that supports GUI testing with a parallel deployment Q-learning, and that can be flexibly configured and extended with multiple state-space abstractions and Q-leaning variants.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {55–59},
numpages = {5},
keywords = {GUI testing, web testing, Q-learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00015,
author = {Zhu, Hao-Nan and Guan, Kevin Z. and Furth, Robert M. and Rubio-Gonz\'{a}lez, Cindy},
title = {ActionsRemaker: Reproducing GitHub Actions},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00015},
doi = {10.1109/ICSE-Companion58688.2023.00015},
abstract = {Mining Continuous Integration and Continuous Delivery (CI/CD) has enabled new research opportunities for the software engineering (SE) research community. However, it remains a challenge to reproduce CI/CD build processes, which is crucial for several areas of research within SE such as fault localization and repair. In this paper, we present ActionsRemaker, a reproducer for GitHub Actions builds. We describe the challenges on reproducing GitHub Actions builds and the design of ActionsRemaker. Evaluation of ActionsRemaker demonstrates its ability to reproduce fail-pass pairs: of 180 pairs from 67 repositories, 130 (72.2%) from 43 repositories are reproducible. We also discuss reasons for unreproducibility. ActionsRemaker is publicly available at https://github.com/bugswarm/actions-remaker, and a demo of the tool can be found at https://youtu.be/flblSqoxeAk.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {11–15},
numpages = {5},
keywords = {software reproducibility, software build, software mining, GitHub actions, CI/CD},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3528231.3536382,
author = {Almeida, Cleuton and Fran\c{c}a, C\'{e}sar},
title = {Improving the PBL method with experiential learning theory in software engineering teaching},
year = {2023},
isbn = {9781450393362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528231.3536382},
doi = {10.1145/3528231.3536382},
abstract = {Context: Problem-Based Learning (PBL) and Experiential Learning Theory (ELT) are convergent active learning approaches widely known for their competent integration between theory and practice. Problem/Objective: However, the usual implementation of PBL leaves out the final active experimentation stage of the experiential learning cycle. In this article, we intend to systematically investigate the impacts of this last stage on the learning outcomes of software engineering students. Methods: A quasi-experiment was designed and applied in three software engineering courses of an undergraduate course, in Rio Branco-Acre / Brazil. Results: students who participated in two of the three treatment groups scored significantly higher on measures of motivation, experience and learning, which means that the PBL method contains gaps that can be significantly improved with the help of ELT, benefiting the learning outcomes of software engineering students.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Education for the Next Generation},
pages = {28–35},
numpages = {8},
keywords = {experiential learning theory, problem-based learning, software engineering education},
location = {Pittsburgh, Pennsylvania},
series = {SEENG '22}
}

@inproceedings{10.1145/3643794.3648289,
author = {Abuserrieh, Lobna and Alalfi, Manar H.},
title = {Towards an MDRE Approach to Verify Security and safety of Heterogeneous IoT Apps},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648289},
doi = {10.1145/3643794.3648289},
abstract = {The rapid growth of IoT technology across various domains presents challenges in coordinating diverse IoT devices operating on different platforms within shared environments. Our study introduces a Model Driven Reverse Engineering (MDRE) approach to analyze and verify critical properties in heterogeneous IoT applications. By applying this approach to platforms such as SmartThings, Open-HAB, and Amazon Alexa, we illustrate its efficacy in bolstering the security and safety of interconnected IoT systems.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {65–72},
numpages = {8},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1109/ICSE48619.2023.00056,
author = {Mordahl, Austin and Zhang, Zenong and Soles, Dakota and Wei, Shiyi},
title = {ECSTATIC: An Extensible Framework for Testing and Debugging Configurable Static Analysis},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00056},
doi = {10.1109/ICSE48619.2023.00056},
abstract = {Testing and debugging the implementation of static analysis is a challenging task, often involving significant manual effort from domain experts in a tedious and unprincipled process. In this work, we propose an approach that greatly improves the automation of this process for static analyzers with configuration options. At the core of our approach is the novel adaptation of the theoretical partial order relations that exist between these options to reason about the correctness of actual results from running the static analyzer with different configurations. This allows for automated testing of static analyzers with clearly defined oracles, followed by automated delta debugging, even in cases where ground truths are not defined over the input programs. To apply this approach to many static analysis tools, we design and implement ECSTATIC, an easy-to-extend, open-source framework. We have integrated four popular static analysis tools, SOOT, WALA, DOOP, and FlowDroid, into ECSTATIC. Our evaluation shows running ECSTATIC detects 74 partial order bugs in the four tools and produces reduced bug-inducing programs to assist debugging. We reported 42 bugs; in all cases where we received responses, the tool developers confirmed the reported tool behavior was unintended. So far, three bugs have been fixed and there are ongoing discussions to fix more.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {550–562},
numpages = {13},
keywords = {testing and debugging, program analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00099,
author = {Ko, Yoonseok and Oh, Hakjoo},
title = {Learning to Boost Disjunctive Static Bug-Finders},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00099},
doi = {10.1109/ICSE48619.2023.00099},
abstract = {We present a new learning-based approach for accelerating disjunctive static bug-finders. Industrial static bug-finders usually perform disjunctive analysis, differentiating program states along different execution paths of a program. Such path-sensitivity is essential for reducing false positives but it also increases analysis costs exponentially. Therefore, practical bug-finders use a state-selection heuristic to keep track of a small number of beneficial states only. However, designing a good heuristic for real-world programs is challenging; as a result, modern static bug-finders still suffer from low cost/bug-finding efficiency. In this paper, we aim to address this problem by learning effective state-selection heuristics from data. To this end, we present a novel data-driven technique that efficiently collects alarm-triggering traces, learns multiple candidate models, and adaptively chooses the best model tailored for each target program. We evaluate our approach with Infer and show that our technique significantly improves Infer's bug-finding efficiency for a range of open-source C programs.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1097–1109},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00012,
author = {Maffey, Katherine R. and Dotterrer, Kyle and Niemann, Jennifer and Cruickshank, Iain and Lewis, Grace A. and K\"{a}stner, Christian},
title = {MLTEing Models: Negotiating, Evaluating, and Documenting Model and System Qualities},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00012},
doi = {10.1109/ICSE-NIER58687.2023.00012},
abstract = {Many organizations seek to ensure that machine learning (ML) and artificial intelligence (AI) systems work as intended in production but currently do not have a cohesive methodology in place to do so. To fill this gap, we propose MLTE (Machine Learning Test and Evaluation, colloquially referred to as "melt"), a framework and implementation to evaluate ML models and systems. The framework compiles state-of-the-art evaluation techniques into an organizational process for interdisciplinary teams, including model developers, software engineers, system owners, and other stakeholders. MLTE tooling supports this process by providing a domain-specific language that teams can use to express model requirements, an infrastructure to define, generate, and collect ML evaluation metrics, and the means to communicate results.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {31–36},
numpages = {6},
keywords = {responsible AI, machine learning evaluation, test and evaluation, machine learning},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00023,
author = {Bogatinovski, Jasmin and Kao, Odej},
title = {Auto-Logging: Al-Centred Logging Instrumentation},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00023},
doi = {10.1109/ICSE-NIER58687.2023.00023},
abstract = {Logging in software development plays a crucial role in bug-fixing, maintaining the code and operating the application. Logs are hints created by human software developers that aim to help human developers and operators in identifying root causes for application bugs or other misbehaviour types. They also serve as a bridge between the Devs and the Ops, allowing the exchange of information. The rise of the DevOps paradigm with the CI/CD pipelines led to a significantly higher number of deployments per month and consequently increased the logging requirements. In response, AI-enabled methods for IT operation (AIOps) are introduced to automate the testing and run-time fault tolerance to a certain extent. However, using logs tailored for human understanding to learn (automatic) AI methods poses an ill-defined problem: AI algorithms need no hints but structured, precise and indicative data. Until now, AIOps researchers adapt the AI algorithms to the properties of the existing human-centred data (e.g., log sentiment), which are not always trivial to model. By pointing out the discrepancy, we envision that there exists an alternative approach: the logging can be adapted such that the produced logs are better tailored towards the strengths of the AI-enabled methods. In response, in this vision paper, we introduce auto-logging, which devises the idea of how to automatically insert log instructions into the code that can better suit AI-enabled methods as end-log consumers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {95–100},
numpages = {6},
keywords = {AIOps, logging, software engineering},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00015,
author = {Kuang, Peng and S\"{o}derberg, Emma and Niehorster, Diederick C. and H\"{o}st, Martin},
title = {Toward Gaze-Assisted Developer Tools},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00015},
doi = {10.1109/ICSE-NIER58687.2023.00015},
abstract = {Many crucial activities in software development are linked to gaze and can potentially benefit from gaze-assisted developer tools. However, despite the maturity of eye trackers and the potential for such tools, we see very few studies of practitioners. Here, we present a systematic mapping study to examine recent developments in the field with a focus on the experimental setup of eye-tracking studies in software engineering research. We identify two gaps regarding studies of practitioners in realistic settings and three challenges in existing experimental setups. We present six recommendations for how to steer the research community toward gaze-assisted developer tools that can benefit practitioners.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {49–54},
numpages = {6},
keywords = {mapping study, software engineering, developer tools, gaze behavior, eye tracking},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00029,
author = {Rahman, Shanto and Li, Chengpeng and Shi, August},
title = {TSVD4J: Thread-Safety Violation Detection for Java},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00029},
doi = {10.1109/ICSE-Companion58688.2023.00029},
abstract = {Concurrency bugs are difficult to detect and debug. One class of concurrency bugs are thread-safety violations, where multiple threads access thread-unsafe data structure at the same time, resulting in unexpected behavior. Prior work proposed an approach TSVD to detect thread-safety violations. TSVD injects delays at API calls that read/write to specific thread-unsafe data structures, tracking whether multiple threads can overlap in their accesses to the same data structure through the delays, showing potential thread-safety violations. We additionally enhance the TSVD approach to also consider read/write operations to object fields. We implement the TSVD approach in Java in our tool TSVD4J. TSVD4J can be integrated as a Maven plugin that can be included in any Maven-based application. Our evaluation on 12 applications shows that TSVD4J can detect 55 pairs of code locations accessing the same shared data structure across multiple threads, representing potential thread-safety violations. We find that the addition of tracking field accesses contributed the most to detecting these pairs. TSVD4J also detects more such pairs than existing tool RV-Predict. The demo video for TSVD4J is available at https://www.youtube.com/watch?v=-wSMzlj5cMY.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {78–82},
numpages = {5},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00070,
author = {Bhutamapuram, Umamaheswara Sharma},
title = {Some Investigations of Machine Learning Models for Software Defects},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00070},
doi = {10.1109/ICSE-Companion58688.2023.00070},
abstract = {Software defect prediction (SDP) and software defect severity prediction (SDSP) models alleviate the burden on the testers by providing the automatic assessment of a newly-developed program in a short amount of time. The research on defect prediction or defect severity prediction is primarily focused on proposing classification frameworks or addressing challenges in developing prediction models; however, the primary yet significant gap in the literature is interpreting the predictions in terms of project objectives. Furthermore, the literature indicates that these models have poor predictive performance. In this thesis, we investigate the use of a diversity-based ensemble learning mechanism for the cross-project defect prediction (CPDP) task and self-training semi-supervised learning for the software defect severity prediction, respectively, for obtaining better prediction performances. We also propose a few project-specific performance measures to interpret the predictions in terms of project objectives (such as a reduction in expenditure, time, and failure chances). Through the empirical analysis, we observe that (1) the diversity-based ensemble learning mechanism improves the prediction performance in terms of both the traditional and proposed measures, and (2) the self-training semi-supervised learning model has a positive impact on predicting the severity of a defective module.Once a potential prediction model is developed, any software organisation may utilise its services. How can an organisation showcase their trust in the developed prediction model? To this end, we investigate the feasibility of SDP models in real-world testing environments by providing proofs using the probabilistic bounds. The proofs summarised show that even if the prediction model has a lower failure probability, the probability of obtaining fewer failures in SDP-tested software than in similar but manually tested software is still exponentially small. This result enables the researchers in SDP to avoid proposing prediction models.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {259–263},
numpages = {5},
keywords = {feasibility study, performance measures, software reliability, software defect severity prediction, cross-project defect prediction, software defect prediction},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00018,
author = {Giamattei, Luca and Pietrantuono, Roberto and Russo, Stefano},
title = {Reasoning-Based Software Testing},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00018},
doi = {10.1109/ICSE-NIER58687.2023.00018},
abstract = {With software systems becoming increasingly pervasive and autonomous, our ability to test for their quality is severely challenged. Many systems are called to operate in uncertain and highly-changing environment, not rarely required to make intelligent decisions by themselves. This easily results in an intractable state space to explore at testing time. The state-of-the-art techniques try to keep the pace, e.g., by augmenting the tester's intuition with some form of (explicit or implicit) learning from observations to search this space efficiently. For instance, they exploit historical data to drive the search (e.g., ML-driven testing) or the tests execution data itself (e.g., adaptive or search-based testing). Despite the indubitable advances, the need for smartening the search in such a huge space keeps to be pressing.We introduce Reasoning-Based Software Testing (RBST), a new way of thinking at the testing problem as a causal reasoning task. Compared to mere intuition-based or state-of-the-art learning-based strategies, we claim that causal reasoning more naturally emulates the process that a human would do to "smartly" search the space. RBST aims to mimic and amplify, with the power of computation, this ability. The conceptual leap can pave the ground to a new trend of techniques, which can be variously instantiated from the proposed framework, by exploiting the numerous tools for causal discovery and inference. Preliminary results reported in this paper are promising.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {66–71},
numpages = {6},
keywords = {software testing, causal reasoning},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00047,
author = {Vigan\`{o}, Enrico and Cornejo, Oscar and Pastore, Fabrizio and Briand, Lionel},
title = {DaMAT: A Data-Driven Mutation Analysis Tool},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00047},
doi = {10.1109/ICSE-Companion58688.2023.00047},
abstract = {We present DaMAT, a tool that implements data-driven mutation analysis. In contrast to traditional code-driven mutation analysis tools it mutates (i.e., modifies) the data exchanged by components instead of the source of the software under test. Such an approach helps ensure that test suites appropriately exercise components interoperability --- essential for safety-critical cyber-physical systems. A user-provided fault model drives the mutation process. We have successfully evaluated DaMAT on software controlling a microsatellite and a set of libraries used in deployed CubeSats. A demo video of DaMAT is available at https://youtu.be/s5M52xWCj84},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {165–169},
numpages = {5},
keywords = {CPS, mutation analysis},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1109/ICSE48619.2023.00153,
author = {Ji, Zhenlan and Ma, Pingchuan and Yuan, Yuanyuan and Wang, Shuai},
title = {CC: Causality-Aware Coverage Criterion for Deep Neural Networks},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00153},
doi = {10.1109/ICSE48619.2023.00153},
abstract = {Deep neural network (DNN) testing approaches have grown fast in recent years to test the correctness and robustness of DNNs. In particular, DNN coverage criteria are frequently used to evaluate the quality of a test suite, and a number of coverage criteria based on neuron-wise, layer-wise, and path-/trace-wise coverage patterns have been published to date. However, we see that existing criteria are insufficient to represent how one neuron would influence subsequent neurons; hence, we lack a concept of how neurons, when functioning as causes and effects, might jointly make a DNN prediction.Given recent advances in interpreting DNN internals using causal inference, we present the first causality-aware DNN coverage criterion, which evaluates a test suite by quantifying the extent to which the suite provides new causal relations for testing DNNs. Performing standard causal inference on DNNs presents both theoretical and practical hurdles. We introduce CC (causal coverage), a practical and efficient coverage criterion that integrates a set of optimizations using DNN domain-specific knowledge. We illustrate the efficacy of CC using diverse, real-world inputs and adversarial inputs, such as adversarial examples (AEs) and backdoor inputs. We demonstrate that CC outperforms previous DNN criteria under various settings with moderate cost.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1788–1800},
numpages = {13},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3643794.3648350,
author = {Minani, Jean Baptiste and Sabir, Fatima and El Fellah, Yahia and Moha, Naouel},
title = {Towards an Automated Approach for Testing IoT Devices},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648350},
doi = {10.1145/3643794.3648350},
abstract = {The Internet of Things (IoT) comprises a network of physical devices embedded with sensors and software to collect and exchange data with other devices and systems via the Internet. IoT devices vary from small devices to complex industrial appliances. Despite the increase in the number of IoT devices, there is a lack of proper testing for these devices, which can impact the functionality of IoT systems. This study focuses on an automated approach for testing IoT systems that use Android-based devices. We propose an approach to generate test cases (TCs) and execute them in physical devices as instrumented tests. Our methodology uses source code as input. We analyze the source code and create an Abstract Syntax Tree (AST). We navigate the AST to identify classes, methods, input parameters, and return types. We manually create a test case (TC) template and use a heuristic search algorithm to generate the test data for each unit test. We populate the TC templates with information extracted from the AST and data generated by a heuristic search algorithm to generate executable TCs. We assess the quality of generated TCs using the mutation analysis technique. The experiment demonstrates that the proposed approach effectively generates executable TCs for conducting functional tests for IoT devices. This study can be beneficial for practitioners, researchers, and device manufacturers towards improvement in the way IoT devices are tested.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {22–29},
numpages = {8},
keywords = {IoT device testing, IoT system testing, websocket in IoT, instrumented tests in IoT, buddy, blue frog robot},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1145/3643662.3643954,
author = {Ramaj, Xhesika and S\'{a}nchez-Gord\'{o}n, Mary and Gkioulos, Vasileios and Colomo-Palacios, Ricardo},
title = {On DevSecOps and Risk Management in Critical Infrastructures: Practitioners' Insights on Needs and Goals},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643954},
doi = {10.1145/3643662.3643954},
abstract = {Risk management is essential for ensuring the sustained viability of organizations over the long term. It plays a pivotal role in business by helping identify potential threats and vulnerabilities, enabling well-informed decision-making. Within the context of critical infrastructures (CIs), it takes on even greater significance. DevSecOps is an innovative approach to bolstering security of software applications. This approach is being heralded as a transformative solution that encourages the adoption of robust security practices, reduces risk, and ensures uninterrupted business continuity. This qualitative study explores the needs and goals of implementing DevSecOps in CIs from the perspective of DevOps, developers, and security experts. Findings show that the relevance of DevSecOps in CIs emerges from the need for proactive work, increased efficiency, automation, monitoring mechanisms, security, and outstanding products and services. Findings also identify the goals for establishing a stronger market presence, increasing revenues, and maintaining a leading position in the market. The study provides valuable insights on DevSevOps in risk management, that can potentially encourage the adoption of DevSecOps and guide practitioners interested in leveraging the inherent benefits of this approach in the context of CIs.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {45–52},
numpages = {8},
keywords = {DevSecOps, risk management, software security, critical infrastructures},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1109/ICSE48619.2023.00118,
author = {Zhu, Shihao and Guo, Yuqi and Zhang, Long and Cai, Yan},
title = {Tolerate Control-Flow Changes for Sound Data Race Prediction},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00118},
doi = {10.1109/ICSE48619.2023.00118},
abstract = {Data races seriously threaten the correctness of concurrent programs. Earlier works can report false positives. Recently, trace-based predictive analysis has achieved sound results by inferring feasible traces based on sound partial orders or constraint solvers. However, they hold the same assumption: any read event may affect the control-flow of a predicted trace. Thus, being control-flow sensitive, they have to enforce any read event (in an inferred trace) to either read the same value or a value from the same event as that in the original trace, albeit some slightly relax this. This (even with relaxation) severely limits their predictive ability and many true data races can be missed.We introduce the concept of Fix-Point Event and propose a new partial order model. This allows us to not only predict races with witness traces (like existing works with no control-flow changes) but also soundly infer existences of witness traces with potential control-flow changes. Thus, we can achieve a higher concurrency coverage and detect more data races soundly. We have implemented above as a tool ToccRace and conducted a set of experiments on a benchmark of seven real-world programs and a large-scale software MySQL, where MySQL produced 427 traces with a total size of 3.4TB. Compared with the state-of-the-art sound data race detector SeqCheck, ToccRace is significantly more effective by detecting 84.4%/200% more unique/dynamic races on the benchmark programs and 52.22%/49.8% more unique/dynamic races on MySQL, incurring reasonable time and memory costs (about 1.1x/43.5x on the benchmark programs and 10x/1.03x on MySQL). Furthermore, ToccRace is sound and is complete on two threads.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1342–1354},
numpages = {13},
keywords = {static information, control flow, data races, concurrency bugs},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3526071.3527519,
author = {Mayr-Dorn, Christoph and Egyed, Alexander and Winterer, Mario and Salomon, Christian and F\"{u}rschu\ss{}, Harald},
title = {Evaluating PDDL for programming production cells: a case study},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527519},
doi = {10.1145/3526071.3527519},
abstract = {A unique selling point for cyber-physical production system manufacturers becomes the easy with which machines and cells can be adapted to new products and production processes. Adaptations, however, are often done by domain experts without in-depth programming know-how. We investigate in this paper, the implications of using a planning-based approach for using a domain expert's knowledge to control the sequences of a robot and injection molding machine (IMM). We find that current engineering support is insufficient to address testing, understanding, and change impact assessment concerns during the evolution of a PDDL/HDDL domain specification.},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {17–24},
numpages = {8},
keywords = {symbolic AI, robot programming, planning, manufacturing automation, end-user programming, PDDL, HDDL},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00029,
author = {Reimann, Lars and Kniesel-W\"{u}nsche, G\"{u}nter},
title = {An Alternative to Cells for Selective Execution of Data Science Pipelines},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00029},
doi = {10.1109/ICSE-NIER58687.2023.00029},
abstract = {Data Scientists often use notebooks to develop Data Science (DS) pipelines, particularly since they allow to selectively execute parts of the pipeline. However, notebooks for DS have many well-known flaws. We focus on the following ones in this paper: (1) Notebooks can become littered with code cells that are not part of the main DS pipeline but exist solely to make decisions (e.g. listing the columns of a tabular dataset). (2) While users are allowed to execute cells in any order, not every ordering is correct, because a cell can depend on declarations from other cells. (3) After making changes to a cell, this cell and all cells that depend on changed declarations must be rerun. (4) Changes to external values necessitate partial re-execution of the notebook. (5) Since cells are the smallest unit of execution, code that is unaffected by changes, can inadvertently be re-executed.To solve these issues, we propose to replace cells as the basis for the selective execution of DS pipelines. Instead, we suggest populating a context-menu for variables with actions fitting their type (like listing columns if the variable is a tabular dataset). These actions are executed based on a data-flow analysis to ensure dependencies between variables are respected and results are updated properly after changes. Our solution separates pipeline code from decision making code and automates dependency management, thus reducing clutter and the risk of making errors.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {129–134},
numpages = {6},
keywords = {machine learning, data science, usability, notebook},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00008,
author = {Chong, Chun Yong and Kang, Eunsuk and Shaw, Mary},
title = {Open Design Case Study - A Crowdsourcing Effort to Curate Software Design Case Studies},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00008},
doi = {10.1109/ICSE-SEET58685.2023.00008},
abstract = {Case study-based learning has been successfully integrated into various courses, including software engineering education. In the context of software design courses, the use of case studies often entails sharing of real successful or failed software development. Using examples of real-world case studies allows educators to reinforce the applicability and usefulness of fundamental design concepts, relate the importance of evaluating design trade-offs with respect to stakeholders' requirements, and highlight the importance of upfront design where students that lack industrial experience tend to overlook. However, the use of real-world case studies is not straightforward because 1.) there is a lack of open source repositories for real software design case studies and 2.) even if case studies are available, they are often reported without a standardized format, which may hinder the alignment between the case and the desired learning outcomes. To address the lack of software design case studies for educational purposes, we propose the idea of Open Design Case Study, a repository to crowdsource, curate, and recruit other educators to contribute case studies for teaching software design courses. The platform will also allow educators and students to share, brainstorm, and discuss design solutions based on case studies shared publicly on the repository.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {23–28},
numpages = {6},
keywords = {case studies, software design, software engineering education},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1145/3643662.3643956,
author = {Ryan, Ita and Roedig, Utz and Stol, Klaas-Jan},
title = {Training Developers to Code Securely: Theory and Practice},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643956},
doi = {10.1145/3643662.3643956},
abstract = {Software security is essential. Flaws in software design and coding produce vulnerabilities that can be exploited by hostile actors, resulting in ransomware, espionage and the hacking of critical infrastructure. Meanwhile, DevOps and continuous integration introduce speed imperatives, often bypassing traditional security gates. Software security responsibility is increasingly shifting to software developers. Industry insiders advise that this extra responsibility should be accompanied by developer training. But is it? Analysis of what constitutes good software security training is sparse, and there is little information on the amount and quality of training actually offered to developers in industry. We analyse recent literature to find the positive features of effective secure development training. We examine training information from a large developer survey (n=962) to assess how training in the field matches key positive features. We find that while some developers experience excellent secure-coding training, others receive inadequate training, and the majority receive none.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {37–44},
numpages = {8},
keywords = {secure software development, security, secure coding training},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1145/3643692.3648259,
author = {Langdon, William and Clark, David},
title = {Deep Mutations have Little Impact},
year = {2024},
isbn = {9798400705731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643692.3648259},
doi = {10.1145/3643692.3648259},
abstract = {Using MAGPIE (Machine Automated General Performance Improvement via Evolution of software), we measure the impact of genetic improvement (GI) on a non-deterministic deeply nested PARSEC VIPS parallel computing multi-threaded image processing benchmark written in C. More than 53% of mutants compile and generate identical results to the original program. We find about 10% Failed Disruption Propagation (FDP). Excluding internal errors and asserts, almost all changes deeper than 30 nested functions which are Executed and Infect data or change control are not Propagated to the output, i.e. these deep PIE changes have no external effect. Suggesting (where it relies on testing) automatic software engineering on deeply nested code will be hard.},
booktitle = {Proceedings of the 13th ACM/IEEE International Workshop on Genetic Improvement},
pages = {1–8},
numpages = {8},
keywords = {software testing, robust software, fault masking, resilience, repair, automatic code optimisation, failed disruption propagation, FDP, PIE (propagation, infection, and execution), fitness landscape, information theory, genetic programming, local search, SBSE},
location = {Lisbon, Portugal},
series = {GI '24}
}

@inproceedings{10.1145/3528226.3528370,
author = {Barboni, Morena and Morichetta, Andrea and Polini, Andrea},
title = {Smart contract testing: challenges and opportunities},
year = {2023},
isbn = {9781450393317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528226.3528370},
doi = {10.1145/3528226.3528370},
abstract = {Blockchain technologies have found important and concrete applications in the real world. Active solutions leverage Smart Contracts for the management of cryptocurrencies, sensitive data, and other valuable assets. One of the core objectives of blockchain-oriented software engineering (BOSE) is ensuring that Smart Contracts receive adequate pre-release testing to guarantee the deployment of reliable code. However, the novelty and the complexity of the blockchain environment pose new challenges to the validation and verification of Smart Contract based software. In this paper, we analyze the aforementioned challenges to foster the discussion on the specific topic of Smart Contract testing and identify relevant research directions.},
booktitle = {Proceedings of the 5th International Workshop on Emerging Trends in Software Engineering for Blockchain},
pages = {21–24},
numpages = {4},
keywords = {testing, smart contract, blockchain, BOSE},
location = {Pittsburgh, Pennsylvania},
series = {WETSEB '22}
}

@inproceedings{10.1145/3643787.3648029,
author = {B\"{a}ckstrand, Emil and Djupedal, Rasmus and \"{O}berg, Lena-Maria and de Oliveira Neto, Francisco Gomes},
title = {Unveiling Disparities: NLP Analysis of Software Industry and Vocational Education Gaps},
year = {2024},
isbn = {9798400705762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643787.3648029},
doi = {10.1145/3643787.3648029},
abstract = {The rapid growth of software industry highlights the importance of the education system in producing competent professionals to meet industry demands. Previous research has identified a gap between industry needs and the content of educational programs. This study presents Vocational Education and Labour Market Analyser (VELMA), a tool designed to extract information from job ads and educational curricula (both from Sweden), utilising topic modelling to identify the diverse technologies and skills in demand within the industry and those covered by professional education. Particularly, we use Latent Dirichlet Allocation (LDA) to categorise keywords into cohesive themes for document frequency analysis. Our findings highlight industry demand for skills in cloud and embedded technologies, security engineering, and software architecture. In contrast, the Higher Vocational Education (HVE) curricula emphasise the education of web developers and general object-oriented programming languages.},
booktitle = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
pages = {9–16},
numpages = {8},
keywords = {topic modelling, NLP analysis, software industry, vocational education},
location = {Lisbon, Portugal},
series = {NLBSE '24}
}

@inproceedings{10.1145/3528588.3528656,
author = {Chatterjee, Preetha},
title = {Automatic identification of informative code in stack overflow posts},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528656},
doi = {10.1145/3528588.3528656},
abstract = {Despite Stack Overflow's popularity as a resource for solving coding problems, identifying relevant information from an individual post remains a challenge. The overload of information in a post can make it difficult for developers to identify specific and targeted code fixes. In this paper, we aim to help users identify informative code segments, once they have narrowed down their search to a post relevant to their task. Specifically, we explore natural language-based approaches to extract problematic and suggested code pairs from a post. The goal of the study is to investigate the potential of designing a browser extension to draw the readers' attention to relevant code segments, and thus improve the experience of software engineers seeking help on Stack Overflow.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {21–24},
numpages = {4},
keywords = {text analysis, stack overflow, mining software repositories},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3639474.3640064,
author = {Lau, Yi Meng and Koh, Christian Michael and Jiang, Lingxiao},
title = {Teaching Software Development for Real-World Problems using a Microservice-Based Collaborative Problem-Solving Approach},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640064},
doi = {10.1145/3639474.3640064},
abstract = {Experienced and skillful software developers are needed in organizations to develop software products effective for their business with shortened time-to-market. Such developers will not only need to code but also be able to work in teams and collaboratively solve real-world problems that organizations are facing. It is challenging for educators to nurture students to become such developers with strong technical, social, and cognitive skills.Towards addressing the challenge, this study presents a Collaborative Software Development Project Framework for a course that focuses on learning microservices architectures and developing a software application for a real-world business. Students get to work in teams to solve a real-world problem of their own choice. They are given opportunities to recognize that the software development process goes beyond writing code and that social and cognitive skills in engaging with each other are also essential. By adopting microservices architectures in the course, students learn to break down the functionalities of their applications into smaller pieces of code with standardized interfaces that can be developed, tested, and deployed independently. This not only helps students to learn various technical skills needed for developing and implementing the functionalities needed by the application in the form of microservices but also facilitates task allocation and coordination among their team members and provides a platform for them to solve problems collaboratively. Upon completion of their projects, students are also asked to reflect on their development process and encouraged to think beyond the basics for better software design and development approaches.The course curriculum incorporates the framework, especially for the student team projects. The earlier teaching weeks introduce a combination of concepts and lab exercises to students as the building blocks. The survey studies show that the framework is effective in enhancing the students' learning of technical, social, and cognitive skills, while further improvements, such as closer collaboration with other courses, can be done to improve a holistic learning curriculum.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {22–33},
numpages = {12},
keywords = {software development, collaborative problem-solving, real-world solutions, microservices architectures},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3643663.3643970,
author = {Dust, Lukas and Ekstr\"{o}m, Mikael and Gu, Rong and Mubeen, Saad and Seceleanu, Cristina},
title = {A Model-Based Methodology for Automated Verification of ROS 2 Systems},
year = {2024},
isbn = {9798400705663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643663.3643970},
doi = {10.1145/3643663.3643970},
abstract = {To simplify the formal verification of ROS 2-based applications, in this paper, we propose a novel approach to the automation of their model-based verification using model-driven engineering techniques. We propose a methodology starting with ROS 2 execution traces, generated by ROS2_tracing and using models and model transformations in Eclipse to automatically initialize pre-defined formal model templates in UPPAAL, with system parameters. While the methodology targets the simplification of formal verification for robotics developers as users, the implementation is at an early stage and the toolchain is not fully implemented and evaluated. Hence, this paper targets tool developers and researchers to give a first overview of the underlying idea of automating ROS 2 verification.Hence, we propose a toolchain that supports verification of implemented and conceptual ROS 2 systems, as well as iterative verification of timing and scheduling parameters. We propose using four different model representations, based on the ROS2_tracing output and self-designed Eclipse Ecore metamodels to model the system from a structural and verification perspective. The different model representations allow traceability throughout the modeling and verification process. Last, an initial proof of concept is implemented containing the core elements of the proposed toolchain and validated given a small ROS 2 system.},
booktitle = {Proceedings of the 2024 ACM/IEEE 6th International Workshop on Robotics Software Engineering},
pages = {35–42},
numpages = {8},
keywords = {ROS 2, robotic systems, formal verification, model checking, model-based engineering},
location = {Lisbon, Portugal},
series = {RoSE '24}
}

@inproceedings{10.1145/3526071.3527515,
author = {Stadler, Marco and Vierhauser, Michael and Cleland-Huang, Jane},
title = {Towards flexible runtime monitoring support for ROS-based applications},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527515},
doi = {10.1145/3526071.3527515},
abstract = {Robotic systems are becoming common in different domains and for various purposes, such as unmanned aerial vehicles performing search and rescue operations, or robots operating in manufacturing plants. Such systems are characterized by close interactions, or even collaborations, between hardware and machinery on the one hand, and humans on the other. Furthermore, as Cyber-Physical Systems (CPS) in general and robotic applications in particular typically operate in an emergent environment, unanticipated events may occur during their operation, making the need for runtime monitoring support a crucial yet often time-consuming task. Runtime monitoring typically requires establishing support for collecting data, aggregating and transporting the data to a monitoring framework for persistence and further processing, and finally, performing checks of functional and non-functional properties. In this paper, we present our initial efforts towards a flexible monitoring framework for ROS-based systems. We report on challenges for establishing runtime monitoring support and present our preliminary architecture that aims to significantly reduce the setup and maintenance effort when creating monitors and establishing constraint checks.},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {43–46},
numpages = {4},
keywords = {runtime monitoring, cyber-physical systems, ROS},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@inproceedings{10.1145/3643794.3648281,
author = {Mauro Junior, Davino and Gama, Kiev},
title = {A Usability Study on the creation of Intrusion Detection Rules on IoT Networks},
year = {2024},
isbn = {9798400705786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643794.3648281},
doi = {10.1145/3643794.3648281},
abstract = {Network Intrusion Detection Systems (IDS) can be used to employ defenses on IoT environments by making use of rules to detect anomalies on network traffic. Usability must be treated as a key feature of these systems, especially on the process of creating the aforementioned rules. In this work, we present IoT-Flows, a platform built on traditional IDS's concepts such as network monitoring and generation of alerts once an anomaly is detected, but focusing on enabling users to create rules in an intuitive way with a user-interface (UI). We compared the usability of our platform with Suricata, a popular open-source IDS. In our experimental design, participants were assigned the task of creating a rule to detect a popular distributed denial-of-service attack (DDoS) attack on both systems. Then, we applied a System Usability Scale questionnaire combined with open-ended questions. The feedback showed that Suricata lacks flexibility and a user-friendly UI, especially for non-experienced users, despite its good documentation. In contrast, IoT-Flows was praised for its UI and flexibility but was slower in rule creation compared to Suricata. We found that usability needs to be considered when developing security systems, especially when targeting IoT contexts, where non-IT users are common.},
booktitle = {Proceedings of the ACM/IEEE 6th International Workshop on Software Engineering Research &amp; Practices for the Internet of Things},
pages = {73–80},
numpages = {8},
keywords = {IoT, usability, security, network intrusion detection systems},
location = {Lisbon, Portugal},
series = {SERP4IoT '24}
}

@inproceedings{10.1145/3641822.3641873,
author = {Padoan, Fernando and Santos, Ronnie De Souza and Medeiros, Rodrigo Pessoa},
title = {Charting a Path to Efficient Onboarding: The Role of Software Visualization},
year = {2024},
isbn = {9798400705335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641822.3641873},
doi = {10.1145/3641822.3641873},
abstract = {Background. Within the software industry, it is commonly estimated that software professionals invest a substantial portion of their work hours in the process of understanding existing systems. In this context, an ineffective technical onboarding process, which introduces newcomers to software under development, can result in a prolonged period for them to absorb the necessary knowledge required to become productive in their roles. Goal. The present study aims to explore the familiarity of managers, leaders, and developers with software visualization tools and how these tools are employed to facilitate the technical onboarding of new team members. Method. To address the research problem, we built upon the insights gained through the literature and embraced a sequential exploratory approach. This approach incorporated quantitative and qualitative analyses of data collected from practitioners using questionnaires and semi-structured interviews. Findings. Our findings demonstrate a gap between the concept of software visualization and the practical use of onboarding tools and techniques. Overall, practitioners do not systematically incorporate software visualization tools into their technical onboarding processes due to a lack of conceptual understanding and awareness of their potential benefits. Conclusion. The software industry could benefit from standardized and evolving onboarding models, improved by incorporating software visualization techniques and tools to support program comprehension of newcomers in the software projects.},
booktitle = {Proceedings of the 2024 IEEE/ACM 17th International Conference on Cooperative and Human Aspects of Software Engineering},
pages = {133–143},
numpages = {11},
keywords = {software visualization, technical onboarding, program comprehension},
location = {Lisbon, Portugal},
series = {CHASE '24}
}

@inproceedings{10.1145/3639477.3639742,
author = {Mao, Ke and \r{A}hs, Cons and Cela, Sopot and Distefano, Dino and Gardner, Nick and Grigore, Radu and Gustafsson, Per and Hajdu, \'{A}kos and Kapus, Timotej and Marescotti, Matteo and Sampaio, Gabriela Cunha and Suzanne, Thibault},
title = {PrivacyCAT: Privacy-Aware Code Analysis at Scale},
year = {2024},
isbn = {9798400705014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639477.3639742},
doi = {10.1145/3639477.3639742},
abstract = {Static and dynamic code analyses have been widely adopted in industry to enhance software reliability, security, and performance by automatically detecting bugs in the code. In this paper, we introduce PrivacyCAT1, a code analysis system developed and deployed at WhatsApp to protect user privacy. PrivacyCAT automatically detects privacy defects in code at early stages (before reaching production and affecting users), and therefore, it prevents such vulnerabilities from evolving into privacy incidents. PrivacyCAT comprises of a collection of static and dynamic taint analysers.We report on the technical development of PrivacyCAT and the results of two years of its large-scale industrial deployment at WhatsApp. We present our experience in designing its system architecture, and continuous integration process. We discuss the unique challenges encountered in developing and deploying such kind of analyses within an industrial context.Since its deployment in 2021, PrivacyCAT has safeguarded data privacy in 74% of privacy site events (SEVs). It has prevented 493 potential privacy SEVs from being introduced into the codebases, enabling developers to maintain a high privacy standard for the code that supports over two billion WhatsApp users.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice},
pages = {106–117},
numpages = {12},
keywords = {program analysis, dynamic analysis, static analysis, privacy},
location = {Lisbon, Portugal},
series = {ICSE-SEIP '24}
}

@inproceedings{10.1145/3597503.3639193,
author = {Kang, Hong Jin and Wang, Kevin and Kim, Miryung},
title = {Scaling Code Pattern Inference with Interactive What-If Analysis},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639193},
doi = {10.1145/3597503.3639193},
abstract = {Programmers often have to search for similar code when detecting and fixing similar bugs. Prior active learning approaches take only instance-level feedback, i.e., positive and negative method instances. This limitation leads to increased labeling burden, when users try to control generality and specificity for a desired code pattern.We present a novel feedback-guided pattern inference approach, called SURF. To reduce users' labelling effort, it actively guides users in assessing the implication of having a particular feature choice in the constructed pattern, and incorporates direct feature-level feedback. The key insight behind SURF is that users can effectively select appropriate features with the aid of impact analysis. SURF provides hints on the global distribution of how each feature is consistent with already labelled positive and negative instances, and how selection of a new feature can yield additional matching instances. Its what-if-analysis contrasts how different feature choices can include (or exclude) more instances in the rest of the population.We performed a user study with 14 participants, designed with two-treatment factorial crossover. Participants were able to provide 30% more correct answers about different API usages in 20% less time. All participants found that what-if-analysis and impact analysis are useful for pattern refinement. 79% of the participants were able to produce the correct, expected pattern with SURF's feature-level guidance, as opposed to 43% of the participants when using the baseline with instance-level feedback only. SURF is the first approach to incorporate feature-level feedback with automated what-if analysis to empower users to control the generality (/ specificity) of a desired code pattern.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {234},
numpages = {12},
keywords = {active learning, code search patterns, API misuse, human feedback},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@proceedings{10.1145/3528230,
title = {Q-SE '22: Proceedings of the 3rd International Workshop on Quantum Software Engineering},
year = {2022},
isbn = {9781450393355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The Q-SE workshop is a platform for researchers and practitioners to discuss challenges in developing quantum software in high-level quantum languages, novel solutions to build correct methods for testing quantum programs, executing quantum software, developing best practices, and creating a research roadmap of quantum software engineering. The Q-SE workshop intends to bring together people from academia and industry and provide opportunities for new collaborations and advance current practices of quantum software engineering.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3643915.3644099,
author = {Filippone, Gianluca and Pi\~{n}era Garc\'{\i}a, Juan Antonio and Autili, Marco and Pelliccione, Patrizio},
title = {Handling uncertainty in the specification of autonomous multi-robot systems through mission adaptation},
year = {2024},
isbn = {9798400705854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643915.3644099},
doi = {10.1145/3643915.3644099},
abstract = {Multi-robot systems (MRS) have gained interest as a versatile paradigm for complex task execution across various domains such as healthcare, logistics, and maintenance. Often, they are called to operate in variable and dynamic environments, which makes uncertainties arise and affect those systems. Uncertainties require the system to be able to adapt its behavior at runtime, in response to the changing and unpredictable conditions in its operating environment. Moreover, often the behavior of the robots cannot be completely anticipated at design time. Consequently, static mission planning is not always suitable: mission specifications need to take into account the uncertainties and, hence, be dynamic and re-configurable at runtime, when the required knowledge is available.This work focuses on the realization of adaptable multi-robot systems, which are capable of dealing with uncertainties by adapting their mission at runtime. We introduce the concept of "adaptable task" that is used in the global mission specification of the MRS to identify the mission tasks affected by uncertainties. Adaptation alternatives are modeled as sub-missions and associated with the adaptable task. At runtime, ad hoc written "trigger functions" executed by robots sense and evaluate the environment and select the most suitable adaptation alternative to be executed.We have experimented with the approach by simulating a use case to assess its validity. The system was able to adapt its behavior in response to the environmental conditions, thus allowing the fulfillment of the mission goals. We also discuss the applicability of the use case on a set of known single- and multi-robot systems.},
booktitle = {Proceedings of the 19th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {25–36},
numpages = {12},
keywords = {robotic software engineering, mission specification, coordinator synthesis},
location = {Lisbon, AA, Portugal},
series = {SEAMS '24}
}

@inproceedings{10.1145/3644032.3644444,
author = {Sterk, Alexander and Wessel, Mairieli and Hooten, Eli and Zaidman, Andy},
title = {Running a Red Light: An Investigation into Why Software Engineers (Occasionally) Ignore Coverage Checks},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644444},
doi = {10.1145/3644032.3644444},
abstract = {Many modern code coverage tools track and report code coverage data generated from running tests during continuous integration. They report code coverage data through a variety of channels, including email, Slack, Mattermost, or through the web interface of social coding platforms such as GitHub. In fact, this ensemble of tools can be configured in such a way that the software engineer gets a failing status check when code coverage drops below a certain threshold. In this study, we broadly investigate the opinions and experience with code coverage tools through a survey among 279 software engineers whose projects use the Codecov coverage tool and bot. In particular, we are investigating why software engineers would ignore a failing status check caused by drop in code coverage. We observe that &gt;80% of software engineers --- at least sometimes --- ignore these failing status checks, and we get insights into the main reasons why software engineers ignore these checks.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {12–22},
numpages = {11},
keywords = {software testing, code coverage, coverage checks},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3644033.3644381,
author = {Weigl, Alexander and Bachmeier, Joshua and Beckert, Bernhard and Ulbrich, Mattias},
title = {Contract Automata: A Specification Language for Mode-Based Systems},
year = {2024},
isbn = {9798400705892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644033.3644381},
doi = {10.1145/3644033.3644381},
abstract = {The comprehensive, understandable and effective formal specification of complex systems is often difficult, especially for reactive and interactive systems like web services or embedded system components. In this paper, we propose contract automata, a new specification formalism for describing the expected behaviour of stateful systems. Contract automata combine two established concepts for formal system specification: contract-based specification and nondeterministic finite state automata. Contract automata restrict the effects that the operations of the specified system may have using input-output-contracts. The automaton structure of a contract automaton describes when contracts are applicable. Contract automata support the refinement and composition of reactive systems, enabling modular verification of systems assembled of multiple subsystems. In this paper, we formally define the semantics of contract automata based on a two-party game between the system under test and its environment. We define the proof obligations and present techniques to prove a refinement relationship between contract automata, the validity of system compositions, and the compliance of source code against a contract automaton. We provide a tool for the generation of the proof obligation that can be discharged with model-checkers or static program analyses. We exemplify the use of contract automata by presenting the specification and verification of an emergency brake assistant.},
booktitle = {Proceedings of the 2024 IEEE/ACM 12th International Conference on Formal Methods in Software Engineering (FormaliSE)},
pages = {1–11},
numpages = {11},
location = {Lisbon, Portugal},
series = {FormaliSE '24}
}

