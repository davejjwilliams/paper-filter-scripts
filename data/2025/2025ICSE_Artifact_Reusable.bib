@inproceedings{10.1109/ICSE55347.2025.00232,
author = {Huang, Ruanqianqian (Lisa) and Ravi, Savitha and He, Michael and Tian, Boyu and Lerner, Sorin and Coblenz, Michael},
title = {How Scientists Use Jupyter Notebooks: Goals, Quality Attributes, and Opportunities},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00232},
doi = {10.1109/ICSE55347.2025.00232},
abstract = {Computational notebooks are intended to prioritize the needs of scientists, but little is known about how scientists interact with notebooks, what requirements drive scientists' software development processes, or what tactics scientists use to meet their requirements. We conducted an observational study of 20 scientists using Jupyter notebooks for their day-to-day tasks, finding that scientists prioritize different quality attributes depending on their goals. A qualitative analysis of their usage shows (1) a collection of goals scientists pursue with Jupyter notebooks, (2) a set of quality attributes that scientists value when they write software, and (3) tactics that scientists leverage to promote quality. In addition, we identify ways scientists incorporated AI tools into their notebook work. From our observations, we derive design recommendations for improving computational notebooks and future programming systems for scientists. Key opportunities pertain to helping scientists create and manage state, dependencies, and abstractions in their software, enabling more effective reuse of clearly-defined components.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1243–1255},
numpages = {13},
keywords = {scientific computing, computational notebooks, end-user software engineering},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00203,
author = {Yadavally, Aashish and Rong, Xiaokai and Nguyen, Phat and Nguyen, Tien N.},
title = {Large Language Models for Safe Minimization},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00203},
doi = {10.1109/ICSE55347.2025.00203},
abstract = {Several tasks in program analysis, verification, and testing are modeled as constraint solving problems, utilizing SMT solvers as the reasoning engine. In this work, we aim to investigate the reasoning capabilities of large language models (LLMs) toward reducing the size of an infeasible string constraint system by exploiting inter-constraint interactions such that the remaining ones are still unsatisfiable. We term this safe minimization.Motivated by preliminary observations of hallucination and error propagation in LLMs, we design SafeMin, a framework leveraging an LLM and SMT solver in tandem to ensure a safe and correct minimization. We test the applicability of our approach on string benchmarks from LeetCode in the computation of minimal unsatisfiable subsets (MUSes). We observed that SafeMin helps safely minimize 94.3% of these constraints, with an average minimization ratio of 98% relative to the MUSes. In addition, we assess SAFEMIN's capabilities in partially enumerating non-unique MUSes, which is baked into our approach via a "sample-and-enumerate" decoding strategy. Overall, we captured 42.1% more non-unique MUSes than without such LLM-based macro-reasoning. Finally, we demonstrate SafeMin's usefulness in detecting infeasible paths in programs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1114–1126},
numpages = {13},
keywords = {large language models, constraint solving, safe minimization, inter-constraint reasoning},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00201,
author = {Ma, Youpeng and Chen, Tao and Li, Ke},
title = {Faster Configuration Performance Bug Testing with Neural Dual-Level Prioritization},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00201},
doi = {10.1109/ICSE55347.2025.00201},
abstract = {As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle.In this paper, we seek to achieve significantly faster CP-Bug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88\texttimes{} testing efficiency speedup over the state-of-the-art tools.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {988–1000},
numpages = {13},
keywords = {performance bug testing, software debugging, testing prioritization, configuration testing, SBSE},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00188,
author = {Choi, Youngjae and Woo, Seunghoon},
title = {Tiver: Identifying Adaptive Versions of C/C++ Third-Party Open-Source Components Using a Code Clustering Technique},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00188},
doi = {10.1109/ICSE55347.2025.00188},
abstract = {Reusing open-source software (OSS) provides significant benefits but also poses risks from propagated vulnerabilities. While tracking OSS component versions helps mitigate threats, existing approaches typically map a single version to the reused codebase. This coarse-grained approach overlooks the coexistence of multiple versions, leading to ineffective OSS management. Moreover, identifying component versions is further complicated by noise codes, such as shared algorithmic code across different OSS, and duplicate components caused by redundant OSS reuse.In this paper, we introduce the concept of the adaptive version, a one-stop solution to represent the version diversity of reused OSS. To identify adaptive versions, we present Tiver, which employs two key techniques: (1) fine-grained function-level versioning and (2) OSS code clustering to identify duplicate components and remove noise. This enables precise identification of OSS reuse locations and adaptive versions, effectively mitigating risks associated with OSS reuse. Evaluation of 2,025 popular C/C++ software revealed that 67% of OSS components contained multiple versions, averaging over three versions per component. Nonetheless, Tiver effectively identified adaptive versions with 88.46% precision and 91.63% recall in duplicate component distinction, and 86% precision and 86.84% recall in eliminating noise, while existing approaches barely achieved 42% recall in distinguishing duplicates and did not address noise. Further experiments showed that Tiver could enhance vulnerability management and be applied to Software Bills of Materials (SBOM) to improve supply chain security.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2458–2469},
numpages = {12},
keywords = {open-source software, third-party library management, version identification, supply chain security},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00125,
author = {Miao, Miao and Mordahl, Austin and Soles, Dakota and Beideck, Alice and Wei, Shiyi},
title = {An Extensive Empirical Study of Nondeterministic Behavior in Static Analysis Tools},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00125},
doi = {10.1109/ICSE55347.2025.00125},
abstract = {Recent research has studied the importance and identified causes of nondeterminism in software. Static analysis tools exhibit many risk factors for nondeterministic behavior, but no work has analyzed the occurrence of such behavior in these tools. To bridge this gap, we perform an extensive empirical study aiming to understand past and ongoing nondeterminism in 12 popular, open-source static analysis tools that target 5 types of projects. We first conduct a qualitative study to understand the extent to which nondeterministic behavior has been found and addressed within the tools under study, and find results in 7 tool repositories. After classifying the issues and commits by root cause, we find that the majority of nondeterminisms are caused by concurrency issues, incorrect analysis logic, or assumed orderings of unordered data structures, which have shared patterns. We also perform a quantitative analysis, where we use two strategies and diverse input programs and configurations to detect yet-unknown nondeterministic behaviors. We discover such behavior in 8 out of the 12 tools, including 3 which had no results from the qualitative analysis. We find that nondeterminism often appears in multiple configurations on a variety of input programs. We communicated all identified nondeterminism to the developers, and received confirmation of five tools. Finally, we detail a case study of fixing FlowDroid's nondeterministic behavior.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1064–1076},
numpages = {13},
keywords = {nondeterminism, staic analysis, software testing},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00113,
author = {Luo, Chuan and Lyu, Shuangyu and Wu, Wei and Zhang, Hongyu and Chu, Dianhui and Hu, Chunming},
title = {Towards High-Strength Combinatorial Interaction Testing for Highly Configurable Software Systems},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00113},
doi = {10.1109/ICSE55347.2025.00113},
abstract = {Highly configurable software systems are crucial in practice to satisfy the rising demand for software customization, and combinatorial interaction testing (CIT) is an important methodology for testing such systems. Constrained covering array generation (CCAG), as the core problem in CIT, is to construct a t-wise covering array (CA) of minimum size, where t represents the testing strength. Extensive studies have demonstrated that high-strength CIT (e.g., 4-wise and 5-wise CIT) has stronger fault detection capability than low-strength CIT (i.e., 2-wise and 3-wise CIT), and there exist certain critical faults that can be disclosed through high-strength CIT. Although existing CCAG algorithm has exhibited effectiveness in solving the low-strength CCAG problem, they suffer the severe high-strength challenge when solving 4-wise and 5-wise CCAG, which urgently calls for effective solutions to solving 4-wise and 5-wise CCAG problems. To alleviate the high-strength challenge, we propose a novel and effective local search algorithm dubbed HSCA. Particularly, HSCA incorporates three new and powerful techniques, i.e., multi-round CA generation mechanism, dynamic priority assigning technique, and variable grouping strategy, to improve its performance. Extensive experiments on 35 real-world and synthetic instances demonstrate that HSCA can generate significantly smaller 4-wise and 5-wise CAs than existing state-of-the-art CCAG algorithms. More encouragingly, among all 35 instances, HSCA successfully builds 4-wise and 5-wise CAs for 35 and 29 instances, respectively, including 11 and 15 instances where existing CCAG algorithms fail. Our results indicate that HSCA can effectively mitigate the high-strength challenge.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1579–1591},
numpages = {13},
keywords = {combinatorial interaction testing, local search},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00094,
author = {Ye, Yulong and Chen, Tao and Li, Miqing},
title = {Distilled Lifelong Self-Adaptation for Configurable Systems},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00094},
doi = {10.1109/ICSE55347.2025.00094},
abstract = {Modern configurable systems provide tremendous opportunities for engineering future intelligent software systems. A key difficulty thereof is how to effectively self-adapt the configuration of a running system such that its performance (e.g., runtime and throughput) can be optimized under time-varying workloads. This unfortunately remains unaddressed in existing approaches as they either overlook the available past knowledge or rely on static exploitation of past knowledge without reasoning the usefulness of information when planning for self-adaptation. In this paper, we tackle this challenging problem by proposing DLiSA, a framework that self-adapts configurable systems. DLiSA comes with two properties: firstly, it supports lifelong planning, and thereby the planning process runs continuously throughout the lifetime of the system, allowing dynamic exploitation of the accumulated knowledge for rapid adaptation. Secondly, the planning for a newly emerged workload is boosted via distilled knowledge seeding, in which the knowledge is dynamically purified such that only useful past configurations are seeded when necessary, mitigating misleading information.Extensive experiments suggest that the proposed DLiSA significantly outperforms state-of-the-art approaches, demonstrating a performance improvement of up to 229% and a resource acceleration of up to 2.22\texttimes{} on generating promising adaptation configurations. All data and sources can be found at our repository: https://github.com/ideas-labo/dlisa.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1333–1345},
numpages = {13},
keywords = {self-adaptive systems, search-based software engineering, dynamic optimization, configuration tuning},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00092,
author = {Richter, Cedric and Chalupa, Marek and Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Cooperative Software Verification via Dynamic Program Splitting},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00092},
doi = {10.1109/ICSE55347.2025.00092},
abstract = {Cooperative software verification divides the task of software verification among several verification tools in order to increase efficiency and effectiveness. The basic approach is to let verifiers work on different parts of a program and at the end join verification results. While this idea is intuitively appealing, cooperative verification is usually hindered by the fact that program decomposition (1) is often static, disregarding strengths and weaknesses of employed verifiers, and (2) often represents the decomposed program parts in a specific proprietary format, thereby making the use of off-the-shelf verifiers in cooperative verification difficult.In this paper, we propose a novel cooperative verification scheme that we call dynamic program splitting (DPS). Splitting decomposes programs into (smaller) programs, and thus directly enables the use of off-the-shelf tools. In DPS, splitting is dynamically applied on demand: Verification starts by giving a verification task (a program plus a correctness specification) to a verifier V1. Whenever V1 finds the current task to be hard to verify, it splits the task (i.e., the program) and restarts verification on subtasks. DPS continues until (1) a violation is found, (2) all subtasks are completed or (3) some user-defined stopping criterion is met. In the latter case, the remaining uncompleted subtasks are merged into a single one and are given to a next verifier V2, repeating the same procedure on the still unverified program parts. This way, the decomposition is steered by what is hard to verify for particular verifiers, leveraging their complementary strengths. We have implemented dynamic program splitting and evaluated it on benchmarks of the annual software verification competition SV-COMP. The evaluation shows that cooperative verification with DPS is able to solve verification tasks that none of the constituent verifiers can solve, without any significant overhead.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2087–2099},
numpages = {13},
keywords = {software verification, cooperation, program splitting, off-the-shelf tools},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00227,
author = {Li, Haofeng and Shi, Chenghang and Lu, Jie and Li, Lian and Zhao, Zixuan},
title = {Module-Aware Context Sensitive Pointer Analysis},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00227},
doi = {10.1109/ICSE55347.2025.00227},
abstract = {The Java Platform Module System (JPMS) has found widespread applications since introduced in Java 9. However, existing pointer analyses fail to leverage the semantics of JPMS. This paper presents a novel module-aware approach to improving the performance of pointer analysis. We model the semantics of keywords provides and uses in JPMS to recover missing points-to relations. We design a module-aware context-sensitive analysis, which can propagate and apply critical contexts (by exploiting modularity) to balance precision and efficiency better. We have implemented our module-aware pointer analysis named MPA in Tai-e and conducted extensive experiments to compare it with standard object-sensitivity. The evaluation results demonstrate that MPA finds more reachable methods and enhances existing context-sensitive approaches, striking a good balance between efficiency and precision. MPA can increase the number of reachable methods up to 90.9\texttimes{} (lombok) under the same analysis. Performance-wise, MPA is nearly as fast as context-insensitivity for most benchmarks, while its precision is superior to that of 1-object-sensitivity on average.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1819–1831},
numpages = {13},
keywords = {pointer analysis, context sensitivity, JPMS},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00222,
author = {Hundal, Rajdeep Singh and Xiao, Yan and Cao, Xiaochun and Dong, Jin Song and Rigger, Manuel},
title = {On the Mistaken Assumption of Interchangeable Deep Reinforcement Learning Implementations},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00222},
doi = {10.1109/ICSE55347.2025.00222},
abstract = {Deep Reinforcement Learning (DRL) is a paradigm of artificial intelligence where an agent uses a neural network to learn which actions to take in a given environment. DRL has recently gained traction from being able to solve complex environments like driving simulators, 3D robotic control, and multiplayer-online-battle-arena video games. Numerous implementations of the state-of-the-art algorithms responsible for training these agents, like the Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) algorithms, currently exist. However, studies make the mistake of assuming implementations of the same algorithm to be consistent and thus, interchangeable. In this paper, through a differential testing lens, we present the results of studying the extent of implementation inconsistencies, their effect on the implementations' performance, as well as their impact on the conclusions of prior studies under the assumption of interchangeable implementations. The outcomes of our differential tests showed significant discrepancies between the tested algorithm implementations, indicating that they are not interchangeable. In particular, out of the five PPO implementations tested on 56 games, three implementations achieved superhuman performance for 50% of their total trials while the other two implementations only achieved superhuman performance for less than 15% of their total trials. Furthermore, the performance among the high-performing PPO implementations was found to differ significantly in nine games. As part of a meticulous manual analysis of the implementations' source code, we analyzed implementation discrepancies and determined that code-level inconsistencies primarily caused these discrepancies. Lastly, we replicated a study and showed that this assumption of implementation interchangeability was sufficient to flip experiment outcomes. Therefore, this calls for a shift in how implementations are being used. In addition, we recommend for (1) replicability studies for studies mistakenly assuming implementation inter-changeability, (2) DRL researchers and practitioners to adopt the differential testing methodology proposed in this paper to combat implementation inconsistencies, and (3) the use of large environment suites.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2225–2237},
numpages = {13},
keywords = {reinforcement learning, differential testing},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00158,
author = {Fratantonio, Yanick and Invernizzi, Luca and Farah, Loua and Thomas, Kurt and Zhang, Marina and Albertini, Ange and Galilee, Francois and Metitieri, Giancarlo and Cretin, Julien and Petit-Bianco, Alex and Tao, David and Bursztein, Elie},
title = {Magika: AI-Powered Content-Type Detection},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00158},
doi = {10.1109/ICSE55347.2025.00158},
abstract = {The task of content-type detection—which entails identifying the data encoded in an arbitrary byte sequence—is critical for operating systems, development, reverse engineering environments, and a variety of security applications. In this paper, we introduce Magika, a novel AI-powered content-type detection tool. Under the hood, Magika employs a deep learning model that can execute on a single CPU with just 1MB of memory to store the model's weights. We show that Magika achieves an average F1 score of 99% across over a hundred content types and a test set of more than 1M files, outperforming all existing content-type detection tools today. To foster adoption and improvements, we open source Magika under an Apache 2 license on GitHub and we make our model and training pipeline publicly available. Our tool has already seen adoption by Gmail and Google Drive for attachment scanning, by VirusTotal to aid with malware analysis, and by prominent open-source projects such as Apache Tika. While this paper focuses on the initial version, Magika continues to evolve with support for over 200 content types now available. The latest developments can be found at https://github.com/google/magika.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2638–2649},
numpages = {12},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00127,
author = {Wu, Xiafa and Demsky, Brian},
title = {GenC2Rust: Towards Generating Generic Rust Code from C},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00127},
doi = {10.1109/ICSE55347.2025.00127},
abstract = {Rust provides an exciting combination of strong safety guarantees and high performance. Many new systems are being implemented in Rust. Nevertheless, there is a large body of existing C code that could greatly benefit from Rust's safety guarantees. Unfortunately, the manual effort required to rewrite C code into Rust is often prohibitively expensive.Researchers have explored tools to assist developers in translating legacy C code into Rust code. However, the mismatch between C abstractions and idiomatic Rust abstractions makes it challenging to automatically utilize Rust's language features, resulting in non-idiomatic Rust code that requires extensive manual effort to further refactor. For example, existing tools often fail to map polymorphic uses of void pointers in C to Rust's generic pointers. In this paper, we present a translation tool, GenC2Rust, that translates non-generic C code into generic Rust code. GenC2Rust statically analyzes the use of void pointers in the C program to compute the typing constraints and then retypes the parametric polymorphic void pointers into generic pointers. We conducted an evaluation of GenC2Rust across 42 C programs that vary in size and span multiple domains to demonstrate its scalability as well as correctness. We discovered GenC2Rust has translated 4,572 void pointers to use generics. We also discuss the limiting factors encountered in the translation process.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {90–102},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00118,
author = {Roque, Enrique Barba and Cruz, Luis and Durieux, Thomas},
title = {Unveiling the Energy Vampires: A Methodology for Debugging Software Energy Consumption},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00118},
doi = {10.1109/ICSE55347.2025.00118},
abstract = {Energy consumption in software systems is becoming increasingly important, especially in large-scale deployments. However, debugging energy-related issues remains challenging due to the lack of specialized tools. This paper presents an energy debugging methodology for identifying and isolating energy consumption hotspots in software systems. We demonstrate the methodology's effectiveness through a case study of Redis, a popular in-memory database. Our analysis reveals significant energy consumption differences between Alpine and Ubuntu distributions, with Alpine consuming up to 20.2% more power in certain operations. We trace this difference to the implementation of the memcpy function in different C standard libraries (musl vs. glibc). By isolating and benchmarking memcpy, we confirm it as the primary cause of the energy discrepancy. Our findings highlight the importance of considering energy efficiency in software dependencies and demonstrate the capability to assist developers in identifying and addressing energy-related issues. This work contributes to the growing field of sustainable software engineering by providing a systematic approach to energy debugging and using it to unveil unexpected energy behaviors in Alpine.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2406–2418},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00059,
author = {Erhabor, Daniel and Udayashankar, Sreeharsha and Nagappan, Meiyappan and Al-Kiswany, Samer},
title = {Measuring the Runtime Performance of C++ Code Written by Humans Using GitHub Copilot},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00059},
doi = {10.1109/ICSE55347.2025.00059},
abstract = {GitHub Copilot is an artificially intelligent programming assistant used by many developers. While a few studies have evaluated the security risks of using Copilot, there has not been any study to show if it aids developers in producing code with better runtime performance. We evaluate the runtime performance of C++ code produced when developers use GitHub Copilot versus when they do not. To this end, we conducted a user study with 32 participants where each participant solved two C++ programming problems, one with Copilot and the other without it and measured the runtime performance of the participants' solutions on our test data. Our results suggest that using Copilot may produce C++ code with (statistically significant) slower runtime performance.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2062–2074},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00187,
author = {Kong, Ziqiao and Li, Shaohua and Huang, Heqing and Su, Zhendong},
title = {Sand: Decoupling Sanitization from Fuzzing for Low Overhead},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00187},
doi = {10.1109/ICSE55347.2025.00187},
abstract = {Sanitizers provide robust test oracles for various vulnerabilities. Fuzzing on sanitizer-enabled programs has been the best practice to find software bugs. Since sanitizers require heavy program instrumentation to insert run-time checks, sanitizer-enabled programs have much higher overhead compared to normally built programs.In this paper, we present Sand, a new fuzzing framework that decouples sanitization from the fuzzing loop. Sand performs fuzzing on a normally built program and only invokes the sanitizer-enabled program when input is shown to be interesting. Since most of the generated inputs are not interesting, i.e., not bug-triggering, Sand allows most of the fuzzing time to be spent on the normally built program. We further introduce execution pattern to practically and effectively identify interesting inputs.We implement Sand on top of AFL++ and evaluate it on 20 real-world programs. Our extensive evaluation highlights its effectiveness: in 24 hours, compared to all the baseline fuzzers, Sand significantly discovers more bugs while not missing any.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {255–267},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00185,
author = {Sens, Yorick and Knopp, Henriette and Peldszus, Sven and Berger, Thorsten},
title = {A Large-Scale Study of Model Integration in ML-Enabled Software Systems},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00185},
doi = {10.1109/ICSE55347.2025.00185},
abstract = {The rise of machine learning (ML) and its integration into software systems has drastically changed development practices. While software engineering traditionally focused on manually created code artifacts with dedicated processes and architectures, ML-enabled systems require additional data-science methods and tools to create ML artifacts—especially ML models and training data. However, integrating models into systems, and managing the many different artifacts involved, is far from trivial. ML-enabled systems can easily have multiple ML models that interact with each other and with traditional code in intricate ways. Unfortunately, while challenges and practices of building ML-enabled systems have been studied, little is known about the characteristics of real-world ML-enabled systems beyond isolated examples. Improving engineering processes and architectures for ML-enabled systems requires improving the empirical understanding of these systems.We present a large-scale study of 2,928 open-source ML-enabled software systems. We classified and analyzed them to determine system characteristics, model and code reuse practices, and architectural aspects of integrating ML models. Our findings show that these systems still mainly consist of traditional source code, and that ML model reuse through code duplication or pre-trained models is common. We also identified different ML integration patterns and related implementation practices. We hope that our results help improve practices for integrating ML models, bringing data science and software engineering closer together.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1165–1177},
numpages = {13},
keywords = {machine learning, AI engineering, SE4AI},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00183,
author = {Kokkonis, Dimitri and Marcozzi, Micha\"{e}l and Decoux, Emilien and Zacchiroli, Stefano},
title = {Rosa: Finding Backdoors with Fuzzing},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00183},
doi = {10.1109/ICSE55347.2025.00183},
abstract = {A code-level backdoor is a hidden access, programmed and concealed within the code of a program. For instance, hard-coded credentials planted in the code of a file server application would enable maliciously logging into all deployed instances of this application. Confirmed software supply-chain attacks have led to the injection of backdoors into popular open-source projects, and backdoors have been discovered in various router firmware. Manual code auditing for backdoors is challenging and existing semi-automated approaches can handle only a limited scope of programs and backdoors, while requiring manual reverse-engineering of the audited (binary) program. Graybox fuzzing (automated semi-randomized testing) has grown in popularity due to its success in discovering vulnerabilities and hence stands as a strong candidate for improved backdoor detection. However, current fuzzing knowledge does not offer any means to detect the triggering of a backdoor at runtime.In this work we introduce Rosa, a novel approach (and tool) which combines a state-of-the-art fuzzer (AFL++) with a new metamorphic test oracle, capable of detecting runtime backdoor triggers. To facilitate the evaluation of Rosa, we have created Rosarum, the first openly available benchmark for assessing the detection of various backdoors in diverse programs. Experimental evaluation shows that Rosa has a level of robustness, speed and automation similar to classical fuzzing. It finds all 17 authentic or synthetic backdooors from Rosarum in 1h30 on average. Compared to existing detection tools, it can handle a diversity of backdoors and programs and it does not rely on manual reverse-engineering of the fuzzed binary code.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2816–2828},
numpages = {13},
keywords = {fuzzing, dynamic analysis, metamorphic testing, backdoors, vulnerability detection},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00159,
author = {She, Yining and Biswas, Sumon and K\"{a}stner, Christian and Kang, Eunsuk},
title = {FairSense: Long-Term Fairness Analysis of ML-Enabled Systems},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00159},
doi = {10.1109/ICSE55347.2025.00159},
abstract = {Algorithmic fairness of machine learning (ML) models has raised significant concern in the recent years. Many testing, verification, and bias mitigation techniques have been proposed to identify and reduce fairness issues in ML models. The existing methods are model-centric and designed to detect fairness issues under static settings. However, many ML-enabled systems operate in a dynamic environment where the predictive decisions made by the system impact the environment, which in turn affects future decision-making. Such a self-reinforcing feedback loop can cause fairness violations in the long term, even if the immediate outcomes are fair. In this paper, we propose a simulation-based framework called FairSense to detect and analyze long-term unfairness in ML-enabled systems. Given a fairness requirement, FairSense performs Monte-Carlo simulation to enumerate evolution traces for each system configuration. Then, FairSense performs sensitivity analysis on the space of possible configurations to understand the impact of design options and environmental factors on the long-term fairness of the system. We demonstrate FairSense's potential utility through three real-world case studies: Loan lending, opioids risk scoring, and predictive policing.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {782–794},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00124,
author = {Gomes, L\'{u}\i{}s F. and Hellendoorn, Vincent J. and Aldrich, Jonathan and Abreu, Rui},
title = {An Exploratory Study of ML Sketches and Visual Code Assistants},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00124},
doi = {10.1109/ICSE55347.2025.00124},
abstract = {This paper explores the integration of Visual Code Assistants in Integrated Development Environments (IDEs). In Software Engineering, whiteboard sketching is often the initial step before coding, serving as a crucial collaboration tool for developers. Previous studies have investigated patterns in SE sketches and how they are used in practice, yet methods for directly using these sketches for code generation remain limited. The emergence of visually-equipped large language models presents an opportunity to bridge this gap, which is the focus of our research. In this paper, we built a first prototype of a Visual Code Assistant to get user feedback regarding in-IDE sketch-to-code tools. We conduct an experiment with 19 data scientists, most of whom regularly sketch as part of their job. We investigate developers' mental models by analyzing patterns commonly observed in their sketches when developing an ML workflow. Analysis indicates that diagrams were the preferred organizational component (52.6%), often accompanied by lists (42.1%) and numbered points (36.8%). Our tool converts their sketches into a Python notebook by querying an LLM. We use an LLM-as-judge setup to score the quality of the generated code, finding that even brief sketching can effectively generate useful code outlines. We also find a positive correlation between sketch time and the quality of the generated code. We conclude the study by conducting extensive interviews to assess the tool's usefulness, explore potential use cases, and understand developers' needs. As noted by participants, promising applications for these assistants include education, prototyping, and collaborative settings. Our findings signal promise for the next generation of Code Assistants to integrate visual information, both to improve code generation and to better leverage developers' existing sketching practices.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1653–1664},
numpages = {12},
keywords = {AI4SE, code generation, visual programming, sketching, machine learning, tool development, human-AI interaction},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00036,
author = {Thomas, Deepak-George and Biagiola, Matteo and Humbatova, Nargiz and Wardat, Mohammad and Jahangirova, Gunel and Rajan, Hridesh and Tonella, Paolo},
title = {μPRL: A Mutation Testing Pipeline for Deep Reinforcement Learning Based on Real Faults},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00036},
doi = {10.1109/ICSE55347.2025.00036},
abstract = {Reinforcement Learning (RL) is increasingly adopted to train agents that can deal with complex sequential tasks, such as driving an autonomous vehicle or controlling a humanoid robot. Correspondingly, novel approaches are needed to ensure that RL agents have been tested adequately before going to production. Among them, mutation testing is quite promising, especially under the assumption that the injected faults (mutations) mimic the real ones.In this paper, we first describe a taxonomy of real RL faults obtained by repository mining. Then, we present the mutation operators derived from such real faults and implemented in the tool μPRL. Finally, we discuss the experimental results, showing that μPRL is effective at discriminating strong from weak test generators, hence providing useful feedback to developers about the adequacy of the generated test scenarios.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2238–2250},
numpages = {13},
keywords = {reinforcement learning, mutation testing, real faults},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00239,
author = {Sherman, Gabriel and Nagy, Stefan},
title = {No Harness, No Problem: Oracle-Guided Harnessing for Auto-Generating C API Fuzzing Harnesses},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00239},
doi = {10.1109/ICSE55347.2025.00239},
abstract = {Library APIs are used by virtually every modern application and system, making them among today's most security-critical software. In recent years, library bug-finding efforts have overwhelmingly adopted the powerful testing strategy of coverage-guided fuzzing. At its core, API fuzzing operates on harnesses: wrapper programs that initialize an API before feeding random inputs to its functions. Successful fuzzing demands correct and thorough harnesses, making manual harnessing challenging without sufficient domain expertise. To overcome this, recent strategies propose "learning" libraries' intended usage to automatically generate their fuzzing harnesses. Yet, despite their high code coverage, resulting harnesses frequently miss key API semantics—bringing with them invalid, unrealistic, or otherwise-impossible data and call sequences—derailing fuzzing with false-positive crashes. Thus, without a precise, semantically-correct harnessing, many critical APIs will remain beyond fuzzing's reach—leaving their hidden vulnerabilities ripe for attackers.This paper introduces Oracle-guided Harnessing: a technique for fully-automatic, semantics-aware API fuzzing harness synthesis. At a high level, Oracle-guided Harnessing mimics the trial-and-error process of manual harness creation—yet automates it via fuzzing. Specifically, we leverage information from API headers to mutationally stitch-together candidate harnesses; and evaluate their validity via a set of Correctness Oracles: compilation, execution, and changes in coverage. By keeping—and further mutating—only correct candidates, our approach produces a diverse set of semantically-correct harnesses for complex, real-world libraries in as little as one hour.We integrate Oracle-guided Harnessing as a prototype, OGHarn; and evaluate it alongside today's leading fully-automatic harnessing approach, Hopper, and a plethora of developer-written harnesses from OSS-Fuzz. Across 20 real-world APIs, OGHarn outperforms developer-written harnesses by a median 14% code coverage, while uncovering 31 and 30 more vulnerabilities than both Hopper and developer-written harnesses, respectively—with zero false-positive crashes. Of the 41 new vulnerabilities found by OGHarn, all 41 are confirmed by developers—40 of which are since fixed—with many found in APIs that, until now, lacked harnesses whatsoever.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {165–177},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00161,
author = {Thompson, Kyle and Saavedra, Nuno and Carrott, Pedro and Fisher, Kevin and Sanchez-Stern, Alex and Brun, Yuriy and Ferreira, Jo\~{a}o F. and Lerner, Sorin and First, Emily},
title = {Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00161},
doi = {10.1109/ICSE55347.2025.00161},
abstract = {Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {347–359},
numpages = {13},
keywords = {formal verification, theorem proving, large language models, retrieval augmentation, software reliability},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00157,
author = {Bouzenia, Islem and Devanbu, Premkumar and Pradel, Michael},
title = {RepairAgent: An Autonomous, LLM-Based Agent for Program Repair},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00157},
doi = {10.1109/ICSE55347.2025.00157},
abstract = {Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270k tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2188–2200},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00143,
author = {Baatartogtokh, Yesugen and Cook, Kaitlyn and Grubb, Alicia M.},
title = {Exploring the Robustness of the Effect of EVO on Intention Valuation through Replication},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00143},
doi = {10.1109/ICSE55347.2025.00143},
abstract = {The development of high-quality software depends on precise and comprehensive requirements that meet the objectives of stakeholders. Goal modeling techniques have been developed to fill this gap by capturing and analyzing stakeholders' needs and allowing them to make trade-off decisions; yet, goal modeling analysis is often difficult for stakeholders to interpret. Recent work found that when subjects are given minimal training on goal modeling and access to a color visualization, called EVO, they are able to use EVO to make goal modeling decisions faster without compromising quality. In this paper, we evaluate the robustness of the empirical evidence for EVO and question the underlying color choices made by the initial designers of EVO. We conduct a pseudo-exact replication (n = 60) of the original EVO study, varying the experimental site and the study population. Even in our heterogeneous sample with less a priori familiarity with requirements and goal modeling, we find that individuals using EVO answered the goal-modeling questions significantly faster than those using the control, expanding the external validity of the original results. However, we find some evidence that the chosen color scheme is not intuitive and make recommendations for the goal modeling community.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {808–820},
numpages = {13},
keywords = {requirements, goal modeling, replication},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00037,
author = {Limpanukorn, Ben and Wang, Jiyuan and Kang, Hong Jin and Zhou, Zitong and Kim, Miryung},
title = {Fuzzing MLIR Compilers with Custom Mutation Synthesis},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00037},
doi = {10.1109/ICSE55347.2025.00037},
abstract = {Compiler technologies in deep learning and domain-specific hardware acceleration are increasingly adopting extensible compiler frameworks such as Multi-Level Intermediate Representation (MLIR) to facilitate more efficient development. With MLIR, compiler developers can easily define their own custom IRs in the form of MLIR dialects. However, the diversity and rapid evolution of such custom IRs make it impractical to manually write a custom test generator for each dialect.To address this problem, we design a new test generator called SynthFuzz that combines grammar-based fuzzing with custom mutation synthesis. The key essence of SynthFuzz is two fold: (1) It automatically infers parameterized context-dependent custom mutations from existing test cases. (2) It then concretizes the mutation's content depending on the target context and reduces the chance of inserting invalid edits by performing k-ancestor and prefix/postfix matching. It obviates the need to manually define custom mutation operators for each dialect.We compare SynthFuzz to three baselines: Grammarinator—a grammar-based fuzzer without custom mutations, MLIRSmith—a custom test generator for MLIR core dialects, and NeuRI—a custom test generator for ML models with parameterization of tensor shapes. We conduct this comprehensive comparison on four different MLIR projects. Each project defines a new set of MLIR dialects where manually writing a custom test generator would take weeks of effort. Our evaluation shows that SynthFuzz on average improves MLIR dialect pair coverage by 1.75X, which increases branch coverage by 1.22X. Further, we show that our context dependent custom mutation increases the proportion of valid tests by up to 1.11X, indicating that SynthFuzz correctly concretizes its parameterized mutations with respect to the target context. Parameterization of the mutations reduces the fraction of tests violating the base MLIR constraints by 0.57X, increasing the time spent fuzzing dialect-specific code.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {217–229},
numpages = {13},
keywords = {grammar-based fuzzing, program synthesis, program transformation, MLIR, compiler testing, code patterns},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00017,
author = {Lian, Xinyu and Chen, Yinfang and Cheng, Runxiang and Huang, Jie and Thakkar, Parth and Zhang, Minjia and Xu, Tianyin},
title = {Large Language Models as Configuration Validators},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00017},
doi = {10.1109/ICSE55347.2025.00017},
abstract = {Misconfigurations are major causes of software failures. Existing practices rely on developer-written rules or test cases to validate configuration values, which are expensive. Machine learning (ML) for configuration validation is considered a promising direction, but has been facing challenges such as the need of large-scale field data and system-specific models. Recent advances in Large Language Models (LLMs) show promise in addressing some of the long-lasting limitations of ML-based configuration validation. We present the first analysis on the feasibility and effectiveness of using LLMs for configuration validation. We empirically evaluate LLMs as configuration validators by developing a generic LLM-based configuration validation framework, named Ciri. Ciri employs effective prompt engineering with few-shot learning based on both valid configuration and misconfiguration data. Ciri checks outputs from LLMs when producing results, addressing hallucination and nondeterminism of LLMs. We evaluate Ciri's validation effectiveness on eight popular LLMs using configuration data of ten widely deployed open-source systems. Our analysis (1) confirms the potential of using LLMs for configuration validation, (2) explores design space of LLM-based validators like Ciri, and (3) reveals open challenges such as ineffectiveness in detecting certain types of misconfigurations and biases towards popular configuration parameters.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1704–1716},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00208,
author = {Chen, Menglong and Tan, Tian and Pan, Minxue and Li, Yue},
title = {PacDroid: A Pointer-Analysis-Centric Framework for Security Vulnerabilities in Android Apps},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00208},
doi = {10.1109/ICSE55347.2025.00208},
abstract = {General frameworks such as FlowDroid, IccTA, P/Taint, Amandroid, and DroidSafe have significantly advanced the development of static analysis tools for Android security by providing fundamental facilities for them. However, while these frameworks have been instrumental in fostering progress, they often operate with inherent inefficiencies, such as redundant computations, reliance on separate tools, and unnecessary complexity, which are rarely scrutinized by the analysis tools that depend on them. This paper introduces PacDroid, a new static analysis framework for detecting security vulnerabilities in Android apps. PacDroid employs a simple yet effective pointer-analysis-centric approach that naturally manages alias information, interprocedural value propagation, and all Android features it supports (including ICC, lifecycles, and miscs), in a unified manner. Our extensive evaluation reveals that PacDroid not only outperforms state-of-the-art frameworks in achieving a superior trade-off between soundness and precision (F-measure) but also surpasses them in both analysis speed and robustness; moreover, PacDroid successfully identifies 77 real security vulnerability flows across 23 real-world Android apps that were missed by all other frameworks. With its ease of extension and provision of essential facilities, PacDroid is expected to serve as a foundational framework for various future analysis applications for Android.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2803–2815},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00186,
author = {Fuch\ss{}, Dominik and Hey, Tobias and Keim, Jan and Liu, Haoyu and Ewald, Niklas and Thirolf, Tobias and Koziolek, Anne},
title = {LiSSA: Toward Generic Traceability Link Recovery through Retrieval-Augmented Generation},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00186},
doi = {10.1109/ICSE55347.2025.00186},
abstract = {There are a multitude of software artifacts which need to be handled during the development and maintenance of a software system. These artifacts interrelate in multiple, complex ways. Therefore, many software engineering tasks are enabled — and even empowered — by a clear understanding of artifact interrelationships and also by the continued advancement of techniques for automated artifact linking.However, current approaches in automatic Traceability Link Recovery (TLR) target mostly the links between specific sets of artifacts, such as those between requirements and code. Fortunately, recent advancements in Large Language Models (LLMs) can enable TLR approaches to achieve broad applicability. Still, it is a nontrivial problem how to provide the LLMs with the specific information needed to perform TLR.In this paper, we present LiSSA, a framework that harnesses LLM performance and enhances them through Retrieval-Augmented Generation (RAG). We empirically evaluate LiSSA on three different TLR tasks, requirements to code, documentation to code, and architecture documentation to architecture models, and we compare our approach to state-of-the-art approaches.Our results show that the RAG-based approach can significantly outperform the state-of-the-art on the code-related tasks. However, further research is required to improve the performance of RAG-based approaches to be applicable in practice.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1396–1408},
numpages = {13},
keywords = {traceability link recovery, large language models, retrieval-augmented generation},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00042,
author = {Hasanov, Sanan and Nagy, Stefan and Gazzillo, Paul},
title = {A Little Goes a Long Way: Tuning Configuration Selection for Continuous Kernel Fuzzing},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00042},
doi = {10.1109/ICSE55347.2025.00042},
abstract = {The Linux kernel is actively-developed and widely-used. It supports billions of devices of all classes, from high-performance computing to the Internet-of-Things, in part because of its sophisticated configuration system, which automatically tailors the source code according to thousands of user-provided configuration options. Fuzzing has been highly successful at finding kernel bugs, being among the top bug reporters. Since the kernel receives 100s of patches per day, fuzzers run continuously, stopping regularly to rebuild the kernel with the latest changes before restarting fuzzing. But kernel fuzzers currently use predefined configuration settings that, as we show, exclude the majority of new patches from the kernel binary, nullifying the benefits of continuous fuzzing. Unfortunately, state-of-the-art configuration testing techniques are generally ill-suited to the needs of continuous fuzzing, excluding necessary options or requiring too many configuration files to be tractable. We distill down the needs of continuous testing into six properties with the most impact, systematically analyze the space of configuration selection strategies, and provide actionable recommendations. Through our analysis, we discover that continuous fuzzers can improve configuration variety without sacrificing performance. We empirically evaluate our discovery by modifying the configuration selection strategy for syzkaller, the most popular Linux kernel fuzzer, which subsequently found more than twice as many new bugs (35 vs. 13) than with the original configuration file and 12x more (24 vs. 2) when considering only unique bugs—with one security vulnerability being assigned a CVE.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {795–807},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00241,
author = {Saha, Antu and Chaparro, Oscar},
title = {Decoding the Issue Resolution Process in Practice via Issue Report Analysis: A Case Study of Firefox},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00241},
doi = {10.1109/ICSE55347.2025.00241},
abstract = {Effectively managing and resolving software issues is critical for maintaining and evolving software systems. Development teams often rely on issue trackers and issue reports to track and manage the work needed during issue resolution, ranging from issue reproduction and analysis to solution design, implementation, verification, and deployment. Despite the issue resolution process being generally known in the software engineering community as a sequential list of activities, it is unknown how developers implement this process in practice and how they discuss it in issue reports. This paper aims to enhance our understanding of the issue resolution process implemented in practice by analyzing the issue reports of Mozilla Firefox. We qualitatively and quantitatively analyzed the discussions found in 356 Firefox issue reports, to identify the sequences of stages that developers go through to address various software problems. We analyzed the sequences to identify the overall resolution process at Firefox and derived a catalog of 47 patterns that represent instances of the process. We analyzed the process and patterns across multiple dimensions, including pattern complexity, issue report types, problem categories, and issue resolution times, resulting in various insights about Mozilla's issue resolution process. We discuss these findings and their implications for different stakeholders on how to better assess and improve the issue resolution process.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2316–2328},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00202,
author = {Kim, Sehoon and Kim, Yonghyeon and Park, Dahyeon and Jeon, Yuseok and Yi, Jooyong and Kim, Mijung},
title = {Lightweight Concolic Testing via Path-Condition Synthesis for Deep Learning Libraries},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00202},
doi = {10.1109/ICSE55347.2025.00202},
abstract = {Many techniques have been recently developed for testing deep learning (DL) libraries. Although these techniques have effectively improved API and code coverage and detected unknown bugs, they rely on blackbox fuzzing for input generation. Concolic testing (also known as dynamic symbolic execution) can be more effective in exploring diverse execution paths, but applying it to DL libraries is extremely challenging due to their inherent complexity. In this paper, we introduce the first concolic testing technique for DL libraries. Our technique offers a lightweight approach that significantly reduces the heavy overhead associated with traditional concolic testing. While symbolic execution maintains symbolic expressions for every variable with non-concrete values to build a path condition, our technique computes approximate path conditions by inferring branch conditions via inductive program synthesis. Despite potential imprecision from approximation, our method's light overhead allows for effective exploration of diverse execution paths within the complex implementations of DL libraries. We have implemented our tool, PathFinder, and evaluated it on PyTorch and TensorFlow. Our results show that PathFinder outperforms existing API-level DL library fuzzers by achieving 67% more branch coverage on average; up to 63% higher than TitanFuzz and 120% higher than FreeFuzz. PathFinder is also effective in bug detection, uncovering 61 crash bugs, 59 of which were confirmed by developers as previously unknown, with 32 already fixed.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2957–2969},
numpages = {13},
keywords = {fuzzing, concolic testing, deep learning libraries},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00194,
author = {Das, Satyaki and Fabiha, Syeda Tasnim and Shafiq, Saad and Medvidovi\'{c}, Nenad},
title = {Are We Learning the Right Features? A Framework for Evaluating DL-Based Software Vulnerability Detection Solutions},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00194},
doi = {10.1109/ICSE55347.2025.00194},
abstract = {Recent research has revealed that the reported results of an emerging body of deep learning-based techniques for detecting software vulnerabilities are not reproducible, either across different datasets or on unseen samples. This paper aims to provide the foundation for properly evaluating the research in this domain. We do so by analyzing prior work and existing vulnerability datasets for the syntactic and semantic features of code that contribute to vulnerability, as well as features that falsely correlate with vulnerability. We provide a novel, uniform representation to capture both sets of features, and use this representation to detect the presence of both vulnerability and spurious features in code. To this end, we design two types of code perturbations: feature preserving perturbations (FPP) ensure that the vulnerability feature remains in a given code sample, while feature eliminating perturbations (FEP) eliminate the feature from the code sample. These perturbations aim to measure the influence of spurious and vulnerability features on the predictions of a given vulnerability detection solution. To evaluate how the two classes of perturbations influence predictions, we conducted a large-scale empirical study on five state-of-the-art DL-based vulnerability detectors. Our study shows that, for vulnerability features, only ~2% of FPPs yield the undesirable effect of a prediction changing among the five detectors on average. However, on average, ~84% of FEPs yield the undesirable effect of retaining the vulnerability predictions. For spurious features, we observed that FPPs yielded a drop in recall up to 29% for graph-based detectors. We present the reasons underlying these results and suggest strategies for improving DNN-based vulnerability detectors. We provide our perturbation-based evaluation framework as a public resource to enable independent future evaluation of vulnerability detectors.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2893–2904},
numpages = {12},
keywords = {vulnerability detection, deep learning, software security, explainable AI},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00172,
author = {Galland, Octavio and B\"{o}hme, Marcel},
title = {Invivo Fuzzing by Amplifying Actual Executions},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00172},
doi = {10.1109/ICSE55347.2025.00172},
abstract = {A major bottleneck that remains when fuzzing software libraries is the need for fuzz drivers, i.e., the glue code between the fuzzer and the library. Despite years of fuzzing, critical security flaws are still found, e.g., by manual auditing, because the fuzz drivers do not cover the complex interactions between the library and the host programs using it.In this work we propose an alternative approach to library fuzzing, which leverages a valid execution context that is set up by a given program using the library (the host), and amplify its execution. More specifically, we execute the host until a designated function from a list of target functions has been reached, and then perform coverage-guided function-level fuzzing on it. Once the fuzzing quota is exhausted, we move on to fuzzing the next target from the list. In this way we not only reduce the amount of manual work needed by a developer to incorporate fuzzing into their workflow, but we also allow the fuzzer to explore parts of the library as they are used in real-world programs that may otherwise not have been tested due to the simplicity of most fuzz drivers.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1566–1578},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00167,
author = {McCormack, Ian and Sunshine, Joshua and Aldrich, Jonathan},
title = {A Study of Undefined Behavior across Foreign Function Boundaries in Rust Libraries},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00167},
doi = {10.1109/ICSE55347.2025.00167},
abstract = {Developers rely on the static safety guarantees of the Rust programming language to write secure and performant applications. However, Rust is frequently used to interoperate with other languages which allow design patterns that conflict with Rust's evolving aliasing models. Miri is currently the only dynamic analysis tool that can validate applications against these models, but it does not support finding bugs in foreign functions, indicating that there may be a critical correctness gap across the Rust ecosystem. We conducted a large-scale evaluation of Rust libraries that call foreign functions to determine whether Miri's dynamic analyses remain useful in this context. We used Miri and an LLVM interpreter to jointly execute applications that call foreign functions, where we found 46 instances of undefined or undesired behavior in 37 libraries. Three bugs were found in libraries that had more than 10,000 daily downloads on average during our observation period, and one was found in a library maintained by the Rust Project. Many of these bugs were violations of Rust's aliasing models, but the latest Tree Borrows model was significantly more permissive than the earlier Stacked Borrows model. The Rust community must invest in new, production-ready tooling for multi-language applications to ensure that developers can detect these errors.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2075–2086},
numpages = {12},
keywords = {rust, interoperation, undefined behavior, aliasing, bugs, foreign functions},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00160,
author = {Zhang, Changjian and Kapoor, Parv and Dardik, Ian and Cui, Leyi and Meira-G\'{o}es, R\^{o}mulo and Garlan, David and Kang, Eunsuk},
title = {Constrained LTL Specification Learning from Examples},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00160},
doi = {10.1109/ICSE55347.2025.00160},
abstract = {Temporal logic specifications play an important role in a wide range of software analysis tasks, such as model checking, automated synthesis, program comprehension, and runtime monitoring. Given a set of positive and negative examples, specified as traces, LTL learning is the problem of synthesizing a specification, in linear temporal logic (LTL), that evaluates to true over the positive traces and false over the negative ones. In this paper, we propose a new type of LTL learning problem called constrained LTL learning, where the user, in addition to positive and negative examples, is given an option to specify one or more constraints over the properties of the LTL formula to be learned. We demonstrate that the ability to specify these additional constraints significantly increases the range of applications for LTL learning, and also allows efficient generation of LTL formulas that satisfy certain desirable properties (such as minimality). We propose an approach for solving the constrained LTL learning problem through an encoding in first-order relational logic and reduction to an instance of the maximal satisfiability (MaxSAT) problem. An experimental evaluation demonstrates that ATLAS, an implementation of our proposed approach, is able to solve new types of learning problems while performing better than or competitively with the state-of-the-art tools in LTL learning.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {629–641},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00152,
author = {Liu, Qikang and He, Yang and Cai, Yanwen and Kwak, Byeongguk and Wang, Yuepeng},
title = {Synthesizing Document Database Queries Using Collection Abstractions},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00152},
doi = {10.1109/ICSE55347.2025.00152},
abstract = {Document databases are increasingly popular in various applications, but their queries are challenging to write due to the flexible and complex data model underlying document databases. This paper presents a synthesis technique that aims to generate document database queries from input-output examples automatically. A new domain-specific language is designed to express a representative set of document database queries in an algebraic style. Furthermore, the synthesis technique leverages a novel abstraction of collections for deduction to efficiently prune the search space and quickly generate the target query. An evaluation of 110 benchmarks from various sources shows that the proposed technique can synthesize 108 benchmarks successfully. On average, the synthesizer can generate document database queries from a small number of input-output examples within tens of seconds.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {476–488},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00126,
author = {Steenhoek, Benjamin and Sivaraman, Kalpathy and Gonzalez, Renata Saldivar and Mohylevskyy, Yevhen and Moghaddam, Roshanak Zilouchian and Le, Wei},
title = {Closing the Gap: A User Study on the Real-World Usefulness of AI-Powered Vulnerability Detection &amp; Repair in the IDE},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00126},
doi = {10.1109/ICSE55347.2025.00126},
abstract = {Security vulnerabilities impose significant costs on users and organizations. Detecting and addressing these vulnerabilities early is crucial to avoid exploits and reduce development costs. Recent studies have shown that deep learning models can effectively detect security vulnerabilities. Yet, little research explores how to adapt these models from benchmark tests to practical applications, and whether they can be useful in practice.This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at this link: https://doi.org/10.6084/m9.figshare.26367139.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2650–2662},
numpages = {13},
keywords = {deep learning, vulnerability detection, vulnerability repair, IDE, user study},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00102,
author = {Patel, Smit and Yadavally, Aashish and Dhulipala, Hridya and Nguyen, Tien N.},
title = {Planning a Large Language Model for Static Detection of Runtime Errors in Code Snippets},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00102},
doi = {10.1109/ICSE55347.2025.00102},
abstract = {Large Language Models (LLMs) have been excellent in generating and reasoning about source code and natural-language texts. They can recognize patterns, syntax, and semantics in code, making them effective in several software engineering tasks. However, they exhibit weaknesses in reasoning about the program execution. They primarily operate on static code representations, failing to capture the dynamic behavior and state changes that occur during program execution.In this paper, we advance the capabilities of LLMs in reasoning about dynamic program behaviors. We propose Orca, a novel approach that instructs an LLM to autonomously formulate a plan to navigate through a control flow graph (CFG) for predictive execution of (in)complete code snippets. It acts as a predictive interpreter to "execute" the code. In Orca, we guide the LLM to pause at the branching point, focusing on the state of the symbol tables for variables' values, thus minimizing error propagation in the LLM's computation. We instruct the LLM not to stop at each step in its execution plan, resulting the use of only one prompt for the entire predictive interpreter, thus much cost-saving. As a downstream task, we use Orca to statically identify any runtime errors for online code snippets. Early detection of runtime errors and defects in these snippets is crucial to prevent costly fixes later in the development cycle after they were adapted into a codebase. Our empirical evaluation showed that Orca is effective and improves over the state-of-the-art approaches in predicting the execution traces and in static detection of runtime errors.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {872–884},
numpages = {13},
keywords = {large language model (LLM) planning, execution prediction, runtime error static detection},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00075,
author = {Zhang, Chenxi and Liang, Yufei and Tan, Tian and Xu, Chang and Kan, Shuangxiang and Sui, Yulei and Li, Yue},
title = {Interactive Cross-Language Pointer Analysis For Resolving Native Code in Java Programs},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00075},
doi = {10.1109/ICSE55347.2025.00075},
abstract = {Java offers the Java Native Interface (JNI), which allows programs running in the Java Virtual Machine to invoke and be manipulated by native applications and libraries written in other languages, typically C. While JNI mechanism significantly enhances the Java platform's capabilities, it also presents challenges for static analysis of Java programs due to the complex behaviors introduced by native code. Therefore, effectively resolving the interactions between Java and native code is crucial for static analysis. In this paper, we introduce JNIFER, the first interactive cross-language pointer analysis for resolving native code in Java programs. JNIFER integrates both Java and C pointer analyses, equipped with advanced native call and JNI function analyses, enabling the simultaneous analysis of both Java and native code. During the analysis of cross-language interactions, the two analyzers interact with each other, constructing cross-language points-to relations and call graphs, thereby approximating the runtime behavior at the interaction sites. Our evaluation shows that JNIFER outperforms state-of-the-art approaches in terms of soundness while maintaining high precision and comparable efficiency, as evidenced by extensive experiments on OpenJDK and real-world Java applications.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1089–1100},
numpages = {12},
keywords = {java native interface, native code, pointer analysis, cross-language analysis},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00230,
author = {Panichella, Annibale},
title = {Metamorphic-Based Many-Objective Distillation of LLMs for Code-Related Tasks},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00230},
doi = {10.1109/ICSE55347.2025.00230},
abstract = {Knowledge distillation compresses large language models (LLMs) into more compact and efficient versions that achieve similar accuracy on code-related tasks. However, as we demonstrate in this study, compressed models are four times less robust than the original LLMs when evaluated with metamorphic code. They exhibit a 440% higher probability of misclassifying code clones due to minor changes in the code fragment under analysis, such as replacing parameter names with synonyms. To address this issue, we propose Morph, a novel method that combines metamorphic testing with many-objective optimization for a robust distillation of LLMs for code. Morph efficiently explores the models' configuration space and generates Pareto-optimal models that effectively balance accuracy, efficiency, and robustness to metamorphic code. Metamorphic testing measures robustness as the number of code fragments for which a model incorrectly makes different predictions between the original and their equivalent metamorphic variants (prediction flips). We evaluate Morph on two tasks—code clone and vulnerability detection—targeting CodeBERT and GraphCodeBERT for distillation. Our comparison includes Morph, the state-of-the-art distillation method Avatar, and the fine-tuned non-distilled LLMs. Compared to Avatar, Morph produces compressed models that are (i) 47% more robust, (ii) 25% more efficient (fewer floating-point operations), while maintaining (iii) equal or higher accuracy (up to +6%), and (iv) similar model size.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1001–1013},
numpages = {13},
keywords = {knowledge distillation, large language models, metamorphic testing, many-objective optimization, green-AI, sustainability, search-based software engineering, AI for SE},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00215,
author = {Souza, Beatriz and Pradel, Michael},
title = {Treefix: Enabling Execution with a Tree of Prefixes},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00215},
doi = {10.1109/ICSE55347.2025.00215},
abstract = {The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2676–2688},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00212,
author = {Liang, Hongyuan and Huang, Yue and Chen, Tao},
title = {The Same Only Different: On Information Modality for Configuration Performance Analysis},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00212},
doi = {10.1109/ICSE55347.2025.00212},
abstract = {Configuration in software systems helps to ensure efficient operation and meet diverse user needs. Yet, some, if not all, configuration options have profound implications for the system's performance. Configuration performance analysis, wherein the key is to understand (or infer) the configuration options' relations and their impacts on performance, is crucial. Two major modalities exist that serve as the source information in the analysis: either the manual or source code. However, it remains unclear what roles they play in configuration performance analysis. Much work that relies on manuals claims their benefits of information richness and naturalness; while work that trusts the source code more prefers the structural information provided therein and criticizes the timeliness of manuals.To fill such a gap, in this paper, we conduct an extensive empirical study over 10 systems, covering 1,694 options, 106,798 words in the manual, and 22,859,552 lines-of-code for investigating the usefulness of manual and code in two important tasks of configuration performance analysis, namely performance-sensitive options identification and the associated dependencies extraction. We reveal several new findings and insights, such as it is beneficial to fuse the manual and code modalities for both tasks; the current automated tools that rely on a single modality are far from being practically useful and generally remain incomparable to human analysis. All those pave the way for further advancing configuration performance analysis.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2522–2534},
numpages = {13},
keywords = {software configuration, performance analysis, manual, source code analysis, configuration dependency},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00206,
author = {Baresi, Luciano and Hu, Davide Yi Xian and Stocco, Andrea and Tonella, Paolo},
title = {Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00206},
doi = {10.1109/ICSE55347.2025.00206},
abstract = {Simulation-based testing is widely used to assess the reliability of Autonomous Driving Systems (ADS), but its effectiveness is limited by the operational design domain (ODD) conditions available in such simulators. To address this limitation, in this work, we explore the integration of generative artificial intelligence techniques with physics-based simulators to enhance ADS system-level testing. Our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models, namely instruction-editing, inpainting, and inpainting with refinement. Specifically, we assess these techniques' capabilities to produce augmented simulator-generated images of driving scenarios representing new ODDs. We employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images. We then performed system-level testing to evaluate the ability of the ADS to generalize to newly synthesized ODDs. Our findings show that diffusion models help to increase the coverage of ODD for system-level ADS testing. Our automated semantic validator achieved a percentage of false positives as low as 3%, retaining the correctness and quality of the images generated for testing. Our approach successfully identified new ADS system failures before real-world testing.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {398–410},
numpages = {13},
keywords = {autonomous driving systems, deep learning testing, diffusion models, generative AI},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00182,
author = {Gropengie\ss{}er, Uwe and Dietz, Elias and Brandherm, Florian and Doula, Achref and Abboud, Osama and Xiao, Xun and M\"{u}hlh\"{a}user, Max},
title = {MARQ: Engineering Mission-Critical AI-Based Software with Automated Result Quality Adaptation},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00182},
doi = {10.1109/ICSE55347.2025.00182},
abstract = {AI-based mission-critical software exposes a blessing and a curse: its inherent statistical nature allows for flexibility in result quality, yet the mission-critical importance demands adherence to stringent constraints such as execution deadlines. This creates a space for trade-offs between the Quality of Result (QoR)—a metric that quantifies the quality of a computational outcome—and other application attributes like execution time and energy, particularly in real-time scenarios. Fluctuating resource constraints, such as data transfer to a remote server over unstable network connections, are prevalent in mobile and edge computing environments—encompassing use cases like Vehicle-to-Everything, drone swarms, or social-VR scenarios. We introduce a novel approach that enables software engineers to easily specify alternative AI service chains—sequences of AI services encapsulated in microservices aiming to achieve a predefined goal—with varying QoR and resource requirements. Our methodology facilitates dynamic optimization at runtime, which is automatically driven by the MARQ framework. Our evaluations show that MARQ can be used effectively for the dynamic selection of AI service chains in real-time while maintaining the required application constraints of mission-critical AI software. Notably, our approach achieves a 100\texttimes{} acceleration in service chain selection and an average 10% improvement in QoR compared to existing methods.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1934–1946},
numpages = {13},
keywords = {mission-critical AI, quality of result, edge computing, approximate computing, software engineering},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00174,
author = {Le, Van-Hoang and Xiao, Yi and Zhang, Hongyu},
title = {Unleashing the True Potential of Semantic-Based Log Parsing with Pre-Trained Language Models},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00174},
doi = {10.1109/ICSE55347.2025.00174},
abstract = {Software-intensive systems often produce console logs for troubleshooting purposes. Log parsing, which aims at parsing a log message into a specific log template, typically serves as the first step toward automated log analytics. To better comprehend the semantic information of log messages, many semantic-based log parsers have been proposed. These log parsers fine-tune a small pre-trained language model (PLM) such as RoBERTa on a few labelled log samples. With the increasing popularity of large language models (LLMs), some recent studies also propose to leverage LLMs such as ChatGPT through in-context learning for automated log parsing and obtain better results than previous semantic-based log parsers with small PLMs. In this paper, we show that semantic-based log parsers with small PLMs can actually achieve better or comparable performance to state-of-the-art LLM-based log parsing models while being more efficient and cost-effective. We propose Unleash, a novel semantic-based log parsing approach, which incorporates three enhancement methods to boost the performance of PLMs for log parsing, including (1) an entropy-based ranking method to select the most informative log samples; (2) a contrastive learning method to enhance the fine-tuning process; and (3) an inference optimization method to improve the log parsing performance. We evaluate Unleash on a set of large-scale, public log datasets and the experimental results show that Unleash is effective and efficient compared to state-of-the-art log parsers.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {975–987},
numpages = {13},
keywords = {log parsing, log analytics, pre-trained LMs},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00128,
author = {Gill, Waris and Anwar, Ali and Gulzar, Muhammad Ali},
title = {TraceFL: Interpretability-Driven Debugging in Federated Learning via Neuron Provenance},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00128},
doi = {10.1109/ICSE55347.2025.00128},
abstract = {In Federated Learning, clients train models on local data and send updates to a central server, which aggregates them into a global model using a fusion algorithm. This collaborative yet privacy-preserving training comes at a cost. FL developers face significant challenges in attributing global model predictions to specific clients. Localizing responsible clients is a crucial step towards (a) excluding clients primarily responsible for incorrect predictions and (b) encouraging clients who contributed high-quality models to continue participating in the future. Existing ML debugging approaches are inherently inapplicable as they are designed for single-model, centralized training.We introduce TraceFL, a fine-grained neuron provenance capturing mechanism that identifies clients responsible for a global model's prediction by tracking the flow of information from individual clients to the global model. Since inference on different inputs activates a different set of neurons of the global model, TraceFL dynamically quantifies the significance of the global model's neurons in a given prediction, identifying the most crucial neurons in the global model. It then maps them to the corresponding neurons in every participating client to determine each client's contribution, ultimately localizing the responsible client. We evaluate TraceFL on six datasets, including two real-world medical imaging datasets and four neural networks, including advanced models such as GPT. TraceFL achieves 99% accuracy in localizing the responsible client in FL tasks spanning both image and text classification tasks. At a time when state-of-the-art ML debugging approaches are mostly domain-specific (e.g., image classification only), TraceFL is the first technique to enable highly accurate automated reasoning across a wide range of FL applications.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2264–2276},
numpages = {13},
keywords = {interpretability, explainability, debugging, machine learning, federated learning, transformer},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00117,
author = {Zhang, Mengxiao and Xu, Zhenyang and Tian, Yongqiang and Cheng, Xinru and Sun, Chengnian},
title = {Toward a Better Understanding of Probabilistic Delta Debugging},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00117},
doi = {10.1109/ICSE55347.2025.00117},
abstract = {Given a list L of elements and a property ψ that L exhibits, ddmin is a classic test input minimization algorithm that aims to automatically remove ψ-irrelevant elements from L. This algorithm has been widely adopted in domains such as test input minimization and software debloating. Recently, ProbDD, a variant of ddmin, has been proposed and achieved state-of-the-art performance. By employing Bayesian optimization, ProbDD estimates the probability of each element in L being relevant to ψ, and statistically decides which and how many elements should be deleted together each time. However, the theoretical probabilistic model of ProbDD is rather intricate, and the underlying details for the superior performance of ProbDD have not been adequately explored.In this paper, we conduct the first in-depth theoretical analysis of ProbDD, clarifying the trends in probability and subset size changes and simplifying the probability model. We complement this analysis with empirical experiments, including success rate analysis, ablation studies, and examinations of trade-offs and limitations, to further comprehend and demystify this state-of-the-art algorithm. Our success rate analysis reveals how ProbDD effectively addresses bottlenecks that slow down ddmin by skipping inefficient queries that attempt to delete complements of subsets and previously tried subsets. The ablation study illustrates that randomness in ProbDD has no significant impact on efficiency. These findings provide valuable insights for future research and applications of test input minimization algorithms.Based on the findings above, we propose CDD, a simplified version of ProbDD, reducing the complexity in both theory and implementation. CDD assists in ① validating the correctness of our key findings, e.g., that probabilities in ProbDD essentially serve as monotonically increasing counters for each element, and ② identifying the main factors that truly contribute to ProbDD's superior performance. Our comprehensive evaluations across 76 benchmarks in test input minimization and software debloating demonstrate that CDD can achieve the same performance as ProbDD, despite being much simplified.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2024–2035},
numpages = {12},
keywords = {program reduction, delta debugging, software debloating, test input minimization},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00114,
author = {Cheng, Mingfei and Xie, Xiaofei and Zhou, Yuan and Wang, Junjie and Meng, Guozhu and Yang, Kairui},
title = {Decictor: Towards Evaluating the Robustness of Decision-Making in Autonomous Driving Systems},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00114},
doi = {10.1109/ICSE55347.2025.00114},
abstract = {Autonomous Driving System (ADS) testing is crucial in ADS development, with the current primary focus being on safety. However, the evaluation of non-safety-critical performance, particularly the ADS's ability to make optimal decisions and produce optimal paths for autonomous vehicles (AVs), is also vital to ensure the intelligence and reduce risks of AVs. Currently, there is little work dedicated to assessing the robustness of ADSs' path-planning decisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an insignificant change in the environment. The key challenges include the lack of clear oracles for assessing PPD optimality and the difficulty in searching for scenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we focus on evaluating the robustness of ADSs' PPDs and propose the first method, Decictor, for generating non-optimal decision scenarios (NoDSs), where the ADS does not plan optimal paths for AVs. Decictor comprises three main components: Non-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle challenge, Non-invasive Mutation is devised to implement conservative modifications, ensuring the preservation of the original optimal path in the mutated scenarios. Subsequently, the Consistency Check is applied to determine the presence of non-optimal PPDs by comparing the driving paths in the original and mutated scenarios. To deal with the challenge of large environment space, we design Feedback metrics that integrate spatial and temporal dimensions of the AV's movement. These metrics are crucial for effectively steering the generation of NoDSs. Therefore, Decictor can generate NoDSs by generating new scenarios and then identifying NoDSs in the new scenarios. We evaluate Decictor on Baidu Apollo, an open-source and production-grade ADS. The experimental results validate the effectiveness of Decictor in detecting non-optimal PPDs of ADSs. It generates 63.9 NoDSs in total, while the best-performing baseline only detects 35.4 NoDSs.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {424–436},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00088,
author = {Chen, Boqi and L\'{o}pez, Jos\'{e} Antonio Hern\'{a}ndez and Mussbacher, Gunter and Varr\'{o}, D\'{a}niel},
title = {The Power of Types: Exploring the Impact of Type Checking on Neural Bug Detection in Dynamically Typed Languages},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00088},
doi = {10.1109/ICSE55347.2025.00088},
abstract = {[Motivation] Automated bug detection in dynamically typed languages such as Python is essential for maintaining code quality. The lack of mandatory type annotations in such languages can lead to errors that are challenging to identify early with traditional static analysis tools. Recent progress in deep neural networks has led to increased use of neural bug detectors. In statically typed languages, a type checker is integrated into the compiler and thus taken into consideration when the neural bug detector is designed for these languages.[Problem] However, prior studies overlook this aspect during the training and testing of neural bug detectors for dynamically typed languages. When an optional type checker is used, assessing existing neural bug detectors on bugs easily detectable by type checkers may impact their performance estimation. Moreover, including these bugs in the training set of neural bug detectors can shift their detection focus toward the wrong type of bugs.[Contribution] We explore the impact of type checking on various neural bug detectors for variable misuse bugs, a common type targeted by neural bug detectors. Existing synthetic and real-world datasets are type-checked to evaluate the prevalence of type-related bugs. Then, we investigate how type-related bugs influence the training and testing of the neural bug detectors.[Findings] Our findings indicate that existing bug detection datasets contain a significant proportion of type-related bugs. Building on this insight, we discover integrating the neural bug detector with a type checker can be beneficial, especially when the code is annotated with types. Further investigation reveals neural bug detectors perform better on type-related bugs than other bugs. Moreover, removing type-related bugs from the training data helps improve neural bug detectors' ability to identify bugs beyond the scope of type checkers.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {489–501},
numpages = {13},
keywords = {type checking, neural bug detection, dynamically typed languages},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00078,
author = {Qin, Qiaolin and Li, Heng and Merlo, Ettore and Lamothe, Maxime},
title = {Automated, Unsupervised, and Auto-Parameterized Inference of Data Patterns and Anomaly Detection},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00078},
doi = {10.1109/ICSE55347.2025.00078},
abstract = {With the advent of data-centric and machine learning (ML) systems, data quality is playing an increasingly critical role for ensuring the overall quality of software systems. Data preparation, an essential step towards high data quality, is known to be a highly effort-intensive process. Although prior studies have dealt with one of the most impacting issues, data pattern violations, these studies usually require data-specific configurations (i.e., parameterized) or use carefully curated data as learning examples (i.e., supervised), relying on domain knowledge and deep understanding of the data, or demanding significant manual effort. In this paper, we introduce RIOLU: Regex Inferencer autO-parameterized Learning with Uncleaned data. RIOLU is fully automated, automatically parameterized, and does not need labeled samples. RIOLU can generate precise patterns from datasets in various domains, with a high F1 score of 97.2%, exceeding the state-of-the-art baseline. In addition, according to our experiment on five datasets with anomalies, RIOLU can automatically estimate a data column's error rate, draw normal patterns, and predict anomalies from unlabeled data with higher performance (up to 800.4% improvement in terms of F1) than the state-of-the-art baseline, even outperforming ChatGPT in terms of both accuracy (12.3% higher F1) and efficiency (10% less inference time). A variant of RIOLU, with user guidance, can further boost its precision, with up to 37.4% improvement in terms of F1. Our evaluation in an industrial setting further demonstrates the practical benefits of RIOLU.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2419–2431},
numpages = {13},
keywords = {pattern anomaly detection, pattern-based data profiling, unsupervised learning, supervised learning},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00016,
author = {Kim, Brian Hyeongseok and Wang, Jingbo and Wang, Chao},
title = {FairQuant: Certifying and Quantifying Fairness of Deep Neural Networks},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00016},
doi = {10.1109/ICSE55347.2025.00016},
abstract = {We propose a method for formally certifying and quantifying individual fairness of deep neural networks (DNN). Individual fairness guarantees that any two individuals who are identical except for a legally protected attribute (e.g., gender or race) receive the same treatment. While there are existing techniques that provide such a guarantee, they tend to suffer from lack of scalability or accuracy as the size and input dimension of the DNN increase. Our method overcomes this limitation by applying abstraction to a symbolic interval based analysis of the DNN followed by iterative refinement guided by the fairness property. Furthermore, our method lifts the symbolic interval based analysis from conventional qualitative certification to quantitative certification, by computing the percentage of individuals whose classification outputs are provably fair, instead of merely deciding if the DNN is fair. We have implemented our method and evaluated it on deep neural networks trained on four popular fairness research datasets. The experimental results show that our method is not only more accurate than state-of-the-art techniques but also several orders-of-magnitude faster.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {527–539},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00007,
author = {Pedro, Rodrigo and Coimbra, Miguel E. and Castro, Daniel and Carreira, Paulo and Santos, Nuno},
title = {Prompt-to-SQL Injections in LLM-Integrated Web Applications: Risks and Defenses},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00007},
doi = {10.1109/ICSE55347.2025.00007},
abstract = {Large Language Models (LLMs) have found widespread applications in various domains, including web applications with chatbot interfaces. Aided by an LLM-integration middleware such as LangChain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users. However, unsanitized user prompts can lead to SQL injection attacks, potentially compromising the security of the database. In this paper, we present a comprehensive examination of prompt-to-SQL (P2SQL) injections targeting web applications based on frameworks such as LangChain and LlamaIndex. We characterize P2SQL injections, exploring their variants and impact on application security through multiple concrete examples. We evaluate seven state-of-the-art LLMs, demonstrating the risks of P2SQL attacks across language models. By employing both manual and automated methods, we discovered P2SQL vulnerabilities in five real-world applications. Our findings indicate that LLM-integrated applications are highly susceptible to P2SQL injection attacks, warranting the adoption of robust defenses. To counter these attacks, we propose four effective defense techniques that can be integrated as extensions to the LangChain framework.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1768–1780},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00181,
author = {Chi, Zhiming and Ma, Jianan and Yang, Pengfei and Huang, Cheng-Chao and Li, Renjue and Wang, Jingyi and Huang, Xiaowei and Zhang, Lijun},
title = {Patch Synthesis for Property Repair of Deep Neural Networks},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00181},
doi = {10.1109/ICSE55347.2025.00181},
abstract = {Deep neural networks (DNNs) are prone to various dependability issues, such as adversarial attacks, which hinder their adoption in safety-critical domains. Recently, NN repair techniques have been proposed to address these issues while preserving original performance by locating and modifying guilty neurons and their parameters. However, existing repair approaches are often limited to specific data sets and do not provide theoretical guarantees for the effectiveness of the repairs. To address these limitations, we introduce PatchPro, a novel patch-based approach for property-level repair of DNNs, focusing on local robustness. The key idea behind PatchPro is to construct patch modules that, when integrated with the original network, provide specialized repairs for all samples within the robustness neighborhood while maintaining the network's original performance. Our method incorporates formal verification and a heuristic mechanism for allocating patch modules, enabling it to defend against adversarial attacks and generalize to other inputs. PatchPro demonstrates superior efficiency, scalability, and repair success rates compared to existing DNN repair methods, i.e., realizing provable property-level repair for 100% cases across multiple high-dimensional datasets.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {1191–1203},
numpages = {13},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1109/ICSE55347.2025.00171,
author = {Amjad, Abdul Haddi and Danish, Muhammad and Jah, Bless and Gulzar, Muhammad Ali},
title = {Accessibility Issues in Ad-Driven Web Applications},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00171},
doi = {10.1109/ICSE55347.2025.00171},
abstract = {Website accessibility is essential for inclusiveness and regulatory compliance. Although third-party advertisements (ads) are a vital revenue source for free web services, they introduce significant accessibility challenges. Leasing a website's space to ad-serving technologies, like DoubleClick, results in developers losing control over ad content accessibility. Even on highly accessible websites, third-party ads can undermine adherence to Web Content Accessibility Guidelines (WCAG). We conduct the first-of-its-kind large-scale investigation of 430K website elements, including nearly 100K ad elements, to understand the accessibility of ads on websites. We seek to understand the prevalence of inaccessible ads and their overall impact on the accessibility of websites. Our findings show that 67% of websites experience increased accessibility violations due to ads, with common violations including Focus Visible (WCAG 2.4.7) and On Input (WCAG 3.2.2). Popular ad-serving technologies like Taboola, DoubleClick, and RevContent often serve ads that fail to comply with WCAG standards. Even when ads are WCAG compliant, 27% of them have alternative text in ad images that misrepresents information, potentially deceiving users. Manual inspection of a sample of these misleading ads revealed that user-identifiable data is collected on 94% of websites through interactions, such as hovering. Since users with disabilities often rely on tools like screen readers that require hover events to access website content, they have no choice but to compromise their privacy to navigate website ads. Based on our findings, we further dissect the root cause of these violations and provide design guidelines to both website developers and ad-serving technologies to achieve WCAG-compliant ad integration.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {2393–2405},
numpages = {13},
keywords = {accessibility, web, ads, privacy, web development},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

